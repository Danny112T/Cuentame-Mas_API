{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to [LoRA fine-tune](https://arxiv.org/abs/2106.09685) an LLM with MLX LM. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it in the right format for MLX LM.\n",
    "2. Setup and run LoRA training. We'll show how to capture the training logs and plot some statistics to visualize the performance.\n",
    "3. Evaluate on the test set. We'll compute the final question-answer accuracy of the fine-tuned model.\n",
    "4. Fuse the resulting adapters into the base model and upload to Hugging Face.\n",
    "5. Discuss tips for debugging accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "664272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install mlx-lm\n",
    "# pip install matplotlib\n",
    "# pip install rouge-score\n",
    "# pip install scikit-learn\n",
    "# pip install tqdm\n",
    "# pip install numpy\n",
    "# pip install json\n",
    "# pip install pathlib\n",
    "# pip install transformers\n",
    "# pip install sentencepiece\n",
    "# pip install datasets\n",
    "# pip install torch\n",
    "# pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1131315",
   "metadata": {},
   "source": [
    "### MLFOW CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d1cf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae62a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d438bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_name=\"finetuning_comparativa\"\n",
    "exp_name=\"MLX1.0\"\n",
    "corrida_name=\"MLX-40_8_4e4\"\n",
    "# base_model = \"Llama-3.2-1B-Instruct\"\n",
    "# four_bits = True \n",
    "# dataset_path = \"FAQ_All.jsonl\"\n",
    "# dataset_type = \"alpaca_chat.load_qa\"\n",
    "# output_dir = \"../trained_models/adapters/adapters.safetensors\"\n",
    "output_path = \"../trained_models/adapters_40_8_4e4/\"\n",
    "output_dir = output_path + 'adapters.safetensors'\n",
    "sequence_len = 2048\n",
    "lora_layers = 8\n",
    "lora_layers_scale = 20.0\n",
    "grad_checkpoint_value = True\n",
    "\n",
    "\n",
    "lora_r = 8\n",
    "# lora_alpha = 16\n",
    "lora_dropout = 0.0\n",
    "# gradient_accumulation_steps = 4\n",
    "optimizer = \"adam\"\n",
    "# weight_decay_value = 0.02\n",
    "lr_scheduler = \"linear\"\n",
    "ds_len = 1024\n",
    "learning_rate_value = 4e-4\n",
    "batch_size = 8\n",
    "\n",
    "epochs = 40\n",
    "steps = ((ds_len // batch_size) * epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c0069b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/880645134898555871', creation_time=1733877767863, experiment_id='880645134898555871', last_update_time=1733877767863, lifecycle_stage='active', name='MLX1.0', tags={'mlflow.sharedViewState.bb15b83a1c8297e8bcab8dc38315c49bda8d8fec1b37088f0195711a12855512': 'deflate;eJzlVktv2kAQ/iuRz6jCJEDNjVCSokJaAY0iRRFZ1oPZsN619uFgEP+9s5iHm+D2UolIPfiw8/zmm/XOrD0NRNH5DeMGlNfyvIonVQjqOvsGGZ6JMYpNrQH9SRuizMSwGI5GbU291oxwDRVvqx87dctr9/toxNkMaEY5HMK3qWGp8w+JIRqM3msenypeLEPg96A0k+LowfnF0AqNPho4UANhR3Ibo6T1WIT3PJJWUXhGw6J04ILqt9IvefpnD9MqjN5dJkSEEHqt9SaX/GBCuPPjzuIrC0MQx/M902zKODPZgCRHt9zMJUXsN73haDzxq5Phz7sRIqAyTogCLKczR65cBWvPWoZpPL95eRlcBrVGsx7Ugvo0qMpIoY/JEhfquj308gwdaYUZy04ey2v5VWQOsDQ6QnaQut67cKvGy5RfpcLxjhwaV9euaUzfggBFtjKjLOyD7dpPqVWEZt6mchrpKo5qs7S2OjvSmT/RVGKaMqSZqV81Y944P1ImCJ8YRZiYcKl1KeJ5VTZq03TxQRCn+P0R70LPUhtwODveBFTCYYn/ZilW2ngJMuPLs2NV0kbgl+KUQZauFurqY+Cslf9dft2P4zj9GDj7pTgtfF6EJjRHnP3eXfccQAsPgJuUAkaxlGYuAAUtTLxsL5keUcJhnAN1NsSNBH1KqIiIIJ9DW888iTaQFCbng9MMCiBQlx2tc3h7Gc5EhWCwbJ2PPRYJfGC/W8MZDulDnVbDLZdTwh/IPtJbTf9QHhPRTl3WId+3zawW++fv0OHB+z/781RcWHb0lq8sBZoF2e6A283rIsfsOAyZTjjJ9kwzPYTtFlloyF8bVLqGvDZN9kLIMfsoQ27jf50eOUkZvA6QyZitCqZ4OftMm/2WeAjQpriUhMjc+2yOfXytEgjvCbege4etMEc4ZyF048Rkv4sj53ONdQjL+e74Zn8l1sghzPB2zLuCTHkhaXTo9zZqR4oZw46vT1zKE3e+/J5uNr8AWwkZ8g=='}>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"https://4z0r6nts-5000.usw3.devtunnels.ms/\") \n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(experiment_name=exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61698208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello stats: 1024 lenght trainging dataset\n",
      "An example:\n",
      "\n",
      "{\n",
      "  \"pregunta\": \"\\u00bfPuedo presentar una p\\u00f3liza de bonos en formato electr\\u00f3nico?\",\n",
      "  \"respuesta\": \"Este tipo de garant\\u00eda es v\\u00e1lida, \\u00bfno?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# jsonl file path\n",
    "\"\"\" save_path = \"./dataset/FAQ_All.jsonl\"\n",
    "with open(save_path, 'r') as file:\n",
    "\tdataset = [json.loads(line) for line in file]\n",
    "\"\"\"\n",
    "\n",
    "# csv file path\n",
    "save_path = \"./datasets/Parph_Data/FAQs_1000.csv\"\n",
    "dataset = []\n",
    "with open(save_path, 'r', encoding='utf-8') as file:\n",
    "\treader = csv.DictReader(file)\n",
    "\tfor row in reader:\n",
    "\t\tdataset.append(row)\n",
    "\n",
    "print(f\"Hello stats: {len(dataset)} lenght trainging dataset\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b607237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "valid_set = [dataset[i] for i in perm[:valid_size]]\n",
    "train_set = [dataset[i] for i in perm[valid_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c38c4e",
   "metadata": {},
   "source": [
    "Finally, put the data splits in the MLX LM training format. The format simply expects the data to be in a container which supports random access to the individual examples (e.g. a Python `list`):\n",
    "```\n",
    "[\"An example for the model.\", \"Another example for the model.\", ...]\n",
    "```\n",
    "For more details, see the [documentation on supported formats](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea738f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\"\"\"def preprocess(dataset):\n",
    "    return [\n",
    "        f\"Question: {clean_text(t['question'])}\\n\"\n",
    "        f\"Answer: {clean_text(t['answer'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\"\"\"\n",
    "def preprocess(dataset):\n",
    "    return [\n",
    "        f\"pregunta: {clean_text(t['pregunta'])}\\n\"\n",
    "        f\"respuesta: {clean_text(t['respuesta'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\n",
    "\n",
    "train_set, valid_set = map(preprocess, (train_set, valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [Phi-3 mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). At 3.8 billion parameters, Phi-3 mini is a high-quality model that is also fast to fine-tune on most Apple silicon machines. Also, it has a [permissive MIT License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE).\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3ff309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm.utils import load, generate\n",
    "from mlx_lm.tuner.trainer import train, evaluate, TrainingArgs\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters and make the training arguments. See the [training argument class](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/trainer.py#L31-L63) for a more detailed list of training parameters. \n",
    "\n",
    "Recall the LoRA update is $W^\\top \\mathbf{x} + c \\cdot \\mathbf{a} \\mathbf{b}^\\top \\mathbf{x}$ where $\\mathbf{a}$ has shape `(D, rank)`.\n",
    "\n",
    "With that in mind, the LoRA parameters to attend to are:\n",
    "- `lora_layers`: The number of Transformer blocks from the top of the model to adapt.\n",
    "- `rank`: The rank of the low-rank adapters. A larger rank implies more adapter parameters per linear layer.\n",
    "- `scale`: This is the constant $c$ that scales the low-rank update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0851dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save the adapter config and weights\n",
    "adapter_path = Path(\"../trained_models/adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"lora_layers\": lora_layers,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": lora_r,\n",
    "    \"scale\": lora_layers_scale,\n",
    "    \"dropout\": lora_dropout,\n",
    "    \"epochs\": epochs\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)    \n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=output_dir,\n",
    "    iters=steps,\n",
    "    steps_per_eval=50,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_length=sequence_len,\n",
    "    grad_checkpoint=grad_checkpoint_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the Phi-3 mini model. Note this may take a few minutes to download from HuggingFace if you haven't downloaded it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb0b16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../original_models/Llama-3.2-1B-Instruct-bf16\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the model, freeze it's parameters so we don't train them. Then convert linear layers to LoRA layers using the MLX LM utility `linear_to_lora_layers`. The adapters in the `LoRA` layers are not frozen, so they will be included in the model's `trainable_parameters`. Check-out the [LoRA layer implementation](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/lora.py#L72-L104) to see how it all works.\n",
    "\n",
    "By default, MLX LM only adapts the query, key, and value projection matrices for Phi-3. You can specify the layers to adapt by setting `lora_parameters[\"keys\"]` to a list of layer names. In this case it defaults to `[\"attn.qkv_proj\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50e1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 425984\n"
     ]
    }
   ],
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"lora_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34ee27",
   "metadata": {},
   "source": [
    "### Evaluate Functions\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each example in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37e55f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > 0.8\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    # Adjust generation parameters\n",
    "    generation_params = {\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    for example in tqdm(dataset):\n",
    "        # Get prompt and true answer\n",
    "        prompt = example[\"prompt\"]\n",
    "        true_answer = example[\"response\"]\n",
    "        \n",
    "        # Generate prediction\n",
    "        response = generate(model, tokenizer, prompt, **generation_params)\n",
    "        \n",
    "        # Store prediction and true label\n",
    "        all_preds.append(response)\n",
    "        all_labels.append(true_answer)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(true_answer, response)\n",
    "        for key, score in scores.items():\n",
    "            rouge_scores[key].append(score.fmeasure)\n",
    "\n",
    "        # Calculate loss/perplexity\n",
    "        tokens = tokenizer.encode(prompt + true_answer)\n",
    "        tokens = mx.array(tokens)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(tokens[None])[0]\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        targets = tokens[1:]\n",
    "        logits = logits[:-1]\n",
    "        loss = mx.mean(nn.losses.cross_entropy(logits, targets))\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = sum(1 for x,y in zip(all_preds, all_labels) if similar(x.strip(), y.strip())) / len(all_preds)\n",
    "    \n",
    "    # Convert predictions and labels to match format for F1\n",
    "    pred_labels = [1 if similar(p.strip(), l.strip()) else 0 for p,l in zip(all_preds, all_labels)]\n",
    "    true_labels = [1] * len(all_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='binary')\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge_scores = {key: np.mean(scores) for key, scores in rouge_scores.items()}\n",
    "\n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    perplexity = float(mx.exp(mx.array(avg_loss)))\n",
    "\n",
    "    return accuracy, f1, perplexity, avg_rouge_scores\n",
    "\n",
    "# Load test dataset\n",
    "test_set_path = \"./datasets/Parph_Data/FAQs_200_testing.jsonl\" \n",
    "with open(test_set_path, 'r') as file:\n",
    "    test_set = [json.loads(line) for line in file]\n",
    "\n",
    "# Define number of test samples\n",
    "num_test = len(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer, but you can specify any [optimizer](https://ml-explore.github.io/mlx/build/html/python/optimizers/common_optimizers.html) with any [scheduler](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html). We also added a custom class to capture the training and validation loss to plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "984516d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder ../trained_models/adapters_40_8_4e4/ created\n",
      "Starting training..., iters: 5120\n",
      "Iter 1: Val loss 3.499, Val took 12.639s\n",
      "Iter 10: Train loss 3.050, Learning Rate 4.000e-04, It/sec 0.929, Tokens/sec 406.990, Trained Tokens 4382, Peak mem 14.672 GB\n",
      "Iter 20: Train loss 3.249, Learning Rate 4.000e-04, It/sec 0.617, Tokens/sec 459.782, Trained Tokens 11828, Peak mem 14.672 GB\n",
      "Iter 30: Train loss 3.211, Learning Rate 4.000e-04, It/sec 0.494, Tokens/sec 427.780, Trained Tokens 20484, Peak mem 14.672 GB\n",
      "Iter 40: Train loss 2.995, Learning Rate 4.000e-04, It/sec 0.875, Tokens/sec 477.276, Trained Tokens 25937, Peak mem 14.672 GB\n",
      "Iter 50: Val loss 2.891, Val took 12.301s\n",
      "Iter 50: Train loss 2.841, Learning Rate 4.000e-04, It/sec 3.741, Tokens/sec 1960.996, Trained Tokens 31179, Peak mem 14.672 GB\n",
      "Iter 60: Train loss 2.794, Learning Rate 4.000e-04, It/sec 0.753, Tokens/sec 468.023, Trained Tokens 37391, Peak mem 14.672 GB\n",
      "Iter 70: Train loss 2.705, Learning Rate 4.000e-04, It/sec 0.669, Tokens/sec 459.455, Trained Tokens 44260, Peak mem 14.672 GB\n",
      "Iter 80: Train loss 2.787, Learning Rate 4.000e-04, It/sec 0.600, Tokens/sec 457.006, Trained Tokens 51871, Peak mem 14.672 GB\n",
      "Iter 90: Train loss 2.635, Learning Rate 4.000e-04, It/sec 0.655, Tokens/sec 444.451, Trained Tokens 58659, Peak mem 14.672 GB\n",
      "Iter 100: Val loss 2.600, Val took 12.552s\n",
      "Iter 100: Train loss 2.440, Learning Rate 4.000e-04, It/sec 12.063, Tokens/sec 5662.589, Trained Tokens 63353, Peak mem 14.672 GB\n",
      "Iter 100: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 2.334, Learning Rate 4.000e-04, It/sec 0.599, Tokens/sec 420.685, Trained Tokens 70371, Peak mem 14.672 GB\n",
      "Iter 120: Train loss 2.396, Learning Rate 4.000e-04, It/sec 0.674, Tokens/sec 472.284, Trained Tokens 77379, Peak mem 14.672 GB\n",
      "Iter 130: Train loss 2.349, Learning Rate 4.000e-04, It/sec 0.747, Tokens/sec 462.823, Trained Tokens 83578, Peak mem 14.672 GB\n",
      "Iter 140: Train loss 2.283, Learning Rate 4.000e-04, It/sec 0.609, Tokens/sec 422.147, Trained Tokens 90508, Peak mem 14.672 GB\n",
      "Iter 150: Val loss 2.496, Val took 13.206s\n",
      "Iter 150: Train loss 2.284, Learning Rate 4.000e-04, It/sec 5.964, Tokens/sec 3731.280, Trained Tokens 96764, Peak mem 14.672 GB\n",
      "Iter 160: Train loss 2.254, Learning Rate 4.000e-04, It/sec 0.680, Tokens/sec 405.887, Trained Tokens 102729, Peak mem 14.672 GB\n",
      "Iter 170: Train loss 2.403, Learning Rate 4.000e-04, It/sec 0.699, Tokens/sec 421.195, Trained Tokens 108758, Peak mem 14.672 GB\n",
      "Iter 180: Train loss 2.385, Learning Rate 4.000e-04, It/sec 0.479, Tokens/sec 465.592, Trained Tokens 118476, Peak mem 14.672 GB\n",
      "Iter 190: Train loss 2.226, Learning Rate 4.000e-04, It/sec 0.860, Tokens/sec 462.912, Trained Tokens 123861, Peak mem 14.672 GB\n",
      "Iter 200: Val loss 2.392, Val took 14.007s\n",
      "Iter 200: Train loss 2.230, Learning Rate 4.000e-04, It/sec 7.523, Tokens/sec 3835.110, Trained Tokens 128959, Peak mem 14.672 GB\n",
      "Iter 200: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 2.151, Learning Rate 4.000e-04, It/sec 0.888, Tokens/sec 443.460, Trained Tokens 133955, Peak mem 14.672 GB\n",
      "Iter 220: Train loss 2.053, Learning Rate 4.000e-04, It/sec 0.430, Tokens/sec 392.915, Trained Tokens 143101, Peak mem 14.672 GB\n",
      "Iter 230: Train loss 1.980, Learning Rate 4.000e-04, It/sec 0.756, Tokens/sec 420.142, Trained Tokens 148662, Peak mem 14.672 GB\n",
      "Iter 240: Train loss 2.007, Learning Rate 4.000e-04, It/sec 0.584, Tokens/sec 381.650, Trained Tokens 155195, Peak mem 14.672 GB\n",
      "Iter 250: Val loss 2.339, Val took 12.203s\n",
      "Iter 250: Train loss 2.099, Learning Rate 4.000e-04, It/sec 13.341, Tokens/sec 7920.651, Trained Tokens 161132, Peak mem 14.672 GB\n",
      "Iter 260: Train loss 2.078, Learning Rate 4.000e-04, It/sec 0.735, Tokens/sec 409.592, Trained Tokens 166703, Peak mem 14.672 GB\n",
      "Iter 270: Train loss 2.046, Learning Rate 4.000e-04, It/sec 0.663, Tokens/sec 437.231, Trained Tokens 173296, Peak mem 14.672 GB\n",
      "Iter 280: Train loss 2.128, Learning Rate 4.000e-04, It/sec 0.545, Tokens/sec 455.672, Trained Tokens 181652, Peak mem 14.672 GB\n",
      "Iter 290: Train loss 1.986, Learning Rate 4.000e-04, It/sec 0.653, Tokens/sec 406.211, Trained Tokens 187869, Peak mem 14.672 GB\n",
      "Iter 300: Val loss 2.306, Val took 14.979s\n",
      "Iter 300: Train loss 1.961, Learning Rate 4.000e-04, It/sec 6.062, Tokens/sec 3572.915, Trained Tokens 193763, Peak mem 14.672 GB\n",
      "Iter 300: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 1.831, Learning Rate 4.000e-04, It/sec 0.850, Tokens/sec 459.929, Trained Tokens 199176, Peak mem 14.672 GB\n",
      "Iter 320: Train loss 1.852, Learning Rate 4.000e-04, It/sec 0.712, Tokens/sec 427.693, Trained Tokens 205185, Peak mem 14.672 GB\n",
      "Iter 330: Train loss 1.814, Learning Rate 4.000e-04, It/sec 0.599, Tokens/sec 400.582, Trained Tokens 211874, Peak mem 14.672 GB\n",
      "Iter 340: Train loss 1.511, Learning Rate 4.000e-04, It/sec 1.008, Tokens/sec 439.838, Trained Tokens 216237, Peak mem 14.672 GB\n",
      "Iter 350: Val loss 2.313, Val took 12.633s\n",
      "Iter 350: Train loss 1.901, Learning Rate 4.000e-04, It/sec 5.531, Tokens/sec 3789.741, Trained Tokens 223089, Peak mem 14.672 GB\n",
      "Iter 360: Train loss 1.928, Learning Rate 4.000e-04, It/sec 0.646, Tokens/sec 417.231, Trained Tokens 229550, Peak mem 14.672 GB\n",
      "Iter 370: Train loss 1.882, Learning Rate 4.000e-04, It/sec 0.531, Tokens/sec 433.586, Trained Tokens 237712, Peak mem 14.672 GB\n",
      "Iter 380: Train loss 1.804, Learning Rate 4.000e-04, It/sec 0.728, Tokens/sec 473.063, Trained Tokens 244207, Peak mem 14.672 GB\n",
      "Iter 390: Train loss 1.754, Learning Rate 4.000e-04, It/sec 0.781, Tokens/sec 436.564, Trained Tokens 249795, Peak mem 14.672 GB\n",
      "Iter 400: Val loss 2.226, Val took 10.835s\n",
      "Iter 400: Train loss 1.674, Learning Rate 4.000e-04, It/sec 10.267, Tokens/sec 5598.454, Trained Tokens 255248, Peak mem 14.672 GB\n",
      "Iter 400: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 1.891, Learning Rate 4.000e-04, It/sec 0.541, Tokens/sec 437.823, Trained Tokens 263346, Peak mem 14.672 GB\n",
      "Iter 420: Train loss 1.348, Learning Rate 4.000e-04, It/sec 0.809, Tokens/sec 408.415, Trained Tokens 268393, Peak mem 14.672 GB\n",
      "Iter 430: Train loss 1.636, Learning Rate 4.000e-04, It/sec 0.625, Tokens/sec 448.787, Trained Tokens 275576, Peak mem 14.672 GB\n",
      "Iter 440: Train loss 1.597, Learning Rate 4.000e-04, It/sec 0.709, Tokens/sec 444.112, Trained Tokens 281836, Peak mem 14.672 GB\n",
      "Iter 450: Val loss 2.237, Val took 13.301s\n",
      "Iter 450: Train loss 1.569, Learning Rate 4.000e-04, It/sec 20.150, Tokens/sec 10691.594, Trained Tokens 287142, Peak mem 14.672 GB\n",
      "Iter 460: Train loss 1.713, Learning Rate 4.000e-04, It/sec 0.532, Tokens/sec 435.524, Trained Tokens 295333, Peak mem 14.672 GB\n",
      "Iter 470: Train loss 1.823, Learning Rate 4.000e-04, It/sec 0.466, Tokens/sec 450.682, Trained Tokens 305010, Peak mem 14.672 GB\n",
      "Iter 480: Train loss 1.622, Learning Rate 4.000e-04, It/sec 0.803, Tokens/sec 448.574, Trained Tokens 310594, Peak mem 14.672 GB\n",
      "Iter 490: Train loss 1.665, Learning Rate 4.000e-04, It/sec 0.657, Tokens/sec 432.579, Trained Tokens 317178, Peak mem 14.672 GB\n",
      "Iter 500: Val loss 2.223, Val took 12.769s\n",
      "Iter 500: Train loss 1.521, Learning Rate 4.000e-04, It/sec 9.943, Tokens/sec 4557.675, Trained Tokens 321762, Peak mem 14.672 GB\n",
      "Iter 500: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 1.559, Learning Rate 4.000e-04, It/sec 0.803, Tokens/sec 432.391, Trained Tokens 327145, Peak mem 14.672 GB\n",
      "Iter 520: Train loss 1.414, Learning Rate 4.000e-04, It/sec 0.619, Tokens/sec 442.737, Trained Tokens 334301, Peak mem 14.672 GB\n",
      "Iter 530: Train loss 1.243, Learning Rate 4.000e-04, It/sec 0.777, Tokens/sec 402.880, Trained Tokens 339489, Peak mem 14.672 GB\n",
      "Iter 540: Train loss 1.536, Learning Rate 4.000e-04, It/sec 0.587, Tokens/sec 419.921, Trained Tokens 346640, Peak mem 14.672 GB\n",
      "Iter 550: Val loss 2.219, Val took 12.208s\n",
      "Iter 550: Train loss 1.358, Learning Rate 4.000e-04, It/sec 8.907, Tokens/sec 4515.716, Trained Tokens 351710, Peak mem 14.672 GB\n",
      "Iter 560: Train loss 1.337, Learning Rate 4.000e-04, It/sec 0.788, Tokens/sec 413.740, Trained Tokens 356963, Peak mem 14.672 GB\n",
      "Iter 570: Train loss 1.490, Learning Rate 4.000e-04, It/sec 0.729, Tokens/sec 432.925, Trained Tokens 362902, Peak mem 14.672 GB\n",
      "Iter 580: Train loss 1.530, Learning Rate 4.000e-04, It/sec 0.425, Tokens/sec 368.705, Trained Tokens 371582, Peak mem 14.672 GB\n",
      "Iter 590: Train loss 1.389, Learning Rate 4.000e-04, It/sec 0.820, Tokens/sec 408.583, Trained Tokens 376564, Peak mem 14.672 GB\n",
      "Iter 600: Val loss 2.158, Val took 10.839s\n",
      "Iter 600: Train loss 1.827, Learning Rate 4.000e-04, It/sec 5.362, Tokens/sec 4821.500, Trained Tokens 385556, Peak mem 14.672 GB\n",
      "Iter 600: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 1.407, Learning Rate 4.000e-04, It/sec 0.703, Tokens/sec 425.068, Trained Tokens 391603, Peak mem 14.672 GB\n",
      "Iter 620: Train loss 1.377, Learning Rate 4.000e-04, It/sec 0.501, Tokens/sec 409.892, Trained Tokens 399782, Peak mem 14.672 GB\n",
      "Iter 630: Train loss 1.206, Learning Rate 4.000e-04, It/sec 0.629, Tokens/sec 406.183, Trained Tokens 406243, Peak mem 14.672 GB\n",
      "Iter 640: Train loss 1.439, Learning Rate 4.000e-04, It/sec 0.542, Tokens/sec 418.473, Trained Tokens 413970, Peak mem 14.672 GB\n",
      "Iter 650: Val loss 2.203, Val took 11.629s\n",
      "Iter 650: Train loss 1.311, Learning Rate 4.000e-04, It/sec 4.452, Tokens/sec 2517.573, Trained Tokens 419625, Peak mem 14.672 GB\n",
      "Iter 660: Train loss 1.379, Learning Rate 4.000e-04, It/sec 0.693, Tokens/sec 489.478, Trained Tokens 426684, Peak mem 14.672 GB\n",
      "Iter 670: Train loss 1.367, Learning Rate 4.000e-04, It/sec 0.647, Tokens/sec 447.906, Trained Tokens 433606, Peak mem 14.672 GB\n",
      "Iter 680: Train loss 1.274, Learning Rate 4.000e-04, It/sec 0.791, Tokens/sec 435.635, Trained Tokens 439111, Peak mem 14.672 GB\n",
      "Iter 690: Train loss 1.259, Learning Rate 4.000e-04, It/sec 0.653, Tokens/sec 420.078, Trained Tokens 445540, Peak mem 14.672 GB\n",
      "Iter 700: Val loss 2.184, Val took 12.054s\n",
      "Iter 700: Train loss 1.224, Learning Rate 4.000e-04, It/sec 12.182, Tokens/sec 5837.724, Trained Tokens 450332, Peak mem 14.672 GB\n",
      "Iter 700: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 1.314, Learning Rate 4.000e-04, It/sec 0.747, Tokens/sec 450.858, Trained Tokens 456370, Peak mem 14.672 GB\n",
      "Iter 720: Train loss 1.304, Learning Rate 4.000e-04, It/sec 0.675, Tokens/sec 444.971, Trained Tokens 462962, Peak mem 14.672 GB\n",
      "Iter 730: Train loss 1.038, Learning Rate 4.000e-04, It/sec 0.878, Tokens/sec 415.704, Trained Tokens 467694, Peak mem 14.672 GB\n",
      "Iter 740: Train loss 1.248, Learning Rate 4.000e-04, It/sec 0.616, Tokens/sec 397.160, Trained Tokens 474138, Peak mem 14.672 GB\n",
      "Iter 750: Val loss 2.203, Val took 15.238s\n",
      "Iter 750: Train loss 1.056, Learning Rate 4.000e-04, It/sec 8.216, Tokens/sec 4733.260, Trained Tokens 479899, Peak mem 14.672 GB\n",
      "Iter 760: Train loss 1.199, Learning Rate 4.000e-04, It/sec 0.544, Tokens/sec 359.184, Trained Tokens 486505, Peak mem 14.672 GB\n",
      "Iter 770: Train loss 1.151, Learning Rate 4.000e-04, It/sec 0.677, Tokens/sec 383.392, Trained Tokens 492169, Peak mem 14.672 GB\n",
      "Iter 780: Train loss 1.138, Learning Rate 4.000e-04, It/sec 0.756, Tokens/sec 363.164, Trained Tokens 496974, Peak mem 14.672 GB\n",
      "Iter 790: Train loss 1.372, Learning Rate 4.000e-04, It/sec 0.452, Tokens/sec 395.594, Trained Tokens 505727, Peak mem 14.672 GB\n",
      "Iter 800: Val loss 2.158, Val took 16.526s\n",
      "Iter 800: Train loss 1.380, Learning Rate 4.000e-04, It/sec 4.317, Tokens/sec 3163.340, Trained Tokens 513054, Peak mem 14.672 GB\n",
      "Iter 800: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 1.231, Learning Rate 4.000e-04, It/sec 0.668, Tokens/sec 419.603, Trained Tokens 519336, Peak mem 14.672 GB\n",
      "Iter 820: Train loss 1.197, Learning Rate 4.000e-04, It/sec 0.623, Tokens/sec 398.126, Trained Tokens 525725, Peak mem 14.672 GB\n",
      "Iter 830: Train loss 1.014, Learning Rate 4.000e-04, It/sec 0.551, Tokens/sec 367.071, Trained Tokens 532384, Peak mem 14.672 GB\n",
      "Iter 840: Train loss 1.070, Learning Rate 4.000e-04, It/sec 0.594, Tokens/sec 369.911, Trained Tokens 538609, Peak mem 14.672 GB\n",
      "Iter 850: Val loss 2.193, Val took 15.629s\n",
      "Iter 850: Train loss 0.969, Learning Rate 4.000e-04, It/sec 10.395, Tokens/sec 5465.628, Trained Tokens 543867, Peak mem 14.672 GB\n",
      "Iter 860: Train loss 1.227, Learning Rate 4.000e-04, It/sec 0.442, Tokens/sec 401.548, Trained Tokens 552949, Peak mem 14.672 GB\n",
      "Iter 870: Train loss 1.054, Learning Rate 4.000e-04, It/sec 0.644, Tokens/sec 373.316, Trained Tokens 558749, Peak mem 14.672 GB\n",
      "Iter 880: Train loss 1.172, Learning Rate 4.000e-04, It/sec 0.538, Tokens/sec 368.267, Trained Tokens 565589, Peak mem 14.672 GB\n",
      "Iter 890: Train loss 1.176, Learning Rate 4.000e-04, It/sec 0.506, Tokens/sec 348.600, Trained Tokens 572478, Peak mem 14.672 GB\n",
      "Iter 900: Val loss 2.182, Val took 13.030s\n",
      "Iter 900: Train loss 1.139, Learning Rate 4.000e-04, It/sec 3.981, Tokens/sec 2325.974, Trained Tokens 578320, Peak mem 14.672 GB\n",
      "Iter 900: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 1.130, Learning Rate 4.000e-04, It/sec 0.716, Tokens/sec 466.243, Trained Tokens 584832, Peak mem 14.672 GB\n",
      "Iter 920: Train loss 0.981, Learning Rate 4.000e-04, It/sec 0.775, Tokens/sec 397.724, Trained Tokens 589964, Peak mem 14.672 GB\n",
      "Iter 930: Train loss 0.909, Learning Rate 4.000e-04, It/sec 0.788, Tokens/sec 435.186, Trained Tokens 595488, Peak mem 14.672 GB\n",
      "Iter 940: Train loss 1.002, Learning Rate 4.000e-04, It/sec 0.608, Tokens/sec 451.547, Trained Tokens 602920, Peak mem 14.672 GB\n",
      "Iter 950: Val loss 2.259, Val took 13.233s\n",
      "Iter 950: Train loss 0.830, Learning Rate 4.000e-04, It/sec 4.889, Tokens/sec 2310.881, Trained Tokens 607647, Peak mem 14.672 GB\n",
      "Iter 960: Train loss 0.931, Learning Rate 4.000e-04, It/sec 0.920, Tokens/sec 465.910, Trained Tokens 612713, Peak mem 14.672 GB\n",
      "Iter 970: Train loss 1.072, Learning Rate 4.000e-04, It/sec 0.547, Tokens/sec 382.039, Trained Tokens 619700, Peak mem 14.672 GB\n",
      "Iter 980: Train loss 0.969, Learning Rate 4.000e-04, It/sec 0.699, Tokens/sec 414.114, Trained Tokens 625625, Peak mem 14.672 GB\n",
      "Iter 990: Train loss 0.976, Learning Rate 4.000e-04, It/sec 0.755, Tokens/sec 467.831, Trained Tokens 631821, Peak mem 14.672 GB\n",
      "Iter 1000: Val loss 2.175, Val took 13.369s\n",
      "Iter 1000: Train loss 1.082, Learning Rate 4.000e-04, It/sec 6.041, Tokens/sec 4463.723, Trained Tokens 639210, Peak mem 14.672 GB\n",
      "Iter 1000: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001000_adapters.safetensors.\n",
      "Iter 1010: Train loss 1.176, Learning Rate 4.000e-04, It/sec 0.465, Tokens/sec 417.297, Trained Tokens 648185, Peak mem 14.672 GB\n",
      "Iter 1020: Train loss 1.092, Learning Rate 4.000e-04, It/sec 0.624, Tokens/sec 380.796, Trained Tokens 654290, Peak mem 14.672 GB\n",
      "Iter 1030: Train loss 0.880, Learning Rate 4.000e-04, It/sec 0.725, Tokens/sec 449.717, Trained Tokens 660489, Peak mem 14.672 GB\n",
      "Iter 1040: Train loss 0.959, Learning Rate 4.000e-04, It/sec 0.573, Tokens/sec 435.851, Trained Tokens 668093, Peak mem 14.672 GB\n",
      "Iter 1050: Val loss 2.221, Val took 12.594s\n",
      "Iter 1050: Train loss 0.904, Learning Rate 4.000e-04, It/sec 4.716, Tokens/sec 2828.478, Trained Tokens 674091, Peak mem 14.672 GB\n",
      "Iter 1060: Train loss 0.876, Learning Rate 4.000e-04, It/sec 0.727, Tokens/sec 451.382, Trained Tokens 680304, Peak mem 14.672 GB\n",
      "Iter 1070: Train loss 0.870, Learning Rate 4.000e-04, It/sec 0.796, Tokens/sec 430.566, Trained Tokens 685716, Peak mem 14.672 GB\n",
      "Iter 1080: Train loss 1.029, Learning Rate 4.000e-04, It/sec 0.570, Tokens/sec 434.530, Trained Tokens 693345, Peak mem 14.672 GB\n",
      "Iter 1090: Train loss 0.842, Learning Rate 4.000e-04, It/sec 0.820, Tokens/sec 436.573, Trained Tokens 698668, Peak mem 14.672 GB\n",
      "Iter 1100: Val loss 2.182, Val took 13.105s\n",
      "Iter 1100: Train loss 0.957, Learning Rate 4.000e-04, It/sec 6.004, Tokens/sec 3621.329, Trained Tokens 704700, Peak mem 14.672 GB\n",
      "Iter 1100: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001100_adapters.safetensors.\n",
      "Iter 1110: Train loss 0.979, Learning Rate 4.000e-04, It/sec 0.622, Tokens/sec 450.720, Trained Tokens 711944, Peak mem 14.672 GB\n",
      "Iter 1120: Train loss 1.045, Learning Rate 4.000e-04, It/sec 0.660, Tokens/sec 452.342, Trained Tokens 718797, Peak mem 14.672 GB\n",
      "Iter 1130: Train loss 0.808, Learning Rate 4.000e-04, It/sec 0.626, Tokens/sec 424.412, Trained Tokens 725581, Peak mem 14.672 GB\n",
      "Iter 1140: Train loss 0.732, Learning Rate 4.000e-04, It/sec 0.746, Tokens/sec 431.943, Trained Tokens 731369, Peak mem 14.672 GB\n",
      "Iter 1150: Val loss 2.246, Val took 14.394s\n",
      "Iter 1150: Train loss 0.724, Learning Rate 4.000e-04, It/sec 18.367, Tokens/sec 9056.654, Trained Tokens 736300, Peak mem 14.672 GB\n",
      "Iter 1160: Train loss 0.767, Learning Rate 4.000e-04, It/sec 0.967, Tokens/sec 434.345, Trained Tokens 740792, Peak mem 14.672 GB\n",
      "Iter 1170: Train loss 0.899, Learning Rate 4.000e-04, It/sec 0.637, Tokens/sec 427.819, Trained Tokens 747513, Peak mem 14.672 GB\n",
      "Iter 1180: Train loss 1.101, Learning Rate 4.000e-04, It/sec 0.423, Tokens/sec 394.802, Trained Tokens 756847, Peak mem 14.672 GB\n",
      "Iter 1190: Train loss 0.995, Learning Rate 4.000e-04, It/sec 0.578, Tokens/sec 463.052, Trained Tokens 764854, Peak mem 14.672 GB\n",
      "Iter 1200: Val loss 2.213, Val took 13.963s\n",
      "Iter 1200: Train loss 0.871, Learning Rate 4.000e-04, It/sec 13.537, Tokens/sec 8842.537, Trained Tokens 771386, Peak mem 14.672 GB\n",
      "Iter 1200: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001200_adapters.safetensors.\n",
      "Iter 1210: Train loss 0.811, Learning Rate 4.000e-04, It/sec 1.035, Tokens/sec 468.014, Trained Tokens 775906, Peak mem 14.672 GB\n",
      "Iter 1220: Train loss 0.945, Learning Rate 4.000e-04, It/sec 0.769, Tokens/sec 425.099, Trained Tokens 781437, Peak mem 14.672 GB\n",
      "Iter 1230: Train loss 1.040, Learning Rate 4.000e-04, It/sec 0.572, Tokens/sec 443.243, Trained Tokens 789181, Peak mem 14.672 GB\n",
      "Iter 1240: Train loss 0.780, Learning Rate 4.000e-04, It/sec 0.581, Tokens/sec 427.376, Trained Tokens 796537, Peak mem 14.672 GB\n",
      "Iter 1250: Val loss 2.232, Val took 12.254s\n",
      "Iter 1250: Train loss 0.879, Learning Rate 4.000e-04, It/sec 2.647, Tokens/sec 1873.145, Trained Tokens 803613, Peak mem 14.672 GB\n",
      "Iter 1260: Train loss 0.802, Learning Rate 4.000e-04, It/sec 0.678, Tokens/sec 458.985, Trained Tokens 810383, Peak mem 14.672 GB\n",
      "Iter 1270: Train loss 0.834, Learning Rate 4.000e-04, It/sec 0.649, Tokens/sec 403.324, Trained Tokens 816600, Peak mem 14.672 GB\n",
      "Iter 1280: Train loss 0.829, Learning Rate 4.000e-04, It/sec 0.642, Tokens/sec 454.862, Trained Tokens 823687, Peak mem 14.672 GB\n",
      "Iter 1290: Train loss 0.729, Learning Rate 4.000e-04, It/sec 0.737, Tokens/sec 438.404, Trained Tokens 829636, Peak mem 14.672 GB\n",
      "Iter 1300: Val loss 2.235, Val took 11.498s\n",
      "Iter 1300: Train loss 0.878, Learning Rate 4.000e-04, It/sec 1.427, Tokens/sec 1021.196, Trained Tokens 836790, Peak mem 14.672 GB\n",
      "Iter 1300: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001300_adapters.safetensors.\n",
      "Iter 1310: Train loss 0.844, Learning Rate 4.000e-04, It/sec 0.577, Tokens/sec 384.937, Trained Tokens 843463, Peak mem 14.672 GB\n",
      "Iter 1320: Train loss 0.839, Learning Rate 4.000e-04, It/sec 0.837, Tokens/sec 425.002, Trained Tokens 848540, Peak mem 14.672 GB\n",
      "Iter 1330: Train loss 0.800, Learning Rate 4.000e-04, It/sec 0.901, Tokens/sec 393.353, Trained Tokens 852906, Peak mem 14.672 GB\n",
      "Iter 1340: Train loss 0.742, Learning Rate 4.000e-04, It/sec 0.732, Tokens/sec 433.848, Trained Tokens 858829, Peak mem 14.672 GB\n",
      "Iter 1350: Val loss 2.341, Val took 12.190s\n",
      "Iter 1350: Train loss 0.682, Learning Rate 4.000e-04, It/sec 11.461, Tokens/sec 6553.667, Trained Tokens 864547, Peak mem 14.672 GB\n",
      "Iter 1360: Train loss 0.699, Learning Rate 4.000e-04, It/sec 1.323, Tokens/sec 448.273, Trained Tokens 867936, Peak mem 14.672 GB\n",
      "Iter 1370: Train loss 0.756, Learning Rate 4.000e-04, It/sec 0.706, Tokens/sec 443.724, Trained Tokens 874219, Peak mem 14.672 GB\n",
      "Iter 1380: Train loss 0.858, Learning Rate 4.000e-04, It/sec 0.547, Tokens/sec 423.947, Trained Tokens 881973, Peak mem 14.672 GB\n",
      "Iter 1390: Train loss 0.784, Learning Rate 4.000e-04, It/sec 0.708, Tokens/sec 424.289, Trained Tokens 887965, Peak mem 14.672 GB\n",
      "Iter 1400: Val loss 2.221, Val took 13.193s\n",
      "Iter 1400: Train loss 0.824, Learning Rate 4.000e-04, It/sec 3.312, Tokens/sec 2308.688, Trained Tokens 894935, Peak mem 14.672 GB\n",
      "Iter 1400: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001400_adapters.safetensors.\n",
      "Iter 1410: Train loss 0.893, Learning Rate 4.000e-04, It/sec 0.541, Tokens/sec 444.430, Trained Tokens 903148, Peak mem 14.672 GB\n",
      "Iter 1420: Train loss 0.824, Learning Rate 4.000e-04, It/sec 0.755, Tokens/sec 493.590, Trained Tokens 909688, Peak mem 14.672 GB\n",
      "Iter 1430: Train loss 0.835, Learning Rate 4.000e-04, It/sec 0.658, Tokens/sec 468.865, Trained Tokens 916818, Peak mem 14.672 GB\n",
      "Iter 1440: Train loss 0.721, Learning Rate 4.000e-04, It/sec 0.643, Tokens/sec 452.825, Trained Tokens 923856, Peak mem 14.672 GB\n",
      "Iter 1450: Val loss 2.225, Val took 11.293s\n",
      "Iter 1450: Train loss 0.680, Learning Rate 4.000e-04, It/sec 17.066, Tokens/sec 8555.047, Trained Tokens 928869, Peak mem 14.672 GB\n",
      "Iter 1460: Train loss 0.700, Learning Rate 4.000e-04, It/sec 0.708, Tokens/sec 502.515, Trained Tokens 935967, Peak mem 14.672 GB\n",
      "Iter 1470: Train loss 0.801, Learning Rate 4.000e-04, It/sec 0.647, Tokens/sec 497.304, Trained Tokens 943657, Peak mem 14.672 GB\n",
      "Iter 1480: Train loss 0.721, Learning Rate 4.000e-04, It/sec 0.673, Tokens/sec 436.867, Trained Tokens 950146, Peak mem 14.672 GB\n",
      "Iter 1490: Train loss 0.663, Learning Rate 4.000e-04, It/sec 0.915, Tokens/sec 414.177, Trained Tokens 954671, Peak mem 14.672 GB\n",
      "Iter 1500: Val loss 2.272, Val took 13.888s\n",
      "Iter 1500: Train loss 0.713, Learning Rate 4.000e-04, It/sec 5.798, Tokens/sec 3430.846, Trained Tokens 960588, Peak mem 14.672 GB\n",
      "Iter 1500: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001500_adapters.safetensors.\n",
      "Iter 1510: Train loss 0.788, Learning Rate 4.000e-04, It/sec 0.770, Tokens/sec 461.524, Trained Tokens 966582, Peak mem 14.672 GB\n",
      "Iter 1520: Train loss 0.724, Learning Rate 4.000e-04, It/sec 0.684, Tokens/sec 405.139, Trained Tokens 972504, Peak mem 14.672 GB\n",
      "Iter 1530: Train loss 0.890, Learning Rate 4.000e-04, It/sec 0.514, Tokens/sec 459.033, Trained Tokens 981435, Peak mem 14.672 GB\n",
      "Iter 1540: Train loss 0.672, Learning Rate 4.000e-04, It/sec 0.670, Tokens/sec 440.723, Trained Tokens 988012, Peak mem 14.672 GB\n",
      "Iter 1550: Val loss 2.270, Val took 10.993s\n",
      "Iter 1550: Train loss 0.637, Learning Rate 4.000e-04, It/sec 9.219, Tokens/sec 6282.036, Trained Tokens 994826, Peak mem 14.672 GB\n",
      "Iter 1560: Train loss 0.685, Learning Rate 4.000e-04, It/sec 0.531, Tokens/sec 426.307, Trained Tokens 1002852, Peak mem 14.672 GB\n",
      "Iter 1570: Train loss 0.764, Learning Rate 4.000e-04, It/sec 0.537, Tokens/sec 464.969, Trained Tokens 1011505, Peak mem 14.672 GB\n",
      "Iter 1580: Train loss 0.647, Learning Rate 4.000e-04, It/sec 0.850, Tokens/sec 461.015, Trained Tokens 1016927, Peak mem 14.672 GB\n",
      "Iter 1590: Train loss 0.683, Learning Rate 4.000e-04, It/sec 0.731, Tokens/sec 433.403, Trained Tokens 1022854, Peak mem 14.672 GB\n",
      "Iter 1600: Val loss 2.290, Val took 12.527s\n",
      "Iter 1600: Train loss 0.654, Learning Rate 4.000e-04, It/sec 9.006, Tokens/sec 4060.088, Trained Tokens 1027362, Peak mem 14.672 GB\n",
      "Iter 1600: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001600_adapters.safetensors.\n",
      "Iter 1610: Train loss 0.659, Learning Rate 4.000e-04, It/sec 0.736, Tokens/sec 448.247, Trained Tokens 1033456, Peak mem 14.672 GB\n",
      "Iter 1620: Train loss 0.765, Learning Rate 4.000e-04, It/sec 0.699, Tokens/sec 413.511, Trained Tokens 1039375, Peak mem 14.672 GB\n",
      "Iter 1630: Train loss 0.697, Learning Rate 4.000e-04, It/sec 0.678, Tokens/sec 455.547, Trained Tokens 1046090, Peak mem 14.672 GB\n",
      "Iter 1640: Train loss 0.714, Learning Rate 4.000e-04, It/sec 0.529, Tokens/sec 416.010, Trained Tokens 1053950, Peak mem 14.672 GB\n",
      "Iter 1650: Val loss 2.269, Val took 13.114s\n",
      "Iter 1650: Train loss 0.724, Learning Rate 4.000e-04, It/sec 15.367, Tokens/sec 13300.468, Trained Tokens 1062605, Peak mem 14.672 GB\n",
      "Iter 1660: Train loss 0.626, Learning Rate 4.000e-04, It/sec 0.562, Tokens/sec 415.822, Trained Tokens 1070003, Peak mem 14.672 GB\n",
      "Iter 1670: Train loss 0.568, Learning Rate 4.000e-04, It/sec 0.731, Tokens/sec 419.424, Trained Tokens 1075742, Peak mem 14.672 GB\n",
      "Iter 1680: Train loss 0.600, Learning Rate 4.000e-04, It/sec 0.819, Tokens/sec 420.056, Trained Tokens 1080873, Peak mem 14.672 GB\n",
      "Iter 1690: Train loss 0.672, Learning Rate 4.000e-04, It/sec 0.649, Tokens/sec 429.584, Trained Tokens 1087493, Peak mem 14.672 GB\n",
      "Iter 1700: Val loss 2.275, Val took 13.074s\n",
      "Iter 1700: Train loss 0.676, Learning Rate 4.000e-04, It/sec 5.007, Tokens/sec 3072.562, Trained Tokens 1093630, Peak mem 14.672 GB\n",
      "Iter 1700: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001700_adapters.safetensors.\n",
      "Iter 1710: Train loss 0.672, Learning Rate 4.000e-04, It/sec 0.714, Tokens/sec 459.881, Trained Tokens 1100073, Peak mem 14.672 GB\n",
      "Iter 1720: Train loss 0.687, Learning Rate 4.000e-04, It/sec 0.840, Tokens/sec 460.319, Trained Tokens 1105554, Peak mem 14.672 GB\n",
      "Iter 1730: Train loss 0.654, Learning Rate 4.000e-04, It/sec 0.807, Tokens/sec 424.666, Trained Tokens 1110818, Peak mem 14.672 GB\n",
      "Iter 1740: Train loss 0.608, Learning Rate 4.000e-04, It/sec 0.897, Tokens/sec 443.068, Trained Tokens 1115759, Peak mem 14.672 GB\n",
      "Iter 1750: Val loss 2.359, Val took 12.700s\n",
      "Iter 1750: Train loss 0.598, Learning Rate 4.000e-04, It/sec 10.013, Tokens/sec 6207.911, Trained Tokens 1121959, Peak mem 14.672 GB\n",
      "Iter 1760: Train loss 0.598, Learning Rate 4.000e-04, It/sec 0.992, Tokens/sec 468.266, Trained Tokens 1126681, Peak mem 14.672 GB\n",
      "Iter 1770: Train loss 0.659, Learning Rate 4.000e-04, It/sec 0.592, Tokens/sec 436.661, Trained Tokens 1134053, Peak mem 14.672 GB\n",
      "Iter 1780: Train loss 0.679, Learning Rate 4.000e-04, It/sec 0.602, Tokens/sec 435.094, Trained Tokens 1141279, Peak mem 14.672 GB\n",
      "Iter 1790: Train loss 0.639, Learning Rate 4.000e-04, It/sec 0.607, Tokens/sec 437.448, Trained Tokens 1148484, Peak mem 14.672 GB\n",
      "Iter 1800: Val loss 2.265, Val took 13.639s\n",
      "Iter 1800: Train loss 0.737, Learning Rate 4.000e-04, It/sec 10.239, Tokens/sec 8059.754, Trained Tokens 1156356, Peak mem 14.672 GB\n",
      "Iter 1800: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001800_adapters.safetensors.\n",
      "Iter 1810: Train loss 0.563, Learning Rate 4.000e-04, It/sec 0.614, Tokens/sec 381.787, Trained Tokens 1162571, Peak mem 14.672 GB\n",
      "Iter 1820: Train loss 0.647, Learning Rate 4.000e-04, It/sec 0.701, Tokens/sec 414.734, Trained Tokens 1168486, Peak mem 14.672 GB\n",
      "Iter 1830: Train loss 0.665, Learning Rate 4.000e-04, It/sec 0.639, Tokens/sec 399.279, Trained Tokens 1174730, Peak mem 14.672 GB\n",
      "Iter 1840: Train loss 0.726, Learning Rate 4.000e-04, It/sec 0.479, Tokens/sec 349.264, Trained Tokens 1182016, Peak mem 14.672 GB\n",
      "Iter 1850: Val loss 2.328, Val took 13.723s\n",
      "Iter 1850: Train loss 0.569, Learning Rate 4.000e-04, It/sec 3.713, Tokens/sec 2147.697, Trained Tokens 1187800, Peak mem 14.672 GB\n",
      "Iter 1860: Train loss 0.584, Learning Rate 4.000e-04, It/sec 0.557, Tokens/sec 433.286, Trained Tokens 1195583, Peak mem 14.672 GB\n",
      "Iter 1870: Train loss 0.524, Learning Rate 4.000e-04, It/sec 0.714, Tokens/sec 440.357, Trained Tokens 1201753, Peak mem 14.672 GB\n",
      "Iter 1880: Train loss 0.583, Learning Rate 4.000e-04, It/sec 0.890, Tokens/sec 383.494, Trained Tokens 1206064, Peak mem 14.672 GB\n",
      "Iter 1890: Train loss 0.637, Learning Rate 4.000e-04, It/sec 0.578, Tokens/sec 412.731, Trained Tokens 1213207, Peak mem 14.672 GB\n",
      "Iter 1900: Val loss 2.302, Val took 14.034s\n",
      "Iter 1900: Train loss 0.570, Learning Rate 4.000e-04, It/sec 5.243, Tokens/sec 3224.228, Trained Tokens 1219357, Peak mem 14.672 GB\n",
      "Iter 1900: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0001900_adapters.safetensors.\n",
      "Iter 1910: Train loss 0.602, Learning Rate 4.000e-04, It/sec 0.514, Tokens/sec 396.053, Trained Tokens 1227067, Peak mem 14.672 GB\n",
      "Iter 1920: Train loss 0.593, Learning Rate 4.000e-04, It/sec 0.724, Tokens/sec 384.881, Trained Tokens 1232382, Peak mem 14.672 GB\n",
      "Iter 1930: Train loss 0.728, Learning Rate 4.000e-04, It/sec 0.700, Tokens/sec 411.477, Trained Tokens 1238257, Peak mem 14.672 GB\n",
      "Iter 1940: Train loss 0.620, Learning Rate 4.000e-04, It/sec 0.661, Tokens/sec 428.100, Trained Tokens 1244734, Peak mem 14.672 GB\n",
      "Iter 1950: Val loss 2.335, Val took 13.508s\n",
      "Iter 1950: Train loss 0.522, Learning Rate 4.000e-04, It/sec 11.896, Tokens/sec 7123.206, Trained Tokens 1250722, Peak mem 14.672 GB\n",
      "Iter 1960: Train loss 0.565, Learning Rate 4.000e-04, It/sec 0.745, Tokens/sec 413.665, Trained Tokens 1256272, Peak mem 14.672 GB\n",
      "Iter 1970: Train loss 0.605, Learning Rate 4.000e-04, It/sec 0.668, Tokens/sec 427.373, Trained Tokens 1262671, Peak mem 14.672 GB\n",
      "Iter 1980: Train loss 0.527, Learning Rate 4.000e-04, It/sec 0.693, Tokens/sec 386.947, Trained Tokens 1268253, Peak mem 14.672 GB\n",
      "Iter 1990: Train loss 0.603, Learning Rate 4.000e-04, It/sec 0.521, Tokens/sec 401.951, Trained Tokens 1275969, Peak mem 14.672 GB\n",
      "Iter 2000: Val loss 2.360, Val took 13.746s\n",
      "Iter 2000: Train loss 0.638, Learning Rate 4.000e-04, It/sec 2.354, Tokens/sec 1535.476, Trained Tokens 1282492, Peak mem 14.672 GB\n",
      "Iter 2000: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002000_adapters.safetensors.\n",
      "Iter 2010: Train loss 0.797, Learning Rate 4.000e-04, It/sec 0.427, Tokens/sec 411.786, Trained Tokens 1292137, Peak mem 14.672 GB\n",
      "Iter 2020: Train loss 0.590, Learning Rate 4.000e-04, It/sec 0.744, Tokens/sec 452.740, Trained Tokens 1298226, Peak mem 14.672 GB\n",
      "Iter 2030: Train loss 0.619, Learning Rate 4.000e-04, It/sec 0.541, Tokens/sec 305.491, Trained Tokens 1303876, Peak mem 14.672 GB\n",
      "Iter 2040: Train loss 0.614, Learning Rate 4.000e-04, It/sec 0.838, Tokens/sec 394.429, Trained Tokens 1308580, Peak mem 14.672 GB\n",
      "Iter 2050: Val loss 2.340, Val took 14.022s\n",
      "Iter 2050: Train loss 0.528, Learning Rate 4.000e-04, It/sec 3.819, Tokens/sec 2191.953, Trained Tokens 1314320, Peak mem 14.672 GB\n",
      "Iter 2060: Train loss 0.701, Learning Rate 4.000e-04, It/sec 0.451, Tokens/sec 414.328, Trained Tokens 1323514, Peak mem 14.672 GB\n",
      "Iter 2070: Train loss 0.554, Learning Rate 4.000e-04, It/sec 0.881, Tokens/sec 475.363, Trained Tokens 1328912, Peak mem 14.672 GB\n",
      "Iter 2080: Train loss 0.509, Learning Rate 4.000e-04, It/sec 0.755, Tokens/sec 389.534, Trained Tokens 1334069, Peak mem 14.672 GB\n",
      "Iter 2090: Train loss 0.599, Learning Rate 4.000e-04, It/sec 0.618, Tokens/sec 423.294, Trained Tokens 1340915, Peak mem 14.672 GB\n",
      "Iter 2100: Val loss 2.380, Val took 12.669s\n",
      "Iter 2100: Train loss 0.594, Learning Rate 4.000e-04, It/sec 9.102, Tokens/sec 6030.089, Trained Tokens 1347540, Peak mem 14.672 GB\n",
      "Iter 2100: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002100_adapters.safetensors.\n",
      "Iter 2110: Train loss 0.585, Learning Rate 4.000e-04, It/sec 0.819, Tokens/sec 453.695, Trained Tokens 1353077, Peak mem 14.672 GB\n",
      "Iter 2120: Train loss 0.626, Learning Rate 4.000e-04, It/sec 0.783, Tokens/sec 417.549, Trained Tokens 1358409, Peak mem 14.672 GB\n",
      "Iter 2130: Train loss 0.606, Learning Rate 4.000e-04, It/sec 0.615, Tokens/sec 407.060, Trained Tokens 1365024, Peak mem 14.672 GB\n",
      "Iter 2140: Train loss 0.634, Learning Rate 4.000e-04, It/sec 0.519, Tokens/sec 398.812, Trained Tokens 1372707, Peak mem 14.672 GB\n",
      "Iter 2150: Val loss 2.386, Val took 12.397s\n",
      "Iter 2150: Train loss 0.510, Learning Rate 4.000e-04, It/sec 5.550, Tokens/sec 3267.620, Trained Tokens 1378595, Peak mem 14.672 GB\n",
      "Iter 2160: Train loss 0.561, Learning Rate 4.000e-04, It/sec 0.623, Tokens/sec 447.741, Trained Tokens 1385778, Peak mem 14.672 GB\n",
      "Iter 2170: Train loss 0.530, Learning Rate 4.000e-04, It/sec 0.712, Tokens/sec 444.179, Trained Tokens 1392018, Peak mem 14.672 GB\n",
      "Iter 2180: Train loss 0.523, Learning Rate 4.000e-04, It/sec 0.745, Tokens/sec 379.649, Trained Tokens 1397115, Peak mem 14.672 GB\n",
      "Iter 2190: Train loss 0.699, Learning Rate 4.000e-04, It/sec 0.439, Tokens/sec 414.231, Trained Tokens 1406542, Peak mem 14.672 GB\n",
      "Iter 2200: Val loss 2.382, Val took 14.241s\n",
      "Iter 2200: Train loss 0.526, Learning Rate 4.000e-04, It/sec 10.047, Tokens/sec 5754.833, Trained Tokens 1412270, Peak mem 14.672 GB\n",
      "Iter 2200: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002200_adapters.safetensors.\n",
      "Iter 2210: Train loss 0.508, Learning Rate 4.000e-04, It/sec 0.740, Tokens/sec 472.839, Trained Tokens 1418656, Peak mem 14.672 GB\n",
      "Iter 2220: Train loss 0.525, Learning Rate 4.000e-04, It/sec 0.813, Tokens/sec 449.752, Trained Tokens 1424188, Peak mem 14.672 GB\n",
      "Iter 2230: Train loss 0.590, Learning Rate 4.000e-04, It/sec 0.661, Tokens/sec 422.088, Trained Tokens 1430571, Peak mem 14.672 GB\n",
      "Iter 2240: Train loss 0.608, Learning Rate 4.000e-04, It/sec 0.632, Tokens/sec 449.462, Trained Tokens 1437688, Peak mem 14.672 GB\n",
      "Iter 2250: Val loss 2.403, Val took 12.872s\n",
      "Iter 2250: Train loss 0.496, Learning Rate 4.000e-04, It/sec 8.970, Tokens/sec 5554.117, Trained Tokens 1443880, Peak mem 14.672 GB\n",
      "Iter 2260: Train loss 0.597, Learning Rate 4.000e-04, It/sec 0.543, Tokens/sec 455.317, Trained Tokens 1452261, Peak mem 14.672 GB\n",
      "Iter 2270: Train loss 0.528, Learning Rate 4.000e-04, It/sec 0.731, Tokens/sec 350.473, Trained Tokens 1457057, Peak mem 14.672 GB\n",
      "Iter 2280: Train loss 0.507, Learning Rate 4.000e-04, It/sec 0.656, Tokens/sec 421.631, Trained Tokens 1463485, Peak mem 14.672 GB\n",
      "Iter 2290: Train loss 0.543, Learning Rate 4.000e-04, It/sec 0.682, Tokens/sec 392.019, Trained Tokens 1469232, Peak mem 14.672 GB\n",
      "Iter 2300: Val loss 2.399, Val took 14.376s\n",
      "Iter 2300: Train loss 0.575, Learning Rate 4.000e-04, It/sec 7.036, Tokens/sec 4174.176, Trained Tokens 1475165, Peak mem 14.672 GB\n",
      "Iter 2300: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002300_adapters.safetensors.\n",
      "Iter 2310: Train loss 0.539, Learning Rate 4.000e-04, It/sec 0.787, Tokens/sec 440.732, Trained Tokens 1480768, Peak mem 14.672 GB\n",
      "Iter 2320: Train loss 0.544, Learning Rate 4.000e-04, It/sec 0.673, Tokens/sec 418.039, Trained Tokens 1486983, Peak mem 14.672 GB\n",
      "Iter 2330: Train loss 0.559, Learning Rate 4.000e-04, It/sec 0.540, Tokens/sec 410.452, Trained Tokens 1494577, Peak mem 14.672 GB\n",
      "Iter 2340: Train loss 0.557, Learning Rate 4.000e-04, It/sec 0.753, Tokens/sec 405.164, Trained Tokens 1499961, Peak mem 14.672 GB\n",
      "Iter 2350: Val loss 2.376, Val took 20.319s\n",
      "Iter 2350: Train loss 0.575, Learning Rate 4.000e-04, It/sec 9.285, Tokens/sec 6674.832, Trained Tokens 1507150, Peak mem 14.672 GB\n",
      "Iter 2360: Train loss 0.554, Learning Rate 4.000e-04, It/sec 0.770, Tokens/sec 406.493, Trained Tokens 1512430, Peak mem 14.672 GB\n",
      "Iter 2370: Train loss 0.525, Learning Rate 4.000e-04, It/sec 0.699, Tokens/sec 402.627, Trained Tokens 1518192, Peak mem 14.672 GB\n",
      "Iter 2380: Train loss 0.632, Learning Rate 4.000e-04, It/sec 0.513, Tokens/sec 391.013, Trained Tokens 1525816, Peak mem 14.672 GB\n",
      "Iter 2390: Train loss 0.535, Learning Rate 4.000e-04, It/sec 0.976, Tokens/sec 425.711, Trained Tokens 1530179, Peak mem 14.672 GB\n",
      "Iter 2400: Val loss 2.389, Val took 14.878s\n",
      "Iter 2400: Train loss 0.503, Learning Rate 4.000e-04, It/sec 5.183, Tokens/sec 3765.521, Trained Tokens 1537444, Peak mem 14.672 GB\n",
      "Iter 2400: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002400_adapters.safetensors.\n",
      "Iter 2410: Train loss 0.518, Learning Rate 4.000e-04, It/sec 0.770, Tokens/sec 476.603, Trained Tokens 1543631, Peak mem 14.672 GB\n",
      "Iter 2420: Train loss 0.565, Learning Rate 4.000e-04, It/sec 0.660, Tokens/sec 399.139, Trained Tokens 1549675, Peak mem 14.672 GB\n",
      "Iter 2430: Train loss 0.560, Learning Rate 4.000e-04, It/sec 0.644, Tokens/sec 430.131, Trained Tokens 1556354, Peak mem 14.672 GB\n",
      "Iter 2440: Train loss 0.601, Learning Rate 4.000e-04, It/sec 0.500, Tokens/sec 409.485, Trained Tokens 1564544, Peak mem 14.672 GB\n",
      "Iter 2450: Val loss 2.396, Val took 12.006s\n",
      "Iter 2450: Train loss 0.478, Learning Rate 4.000e-04, It/sec 4.150, Tokens/sec 2806.625, Trained Tokens 1571307, Peak mem 14.672 GB\n",
      "Iter 2460: Train loss 0.455, Learning Rate 4.000e-04, It/sec 0.571, Tokens/sec 419.823, Trained Tokens 1578657, Peak mem 14.672 GB\n",
      "Iter 2470: Train loss 0.535, Learning Rate 4.000e-04, It/sec 0.928, Tokens/sec 419.781, Trained Tokens 1583180, Peak mem 14.672 GB\n",
      "Iter 2480: Train loss 0.601, Learning Rate 4.000e-04, It/sec 0.504, Tokens/sec 403.727, Trained Tokens 1591184, Peak mem 14.672 GB\n",
      "Iter 2490: Train loss 0.493, Learning Rate 4.000e-04, It/sec 0.664, Tokens/sec 413.804, Trained Tokens 1597417, Peak mem 14.672 GB\n",
      "Iter 2500: Val loss 2.425, Val took 12.801s\n",
      "Iter 2500: Train loss 0.475, Learning Rate 4.000e-04, It/sec 12.138, Tokens/sec 7004.902, Trained Tokens 1603188, Peak mem 14.672 GB\n",
      "Iter 2500: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002500_adapters.safetensors.\n",
      "Iter 2510: Train loss 0.584, Learning Rate 4.000e-04, It/sec 0.516, Tokens/sec 394.834, Trained Tokens 1610834, Peak mem 14.672 GB\n",
      "Iter 2520: Train loss 0.542, Learning Rate 4.000e-04, It/sec 0.514, Tokens/sec 374.311, Trained Tokens 1618122, Peak mem 14.672 GB\n",
      "Iter 2530: Train loss 0.501, Learning Rate 4.000e-04, It/sec 0.684, Tokens/sec 401.353, Trained Tokens 1623987, Peak mem 14.672 GB\n",
      "Iter 2540: Train loss 0.568, Learning Rate 4.000e-04, It/sec 0.604, Tokens/sec 412.808, Trained Tokens 1630823, Peak mem 14.672 GB\n",
      "Iter 2550: Val loss 2.400, Val took 14.498s\n",
      "Iter 2550: Train loss 0.516, Learning Rate 4.000e-04, It/sec 16.897, Tokens/sec 8282.709, Trained Tokens 1635725, Peak mem 14.672 GB\n",
      "Iter 2560: Train loss 0.479, Learning Rate 4.000e-04, It/sec 0.746, Tokens/sec 412.661, Trained Tokens 1641257, Peak mem 14.672 GB\n",
      "Iter 2570: Train loss 0.421, Learning Rate 4.000e-04, It/sec 0.714, Tokens/sec 426.625, Trained Tokens 1647234, Peak mem 14.672 GB\n",
      "Iter 2580: Train loss 0.573, Learning Rate 4.000e-04, It/sec 0.984, Tokens/sec 420.871, Trained Tokens 1651512, Peak mem 14.672 GB\n",
      "Iter 2590: Train loss 0.450, Learning Rate 4.000e-04, It/sec 0.659, Tokens/sec 395.297, Trained Tokens 1657510, Peak mem 14.672 GB\n",
      "Iter 2600: Val loss 2.386, Val took 14.361s\n",
      "Iter 2600: Train loss 0.540, Learning Rate 4.000e-04, It/sec 9.712, Tokens/sec 6907.930, Trained Tokens 1664623, Peak mem 14.672 GB\n",
      "Iter 2600: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002600_adapters.safetensors.\n",
      "Iter 2610: Train loss 0.554, Learning Rate 4.000e-04, It/sec 0.911, Tokens/sec 465.183, Trained Tokens 1669728, Peak mem 14.672 GB\n",
      "Iter 2620: Train loss 0.554, Learning Rate 4.000e-04, It/sec 0.992, Tokens/sec 433.930, Trained Tokens 1674103, Peak mem 14.672 GB\n",
      "Iter 2630: Train loss 0.500, Learning Rate 4.000e-04, It/sec 0.477, Tokens/sec 421.803, Trained Tokens 1682954, Peak mem 14.672 GB\n",
      "Iter 2640: Train loss 0.595, Learning Rate 4.000e-04, It/sec 0.430, Tokens/sec 478.034, Trained Tokens 1694082, Peak mem 14.672 GB\n",
      "Iter 2650: Val loss 2.400, Val took 11.890s\n",
      "Iter 2650: Train loss 0.541, Learning Rate 4.000e-04, It/sec 7.859, Tokens/sec 4319.907, Trained Tokens 1699579, Peak mem 14.672 GB\n",
      "Iter 2660: Train loss 0.542, Learning Rate 4.000e-04, It/sec 0.740, Tokens/sec 390.125, Trained Tokens 1704851, Peak mem 14.672 GB\n",
      "Iter 2670: Train loss 0.555, Learning Rate 4.000e-04, It/sec 0.638, Tokens/sec 368.914, Trained Tokens 1710629, Peak mem 14.672 GB\n",
      "Iter 2680: Train loss 0.475, Learning Rate 4.000e-04, It/sec 0.654, Tokens/sec 435.582, Trained Tokens 1717289, Peak mem 14.672 GB\n",
      "Iter 2690: Train loss 0.498, Learning Rate 4.000e-04, It/sec 0.706, Tokens/sec 407.189, Trained Tokens 1723056, Peak mem 14.672 GB\n",
      "Iter 2700: Val loss 2.401, Val took 16.427s\n",
      "Iter 2700: Train loss 0.499, Learning Rate 4.000e-04, It/sec 4.613, Tokens/sec 2334.350, Trained Tokens 1728116, Peak mem 14.672 GB\n",
      "Iter 2700: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002700_adapters.safetensors.\n",
      "Iter 2710: Train loss 0.528, Learning Rate 4.000e-04, It/sec 0.819, Tokens/sec 428.704, Trained Tokens 1733349, Peak mem 14.672 GB\n",
      "Iter 2720: Train loss 0.622, Learning Rate 4.000e-04, It/sec 0.460, Tokens/sec 400.569, Trained Tokens 1742050, Peak mem 14.672 GB\n",
      "Iter 2730: Train loss 0.545, Learning Rate 4.000e-04, It/sec 0.581, Tokens/sec 360.014, Trained Tokens 1748248, Peak mem 14.672 GB\n",
      "Iter 2740: Train loss 0.553, Learning Rate 4.000e-04, It/sec 0.434, Tokens/sec 369.666, Trained Tokens 1756758, Peak mem 14.672 GB\n",
      "Iter 2750: Val loss 2.415, Val took 12.118s\n",
      "Iter 2750: Train loss 0.526, Learning Rate 4.000e-04, It/sec 5.878, Tokens/sec 4256.361, Trained Tokens 1763999, Peak mem 14.672 GB\n",
      "Iter 2760: Train loss 0.485, Learning Rate 4.000e-04, It/sec 0.803, Tokens/sec 442.025, Trained Tokens 1769504, Peak mem 14.672 GB\n",
      "Iter 2770: Train loss 0.485, Learning Rate 4.000e-04, It/sec 0.684, Tokens/sec 393.722, Trained Tokens 1775256, Peak mem 14.672 GB\n",
      "Iter 2780: Train loss 0.480, Learning Rate 4.000e-04, It/sec 0.540, Tokens/sec 435.238, Trained Tokens 1783321, Peak mem 14.672 GB\n",
      "Iter 2790: Train loss 0.463, Learning Rate 4.000e-04, It/sec 0.716, Tokens/sec 427.933, Trained Tokens 1789296, Peak mem 14.672 GB\n",
      "Iter 2800: Val loss 2.409, Val took 11.992s\n",
      "Iter 2800: Train loss 0.507, Learning Rate 4.000e-04, It/sec 11.916, Tokens/sec 8490.013, Trained Tokens 1796421, Peak mem 14.672 GB\n",
      "Iter 2800: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002800_adapters.safetensors.\n",
      "Iter 2810: Train loss 0.561, Learning Rate 4.000e-04, It/sec 0.583, Tokens/sec 401.141, Trained Tokens 1803299, Peak mem 14.672 GB\n",
      "Iter 2820: Train loss 0.546, Learning Rate 4.000e-04, It/sec 0.661, Tokens/sec 414.511, Trained Tokens 1809567, Peak mem 14.672 GB\n",
      "Iter 2830: Train loss 0.658, Learning Rate 4.000e-04, It/sec 0.646, Tokens/sec 443.873, Trained Tokens 1816437, Peak mem 14.672 GB\n",
      "Iter 2840: Train loss 0.570, Learning Rate 4.000e-04, It/sec 0.866, Tokens/sec 452.092, Trained Tokens 1821659, Peak mem 14.672 GB\n",
      "Iter 2850: Val loss 2.431, Val took 13.086s\n",
      "Iter 2850: Train loss 0.526, Learning Rate 4.000e-04, It/sec 7.027, Tokens/sec 4903.147, Trained Tokens 1828637, Peak mem 14.672 GB\n",
      "Iter 2860: Train loss 0.532, Learning Rate 4.000e-04, It/sec 0.724, Tokens/sec 414.741, Trained Tokens 1834368, Peak mem 14.672 GB\n",
      "Iter 2870: Train loss 0.523, Learning Rate 4.000e-04, It/sec 0.622, Tokens/sec 396.496, Trained Tokens 1840741, Peak mem 14.672 GB\n",
      "Iter 2880: Train loss 0.592, Learning Rate 4.000e-04, It/sec 0.499, Tokens/sec 442.214, Trained Tokens 1849602, Peak mem 14.672 GB\n",
      "Iter 2890: Train loss 0.556, Learning Rate 4.000e-04, It/sec 1.015, Tokens/sec 460.728, Trained Tokens 1854140, Peak mem 14.672 GB\n",
      "Iter 2900: Val loss 2.479, Val took 14.644s\n",
      "Iter 2900: Train loss 0.486, Learning Rate 4.000e-04, It/sec 5.870, Tokens/sec 3690.858, Trained Tokens 1860428, Peak mem 14.672 GB\n",
      "Iter 2900: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0002900_adapters.safetensors.\n",
      "Iter 2910: Train loss 0.504, Learning Rate 4.000e-04, It/sec 0.801, Tokens/sec 429.665, Trained Tokens 1865794, Peak mem 14.672 GB\n",
      "Iter 2920: Train loss 0.471, Learning Rate 4.000e-04, It/sec 0.625, Tokens/sec 439.525, Trained Tokens 1872823, Peak mem 14.672 GB\n",
      "Iter 2930: Train loss 0.537, Learning Rate 4.000e-04, It/sec 0.692, Tokens/sec 407.622, Trained Tokens 1878717, Peak mem 14.672 GB\n",
      "Iter 2940: Train loss 0.533, Learning Rate 4.000e-04, It/sec 0.693, Tokens/sec 421.921, Trained Tokens 1884805, Peak mem 14.672 GB\n",
      "Iter 2950: Val loss 2.399, Val took 15.473s\n",
      "Iter 2950: Train loss 0.593, Learning Rate 4.000e-04, It/sec 6.769, Tokens/sec 3700.560, Trained Tokens 1890272, Peak mem 14.672 GB\n",
      "Iter 2960: Train loss 0.690, Learning Rate 4.000e-04, It/sec 0.341, Tokens/sec 372.980, Trained Tokens 1901213, Peak mem 14.672 GB\n",
      "Iter 2970: Train loss 0.494, Learning Rate 4.000e-04, It/sec 0.661, Tokens/sec 377.114, Trained Tokens 1906916, Peak mem 14.672 GB\n",
      "Iter 2980: Train loss 0.519, Learning Rate 4.000e-04, It/sec 0.931, Tokens/sec 430.937, Trained Tokens 1911543, Peak mem 14.672 GB\n",
      "Iter 2990: Train loss 0.456, Learning Rate 4.000e-04, It/sec 0.597, Tokens/sec 402.776, Trained Tokens 1918291, Peak mem 14.672 GB\n",
      "Iter 3000: Val loss 2.422, Val took 24.589s\n",
      "Iter 3000: Train loss 0.521, Learning Rate 4.000e-04, It/sec 11.180, Tokens/sec 7088.233, Trained Tokens 1924631, Peak mem 14.672 GB\n",
      "Iter 3000: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003000_adapters.safetensors.\n",
      "Iter 3010: Train loss 0.504, Learning Rate 4.000e-04, It/sec 0.826, Tokens/sec 395.783, Trained Tokens 1929424, Peak mem 14.672 GB\n",
      "Iter 3020: Train loss 0.507, Learning Rate 4.000e-04, It/sec 0.556, Tokens/sec 318.888, Trained Tokens 1935162, Peak mem 14.672 GB\n",
      "Iter 3030: Train loss 0.537, Learning Rate 4.000e-04, It/sec 0.556, Tokens/sec 385.021, Trained Tokens 1942081, Peak mem 14.672 GB\n",
      "Iter 3040: Train loss 0.574, Learning Rate 4.000e-04, It/sec 0.383, Tokens/sec 231.329, Trained Tokens 1948115, Peak mem 14.672 GB\n",
      "Iter 3050: Val loss 2.406, Val took 13.388s\n",
      "Iter 3050: Train loss 0.455, Learning Rate 4.000e-04, It/sec 3.402, Tokens/sec 2627.384, Trained Tokens 1955838, Peak mem 14.672 GB\n",
      "Iter 3060: Train loss 0.569, Learning Rate 4.000e-04, It/sec 0.579, Tokens/sec 406.837, Trained Tokens 1962870, Peak mem 14.672 GB\n",
      "Iter 3070: Train loss 0.480, Learning Rate 4.000e-04, It/sec 0.745, Tokens/sec 427.834, Trained Tokens 1968610, Peak mem 14.672 GB\n",
      "Iter 3080: Train loss 0.493, Learning Rate 4.000e-04, It/sec 0.695, Tokens/sec 424.935, Trained Tokens 1974724, Peak mem 14.672 GB\n",
      "Iter 3090: Train loss 0.461, Learning Rate 4.000e-04, It/sec 0.782, Tokens/sec 408.501, Trained Tokens 1979948, Peak mem 14.672 GB\n",
      "Iter 3100: Val loss 2.400, Val took 13.545s\n",
      "Iter 3100: Train loss 0.500, Learning Rate 4.000e-04, It/sec 5.897, Tokens/sec 5892.310, Trained Tokens 1989940, Peak mem 14.672 GB\n",
      "Iter 3100: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003100_adapters.safetensors.\n",
      "Iter 3110: Train loss 0.490, Learning Rate 4.000e-04, It/sec 0.628, Tokens/sec 423.595, Trained Tokens 1996686, Peak mem 14.672 GB\n",
      "Iter 3120: Train loss 0.520, Learning Rate 4.000e-04, It/sec 0.497, Tokens/sec 309.569, Trained Tokens 2002915, Peak mem 14.672 GB\n",
      "Iter 3130: Train loss 0.510, Learning Rate 4.000e-04, It/sec 0.594, Tokens/sec 312.014, Trained Tokens 2008166, Peak mem 14.672 GB\n",
      "Iter 3140: Train loss 0.544, Learning Rate 4.000e-04, It/sec 0.701, Tokens/sec 397.320, Trained Tokens 2013831, Peak mem 14.672 GB\n",
      "Iter 3150: Val loss 2.385, Val took 15.869s\n",
      "Iter 3150: Train loss 0.514, Learning Rate 4.000e-04, It/sec 5.132, Tokens/sec 3792.593, Trained Tokens 2021221, Peak mem 14.672 GB\n",
      "Iter 3160: Train loss 0.538, Learning Rate 4.000e-04, It/sec 0.769, Tokens/sec 435.363, Trained Tokens 2026883, Peak mem 14.672 GB\n",
      "Iter 3170: Train loss 0.509, Learning Rate 4.000e-04, It/sec 0.823, Tokens/sec 429.831, Trained Tokens 2032104, Peak mem 14.672 GB\n",
      "Iter 3180: Train loss 0.477, Learning Rate 4.000e-04, It/sec 0.780, Tokens/sec 407.720, Trained Tokens 2037328, Peak mem 14.672 GB\n",
      "Iter 3190: Train loss 0.586, Learning Rate 4.000e-04, It/sec 0.436, Tokens/sec 368.632, Trained Tokens 2045792, Peak mem 14.672 GB\n",
      "Iter 3200: Val loss 2.454, Val took 11.782s\n",
      "Iter 3200: Train loss 0.462, Learning Rate 4.000e-04, It/sec 4.883, Tokens/sec 3387.557, Trained Tokens 2052730, Peak mem 14.672 GB\n",
      "Iter 3200: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003200_adapters.safetensors.\n",
      "Iter 3210: Train loss 0.494, Learning Rate 4.000e-04, It/sec 0.744, Tokens/sec 398.070, Trained Tokens 2058078, Peak mem 14.672 GB\n",
      "Iter 3220: Train loss 0.489, Learning Rate 4.000e-04, It/sec 0.579, Tokens/sec 425.218, Trained Tokens 2065425, Peak mem 14.672 GB\n",
      "Iter 3230: Train loss 0.508, Learning Rate 4.000e-04, It/sec 0.834, Tokens/sec 404.536, Trained Tokens 2070276, Peak mem 14.672 GB\n",
      "Iter 3240: Train loss 0.455, Learning Rate 4.000e-04, It/sec 0.517, Tokens/sec 460.300, Trained Tokens 2079182, Peak mem 14.672 GB\n",
      "Iter 3250: Val loss 2.450, Val took 12.403s\n",
      "Iter 3250: Train loss 0.486, Learning Rate 4.000e-04, It/sec 7.764, Tokens/sec 5035.223, Trained Tokens 2085667, Peak mem 14.672 GB\n",
      "Iter 3260: Train loss 0.472, Learning Rate 4.000e-04, It/sec 0.850, Tokens/sec 405.825, Trained Tokens 2090441, Peak mem 14.672 GB\n",
      "Iter 3270: Train loss 0.522, Learning Rate 4.000e-04, It/sec 0.593, Tokens/sec 402.253, Trained Tokens 2097223, Peak mem 14.672 GB\n",
      "Iter 3280: Train loss 0.453, Learning Rate 4.000e-04, It/sec 0.637, Tokens/sec 348.569, Trained Tokens 2102693, Peak mem 14.672 GB\n",
      "Iter 3290: Train loss 0.500, Learning Rate 4.000e-04, It/sec 0.658, Tokens/sec 413.198, Trained Tokens 2108968, Peak mem 14.672 GB\n",
      "Iter 3300: Val loss 2.386, Val took 13.431s\n",
      "Iter 3300: Train loss 0.593, Learning Rate 4.000e-04, It/sec 6.255, Tokens/sec 4872.913, Trained Tokens 2116759, Peak mem 14.672 GB\n",
      "Iter 3300: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003300_adapters.safetensors.\n",
      "Iter 3310: Train loss 0.538, Learning Rate 4.000e-04, It/sec 0.824, Tokens/sec 416.225, Trained Tokens 2121813, Peak mem 14.672 GB\n",
      "Iter 3320: Train loss 0.454, Learning Rate 4.000e-04, It/sec 0.604, Tokens/sec 391.458, Trained Tokens 2128291, Peak mem 14.672 GB\n",
      "Iter 3330: Train loss 0.442, Learning Rate 4.000e-04, It/sec 0.586, Tokens/sec 407.879, Trained Tokens 2135251, Peak mem 14.672 GB\n",
      "Iter 3340: Train loss 0.486, Learning Rate 4.000e-04, It/sec 0.433, Tokens/sec 393.252, Trained Tokens 2144333, Peak mem 14.672 GB\n",
      "Iter 3350: Val loss 2.411, Val took 12.351s\n",
      "Iter 3350: Train loss 0.455, Learning Rate 4.000e-04, It/sec 3.643, Tokens/sec 2059.555, Trained Tokens 2149986, Peak mem 14.672 GB\n",
      "Iter 3360: Train loss 0.426, Learning Rate 4.000e-04, It/sec 0.742, Tokens/sec 450.177, Trained Tokens 2156050, Peak mem 14.672 GB\n",
      "Iter 3370: Train loss 0.461, Learning Rate 4.000e-04, It/sec 0.821, Tokens/sec 406.206, Trained Tokens 2160999, Peak mem 14.672 GB\n",
      "Iter 3380: Train loss 0.427, Learning Rate 4.000e-04, It/sec 0.612, Tokens/sec 437.373, Trained Tokens 2168151, Peak mem 14.672 GB\n",
      "Iter 3390: Train loss 0.523, Learning Rate 4.000e-04, It/sec 1.049, Tokens/sec 417.575, Trained Tokens 2172132, Peak mem 14.672 GB\n",
      "Iter 3400: Val loss 2.482, Val took 19.041s\n",
      "Iter 3400: Train loss 0.441, Learning Rate 4.000e-04, It/sec 13.081, Tokens/sec 7423.464, Trained Tokens 2177807, Peak mem 14.672 GB\n",
      "Iter 3400: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003400_adapters.safetensors.\n",
      "Iter 3410: Train loss 0.520, Learning Rate 4.000e-04, It/sec 0.357, Tokens/sec 345.506, Trained Tokens 2187483, Peak mem 14.672 GB\n",
      "Iter 3420: Train loss 0.456, Learning Rate 4.000e-04, It/sec 0.645, Tokens/sec 386.903, Trained Tokens 2193485, Peak mem 14.672 GB\n",
      "Iter 3430: Train loss 0.474, Learning Rate 4.000e-04, It/sec 0.577, Tokens/sec 397.971, Trained Tokens 2200382, Peak mem 14.672 GB\n",
      "Iter 3440: Train loss 0.457, Learning Rate 4.000e-04, It/sec 0.753, Tokens/sec 442.303, Trained Tokens 2206255, Peak mem 14.672 GB\n",
      "Iter 3450: Val loss 2.427, Val took 13.492s\n",
      "Iter 3450: Train loss 0.554, Learning Rate 4.000e-04, It/sec 4.703, Tokens/sec 3936.834, Trained Tokens 2214626, Peak mem 14.672 GB\n",
      "Iter 3460: Train loss 0.477, Learning Rate 4.000e-04, It/sec 0.648, Tokens/sec 385.153, Trained Tokens 2220572, Peak mem 14.672 GB\n",
      "Iter 3470: Train loss 0.450, Learning Rate 4.000e-04, It/sec 0.801, Tokens/sec 430.467, Trained Tokens 2225949, Peak mem 14.672 GB\n",
      "Iter 3480: Train loss 0.456, Learning Rate 4.000e-04, It/sec 0.851, Tokens/sec 430.876, Trained Tokens 2231014, Peak mem 14.672 GB\n",
      "Iter 3490: Train loss 0.407, Learning Rate 4.000e-04, It/sec 0.449, Tokens/sec 397.231, Trained Tokens 2239856, Peak mem 14.672 GB\n",
      "Iter 3500: Val loss 2.455, Val took 12.276s\n",
      "Iter 3500: Train loss 0.470, Learning Rate 4.000e-04, It/sec 8.662, Tokens/sec 5846.719, Trained Tokens 2246606, Peak mem 14.672 GB\n",
      "Iter 3500: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003500_adapters.safetensors.\n",
      "Iter 3510: Train loss 0.447, Learning Rate 4.000e-04, It/sec 0.646, Tokens/sec 377.408, Trained Tokens 2252451, Peak mem 14.672 GB\n",
      "Iter 3520: Train loss 0.465, Learning Rate 4.000e-04, It/sec 0.624, Tokens/sec 434.434, Trained Tokens 2259415, Peak mem 14.672 GB\n",
      "Iter 3530: Train loss 0.526, Learning Rate 4.000e-04, It/sec 0.528, Tokens/sec 403.352, Trained Tokens 2267060, Peak mem 14.672 GB\n",
      "Iter 3540: Train loss 0.472, Learning Rate 4.000e-04, It/sec 0.440, Tokens/sec 310.511, Trained Tokens 2274122, Peak mem 14.672 GB\n",
      "Iter 3550: Val loss 2.466, Val took 12.010s\n",
      "Iter 3550: Train loss 0.488, Learning Rate 4.000e-04, It/sec 5.390, Tokens/sec 2593.872, Trained Tokens 2278934, Peak mem 14.672 GB\n",
      "Iter 3560: Train loss 0.516, Learning Rate 4.000e-04, It/sec 0.836, Tokens/sec 432.375, Trained Tokens 2284104, Peak mem 14.672 GB\n",
      "Iter 3570: Train loss 0.517, Learning Rate 4.000e-04, It/sec 0.660, Tokens/sec 390.287, Trained Tokens 2290015, Peak mem 14.672 GB\n",
      "Iter 3580: Train loss 0.515, Learning Rate 4.000e-04, It/sec 0.608, Tokens/sec 398.179, Trained Tokens 2296565, Peak mem 14.672 GB\n",
      "Iter 3590: Train loss 0.521, Learning Rate 4.000e-04, It/sec 0.759, Tokens/sec 414.127, Trained Tokens 2302021, Peak mem 14.672 GB\n",
      "Iter 3600: Val loss 2.408, Val took 12.648s\n",
      "Iter 3600: Train loss 0.555, Learning Rate 4.000e-04, It/sec 7.658, Tokens/sec 6712.120, Trained Tokens 2310786, Peak mem 14.672 GB\n",
      "Iter 3600: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003600_adapters.safetensors.\n",
      "Iter 3610: Train loss 0.467, Learning Rate 4.000e-04, It/sec 0.624, Tokens/sec 431.892, Trained Tokens 2317707, Peak mem 14.672 GB\n",
      "Iter 3620: Train loss 0.484, Learning Rate 4.000e-04, It/sec 0.780, Tokens/sec 378.534, Trained Tokens 2322558, Peak mem 14.672 GB\n",
      "Iter 3630: Train loss 0.463, Learning Rate 4.000e-04, It/sec 0.655, Tokens/sec 454.296, Trained Tokens 2329492, Peak mem 14.672 GB\n",
      "Iter 3640: Train loss 0.549, Learning Rate 4.000e-04, It/sec 0.568, Tokens/sec 419.714, Trained Tokens 2336877, Peak mem 14.672 GB\n",
      "Iter 3650: Val loss 2.500, Val took 12.677s\n",
      "Iter 3650: Train loss 0.523, Learning Rate 4.000e-04, It/sec 4.664, Tokens/sec 2548.395, Trained Tokens 2342341, Peak mem 14.672 GB\n",
      "Iter 3660: Train loss 0.482, Learning Rate 4.000e-04, It/sec 0.666, Tokens/sec 390.421, Trained Tokens 2348204, Peak mem 14.672 GB\n",
      "Iter 3670: Train loss 0.540, Learning Rate 4.000e-04, It/sec 0.700, Tokens/sec 436.139, Trained Tokens 2354435, Peak mem 14.672 GB\n",
      "Iter 3680: Train loss 0.447, Learning Rate 4.000e-04, It/sec 0.773, Tokens/sec 434.477, Trained Tokens 2360055, Peak mem 14.672 GB\n",
      "Iter 3690: Train loss 0.534, Learning Rate 4.000e-04, It/sec 0.885, Tokens/sec 432.436, Trained Tokens 2364942, Peak mem 14.672 GB\n",
      "Iter 3700: Val loss 2.463, Val took 11.764s\n",
      "Iter 3700: Train loss 0.474, Learning Rate 4.000e-04, It/sec 3.162, Tokens/sec 1848.728, Trained Tokens 2370788, Peak mem 14.672 GB\n",
      "Iter 3700: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003700_adapters.safetensors.\n",
      "Iter 3710: Train loss 0.463, Learning Rate 4.000e-04, It/sec 0.621, Tokens/sec 425.566, Trained Tokens 2377643, Peak mem 14.672 GB\n",
      "Iter 3720: Train loss 0.495, Learning Rate 4.000e-04, It/sec 0.611, Tokens/sec 300.511, Trained Tokens 2382559, Peak mem 14.672 GB\n",
      "Iter 3730: Train loss 0.485, Learning Rate 4.000e-04, It/sec 0.439, Tokens/sec 326.292, Trained Tokens 2389994, Peak mem 14.672 GB\n",
      "Iter 3740: Train loss 0.512, Learning Rate 4.000e-04, It/sec 0.667, Tokens/sec 396.262, Trained Tokens 2395932, Peak mem 14.672 GB\n",
      "Iter 3750: Val loss 2.455, Val took 15.730s\n",
      "Iter 3750: Train loss 0.496, Learning Rate 4.000e-04, It/sec 7.463, Tokens/sec 6450.014, Trained Tokens 2404575, Peak mem 14.672 GB\n",
      "Iter 3760: Train loss 0.522, Learning Rate 4.000e-04, It/sec 0.406, Tokens/sec 392.364, Trained Tokens 2414239, Peak mem 14.672 GB\n",
      "Iter 3770: Train loss 0.541, Learning Rate 4.000e-04, It/sec 1.011, Tokens/sec 424.500, Trained Tokens 2418439, Peak mem 14.672 GB\n",
      "Iter 3780: Train loss 0.452, Learning Rate 4.000e-04, It/sec 0.505, Tokens/sec 321.004, Trained Tokens 2424793, Peak mem 14.672 GB\n",
      "Iter 3790: Train loss 0.481, Learning Rate 4.000e-04, It/sec 0.495, Tokens/sec 413.711, Trained Tokens 2433153, Peak mem 14.672 GB\n",
      "Iter 3800: Val loss 2.472, Val took 13.388s\n",
      "Iter 3800: Train loss 0.511, Learning Rate 4.000e-04, It/sec 7.438, Tokens/sec 5798.728, Trained Tokens 2440949, Peak mem 14.672 GB\n",
      "Iter 3800: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003800_adapters.safetensors.\n",
      "Iter 3810: Train loss 0.510, Learning Rate 4.000e-04, It/sec 0.679, Tokens/sec 400.455, Trained Tokens 2446847, Peak mem 14.672 GB\n",
      "Iter 3820: Train loss 0.480, Learning Rate 4.000e-04, It/sec 0.868, Tokens/sec 427.432, Trained Tokens 2451769, Peak mem 14.672 GB\n",
      "Iter 3830: Train loss 0.455, Learning Rate 4.000e-04, It/sec 0.679, Tokens/sec 396.949, Trained Tokens 2457614, Peak mem 14.672 GB\n",
      "Iter 3840: Train loss 0.492, Learning Rate 4.000e-04, It/sec 0.609, Tokens/sec 375.744, Trained Tokens 2463785, Peak mem 14.672 GB\n",
      "Iter 3850: Val loss 2.484, Val took 13.059s\n",
      "Iter 3850: Train loss 0.511, Learning Rate 4.000e-04, It/sec 3.663, Tokens/sec 2367.868, Trained Tokens 2470250, Peak mem 14.672 GB\n",
      "Iter 3860: Train loss 0.550, Learning Rate 4.000e-04, It/sec 0.928, Tokens/sec 437.841, Trained Tokens 2474969, Peak mem 14.672 GB\n",
      "Iter 3870: Train loss 0.481, Learning Rate 4.000e-04, It/sec 0.530, Tokens/sec 385.641, Trained Tokens 2482250, Peak mem 14.672 GB\n",
      "Iter 3880: Train loss 0.426, Learning Rate 4.000e-04, It/sec 0.576, Tokens/sec 370.715, Trained Tokens 2488684, Peak mem 14.672 GB\n",
      "Iter 3890: Train loss 0.403, Learning Rate 4.000e-04, It/sec 0.707, Tokens/sec 430.792, Trained Tokens 2494776, Peak mem 14.672 GB\n",
      "Iter 3900: Val loss 2.488, Val took 11.661s\n",
      "Iter 3900: Train loss 0.402, Learning Rate 4.000e-04, It/sec 3.675, Tokens/sec 2865.462, Trained Tokens 2502574, Peak mem 14.672 GB\n",
      "Iter 3900: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0003900_adapters.safetensors.\n",
      "Iter 3910: Train loss 0.436, Learning Rate 4.000e-04, It/sec 0.576, Tokens/sec 412.281, Trained Tokens 2509737, Peak mem 14.672 GB\n",
      "Iter 3920: Train loss 0.445, Learning Rate 4.000e-04, It/sec 0.664, Tokens/sec 414.798, Trained Tokens 2515983, Peak mem 14.672 GB\n",
      "Iter 3930: Train loss 0.436, Learning Rate 4.000e-04, It/sec 0.702, Tokens/sec 416.265, Trained Tokens 2521910, Peak mem 14.672 GB\n",
      "Iter 3940: Train loss 0.435, Learning Rate 4.000e-04, It/sec 0.583, Tokens/sec 419.194, Trained Tokens 2529103, Peak mem 14.672 GB\n",
      "Iter 3950: Val loss 2.510, Val took 11.833s\n",
      "Iter 3950: Train loss 0.487, Learning Rate 4.000e-04, It/sec 15.136, Tokens/sec 7431.870, Trained Tokens 2534013, Peak mem 14.672 GB\n",
      "Iter 3960: Train loss 0.580, Learning Rate 4.000e-04, It/sec 0.863, Tokens/sec 406.689, Trained Tokens 2538724, Peak mem 14.672 GB\n",
      "Iter 3970: Train loss 0.615, Learning Rate 4.000e-04, It/sec 0.487, Tokens/sec 398.965, Trained Tokens 2546910, Peak mem 14.672 GB\n",
      "Iter 3980: Train loss 0.432, Learning Rate 4.000e-04, It/sec 0.569, Tokens/sec 435.325, Trained Tokens 2554555, Peak mem 14.672 GB\n",
      "Iter 3990: Train loss 0.425, Learning Rate 4.000e-04, It/sec 0.689, Tokens/sec 433.558, Trained Tokens 2560852, Peak mem 14.672 GB\n",
      "Iter 4000: Val loss 2.565, Val took 12.112s\n",
      "Iter 4000: Train loss 0.491, Learning Rate 4.000e-04, It/sec 9.453, Tokens/sec 8031.198, Trained Tokens 2569348, Peak mem 14.672 GB\n",
      "Iter 4000: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004000_adapters.safetensors.\n",
      "Iter 4010: Train loss 0.407, Learning Rate 4.000e-04, It/sec 0.606, Tokens/sec 430.664, Trained Tokens 2576454, Peak mem 14.672 GB\n",
      "Iter 4020: Train loss 0.413, Learning Rate 4.000e-04, It/sec 0.692, Tokens/sec 420.175, Trained Tokens 2582524, Peak mem 14.672 GB\n",
      "Iter 4030: Train loss 0.474, Learning Rate 4.000e-04, It/sec 0.933, Tokens/sec 439.704, Trained Tokens 2587235, Peak mem 14.672 GB\n",
      "Iter 4040: Train loss 0.486, Learning Rate 4.000e-04, It/sec 0.730, Tokens/sec 408.621, Trained Tokens 2592836, Peak mem 14.672 GB\n",
      "Iter 4050: Val loss 2.432, Val took 13.095s\n",
      "Iter 4050: Train loss 0.427, Learning Rate 4.000e-04, It/sec 5.557, Tokens/sec 3443.479, Trained Tokens 2599033, Peak mem 14.672 GB\n",
      "Iter 4060: Train loss 0.452, Learning Rate 4.000e-04, It/sec 0.865, Tokens/sec 451.515, Trained Tokens 2604254, Peak mem 14.672 GB\n",
      "Iter 4070: Train loss 0.426, Learning Rate 4.000e-04, It/sec 0.645, Tokens/sec 402.365, Trained Tokens 2610496, Peak mem 14.672 GB\n",
      "Iter 4080: Train loss 0.456, Learning Rate 4.000e-04, It/sec 0.628, Tokens/sec 418.451, Trained Tokens 2617160, Peak mem 14.672 GB\n",
      "Iter 4090: Train loss 0.399, Learning Rate 4.000e-04, It/sec 0.764, Tokens/sec 412.218, Trained Tokens 2622557, Peak mem 14.672 GB\n",
      "Iter 4100: Val loss 2.455, Val took 12.040s\n",
      "Iter 4100: Train loss 0.413, Learning Rate 4.000e-04, It/sec 1.905, Tokens/sec 1753.998, Trained Tokens 2631766, Peak mem 14.672 GB\n",
      "Iter 4100: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004100_adapters.safetensors.\n",
      "Iter 4110: Train loss 0.503, Learning Rate 4.000e-04, It/sec 1.036, Tokens/sec 421.617, Trained Tokens 2635834, Peak mem 14.672 GB\n",
      "Iter 4120: Train loss 0.397, Learning Rate 4.000e-04, It/sec 0.656, Tokens/sec 414.695, Trained Tokens 2642156, Peak mem 14.672 GB\n",
      "Iter 4130: Train loss 0.473, Learning Rate 4.000e-04, It/sec 0.436, Tokens/sec 361.964, Trained Tokens 2650459, Peak mem 14.672 GB\n",
      "Iter 4140: Train loss 0.449, Learning Rate 4.000e-04, It/sec 0.532, Tokens/sec 338.262, Trained Tokens 2656815, Peak mem 14.672 GB\n",
      "Iter 4150: Val loss 2.511, Val took 14.655s\n",
      "Iter 4150: Train loss 0.416, Learning Rate 4.000e-04, It/sec 6.742, Tokens/sec 3609.666, Trained Tokens 2662169, Peak mem 14.672 GB\n",
      "Iter 4160: Train loss 0.497, Learning Rate 4.000e-04, It/sec 0.889, Tokens/sec 426.812, Trained Tokens 2666968, Peak mem 14.672 GB\n",
      "Iter 4170: Train loss 0.439, Learning Rate 4.000e-04, It/sec 0.628, Tokens/sec 410.577, Trained Tokens 2673501, Peak mem 14.672 GB\n",
      "Iter 4180: Train loss 0.414, Learning Rate 4.000e-04, It/sec 0.595, Tokens/sec 419.419, Trained Tokens 2680555, Peak mem 14.672 GB\n",
      "Iter 4190: Train loss 0.432, Learning Rate 4.000e-04, It/sec 0.655, Tokens/sec 419.453, Trained Tokens 2686955, Peak mem 14.672 GB\n",
      "Iter 4200: Val loss 2.506, Val took 12.893s\n",
      "Iter 4200: Train loss 0.317, Learning Rate 4.000e-04, It/sec 8.980, Tokens/sec 6908.330, Trained Tokens 2694648, Peak mem 14.672 GB\n",
      "Iter 4200: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004200_adapters.safetensors.\n",
      "Iter 4210: Train loss 0.382, Learning Rate 4.000e-04, It/sec 0.585, Tokens/sec 411.906, Trained Tokens 2701684, Peak mem 14.672 GB\n",
      "Iter 4220: Train loss 0.402, Learning Rate 4.000e-04, It/sec 0.715, Tokens/sec 439.106, Trained Tokens 2707822, Peak mem 14.672 GB\n",
      "Iter 4230: Train loss 0.383, Learning Rate 4.000e-04, It/sec 0.727, Tokens/sec 423.907, Trained Tokens 2713653, Peak mem 14.672 GB\n",
      "Iter 4240: Train loss 0.376, Learning Rate 4.000e-04, It/sec 0.652, Tokens/sec 423.360, Trained Tokens 2720149, Peak mem 14.672 GB\n",
      "Iter 4250: Val loss 2.499, Val took 14.785s\n",
      "Iter 4250: Train loss 0.469, Learning Rate 4.000e-04, It/sec 7.544, Tokens/sec 3428.085, Trained Tokens 2724693, Peak mem 14.672 GB\n",
      "Iter 4260: Train loss 0.481, Learning Rate 4.000e-04, It/sec 1.048, Tokens/sec 459.896, Trained Tokens 2729081, Peak mem 14.672 GB\n",
      "Iter 4270: Train loss 0.458, Learning Rate 4.000e-04, It/sec 0.397, Tokens/sec 399.046, Trained Tokens 2739130, Peak mem 14.672 GB\n",
      "Iter 4280: Train loss 0.528, Learning Rate 4.000e-04, It/sec 0.660, Tokens/sec 420.823, Trained Tokens 2745503, Peak mem 14.672 GB\n",
      "Iter 4290: Train loss 0.449, Learning Rate 4.000e-04, It/sec 0.711, Tokens/sec 425.977, Trained Tokens 2751498, Peak mem 14.672 GB\n",
      "Iter 4300: Val loss 2.512, Val took 13.248s\n",
      "Iter 4300: Train loss 0.418, Learning Rate 4.000e-04, It/sec 12.841, Tokens/sec 7575.115, Trained Tokens 2757397, Peak mem 14.672 GB\n",
      "Iter 4300: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004300_adapters.safetensors.\n",
      "Iter 4310: Train loss 0.351, Learning Rate 4.000e-04, It/sec 0.484, Tokens/sec 397.527, Trained Tokens 2765610, Peak mem 14.672 GB\n",
      "Iter 4320: Train loss 0.407, Learning Rate 4.000e-04, It/sec 0.838, Tokens/sec 424.021, Trained Tokens 2770671, Peak mem 14.672 GB\n",
      "Iter 4330: Train loss 0.422, Learning Rate 4.000e-04, It/sec 0.603, Tokens/sec 320.057, Trained Tokens 2775978, Peak mem 14.672 GB\n",
      "Iter 4340: Train loss 0.460, Learning Rate 4.000e-04, It/sec 0.750, Tokens/sec 400.933, Trained Tokens 2781326, Peak mem 14.672 GB\n",
      "Iter 4350: Val loss 2.506, Val took 13.494s\n",
      "Iter 4350: Train loss 0.406, Learning Rate 4.000e-04, It/sec 6.857, Tokens/sec 3948.100, Trained Tokens 2787084, Peak mem 14.672 GB\n",
      "Iter 4360: Train loss 0.403, Learning Rate 4.000e-04, It/sec 0.629, Tokens/sec 410.514, Trained Tokens 2793612, Peak mem 14.672 GB\n",
      "Iter 4370: Train loss 0.523, Learning Rate 4.000e-04, It/sec 0.818, Tokens/sec 413.664, Trained Tokens 2798670, Peak mem 14.672 GB\n",
      "Iter 4380: Train loss 0.483, Learning Rate 4.000e-04, It/sec 0.404, Tokens/sec 436.990, Trained Tokens 2809498, Peak mem 14.672 GB\n",
      "Iter 4390: Train loss 0.391, Learning Rate 4.000e-04, It/sec 0.638, Tokens/sec 422.956, Trained Tokens 2816131, Peak mem 14.672 GB\n",
      "Iter 4400: Val loss 2.563, Val took 13.070s\n",
      "Iter 4400: Train loss 0.433, Learning Rate 4.000e-04, It/sec 12.002, Tokens/sec 6351.220, Trained Tokens 2821423, Peak mem 14.672 GB\n",
      "Iter 4400: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004400_adapters.safetensors.\n",
      "Iter 4410: Train loss 0.478, Learning Rate 4.000e-04, It/sec 0.452, Tokens/sec 424.785, Trained Tokens 2830816, Peak mem 14.672 GB\n",
      "Iter 4420: Train loss 0.415, Learning Rate 4.000e-04, It/sec 0.786, Tokens/sec 487.235, Trained Tokens 2837015, Peak mem 14.672 GB\n",
      "Iter 4430: Train loss 0.477, Learning Rate 4.000e-04, It/sec 1.018, Tokens/sec 482.356, Trained Tokens 2841752, Peak mem 14.672 GB\n",
      "Iter 4440: Train loss 0.418, Learning Rate 4.000e-04, It/sec 0.683, Tokens/sec 461.201, Trained Tokens 2848503, Peak mem 14.672 GB\n",
      "Iter 4450: Val loss 2.541, Val took 13.099s\n",
      "Iter 4450: Train loss 0.464, Learning Rate 4.000e-04, It/sec 6.787, Tokens/sec 3774.989, Trained Tokens 2854065, Peak mem 14.672 GB\n",
      "Iter 4460: Train loss 0.473, Learning Rate 4.000e-04, It/sec 0.865, Tokens/sec 442.127, Trained Tokens 2859179, Peak mem 14.672 GB\n",
      "Iter 4470: Train loss 0.459, Learning Rate 4.000e-04, It/sec 0.796, Tokens/sec 469.375, Trained Tokens 2865076, Peak mem 14.672 GB\n",
      "Iter 4480: Train loss 0.474, Learning Rate 4.000e-04, It/sec 0.526, Tokens/sec 409.473, Trained Tokens 2872863, Peak mem 14.672 GB\n",
      "Iter 4490: Train loss 0.428, Learning Rate 4.000e-04, It/sec 0.532, Tokens/sec 381.020, Trained Tokens 2880021, Peak mem 14.672 GB\n",
      "Iter 4500: Val loss 2.514, Val took 13.802s\n",
      "Iter 4500: Train loss 0.410, Learning Rate 4.000e-04, It/sec 16.433, Tokens/sec 10247.560, Trained Tokens 2886257, Peak mem 14.672 GB\n",
      "Iter 4500: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004500_adapters.safetensors.\n",
      "Iter 4510: Train loss 0.396, Learning Rate 4.000e-04, It/sec 0.590, Tokens/sec 399.799, Trained Tokens 2893038, Peak mem 14.672 GB\n",
      "Iter 4520: Train loss 0.401, Learning Rate 4.000e-04, It/sec 0.649, Tokens/sec 423.975, Trained Tokens 2899572, Peak mem 14.672 GB\n",
      "Iter 4530: Train loss 0.477, Learning Rate 4.000e-04, It/sec 0.815, Tokens/sec 433.050, Trained Tokens 2904884, Peak mem 14.672 GB\n",
      "Iter 4540: Train loss 0.467, Learning Rate 4.000e-04, It/sec 0.750, Tokens/sec 397.109, Trained Tokens 2910180, Peak mem 14.672 GB\n",
      "Iter 4550: Val loss 2.558, Val took 13.117s\n",
      "Iter 4550: Train loss 0.492, Learning Rate 4.000e-04, It/sec 19.104, Tokens/sec 11989.535, Trained Tokens 2916456, Peak mem 14.672 GB\n",
      "Iter 4560: Train loss 0.447, Learning Rate 4.000e-04, It/sec 0.443, Tokens/sec 456.195, Trained Tokens 2926755, Peak mem 14.672 GB\n",
      "Iter 4570: Train loss 0.445, Learning Rate 4.000e-04, It/sec 0.741, Tokens/sec 429.915, Trained Tokens 2932557, Peak mem 14.672 GB\n",
      "Iter 4580: Train loss 0.472, Learning Rate 4.000e-04, It/sec 0.556, Tokens/sec 401.887, Trained Tokens 2939791, Peak mem 14.672 GB\n",
      "Iter 4590: Train loss 0.510, Learning Rate 4.000e-04, It/sec 0.872, Tokens/sec 393.771, Trained Tokens 2944305, Peak mem 14.672 GB\n",
      "Iter 4600: Val loss 2.537, Val took 12.853s\n",
      "Iter 4600: Train loss 0.517, Learning Rate 4.000e-04, It/sec 3.824, Tokens/sec 2752.282, Trained Tokens 2951502, Peak mem 14.672 GB\n",
      "Iter 4600: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004600_adapters.safetensors.\n",
      "Iter 4610: Train loss 0.451, Learning Rate 4.000e-04, It/sec 0.675, Tokens/sec 421.773, Trained Tokens 2957755, Peak mem 14.672 GB\n",
      "Iter 4620: Train loss 0.463, Learning Rate 4.000e-04, It/sec 0.770, Tokens/sec 441.400, Trained Tokens 2963485, Peak mem 14.672 GB\n",
      "Iter 4630: Train loss 0.490, Learning Rate 4.000e-04, It/sec 0.627, Tokens/sec 420.939, Trained Tokens 2970198, Peak mem 14.672 GB\n",
      "Iter 4640: Train loss 0.449, Learning Rate 4.000e-04, It/sec 0.464, Tokens/sec 401.836, Trained Tokens 2978860, Peak mem 14.672 GB\n",
      "Iter 4650: Val loss 2.485, Val took 14.144s\n",
      "Iter 4650: Train loss 0.468, Learning Rate 4.000e-04, It/sec 8.966, Tokens/sec 5719.352, Trained Tokens 2985239, Peak mem 14.672 GB\n",
      "Iter 4660: Train loss 0.517, Learning Rate 4.000e-04, It/sec 0.627, Tokens/sec 440.957, Trained Tokens 2992275, Peak mem 14.672 GB\n",
      "Iter 4670: Train loss 0.523, Learning Rate 4.000e-04, It/sec 0.874, Tokens/sec 433.807, Trained Tokens 2997241, Peak mem 14.672 GB\n",
      "Iter 4680: Train loss 0.541, Learning Rate 4.000e-04, It/sec 0.765, Tokens/sec 403.218, Trained Tokens 3002512, Peak mem 14.672 GB\n",
      "Iter 4690: Train loss 0.547, Learning Rate 4.000e-04, It/sec 0.677, Tokens/sec 420.551, Trained Tokens 3008724, Peak mem 14.672 GB\n",
      "Iter 4700: Val loss 2.499, Val took 12.304s\n",
      "Iter 4700: Train loss 0.514, Learning Rate 4.000e-04, It/sec 13.681, Tokens/sec 8650.318, Trained Tokens 3015047, Peak mem 14.672 GB\n",
      "Iter 4700: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004700_adapters.safetensors.\n",
      "Iter 4710: Train loss 0.578, Learning Rate 4.000e-04, It/sec 0.633, Tokens/sec 367.769, Trained Tokens 3020858, Peak mem 14.672 GB\n",
      "Iter 4720: Train loss 0.455, Learning Rate 4.000e-04, It/sec 0.606, Tokens/sec 414.528, Trained Tokens 3027704, Peak mem 14.672 GB\n",
      "Iter 4730: Train loss 0.539, Learning Rate 4.000e-04, It/sec 0.652, Tokens/sec 411.064, Trained Tokens 3034011, Peak mem 14.672 GB\n",
      "Iter 4740: Train loss 0.493, Learning Rate 4.000e-04, It/sec 0.684, Tokens/sec 422.528, Trained Tokens 3040191, Peak mem 14.672 GB\n",
      "Iter 4750: Val loss 2.512, Val took 14.568s\n",
      "Iter 4750: Train loss 0.466, Learning Rate 4.000e-04, It/sec 8.894, Tokens/sec 6264.940, Trained Tokens 3047235, Peak mem 14.672 GB\n",
      "Iter 4760: Train loss 0.560, Learning Rate 4.000e-04, It/sec 0.473, Tokens/sec 420.604, Trained Tokens 3056127, Peak mem 14.672 GB\n",
      "Iter 4770: Train loss 0.487, Learning Rate 4.000e-04, It/sec 0.710, Tokens/sec 430.029, Trained Tokens 3062187, Peak mem 14.672 GB\n",
      "Iter 4780: Train loss 0.514, Learning Rate 4.000e-04, It/sec 0.755, Tokens/sec 401.505, Trained Tokens 3067507, Peak mem 14.672 GB\n",
      "Iter 4790: Train loss 0.523, Learning Rate 4.000e-04, It/sec 0.793, Tokens/sec 439.051, Trained Tokens 3073041, Peak mem 14.672 GB\n",
      "Iter 4800: Val loss 2.463, Val took 13.070s\n",
      "Iter 4800: Train loss 0.570, Learning Rate 4.000e-04, It/sec 10.702, Tokens/sec 8027.586, Trained Tokens 3080542, Peak mem 14.672 GB\n",
      "Iter 4800: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004800_adapters.safetensors.\n",
      "Iter 4810: Train loss 0.396, Learning Rate 4.000e-04, It/sec 0.605, Tokens/sec 421.018, Trained Tokens 3087504, Peak mem 14.672 GB\n",
      "Iter 4820: Train loss 0.496, Learning Rate 4.000e-04, It/sec 0.586, Tokens/sec 432.255, Trained Tokens 3094879, Peak mem 14.672 GB\n",
      "Iter 4830: Train loss 0.470, Learning Rate 4.000e-04, It/sec 0.746, Tokens/sec 433.706, Trained Tokens 3100690, Peak mem 14.672 GB\n",
      "Iter 4840: Train loss 0.437, Learning Rate 4.000e-04, It/sec 0.673, Tokens/sec 367.725, Trained Tokens 3106153, Peak mem 14.672 GB\n",
      "Iter 4850: Val loss 2.514, Val took 17.129s\n",
      "Iter 4850: Train loss 0.493, Learning Rate 4.000e-04, It/sec 6.634, Tokens/sec 4619.590, Trained Tokens 3113117, Peak mem 14.672 GB\n",
      "Iter 4860: Train loss 0.493, Learning Rate 4.000e-04, It/sec 0.663, Tokens/sec 428.340, Trained Tokens 3119580, Peak mem 14.672 GB\n",
      "Iter 4870: Train loss 0.496, Learning Rate 4.000e-04, It/sec 0.601, Tokens/sec 396.284, Trained Tokens 3126177, Peak mem 14.672 GB\n",
      "Iter 4880: Train loss 0.512, Learning Rate 4.000e-04, It/sec 0.679, Tokens/sec 393.585, Trained Tokens 3131973, Peak mem 14.672 GB\n",
      "Iter 4890: Train loss 0.501, Learning Rate 4.000e-04, It/sec 0.653, Tokens/sec 399.136, Trained Tokens 3138086, Peak mem 14.672 GB\n",
      "Iter 4900: Val loss 2.479, Val took 12.487s\n",
      "Iter 4900: Train loss 0.533, Learning Rate 4.000e-04, It/sec 4.721, Tokens/sec 2322.026, Trained Tokens 3143005, Peak mem 14.672 GB\n",
      "Iter 4900: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0004900_adapters.safetensors.\n",
      "Iter 4910: Train loss 0.454, Learning Rate 4.000e-04, It/sec 0.883, Tokens/sec 445.032, Trained Tokens 3148045, Peak mem 14.672 GB\n",
      "Iter 4920: Train loss 0.403, Learning Rate 4.000e-04, It/sec 0.631, Tokens/sec 391.185, Trained Tokens 3154245, Peak mem 14.672 GB\n",
      "Iter 4930: Train loss 0.452, Learning Rate 4.000e-04, It/sec 0.471, Tokens/sec 405.183, Trained Tokens 3162855, Peak mem 14.672 GB\n",
      "Iter 4940: Train loss 0.443, Learning Rate 4.000e-04, It/sec 0.736, Tokens/sec 417.779, Trained Tokens 3168532, Peak mem 14.672 GB\n",
      "Iter 4950: Val loss 2.478, Val took 13.106s\n",
      "Iter 4950: Train loss 0.487, Learning Rate 4.000e-04, It/sec 11.121, Tokens/sec 6423.746, Trained Tokens 3174308, Peak mem 14.672 GB\n",
      "Iter 4960: Train loss 0.547, Learning Rate 4.000e-04, It/sec 0.551, Tokens/sec 404.397, Trained Tokens 3181652, Peak mem 14.672 GB\n",
      "Iter 4970: Train loss 0.521, Learning Rate 4.000e-04, It/sec 0.446, Tokens/sec 421.416, Trained Tokens 3191101, Peak mem 14.672 GB\n",
      "Iter 4980: Train loss 0.493, Learning Rate 4.000e-04, It/sec 0.616, Tokens/sec 350.532, Trained Tokens 3196796, Peak mem 14.672 GB\n",
      "Iter 4990: Train loss 0.489, Learning Rate 4.000e-04, It/sec 0.735, Tokens/sec 387.980, Trained Tokens 3202073, Peak mem 14.672 GB\n",
      "Iter 5000: Val loss 2.541, Val took 13.794s\n",
      "Iter 5000: Train loss 0.507, Learning Rate 4.000e-04, It/sec 6.632, Tokens/sec 3180.202, Trained Tokens 3206868, Peak mem 14.672 GB\n",
      "Iter 5000: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0005000_adapters.safetensors.\n",
      "Iter 5010: Train loss 0.460, Learning Rate 4.000e-04, It/sec 0.565, Tokens/sec 328.850, Trained Tokens 3212685, Peak mem 14.672 GB\n",
      "Iter 5020: Train loss 0.442, Learning Rate 4.000e-04, It/sec 0.497, Tokens/sec 330.658, Trained Tokens 3219339, Peak mem 14.672 GB\n",
      "Iter 5030: Train loss 0.441, Learning Rate 4.000e-04, It/sec 0.510, Tokens/sec 295.810, Trained Tokens 3225140, Peak mem 14.672 GB\n",
      "Iter 5040: Train loss 0.451, Learning Rate 4.000e-04, It/sec 0.500, Tokens/sec 297.514, Trained Tokens 3231086, Peak mem 14.672 GB\n",
      "Iter 5050: Val loss 2.484, Val took 44.537s\n",
      "Iter 5050: Train loss 0.526, Learning Rate 4.000e-04, It/sec 18.301, Tokens/sec 9101.240, Trained Tokens 3236059, Peak mem 14.672 GB\n",
      "Iter 5060: Train loss 0.498, Learning Rate 4.000e-04, It/sec 0.470, Tokens/sec 395.454, Trained Tokens 3244472, Peak mem 14.672 GB\n",
      "Iter 5070: Train loss 0.482, Learning Rate 4.000e-04, It/sec 0.604, Tokens/sec 337.228, Trained Tokens 3250057, Peak mem 14.672 GB\n",
      "Iter 5080: Train loss 0.485, Learning Rate 4.000e-04, It/sec 0.613, Tokens/sec 424.888, Trained Tokens 3256993, Peak mem 14.672 GB\n",
      "Iter 5090: Train loss 0.528, Learning Rate 4.000e-04, It/sec 0.842, Tokens/sec 460.465, Trained Tokens 3262464, Peak mem 14.672 GB\n",
      "Iter 5100: Val loss 2.518, Val took 12.533s\n",
      "Iter 5100: Train loss 0.486, Learning Rate 4.000e-04, It/sec 5.034, Tokens/sec 4523.453, Trained Tokens 3271450, Peak mem 14.672 GB\n",
      "Iter 5100: Saved adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors and ../trained_models/adapters_40_8_4e4/0005100_adapters.safetensors.\n",
      "Iter 5110: Train loss 0.435, Learning Rate 4.000e-04, It/sec 0.904, Tokens/sec 437.810, Trained Tokens 3276294, Peak mem 14.672 GB\n",
      "Iter 5120: Val loss 2.489, Val took 14.630s\n",
      "Iter 5120: Train loss 0.454, Learning Rate 4.000e-04, It/sec 14.019, Tokens/sec 7146.992, Trained Tokens 3281392, Peak mem 14.672 GB\n",
      "Saved final adapter weights to ../trained_models/adapters_40_8_4e4/adapters.safetensors.\n",
      "\n",
      " Starting Evaluation\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [07:04<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on 205 test samples:\n",
      "Accuracy: 0.000\n",
      "F1 Score: 0.000\n",
      "Perplexity: 7.966\n",
      "\n",
      "ROUGE Scores:\n",
      "rouge1: 0.180\n",
      "rouge2: 0.049\n",
      "rougeL: 0.134\n",
      "🏃 View run MLX-40_8_4e4 at: http://127.0.0.1:5000/#/experiments/880645134898555871/runs/16366a516e1b44579bcb6c7f9e999525\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/880645134898555871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Checl if the output folder exists, if not, create it:\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Output folder {output_path} created\")\n",
    "else:\n",
    "    print(f\"Output folder {output_path} already exists\")\n",
    "\n",
    "\n",
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "if optimizer == \"adam\":\n",
    "    opt = optim.Adam(learning_rate=learning_rate_value)\n",
    "else:\n",
    "    opt = optim.AdamW(learning_rate=learning_rate_value, weight_decay=weight_decay_value)\n",
    "\n",
    "# Make a class to record the training stats:\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "        try:\n",
    "            mlflow.log_metric(\"train_loss\", info[\"train_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log train metric to MLflow: {e}\")\n",
    "            \n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        self.val_accuracies.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        # log validation loss\n",
    "        try:\n",
    "            mlflow.log_metric(\"val_loss\", info[\"val_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log validation metric to MLflow: {e}\")\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "with mlflow.start_run(run_name=corrida_name):\n",
    "    mlflow.log_params({\n",
    "        \"num_train_epoch\": lora_config[\"lora_parameters\"][\"epochs\"],\n",
    "        \"max_steps\": training_args.iters,\n",
    "        \"lora_r\": lora_config[\"lora_parameters\"][\"rank\"],\n",
    "        \"lora_dropout\":lora_config[\"lora_parameters\"][\"dropout\"],\n",
    "        \"lora_layers\":lora_config[\"lora_layers\"],\n",
    "        \"lora_layeres_scale\":lora_config[\"lora_parameters\"][\"scale\"],\n",
    "        \"batch_size\":training_args.batch_size,\n",
    "        \"optimizer\":optimizer,\n",
    "        \"learning_rate\":learning_rate_value,\n",
    "        # \"weight_decay\": weight_decay_value,\n",
    "        \"scheduler\":lr_scheduler\n",
    "    })\n",
    "\n",
    "    # Train model:\n",
    "    train(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        optimizer=opt,\n",
    "        train_dataset=train_set,\n",
    "        val_dataset=valid_set,\n",
    "        training_callback=metrics,\n",
    "    )\n",
    "\n",
    "    print(\"\\n Starting Evaluation\")\n",
    "    # Evaluate model and log metrics in the same run\n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, f1, perplexity, rouge_scores = evaluate_model(model, tokenizer, test_set[:num_test])\n",
    "\n",
    "    # Log final metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": metrics.train_losses[-1][1],\n",
    "        \"final_val_loss\": metrics.val_losses[-1][1],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"rouge1\": rouge_scores['rouge1'],\n",
    "        \"rouge2\": rouge_scores['rouge2'],\n",
    "        \"rougeL\": rouge_scores['rougeL']\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nResults on {num_test} test samples:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\") \n",
    "    print(f\"Perplexity: {perplexity:.3f}\")\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for key, score in rouge_scores.items():\n",
    "        print(f\"{key}: {score:.3f}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved every 100 iterations along with the final adapters in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac329358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000100_adapters.safetensors 0001200_adapters.safetensors\n",
      "0000200_adapters.safetensors 0001300_adapters.safetensors\n",
      "0000300_adapters.safetensors 0001400_adapters.safetensors\n",
      "0000400_adapters.safetensors 0001500_adapters.safetensors\n",
      "0000500_adapters.safetensors 0001600_adapters.safetensors\n",
      "0000600_adapters.safetensors 0001700_adapters.safetensors\n",
      "0000700_adapters.safetensors 0001800_adapters.safetensors\n",
      "0000800_adapters.safetensors 0001900_adapters.safetensors\n",
      "0000900_adapters.safetensors 0002000_adapters.safetensors\n",
      "0001000_adapters.safetensors adapters.safetensors\n",
      "0001100_adapters.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls ../trained_models/adapters2k/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training and validation losses to see how well the adapters fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3b784a720>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABvsUlEQVR4nO3dd3gU1foH8O9MQgIkmwKEhBvKpagIRLqKggFDBMsVkGa7RryiiPqjWCCIYoOIIuBF0CsoxoaFFmxUQ0QJKARCUEBpAjEN0kP6nN8fYZfdzZbZzW625Pt5nOdJZmZnzw4x8+ac97xHAiBARERE5CVkVzeAiIiIyJEY3BAREZFXYXBDREREXoXBDREREXkVBjdERETkVRjcEBERkVdhcENERERexdfVDXCFf/zjHygpKXF1M4iIiMgGGo0Gf//9t9Xzmlxw849//AOZmZmubgYRERHZITIy0mqA0+SCG22PTWRkJHtviIiIPIRGo0FmZqaqZ3eTC260SkpKGNwQERF5ISYUExERkVdhcENERERehcENEREReZUmm3NDRETkSL6+vmjXrh1kmf0G9lAUBVlZWaipqWnwtRjcEBERNVDbtm3x6quvonnz5q5uikerqKjAc889h7y8vAZdh8ENERFRA0iShIcffhilpaVYtGgRKisrXd0kj+Tv748pU6Zg8uTJSEhIgBDC7msxuCEiImqAkJAQdO/eHStWrMAff/zh6uZ4tC+//BJTp05FcHAwCgsL7b4OBwaJiIgaQKPRAAByc3Nd3BLPp72HQUFBDboOgxsiIqIGkCQJAFBbW+vilng+7T3U3lN7cVjKQWRZxpAhPdCuXStkZeVj167foSiKq5tFRETU5Li052bKlClIT09HUVERioqKsHv3bowcOdLs+XFxcRBCGGzl5eWN2GLTxowZhFOnVyF5ZwI+W/MMkncm4NTpVRgzZpCrm0ZERNTkuDS4OXfuHGbPno3+/ftjwIAB+OGHH5CUlIQePXqYfU1RUREiIiJ0W6dOnRqxxfWNGTMIX62NR2RkG4P9kZGt8dXaeAY4RESkiiTL6DqgL/reGouuA/pC8sB6OadOncK0adNc3QzXDkt98803Bt/PnTsXjz32GK6//nr8/vvvJl8jhEBOTk5jNM8qWZax9K3Jl76W6h1TFAVLlk5GUtJeDlEREZFZUTHRGD17BkIiwnX7CrNzsPG1JcjYkeLw97M2zfrFF1/ESy+9ZPN1Bw4ciLKyMnub5TBuExbKsoyJEyciICAAqampZs8LDAzE6dOncebMGWzcuNFiLw8A+Pn5QaPRGGyOMmRID3ToEFYvsNGSZRkdO4ZhyBDLbSQioqYrKiYacYsTENw2zGB/cNswxC1OQFRMtMPfU38EZNq0afVGRRYtWmRwvo+Pj6rrnj9/3i3SRVwe3PTq1QslJSWorKzEu+++izFjxuDIkSMmzz127BgeeughjBo1Cvfffz9kWcbu3bsRGRlp9vrx8fEoLi7WbZmZmQ5re7t2rRx6HhEReQe/Fs1Vbf4BLTE6fiYA1BuGqvteYPTsGfAPaKnqemrl5OTotqKiIt2oSE5ODrp3747S0lKMHDkS+/btQ2VlJQYPHowuXbpg48aNyM7ORklJCX755RfExMQYXNd4WEoIgf/85z9Yv349ysrK8Mcff+Bf//qX/TdWJZfPljp27Bj69OmD4OBgjBs3DomJiYiOjjYZ4OzZswd79uzRfb97924cOXIEjz76KF544QWT109ISMDixYt132s0GocFOFlZ+Q49j4iIPJ9fi+ZI+CXZIdeSZBkhEeFYsGeHqvPjrx2GqvIKh7z3a6+9hqeffhonT55EQUEBOnTogO+++w7PPfccKisr8cADD+Drr7/GVVddhbNnz5q9zrx58/Dss8/imWeewZNPPolPP/0UnTp1QkFBgUPaaYrLe26qq6tx4sQJpKWlYc6cOUhPT1edjFRTU4MDBw6gW7duZs+pqqpCSUmJweYou3b9jrNn88zm0yiKgjNn8rBrl+n8ISIiInf1wgsvYPv27brg5tChQ3jvvffw22+/4fjx43jhhRdw4sQJ3HnnnRav8+GHH+Lzzz/HiRMnMGfOHGg0Glx77bVObbvLe26MybIMf39/1edGRUXhu+++c3KrTFMUBdOnrcRXa+OhKIrBSrB1AY+EGdNXMpmYiKgJqSqvQPy1w1Sd27lfbzzy7lKr5703ZTpOpaWrem9H2bdvn8H3AQEBePHFF3H77bejXbt28PX1RYsWLdCxY0eL1zl06JDu64sXL6KoqAht27Z1WDtNcWlws2DBAnz//fc4c+YMNBoN7r33XgwdOhQjRowAACQmJiIzMxNz5swBADz//PPYs2cPjh8/jpCQEDzzzDPo1KkTVq1a5bLPsGFDKsaPS8DStyajQ4fLyWDnzl3AjOkrsWGD+eRoIiLyTmqDjD9Sf0Vhdg6C24aZnPotFAWFObn4I/VXiEb+Q9l41tOiRYsQGxuLp59+GsePH0d5eTnWrl0LPz8/i9eprq42+F4IYdAZ4AwuDW7atm2Ljz76CO3atUNRUREOHTqEESNGYPv27QCAjh07GvR6hIaGYuXKlYiIiEBBQQH279+PG264wWwCcmPZsCEVSUl7cfLUKnTsGIZp//celi//lj02RERkkVAUbHxtCeIWJ0AoikGAIy6NACQtXNrogY0pN954Iz788ENs3LgRQF1Pzj//+U+XtskclwY3Dz/8sMXjw4YZduvNnDkTM2fOdGaT7KYoCv7++wI6dgzDX3/lMrAhIiJVMnakIHFmfP06Nzm5SFq41Cl1buzx559/4q677sLXX38NIQReeeUVp/fA2Mvtcm48WUFBXRdeaGigi1tCRESeJGNHCg4n70KXfr0RFNYGxXnncTIt3S16bLRmzpyJDz74ALt378b58+excOHCBq/e7SwMbhyooKAUANCqFYMbIiKyjVAUnNh3oNHfNzExEYmJibrvU1JSTK7K/ddff9Wra7NixQqD7zt37mzwvanrhIaGNqS5qrhnf5KHKsivm2bOnhsiIiLXYXDjQNqeGwY3RERErsPgxoG0wU0IgxsiIiKXYXDjQJdzbhy3OCcRERHZhsGNA+Xna4elAlzcEiIioqaLwY0DMeeGiIjI9RjcOBCDGyIiItdjcONADG6IiIhcj8GNA2mDm2bNfBEY2MLFrSEiInKe5ORkLFmyRPf9qVOnMG3aNIuvEUJg1KhRzm4aKxQ70sWLlaisrIa/fzOEhgaitLTc1U0iIiIPIcsyhgzpgXbtWiErKx+7dv3utHUKN23ahGbNmuHWW2+td2zw4MHYtWsXrrnmGmRkZKi+5sCBA+utJO4qDG4crKCgFBERoQgNDcDZs3mubg4REXmAMWMGYelbk9GhQ5hu39mzeZg+bSU2bEh1+Pu9//77WLduHSIjI5GZmWlwbNKkSfj1119tCmwA4Pz5845sYoNwWMrBmHdDRES2GDNmEL5aG4/IyDYG+yMjW+OrtfEYM2aQw9/zm2++QV5eHh588EGD/QEBARg/fjw2btyIzz77DOfOnUNZWRkOHTqEu+++2+I1jYelunXrhpSUFJSXl+O3337D8OHDHf45zGFw40CSLKO8WgAAelx7DSQ3XQqeiIicq2VLf1VbYGALvPXfRwAAsmy4yKQsywAElr71CAIDW6i6nlq1tbX46KOP6gU348ePh4+PDz755BPs378ft99+O3r16oX33nsPH3/8MQYOHKjq+pIkYf369aiqqsJ1112HKVOmYOHCharb11AclnKQqJhojJ49AyGdmwOoxqhpk9HmlgnY+NoSZOxIcXXziIiokbRs6Y/SsrUOuZYsy+jQoQ2KS75UdX5gwDhcvFip6twPPvgAzz77LKKjo5GSUvecmjRpEtatW4czZ87gzTff1J379ttvY8SIEZgwYQJ+/fVXq9cePnw4unfvjhEjRiArKwsAMGfOHGzevFlV2xqKXQsOEBUTjbjFCQhuG4bK2rpb2lxWENw2DHGLExAVE+3iFhIRERk6duwYfv75Zzz00EMAgK5du+Kmm27C+++/D1mWMXfuXBw6dAgXLlxASUkJRowYgY4dO6q69tVXX42zZ8/qAhsASE11fO6QOey5aSBJljF69gwAApIso6K2rlvR36fue6EoGDVrOg4n74JwUtY7ERG5j4sXKxEYME7VuUOG9MT3m1+yet6tI+dh167fVL23Ld5//30sW7YMjz/+OCZNmoTjx48jJSUFs2bNwrRp0zB9+nRkZGSgrKwMS5cuhZ+fn03XdxX23DRQl369ERIRrsuvqbwU3DT3qcu9kWQZoe0i0KVfb5e1kYiIGtfFi5Wqtm3bDuLs2TyzU74VRcGZM3nYtu2gquvZ6ssvv4SiKLj33nvxwAMP4IMPPgAA3HjjjUhKSsKnn36KQ4cO4eTJk7jyyitVX/fIkSPo0KEDIiIidPuuv/56m9tnLwY3DRQUZpjdXqEdlvJRLJ5HRESkKAqmT1sJQKoX4NR9L2HG9JVOq3dTVlaGL774AgkJCWjXrh0+/PBDAMCff/6J2NhYDBo0CN27d8f//vc/hIeHq77u9u3b8ccffyAxMRHXXHMNBg8ejPnz5zvlM5jC4KaBivMM5/XrD0vpa9OxfaO1iYiIPMeGDakYPy4BmZkXDPafO3cB48clOKXOjb73338frVq1wpYtW3Q5Mq+++irS0tKwZcsW7Ny5E9nZ2di4caPqawohMGbMGLRo0QK//PILVq1aheeee85Jn6A+5tw00Mm0dBRm5yA4vC0kSao3LAXU/SNfN3YUtq9MZN4NERHVs2FDKpKS9jZahWJ9e/bsgSQZTkMvKCjAmDFjLL5u2LBhBt937tzZ4Ps///wTN910k8E+4/dxFvbcNJBQFOxZm6T7B9MOS/nrDUtJkoTQduHMuyEiIrMURUFKymF8/vmPSEk53CiBjbdicOMA58+c031tqudGi3k3REREzsfgxgH0824Mc26E2fOIiIjIORjcOIA270Yoim5YSpYAP7kuuBGKgoKsbJxMS3dlM4mIiJoEBjcOIBQFG19bAkBCjQLUXBombe4jLiUQS0hauJTJxEREXkiIuj9kfXx8XNwSz6e9h9p7ai8GNw6SsSMFiTPjDXpv/H0ECnNykTgznutLERF5qZKSEgBA27ZtXdwSz6e9h8XFxQ26DqeCO1DGjhRUlJWhslZCYDMg98hhzB/9OHtsiIi8WGFhIY4ePYoJEyYgPz8flZW2VwomwN/fHxMmTMDRo0dRVFTUoGsxuHEwvxYtUFFbBgBoJqoZ2BAReTkhBFauXIn58+dj7ty5rm6OR6uoqEBCQkKDh6UY3DiQ7OsD32bNdMNSQYHNXdwiIiJqDHl5eZg6dSoiIiKYe2On2tpaZGdno6ampsHXYnDjQH7N64IZba2boEB/VzaHiIgaUU1NDc6dO2f9RHI6JhQ7kF+LFgAu17oJbOkZS8MTERF5EwY3DuTXQttzU3dbA1uyY4yIiKixMbhxoHo9N80Z3BARETU2BjcO5N+yJYDLwU1LfyaVERERNTYGNw4SFRONB5cmALg8LBXQwhdRMdGubBYREVGTw+DGAaJiohG3OAEBoSEALvfcNPcViFucwACHiIioETG4aSBJljF69gwAApJUF9RUKpeCm0srg4+aNR2SzFtNRETUGPjEbaAu/XojJCLcIHipqLm0tpQsIMkSQttFoEu/3q5qIhERUZPC4KaBgsLa1NtXcannRpLqAhxz5xEREZHjMbhpoOK88/X2KUJC9aUlpeqGpkyfR0RERI7n0uBmypQpSE9PR1FREYqKirB7926MHDnS4mvGjRuHI0eOoLy8HIcOHcKtt97aSK017WRaOgqzc+otkKldX6q5jwKlthYtLyUbExERkXO5NLg5d+4cZs+ejf79+2PAgAH44YcfkJSUhB49epg8f9CgQVizZg3ef/999O3bFxs3bsTGjRvRs2fPRm75ZUJRsPG1JQAkgwBHu76Uv4+AJMuIWzSfs6aIiIgagQSgYeuKO9iFCxfwzDPP4IMPPqh37PPPP0dAQAD+9a9/6falpqbi4MGDeOyxx1RdX6PRoLi4GEFBQSgpKXFYu68ZPhT/XvQq5EurwY7vXIj2ATX49owGfxT7QygKCnNyMX/k2Hq9PERERGSZLc9vt8m5kWUZEydOREBAAFJTU02eM2jQIGzfvt1g35YtWzBo0CCz1/Xz84NGozHYnKGssEgX2ACXh6X8feoCGUmWOWuKiIioEbg8uOnVqxdKSkpQWVmJd999F2PGjMGRI0dMnhsREYGcnByDfTk5OYiIiDB7/fj4eBQXF+u2zMxMh7Zfy3g2lHZYSptQbO48IiIiciyXBzfHjh1Dnz59cN111+Gdd95BYmIirr76aoddPyEhAUFBQbotMjLSYdfWZzwbqsJMcMNZU0RERM7l8mWrq6urceLECQBAWloaBg4ciGnTpmHKlCn1zs3OzkZ4eLjBvvDwcGRnZ5u9flVVFaqqqhzbaBO0s6aC24ZBkmXd+lLaYSltzs3JtHSnt4WIiKgpc3nPjTFZluHv72/yWGpqKmJiYgz2xcbGms3RaUzGs6b0e27qEoglJC1cymRiIiKiRiBctS1YsEAMGTJEdOrUSfTq1UssWLBA1NbWiuHDhwsAIjExUSxYsEB3/qBBg0RVVZWYOXOmuOqqq8S8efNEZWWl6Nmzp+r31Gg0QgghNBqNUz5TVEy0eH7bRvHNmWShiK/F6ZLNYu7WDSIqJtpl95kbN27cuHHz9M3G57frGrpq1Spx6tQpUVFRIXJycsS2bdt0gQ0AkZycLFavXm3wmnHjxomjR4+KiooKkZGRIW699VZn3hy7NkmWxYJP36gLbnK/EpIsu/yHghs3bty4cfPkzWOCGw+4OXZvj730uFDE16KwbL2Iju4lZAY43Lhx48aNm92bLc9vt8u58QZjxgzCS0/eBAAIatkMyTsTcOr0KowZY74eDxERETkGgxsHGzNmEL5aG4/WIS0M9kdGtsZXa+MZ4BARETkZgxsHkmUZS9+aDACQJKneMUBgydLJl74mIiIiZ+BT1oGGDOmBDh3CIMuSyeOyLKNjxzAMGWJ6YVAiIiJqOAY3DtSuXSuHnkdERES2Y3DjQFlZ+Q49j4iIiGzH4MaBdu36Hdm5JRDC9HEhgKzcYuza9XvjNoyIiKgJYXDjQALAzqyWdV8bBTja73dmBcBM7ENEREQOwODGgbr0640sn3B8c1aD0mrDW1tSLeObsxpk+4SjS7/eLmohERGR93P5quDeJCisDQDgeLE/ThT74crgStzWoRQ1CvDBHyEQl2JJ7XlERETkeOy5caDivPO6rwUk/FnkD0UAvjIQ4Ht5MKrk/AVXNI+IiKhJYHDjQCfT0lGYnQOhKAAABRKKqupucYh/re68BxbPR1RMtEvaSERE5O0Y3DiQUBQkLVwKSBLEpQziwiofAECIn6I7r2VwMOIWJzDAISIicgIGNw5WVlgESZJ0yy9og5tQv8s9N9pjo2ZNh8SlGIiIiByKT1YHM04WLqysC26C9YIbAJBkCaHtIjhzioiIyMEY3DiYflIxoNdz419r6nTOnCIiInIwBjcOZpxUXKDLuakFTJTvMw6GiIiIqGEY3DiYUBRsfG0JgLqk4uIqGbWXpoMH+ioG5xVkZeNkWrrrGktEROSFGNw4QcaOFCTOjMfFwiIISCg2mg5e16sjIWnhUl0PDxERETkGgxsnydiRgnlDb8epA+l6M6bqApnCnFwkzoxHxo4UVzaRiIjIKzG4caJew4YgolsXvVo3tSgrLMKm199iYENEROQkDG6cJComGnGLE9A8MFA3HTzEvxYtgzR44M0FLOBHRETkJAxunECSZYyePQOAgCRJBjOm6or2CRbwIyIichI+XZ2gS7/eCIkI1wUvhUbTwSVZZgE/IiIiJ2Fw4wTGhfkMpoM3U8yeR0RERA3H4MYJjAvzGUwH11uGgQX8iIiIHI/BjRMYVykGYDAdnAX8iIiInIfBjRMYVCm+FODoZkxdSir2a94cvYYNcWEriYiIvBODGyfRVSkuKgagl1R8qUpxy+AgxC1O4JRwIiIiB2Nw40SHk3ehurISQgijBTTBKeFEREROwqeqE+mmhEuSXs5NLa4KrkD7gCrIssQp4URERA7m6+oGeDP9qd5hzashBOAjA7d1KAUAlFTJ2JkdwCnhREREDsSeGyfSTvXuFlSJOy4FNPoCmym4o0MJru8V1thNIyIi8loMbpzoZFo6irJzMDSiLrCRJMPjdd8LzHpiOGTm3RARETkEh6WcSCgKzn6/FprhE82eI0kSOnYMw5AhPZCScrgRW0dE5B1kWcaQIT3Qrl0rZGXlY9eu36Ho1RnzhLa402fwBgxunKz87zOqzmvXrpWTW0JE5H3GjBmEpW9NRocOl4f3z57Nw/RpK7FhQ6pHtMWdPoO34FiIk2Vl5Tv0PCIiqjNmzCB8tTYekZGGkzIiI1vjq7XxGDNmkNu0Ze7cibj77psQHd3LIA3BnT6DObIsIzq6l8n2uysJgHB1IxqTRqNBcXExgoKCUFJS4vT3k2UZp06vQmRka5M/EIqi4Ny5C+jS+WF2QRIRqXT5d2sbyLJU73hj/m611hYhBCS9pEttr0xS0l6rnyE/vxQTJyxESsphlzwj3KlXyZbnt/uHXx5OURRMn7YSgAQhjI8JABJmTF/JwIaIyAZDhvRAhw5hJoMCoC7g0OYzurotktFsEm2vzJw5461+hjZtgrDjh/k4dXpVo/fi2NKr5G69OwxuGsGGDamYePfrKK02vN2lpeUYPy6BY6pERDaQZRk336yu+Km5fEZ7H8amXmdrzqR8qUL908/cpfo1aoap1HwmtZ9blmUsfWvypa+lescAgSVLJ0OWZYwZMwinTq9C8s4EfLbmGSTvTHBJMKaPCcWNZN1XP+H6ORp0DBbI2fwlnnhsJCoqqvDtt7+6umlERA3WWLN9TA2TWGIqn9HcUMvMGatw/nyx2c9g7nXbth60+XPIsoygoJY2na8oCpYsnYykpL317q2a4SNbhpi0vVGW2tOxYxjmzBmPF1+6r95xbTDmqj/gmXPTiF7dvQ0tNIF4fdTd2PfjS4iMbI2XX16Do0fOceofEXmsxsrL0A6TAPV7E4yZy7kxdw1FUSBJUr3cmJXvbcHx41no1q2d7iGu/zrjfJrGMGxoPFJSDusCyjvvvA7TZ4yCEPU/EyBh/LgEADD7ubXn6P9b3X33TfhszTNW23LhQjFCQzWNkvdky/PbpT03s2fPxl133YXu3bujvLwcu3fvxqxZs/DHH3+YfU1cXBw+/PBDg30VFRVo0aKFk1vbcFXl5WihCYSvnz9+/ul3TJg4BC+8cI/uOKf+EZGzObqHRT9Y0GfvX+7G7fv556O48cbu+Mc/WmPJ0ocvnWM5mBDCdD6jtaEWYZQY2b59G7z8yv0G1zUOZIy/tzfYseV1N9/cG63bBGHJkocNAkrjl+v39mivbepzm+oRUjuDt3XrILPH9POeGruOm0uDm+joaCxfvhy//vorfH19sWDBAmzduhU9evTAxYsXzb6uqKgIV111le574x9Id1VVXgEAuOO2fhg3fnC9H2ZXd+MRke08qfiao3tYrAULloZRtOfo3ztTD+yamlr4+vrY1K7Kymrcd+8im4darAUuaoIPe4MdWwKi51+4W/VzTxtgqDnniSdvx9vLvr20T8KFC8Vo1Upjsm22BGN33nldowc3bjUs1aZNG+Tl5eGmm27Crl27TJ4TFxeHpUuXIjQ0VNU1/fz84O/vr/teo9EgMzPTJcNSM79KRPvuV2Bi2EmEhwW5fPoiETWMO02TtcbScIypYQl95gK46OheSN6ZYPW9X3n5c/zwQ7pB4Gfq3mkf2PoPTXt6QmpqatE27D4UFpYZ7Fc71OIoH3ywFQ8+GAtAqE5YLikpR0BAc1W9U84YDjt/vgiAhDZtLvfImHovW95fCIFxYxv+R7vHTgUPDg4GAOTnW+4OCwwMxOnTp3HmzBls3LgRPXqYn+oXHx+P4uJi3ZaZmenQNtuiqrwCkQHVaBce7BbTF4nIfq4ovtaQGT5qZ74Yv8/cuRNx6vT7JmfCqJ0l9PwLd+teN3fuRLz55n+wdl39ewfY11ui7+TJbPj6+uCOO66t9znbhgfbdK2G2r4tHePHJSAz84Lq17zx+joAsPrHrbPyfFq3DkLr1hqL59gaWAlh+PPVGNym50aSJGzatAkhISEYMmSI2fOuv/56XHHFFTh06BCCg4Px9NNP46abbkLPnj1NBi7u1HPzyP+W4s5b++C2Dtbf99573sDnn//YCK0iIlu5ooCcuV4ibcKrpSExtT0sw4bGo1UrjckeFf2Hmba358V5nxrkpFjjzORb7T3/+KMf8NzciVi3brcukdbWGVaOop/4Gx3dE19+NVtV8u2oUdc5rL2OuueKoqCkpBzBwQF2X0N7P+zlkT03y5cvR69evXD33XdbPG/Pnj34+OOPkZ6ejh9//BF33XUX8vLy8Oijj5o8v6qqCiUlJQabq1SVV6CsRt0PGZdjIHJfjV1AzlwvkTbh1VptEbU9LHfeeZ3J9zF+OGp7ex6ePAJnz55XHcA5M7DRJhCvX1839DFyZD80b+5n9t4Z56w4MndTURScOZOHXbt+132fnJyBRya/rdde0+1XFAUbNqSi8z8fxisvf97gtjjqnsuy3KDABmjcNRTdIrhZtmwZ7rjjDgwbNszmYaOamhocOHAA3bp1c1LrHCMqJhpX3XAtMsuaoaRKrletWMv4fwoiqs+V1VAdUUDO1vczN6RkrvKtcYCj9o+lh/5zCyTJ+mwkbbs6dgzDyvc2N/pUaGPnzl3Q5QwdOHACZ87kISCgOV559T688+7US+21PtxVVzXeMmtBkXGgom/DhlSTw1T67de/zg8/pFttjydpzD/aXV7Eb9myZRgzZgyGDh2K06dP2/x6WZYRFRWF7777zvGNc5ComGjELU4AJEBAws7sANzRoQRCGE7dMzd9kYguc2YSr7WZT/YWkGvIjCprM3yM229qhtKuXb/j7Nk8q2sfBQerLyqndfx4Fn7//Sx69uxo82vtpSgK8vKKMXPmKvydeaFeonJoaCAA4Kmn1FUAnj79PZw7d6HeTC1zw3HzXvhEV/tm8iMj0aHD5V6hc+cuYMZ08z+LGzakIilpr6qfh8v/bqbXJjTXvs3f78Nttw9U9dltlZtbiDZtglS1R79d585daNQ/2l0a3Cxfvhz33nsvRo0ahZKSEoSHhwOom+pdUVE3bToxMRGZmZmYM2cOAOD555/Hnj17cPz4cYSEhOCZZ55Bp06dsGrVKpd9DkskWcbo2TMACEhS3Q/D8WJ/fHMWGBpRBo3f5R9oRRG479433G6WBZG7cHRNFeNrW8pp0S/iZo3+L/OG5MoAtvf+GNcW0QZW69fvxv/93502XUuN4cN7o2fPjqiqqpt+fc01nfH8C5bTCywxfkCae4BPfWxFvX9r7c+HrR1JuTlFWL9uNzZu2KMLOtQGLgsWfGVz4KooiqrcE+3ahF+tjYeiKAYBhbbooD5t+/LzS1QFN4oiVPXSad/v3LkLmDlzFb78crbZ9hhf01JPljO5NKHY3Bjngw8+iMTERABAcnIyTp8+jUmTJgEAFi9ejLvuugsREREoKCjA/v37MXfuXBw8eFDVezZ2heKuA/pi6uoVJo9JEIgMqEagr4LrgvLQKrgFnnp+LZYu+BiCPTfkBcwVZLO1B0M/IdNc3Q1rKyib6j0BYLHCq7UHrTlCCMx74VOEhgaquq6l0v9qk4GN3XvPG6isrLaaHOyoonNlZRV44N+L9Va6Nt3bYImpKsHGdW7OnMkz2TNiLcnbEnOJru5Sw8hUgHzmTB6emmn6Z+byvTD9b6AoCi5cKLlUgM9wmrqp6fjG5QLMtefzNT/inntvqrffUk+WLWx5frvNbKnG0tjBTd9bY3H/6y9bPe+6sIu4Ifwi8spl7Dxeg80ffoXEpZ9xeIrclj1DOMYPKlPDSWoKu1ljHCyY+ivcVD0PR6mtVeDjo/7Bbq70//RpK5GUtBen/3ofkZGtbQpCLtdYMa5rU/crf93anzF+wmDV17NGe13DUv/q67sAph/YagNie4JAT6orZmugdbmXU9TrYdFfksH4/9G8vCJIkuH/F6YCFHPtcWZAyODGAnfqudHXM6QcsZFlBt2p2bkleHzKMg5TkVvQ/6VlKljQDyjU9oToTyk2l8Ng6i9Ja0wFC6Z6LGy9rjXr1+3GmLsGOaQnRHtv3lqahIHXXonBg3vYVV/EUi+X2sBObW+PtenMlj7jpk17G/QgtLVAn5rihZ7OXA+LfqBiqUfT1T1WxhjcWNDYwY0ky5i7ZT2Cw9ua/aXULagSd1yqfWOYYFz3y8Cb/+cjz2Cumqzxg8o4oFDD2oOzMdbqcZTc3EKEhQU77X2Liy4iSC/pt7E+45kzeVi1cguaNfNVlU9jvLCjuYDYkUMWtvbcOPK93Zm7DK05gscsnNkUCEXBnrVJGPnEIyaPSxAYGlFXItz4d5Qk1UWeltZmoabFWb+oLF3XXBKvqdon9tQKccR6Pmqu60za2Tvh4SFOew8hBAI1LfDC8+Zn6tiqbkXnQLN5GcY5THfffZOq62qToI0TZ+1JvlXL2swiSzOsvJna5GVvw+CmEZw/c87ssciAaoMZU8ZkSXLZqqrkXpw1BdrSdZOS9pqtsWKKq+uduIJ2eOOzT3dixszRTnsfSZIghIKHJ4/Q5YgsWPAVnnjydixdavqPJ2veWroJL750n8mZL4CERx95G8nJh3T71dYpMXeeMx+01mYWmZthRd7JLYr4ebvivPNmjwX4qvtL966xNzR6sTKq48qCcVrOWsfI2nXXrHnGYiXepsi4d0pbgG3Tpr1Of2/jyseKoiA3p8jm62iLhS5Y8JXqonLA5d4Rcz0eri5CakuRPPJuzLlpBLq8m7ZhkIwejO0DqjC+c7Hqa7nrisPeyh1WfXbGOkZqpla7O0VRLhXClJwWfFlLgLZl+q2l69qaO6O/9pytuSamEmltGe5UMwvH1b+fvCnPhC7zyLWlvJlQFKR9v61+Ug1gdTkG4/3OXHGYDLli1WdTbF3HyFpP05gxg3Dq9Crs+GEBWrcOatTAxt71e8yVuH9z0Qa97217HyGEyesqisCSxRvxwvOf4Nw50z0Ar776BT7//EeDejraYRFAMrl2kKXr1p2j/t7oD/tY600x19NkXOo/JeVwvc9kiif0jtjyecg7seemEWiXX5DMPJwszZYy9dwx/kudf6U4nitWfTZH7RRXc0Xb9Hua9JODHd3boab3wbjOjZrXmJqFpT/TxdxMLmvT0NXU87Dn/y17pt+qreVj7ufOWm+KqZ6mhuLvHWpsnApugcumgpsYktLXLaiy3nIM1phbD4VDVw2ntqvfXGVTV7Tlhec/0S0PYKr8+X/fSsK99w1FmzbBDglsTA2tGAcUxnVM9AuyXV7OoH6FVONAxlwlVi01wYJ2OrP+Qx5wTj0Pex782tdcrhFk27CPmqCKyJMxuLHAXYv4AZeXY+gWVIW+rStUvUZNqWyynS29JdrcB2dR24skSdKlfI/GGWYyDhbMBRSWHq7mHsjGQYg9AYcn9yzYG6h48mcmsobBjQXuuvyCVlVFBQ6t/C8+/d9Dqs5XUymUv9xs1xg9N7Y8iMaOvQFffjW73r+1trfkxXmf4uVX7rerHbZ65eXP8cMP6Sbb25AeCz6QDfG+EBliET83YmkauCm+fn748v1NeG3uv1TNujCXr2C8MjDZ5nJBMMu9JfZOeVUzC0v/4XbNNf+EJEn11iyqrVVwz92vo1kz5/+vrP3ML720xuJUYFt/3ppqkTFreF+I7MfgxslOpqWjMDvHas6NlizL6D0ixmwxKltpK4WSbbQzX9auq1+ZVzvsN2P6SrMPeUurYV/ONTGknYX11tIkFBSUmqw++9lnO/HB+9vQsWMYlr71CEJDA9GrVyeH1N5RFAWlpRXQaFqYzfew9JmJiNwFh6UagXa2FCBUBThFuXl4JXY0RptYeM5WjZHw6mju0h3fo0dHHP5teb2hv5ycQouVTtWshq1mlpC5hF1tLtXnnz+LCROH2PvxDFhbKZiJqUTkahyWcjMZO1KQODMe4154FoGtrPekBLcNQ5d+vbFhQyqSkvbaVV69ocMmruLIonkNDZImT74FQF1dj2X//RrzXrwXQ4dGYcP63RYDG1PrMOkPJQHqlikwtcaSEAqWLJ0MSZYwbvxgk0GSPYspnjt3wSB4SUra6xYBJhGRPdhz04j63nYL7l/4kqpzUz5ag01v/BeA/mwZ0zk49auo1v2Tav/Cd5eeEGvM1WCxZ/aXvUGS9l517BiGZW9PQVBQS9w6ch62bElDbGxfbNn6MvLyihD5jzjU1NTWe62lWU2OlJtbaHZKt9raMU1xEUEi8lzsuXFTxbl5qs8dOOp2fLNkBZSaGqsLwhk/yGpqavHqK5/D378Z5s6dWC93wx3r4MiybHaBRlmWoSiK6tXRzfWeaHNazJWdN7XKck1NLQICmwMAfvghHTk5BQgPD0VMTG9s2ZJmcI2bb+7doCFEW7RtG2L2mPbn4asvf8K48TeazZ/hIoJE5K3Yc9OIdAX9wtuqGjYoLynFF8+/iowdKQDM177QFjjr2q0dli+fAn9/P4PrqFkfRz/h1RW9O7ZOvTbXG2VLZeFRJnKarOW5LFv2KB5/4g5s3rwfHyX+YDIgchfmKhYzf4aIPBHr3FjgyuAGsL4Ugz5tgb7EGfG6AMfSENOYMYOwdl28qsDJ+CFunPDa2L07jlpiID+/RFWQ9MEHW/Hgg7EArC9DoB8QPTtrLBYseMDguD05LvbSDieFh4dYPddaIEhE5EkY3Fjg6uAGAGIfnYSRT6hLEBZCoDA7F/NH3gVh4YHU0HwPc707tq4cbO+D1FFLDLy1NAkzZo62/oFhe1CifW9JUpcQ7Oj3137GiRNew+IlD5vNwWIBRyLyRgxuLHCH4ObyelNtVfXgAMCKSVNxYt8Bs8fVBge2sDaEY9y7ozaJ11QABKDBSwzY0qthjwsXihEaqnFo8PjivE8RGhpoci0hU2ss6S8WaWmhRC69QUTehsGNBe4Q3AC2DU8BwCfPvoAD328ze1ztsI491PSWFBSUWjxH+7A1FwCtfG+L7iFvqjdDCIF5L6hbYqCo6CKCglo02lCRGsbDfsZ5L/asscSFEomoKWFwY4G7BDeAbcNTrui50VLbY2FtnauZM1fhyy9nAzAMgEwl8Rpf5/z5Irz33hbMmTNBdbsdlQujKAry80vRpk2Qza/VrsOkJmGb6zIREZnH4MYCdwpu1Myesj3nxvp6VK5SUFCK4OAAVUGSEMBbS5Pw/eb9WLPmWbRurbH5/RwR3OgPH9myMCXzXoiIHMuW57d7PgWbCKEo2PjaEkBcnhllcPzSvqSFSywGNsDltZAAqd7D1Pjapt7LZPtUnqdWaGigqnyVuoBEYOy4GxEU1BKtWgXa9X6SVP9eWGP8mc+du4Dx4xKwYMFXOHs2T9X1uA4TEZFrsefGDUTFRGP8vNkICA2pd6ysoBBfvfSabiq4NdZyNy4v2mg5edXcvsZmqRKvLRRFqKrmaynPxVwSr6XEXyIicgwOS1ngjsENAETFDkPcm/MBGE4zFpd6ARJnxqsOcKzlYThqYUdPUjfUZXpmkX4xQ2s5K/Yk/hIRUcMxuLHAHYOby1PDw0yuGi4UBYU5ufh87qvQtG6F4rzzOJmWbnWoyhLjAEib8Hrzzb3x/At3N+TjuKUXnv8Ekx8Z4ZCZRUziJSJqfAxuLHDH4KbrgL6YunqFTa8pzM7BxteWqO7NUauhU8odOUNJbc2auqGrIKsF7QAwKCEi8lBMKPYwQWG2r0sU3DYMcYsTEBUT7dC2ZGXlqzpPCFEvMFAUBUIAH7xvvh6PuWsZXweQ8MTj71hM4lUUBWfO5GHq1HdgKpHaOLFXURSkpBzG55//iJSUwwxsiIi8FIMbN1Ccd97m19QNXwmMmjXd5FCWvXbt+l1VQDF+/GvIzLxgcEw7s+iRR962eo2cnELcd98ivPD8Jzh3zvR11q3bbXYGmH7gsn7dbowfl2C2PUzsJSJqWjgs5Qas5dxYY63An63Ulva3toin2uUB7EmANpUvw1wYIiLvxZwbC9wxuAEuL8cACJsDHGtLM9jDEaX9Hbk8AAMXIqKmjcGNBe4a3AB1Ac7o2TMQEhFu0+sc3XOj5YiAgkEJERE5AoMbC9w5uAHqhqjuWfAC+t8+wuq5dUsz5GD+yLENmhZORETk7jhbyoMJRUFRTq7q85Pe+C8DGyIiIj0MbtxMVEw0rh87StW5kiThYkGhcxtERETkYRjcuBFtUnGLIPUrYPccNsSJLSIiIvI8DG7chCTLGD17BgDbKvz2u32EQ+vcEBEReTo+Fd1El369ERIRbnOgomndCl369XZSq4iIiDwPgxs3Yc8SDI54LRERkbdhcOMm7FmCwRGvJSIi8jYuDW5mz56NX375BcXFxcjJycGGDRtw5ZVXWn3duHHjcOTIEZSXl+PQoUO49dZbG6G1znUyLR2F2Tk2TesWioKCrGycTEt3YsuIiIg8i0uDm+joaCxfvhzXX389YmNj0axZM2zduhUtW7Y0+5pBgwZhzZo1eP/999G3b19s3LgRGzduRM+ePRux5Y4nFAUbX1uCurqKKkkSDny/jXVuiIiI9NhVobh9+/YQQiAzMxMAMHDgQNx77734/fffsXLlSrsb06ZNG+Tl5eGmm27Crl27TJ7z+eefIyAgAP/61790+1JTU3Hw4EE89thj9c738/ODv7+/7nuNRoPMzEy3rVD80LLX0XOouundQghAAIkz45GxI8XJLSMiInIdp1co/uyzzzBs2DAAQHh4OLZt24Zrr70W8+fPx/PPP2/PJQEAwcHBAID8/Hyz5wwaNAjbt2832LdlyxYMGjTI5Pnx8fEoLi7WbdqAzB1JsoxO16jvgZIkCZCAcfNmcTo4ERHRJXY9EXv16oVffvkFADBhwgQcPnwYN954I+677z48+OCDdjVEkiQsXboUP/30E3777Tez50VERCAnJ8dgX05ODiIiIkyen5CQgKCgIN0WGRlpV/saQ5d+vRHYqpVNr5EkCYGhoRg+Oc5JrSIiIvIsdgU3zZo1Q2VlJQBg+PDh2LRpEwDg6NGjaNeunV0NWb58OXr16oW7777brtebU1VVhZKSEoPNXTVkSveQ+yaw94aIiAh2Bje//fYbpkyZgsGDByM2NhabN28GAPzjH//AhQsXbL7esmXLcMcdd2DYsGFWh42ys7MRHh5usC88PBzZ2dk2v6+7aciU7oDQENzy2H/QdUBfBjlERNSk2fUUnDVrFh599FHs3LkTa9aswaFDhwAAd955p264Sq1ly5ZhzJgxuPnmm3H69Gmr56empiImJsZgX2xsLFJTU216X3dkz3RwfbdMeQhTV6/A3C3rERUT7eDWEREReQa7ZksBgCzLCAoKQmFhoW5fp06dcPHiReTl5am6xvLly3Hvvfdi1KhROHbsmG5/UVERKioqAACJiYnIzMzEnDlzANQlFKekpGD27Nn49ttvcffdd2POnDno16+fxVwdLVuyrV1Bu3gmIOzugakLjiTOoiIiIq/h9NlSzZs3h7+/vy6w6dixI6ZNm4arrrpKdWADAFOnTkVISAhSUlKQnZ2t2yZOnKg7p2PHjgZ5PKmpqbj33nvxyCOPID09HePGjcPo0aNVBTaeIGNHChJnxqMo1/A+KopSN/VbhbqgSGDUrOkcoiIioibHrp6bLVu2YP369fjf//6H4OBgHD16FNXV1WjTpg1mzpyJd9991wlNdQx377nRkmQZXfr1RlBYGxTnnUfL0BDELZp/6Zj6Qn9b3/0Ax/fuw8m0dBb7IyIij2XL89uu4CYvLw/R0dH4/fff8Z///AdPPvkk+vbti7Fjx+Lll19Gjx497G2703lKcGNKVEw0Rs+egZCIcOsnGynMzsHG15ZwmIqIiDyS04elWrZsqbvwLbfcgvXr10MIgT179qBTp072XJJUyNiRgldH3IWDW3bY/NrgtmGIW5zARGMiIvJ6dgU3x48fx+jRo9G+fXuMGDECW7duBQC0bdsWxcXFDm0gGRKKgqw/T9j8OubhEBFRU2HXU+7ll1/GokWLcPr0afzyyy/Ys2cPgLpenAMHDji0gVRfhZ3DaZIsI7RdBLr06+3gFhEREbkPX3tetG7dOt0spvT0dN3+HTt2YMOGDQ5rHJnWukP7Br2+IZWQiYiI3J1dwQ1Qt55TTk6Obq2mzMxM/Prrrw5rGJkWFRONIfdNaNA11FZCNp6xxRlXRETkCewKbiRJwty5c/HUU08hMDAQAFBSUoI333wT8+fPV12PhWwjyTLGz5tt9+uFoqC0oBBBbcPQdUBfi8GKqZlZnHFFRESewK7gZv78+fjPf/6D2bNn4+effwYADB48GC+++CKaN2+OuXPnOrSRVGf45DgEhIbY9VqhKIAkQdO6Fe5f+BIA88GKfpVkfdoZV6x8TERE7syuOjeZmZmYMmUKvv76a4P9d955J1asWIH27RuWE+JMnlrnRpJlvJTyHQJCglWdL4SAJEkG3wMw3GdimQZJljF3y3oEtw0zOatKKAoKc3Ixf+RYDlEREVGjcXqdm1atWuHo0aP19h89ehStWrWy55JkRZd+vVUHNoBhEFOaXwAYBTuA6enhXfr1RkhEuNnp4pxxRURE7s6u4CY9PR1PPPFEvf1PPPGEboVwcqyGzHAKbBWqOlhR+z6ccUVERO7KrpybZ599Ft9++y2GDx+O1NRUAHWrdXfo0AG33XabQxtIddTOcLKXNlhR+z7Obg8REZG97Oq5+fHHH3HllVdiw4YNCAkJQUhICNavX4+ePXvi3//+t6PbSABOpqWjMDvHaXku2mDF2vsIRUFBVjZOpqWbPE5ERORqdiUUm3PNNdcgLS0Nvr52l89xOk9NKAYuz2KyZVVwLeMEY91+EwnCutlSkvUEZCIiosbg9IRico2MHSn46Kk5UGpr7Xq9cW+MNlhJWrjU4FjGjhQkzoyHUAzj3sKcXAY2RETk9hjceJhD23fio2eehxBCdbFEIQSSP/wMZYVFBvstBSsZO1Jwsejy+SsmTcX8kWMZ2BARkdtjcOOBMrYlI3FGPIrzLhjstxTs9B0Zg42vLdF9n/LRGqvBiv71Tuw7wLo2RETkEWxKjlm3bp3F4yEhIQ1pC9kgY0cKTqcfxovJ3+j2mcqp0e4PbReBDr2u1u0rzj3PYIWIiLySTcFNUVGR1eMfffRRgxpE6lVXVNh0fmT3K3VfN2vZwtHNISIicgs2BTcPPfSQs9pBdqipqrbp/G7X9td9PfSBe5D9x3Hm0BARkddhzo0Hq602DG7M1qYxkXzsH9AScYsTEBUT7bT2ERERuQKDGw8mhEBNVdXlHZJkNsCpt66UVFfiSH9dKSIiIm/Ap5qH0x+a2vTGWyjKzat3jtlEYy6CSUREXojBjYfT77k5+tMebHz9LVSUldl0jeC2YY5uFhERkcu47zoJpEqNXt5Nt4H9cddzT9ctqmGDUbOmo7qysl5ysdoigURERO6EPTceTr/nZviUSQBMryFlSUBIMJOLiYjIazC48XD6OTfBYW3sSg6uew2Ti4mIyDvwSebhlJoah1zHZHIxh6WIiMgDMbjxcI7OiwkKa+PQ6xERETU2BjceTiiXg5vCnNwGrxdVnHfe5H5b83iIiIhchcGNhxO4HNxsXLgUgPlCfhavIwQKsnJwMi3d5HHZlxPriIjIMzC48XD6gUzGtmQkzozHxeISm68jSRL2rksyuJ7+kJcPgxsiIvIQDG48nVHKTcaOFHz01HN2Xer8mXNmj/k0Y3BDRESegcGNhzM1BHUyLR1Kba3NycbG+Tb6eTbsuSEiIk/BJ5aHE8ZdNwA694mC7OOj/hqKgtKCQpw6mGGwX/8azLkhIiJPwZ4bD6c/W0rL1unckixD07oVnvt+rUGVYv3eGh9f9cESERGRKzG48XCmhqXMTee2JrhtGOIWJ+Ca4UMBGPbWcFiKiIg8BYMbD2dqWOpkWjpK8/NtvpYky5BkCf9e9CpipzwEX79mumMMboiIyFMwuPFwpoalhKJg/zdb7L6m7OODkY9PNghorho8yO7rERERNSYGNx5Of1iq64C+uoUvf0ve5dD3ufPpJ7lqOBEReQQGNx4sKiYa/+wTpft+6uoVmLtlPaJionEyLR2V5eWOezMBrhpOREQewaVPqiFDhmDTpk3IzMyEEAKjRo2yeH50dDSEEPW28PDwRmqx+4iKiUbc4gT4+vkZ7NcmBfcaNgR/pR82+Vp7FtuUZKn+quFERERuyKXBTUBAANLT0/H444/b9Lorr7wSERERui03N9dJLXRPkixj9OwZAES9BS3relYERs2ajgtnM02/vgGLYHLVcCIicncunQKzefNmbN682ebX5ebmoqioyAkt8gxd+vVGSIT53ipJlhHaLgKBrUN1+75e/DZCI8Ix+N7xDXpve6eZExERNRaPTKA4ePAg/v77b2zduhU33HCDxXP9/Pyg0WgMNk+ntvdEf8jq7OEjOLQt2e73tLZqOBERkbvwqOAmKysLjz76KMaOHYuxY8fi7Nmz2LlzJ/r27Wv2NfHx8SguLtZtmZmmh2o8idrek8rSMt3XNVVVOJmWjsLsHJOF/6wxtWo4ERGRO/Ko4OaPP/7Ae++9h7S0NKSmpuI///kPdu/ejRkzZph9TUJCAoKCgnRbZGRkI7bYOawFKUJRUJCVjfysbN2+mqoqCEXBxteWAJDsClJ8fH3R99ZYgynnRERE7sbjn1C//PILunXrZvZ4VVUVSkpKDDZPZylIqfteQtLCpVCqa3T7ayqrAAAZO1KQODMeRbl5Nr9v7JSHcP/rLxtMOSciInI3Hh/c9OnTB1lZWa5uRqMzF6QU5uQicWY8MnakQKmt1e2vqao2eO2rI+7CiklTUZCVo+r9jKePa6ecM8AhIiJ349LZUgEBAQa9Lp07d0bv3r2Rn5+Ps2fPYsGCBYiMjERcXBwAYNq0aTh16hR+++03NG/eHA8//DBuvvlm3HLLLa76CC6VsSMFh5N3oUu/3ggKa4PivPM4mZau682p1Q9uqqsMXisUBSf2HUBRTi5C21mvE2RqyrlQFIyaNR2Hk3cxF4eIiNyGS4ObAQMGYOfOnbrvlyxZAgD48MMPMWnSJLRr1w4dO3bUHffz88Obb76JyMhIXLx4EYcOHcLw4cMNrtHUaIMUc8e0tMNS+qJiohF59ZV2v7d2ynmXfr3NtoGIiKixuTS4SUlJsVhQbtKkSQbfv/HGG3jjjTec3SyvIfv46L7WH5YCLlc4hv31/HRY2I+IiNyJx+fckHn6wU11VaXua0sVju3Bwn5EROROXNpzQ84l603XVmou599Yq3CsllAUFObkmi3sJ8kyuvbvg27X9ocAcOLXNJzYd4D5OURE5FQMbryY7Otjcr+jhpEkWUbzwEBE3XwTDm3faXAsKiYa4+fNRkBoyOWdUx5CWUEhvnrpNWTsSHFIG4iIiIxxWMqLybLp4MaRw0gtNIF4YPEC3D5jqm6fNp+nZUhwvfNbhgQjbgmnkBMRkfMwuPFisu/ljjn9qsINWYbBnGGT7kdU7LDL+TyS6dXHtftGzZrBKsdEROQUfLp4qaiYaFw35g7d9/pVhQ0qHBsV57OHJEmQJAljn3saXQf0RUhEuMVEZUmSENouHF369W7wexMRERljcOOFtMNCfi1bGOzXryqcsSMFHz01xyHBjZamdSt0HdhP9fmcQk5ERM7A4MbLWJrmXTcMJDBq1nRIsoyywiKDGVUOeX8bzuUUciIicgYGN15GO83bXD6LflVhZ/ScnPg1rS6fx0KPkBACBVk5ZqeQExERNQSDGy+jNmDRrkXlaPe/8TIOfLcNEPUX2wQu70tauIT1boiIyCkY3HgZtQGLdpHNi0XFDn3/wFatMHTSfTicnIKLhUX1jpcVFiFxRjzr3BARkdMwuPEy1qZ5C0VBQVa2bvXwn9esc0o7et0cjbWvvoEyvQDn0PadeHHo7QxsiIjIqRjceBmDad5GAU7d9xKSFi7VHdv67vtWr2cr7dTwu+Y8ZZCwXFlWxqEoIiJyOgY3XihjRwoSZ8ajKDfPYH9hTi4SZxoOCSm1l9ecMs6R0QZD9tK0bgW/Fs113zcPDLT7WkRERGpxbSkvlbEjBYeTd+lmRWlzbCz1nNTW1MC3WTPd94U5udi7bhNGPvGI3e3w0bte84AAu69DRESkFoMbLyYUBSf2HbB4jv6U8aKcXHw5LwGa1q10wRAA3HjPOGhat2pwe/wDWzb4GkRERNZwWKoJi4qJxtwt63Xft24fiXvmP4+aqiqc2HcAQlEgFAU/fPCJXdfXH/IC6iok669xRURE5AwSAMfV3/cAGo0GxcXFCAoKQklJiaub4zLaJRoAYRBsaPNs9HNzWrePxJzv19r8HkLUr5IMAIXZOdizNgnnz5xTNVxGRERky/Obw1JNkMESDUa9KJIsQygKRs2ajsPJuyAUBZXlF+17HzOLZwaHtzXI4ynMzsHG15ZwijgRETkExweaIFuWaACAqvIK3bGqisoGv79x0KO/oCcREVFDMbhpgmxZoiEqJhqzktbo9vk194cQwqGriRsv6ElERNQQfJI0QWqXaGjTsT3iFicguG2YwX5tkT5HMu4tIiIishdzbpog7RINwW3DTPaUCEVBYU4urh83CqbycpzJOJDSJ8myTXV7iIioaWJw0wRpl2iIW5wAoSgmZ0s1tHifvUbNmo7qysp6ycVRMdEYPXsGQiLCdfuYiExERKZwWKqJsrZEw/kz51zSroDQkHrJxdpp68a9OkxEJiIiU9hz04RZWqKh64C+LmmTJEkQl5KLDyfvAgCbpq0TERExuGnizC3RoCYvRwgB2cfH4W2SJMkguVh/KKreuXqJyNaWmiAioqaBwQ2ZpCYv5+Onn0NZYRGCwtqgy4A+uGHCXQ5tg9op67aeS0RE3o05N2SWtbycQ9t34sS+Azjw/TZkHz/l8PfvOXSw6mnras8jIiLvx54bsshSXo6+msqGVy7WJ4RAn1tjcWhHiqpp69oVzImIiBjckFXm8nL0VVdVqb5edWUlfP38LBYC1B67a85TWDd/EeIWza+3EKd2eCxp4VKrycSskUNE1HQwuCGHqKlUH9yc/e0oOve9RtW5mtatcLGgEIkz43Hfay+hWXN/3bHSgkKsf/UNq3VuWCOHiKhpYc4NOUS1DcHNiV/TbOrpCQprg8PJu1BWWGSwX9O6FUbNmm6xzg1r5BARNT0MbsghalQEK9rFNpv5+wG16oeEesVE45WftiAkom29Y5aCFEmWLdbI4WKdRETeib/VySGsJRTrryI+9MH74Neyhcljpl7TZ0QMWmgCTZ5jKUjp0q83QiLCzQYvXKyTiMg7Mbghh7A2LGU8pGSNuYDHFHNBitraN6yRQ0TkXRjckEOYG5YqKyzC5uUrLfbsSJJkMpixNJvKlKCwNpBkGV0H9EXfW2MR2CpU1etYI4eIyLtwthQ5RJcB/UzubxmkwYjHHoYkWw5UtAGONqCxNbABgDYd22PulvUGs6KU2lpIsmzyekJRUFZUXHf80jpVRETk+SQA6vv/vYBGo0FxcTGCgoJQUlLi6uZ4hWuGD8W/F71qdp0p4+UbzDGuY6OWUAQqykrRPLAuL6deLRwTAZPxe3FqOBGRe7Pl+c1hKWqQqJhoPPDmAosLaKqdjWRXYCMEIAEtNBpIklTvGpIsA0LUbRZwajgRkfdgcEN2059qrYbaJOHKi+UOHSLSDjvpt8NkEMSp4UREXoG/xclu1qZa2+v0wUOAmSRjLaWmFuUlpYAdQ1nmzufUcCIi7+DS4GbIkCHYtGkTMjMzIYTAqFGjrL4mOjoa+/fvR0VFBf7880/ExcU1QkvJFLVTqIUQqvJptMFMp2t6mRxiAoC8v84CqFt6oYUm0Cm9LJwaTkTk2Vwa3AQEBCA9PR2PP/64qvP/+c9/4ttvv0VycjL69OmDpUuXYtWqVbjllluc3FIyRe0UanOBij6hXO6l8Q9oafa8zGN/AoDBGlPWCEWxqW4Op4YTEXk2l04F37x5MzZv3qz6/ClTpuDUqVN4+umnAQBHjx7F4MGDMWPGDGzdutVZzSQzTqalozA7B8Ftw8z2oCiKAllF70rlxYvwb9kCsBII9bnlZgCXlnBQoS7hWN2wlVAUFObk4mRauqrziYjIPXlUzs2gQYOwfft2g31btmzBoEGDzL7Gz88PGo3GYCPHEIqCja8tASDVSwAWigKhCGxdsUrVtfwDWto0xOTTrBkqysoMenzMUd9zJCFp4VLWuyEi8nAeFdxEREQgJyfHYF9OTg6Cg4PRvHlzk6+Jj49HcXGxbsvMzGyMpjYZGTtSkDgzHkW5eQb7C3NykTgzHttXJqIwO8diwGDLkJGWJEloHhAASbaceKw22fhiURESZ8azzg0RkRfw+grFCQkJWLx4se57jUbDAMfBMnak4HDyLnTp1xtBYW1QnHceJ9PSdQHNxteWIG5xgtmkYnvq2zjaR089h+O/prm6GURE5AAeFdxkZ2cjPDzcYF94eDiKiopQUVFh8jVVVVWoMrPuETmOUBSc2HfA5LGMHSnYsmIlRj7xiFPeu6HBkaIoaBES7KDWEBGRq3nUsFRqaipiYmIM9sXGxiI1NdVFLSK1zp855+ommCVJEuIWzWd1YiIiL+HyqeC9e/dG7951RdM6d+6M3r17o0OHDgCABQsWIDExUXf+u+++iy5dumDhwoW46qqr8Nhjj2HChAlYsmSJS9pP6rnz9Oq6nh9WJyYi8hYu/U0+YMAAHDx4EAcPHgQALFmyBAcPHsTLL78MAGjXrh06duyoO//06dO4/fbbERsbi/T0dDz11FN4+OGHOQ3cA2injZtLLNYW+nMVVicmIvIeXBWcGk1UTDTiFicAEIZrPSkKAKnuPwcnF2uDJjW1dgDgk2dfwIHvt1k9T5JlswnUDTmXiIhMs+X57VEJxeTZtNPGR8+egZCIy4nhhTm58GveHC2Dg1QV3FOzlIP2PMC2gEnN8FlUTHT9z5Cdg42vLak3ldyWc4mIyDHYc0ONzrgnQ5JlPPb+2w5/H1uCG6EoKC0oxCu3jME/r+lptpfFWu+Tfq0cW84lIiLLbHl+M7ghl+t7ayzuf/1lq+ep7bE5unsvut9wnV1tUWprIfv46L7X72WRZBlzt6w3u9yEdvmG+SPHAoDqczlERURknS3Pb04NIZdz9EyqgOAgu19rHIgEtw1D3OIERMVEo+uAvgiJCDc7o0o/KblLv96qzyUiIsdizg25nJoFOAH1uTPte3S3uy3G7yHJMoSiYNy8WZAkdX8LBIW1Uf1+tpxLRETqsOeGXM7SApx1x20bOdUPUBwx5CPJMgJDQ+sSnlUozjuvujfKnev/EBF5KgY35BbMLcAJAFtWrLT/wg6cWq6m50iprUXL0BDrdX0UBQVZ2TiZlu6w9hERUR0GN+Q2MnakYOPrb6E0v8Bg//XjRqGsoNCuXpjfknc5qnmqSLKMuEXz0WvYEF1vlDHtbKmkhUuZTExE5AQMbshtRMVEI27RfAQYLWIZ3DYMLYODAcn0sJUlnfv1RuXFi45spkWSVFeMcNSs6TicvAuJM+PrnVOYk8tp4ERETsSEYnILkixj9OwZMK4Joz0mFAVlhUWoqawyKIhnTUBIMGqqqwGon0reUJIkIbRdBLr271MvgFkxaarbVihmJWUi8hYMbsgtaKdOm6NN6n3nocchhEB03D3oET1YVbDi4+uaH/MHFi/Ark++MNh3Yt8Bl7TFGlZSJiJvwmEpcgtqp0Rr2rTGiX0H8MGTz2LLilWqXtMYvTWmtAwOwsgnHjHYFxUT7ZK2WKKtpBzcNsxgv36NHyIiT8LghtyCPVOnt7/34aUZSeqmijd2kGPq/dwtWLA2HAgIjJo13WL9ISIid8PfWOQW7Jk6fbk+jmPq2TQOgbHPP4u+t92CrgP6Qvb1RdcBfdH31lh0HdC30YMIVlImIm/EnBtyC9pAJW5xAoSimFxo0tTUaW19nHEvPIvAVq0audW2k2QZmtatcP/ClwBYXsuqMagdDmQlZSLyJOy5IbdhrpCftanTGTtS8FLMKJTmF+hWAvcUltayagyspExE3og9N+RWMnak4HDyLpunJCs1NVj78sK6nh+hNHh4pzGnjRt8f2nau7ZOjrXPbev0bePzTx3MsLiul3b1clZSJiJPwuCG3I5QFLumTGt7fu5+9Xk0DwxoUBskSbI5wHFUQKTNc+navw+O/5pm9jxz07f3rE3C+TPn6gU75s5P+24rhj14f732s5IyEXkqCYBn9eM3kEajQXFxMYKCglBSUuLq5pCDaac1S3LDgwy1wYqzennKCovw1YsJJofjtJ/TeJaTcVu0OTwATJ9/KYBJ/vAT3DhxLPwDWuqOFWRlI2nhUta5ISK3YMvzm8ENeQ1JljF3y3qzQyxaurwcUX/6sz2cFdwIIQCBevlGaj8ncDl4uVhUhJbBQRaHnn7f9TNunDAWAPDeo9Px5y/70blPFCsWE5FbsOX5zWEp8hrWqhzrS/7wM/S7dbhNSzmY46zcHEmSIC7VmdHPv7Hlc2pzeAJCQyyeE9ouAsFhbXX7NG1a47nv19pUsZjLNxCRu2BwQ15D7XRlSZJw9Mef8d3SFbjlsf/glikPObll5lnr9dGuUzX4nnEozS9Acd55BBlVErZGbe9UyyCN7uu7X50LGM08087kMjVzjcs3GGKgR+RaDG7Ia9gyXTkorA2EouD43n12BTcOSx5WeY26KsJ1SvPzG/y+Jtsi15+5Zfy9qZlc+vk/+iwFQ96MgR6R67HODXmNk2npqh/82kDIamVkM3VzXLVeFQAEhIZCCGFzTR9zy1Roqz/XVFXr9pn7fMYVi7l8gyGu00XkHprGbxxqEoSiYN0rb1h88AshDJZxuLyEg1QvwGmsWje2srVNQlFQWlBQ97XRfdGf7m3L9HntEGBTWr5BkmWLS2Uw0CNyH/y/jLzKoe07kbz6E5PHtLOPjOu2mKuM7I6BjZYkSaqnqUOSsPalhUicGY/Ki+UGx/WrP/u3bGnmKvVpe76ayvINUTHRmLtlPaauXoH7X38ZU1evwNwt6w16YppSoEfk7phzQ17n2yUrcObwEYyb+wwCW4Xq9hdm55it22JcGbltl3+6NNHYkSpKS+Hr74/i3Dwc2voDrh1zBwDgi3kL8OvGb3VreenPqDLXa6WdNn7qYAa6DuiL8C7/VNUGT16+QW1OUVMJ9Ig8AYMb8koZ25JxeEeKTTNW9Csjdx3Q1yuCG0mS0EKj0S3UWVVRoTtWmJUNoSi6BNiAkGCD15qrWHz6wCG8mPyNwfnWgiE1yze44wwja0NN+gnWXKeLyH0wuCGvZe8yDsDlRGOzay65aT6ONc38/XVf9xw6BJ2u6YURUydDTS3PsqIi+DbzQ9/bbjF5vCHLN7hyhpGloMpaTSH9oSarPzNcp8uh3DEYJvfB4IbIBG2icdziBN2wjbGK0jL4t2zhUQmi+sHH4HvHA9AGJbLZ8wAg6/hJRHTpXFfT3Mx1jZOVC3NysXfdJvj6+aHrgL5mHz6unEpuLaiyZajJ0s8M1+lyLE63J2u4/AKRBaZ+iZZcyMf6V+tmZZlar8kbaQMXtb1Vm99+D9ePG2Vw38oKi7Drky+wfWWi7gFvbSkJbW/H53NfhaZ1K5ScvwBIEjStWzX4r3Wz63NdCkQSZ8bjYlExpq5eYfVaKyZN1fUS1v3MzERIxOWKz1yny3HU/LvxPnsnri1lAYMbspWl7m9TwY+nDlk5klAEIJkOhsoKCvHVS68hY0cKug7oqyp4MEftX+vG/4anDmbgue/XWg2qFtw23up5ZUXF+PjpuTix74Du5yI4vC1e2J4EAMjP/BsJd0yEUlNj9+c09RlMBXbePlSjNhieP3KsV31uqsPgxgIGN+Ro+g+UNh3b1+uxaIosBXj6C4L6+vnh/tdftv99Lv21vmXFSoMeIX2mAtDS/HwEtmpl9forJk1Fy+Cgup4Co2DN0grsY+c+A02b1vWO2dujoGYYpikM1agNhvV70rydtwe0+hjcWMDghpxN+8smuG0YRs2ajoCQYK8ftrKVUARKCwrwyax5eGzVModcUzvsteP9j3Wrmbfp2F6XMG08hKHm3+STZ1/Age+3ISomGhNeikfLYPMzxLSBljYvydQx7ZCJJMvo2r8Pul3bHwLAiV/TDHp+9KkZhgHQJIZq+t4aqyoY1v67eTs1Aa03BT8MbixgcEONydyDSV95SSl8mvmimZ9/vfWdvF3lxXL4Nfd3aPCnKApk/Qd8A4YJV0yaipNp6ejSrzdGTJ2MrgP7WjzfUm6Sdsgk6Y3/Yvzzz9ZbqV1/uE5L7TCMJElNYqiGPTeXWQt6f/zkc5QXl9TrSfbk3jwGNxYwuKHGZi4pOe3bLfgteRdOpqWj17AhiFuc4JDgpqnn/Dji8+sHIqOfnebQYUZzAZB2f+KMy70sDc1J0mfpgW/PX/f6r3Fkorel9si+vli4byckWTYdQAoBoSiYNWBog3Oc3Jm1oFefuR5GT+zNs+X5zangRE5mXP3Y1C//w8m7UFFWhhaawAa/X1MObAA71t4yEQxJsowWGg3i3pwP2LhAqRqm2qidTj9q1gz8lvIzOveJwjWxwxz2nuamtduTq2PqNfpK8/Ox/5vLwbstic+W2nOxqBiyj4/ZzyhJEiQfH3TuE+XVPTddB/RVHXCb+tnWLz5pLgj19OEsBjdEjcBaQcEu/Xo7JLAhx/EPqFtry9H5UpaCL0mSENouHPN2bDJYOsQRTFVGtqfGkLnX6Ats1QrRD9yD6AfuqVcCwFLwAsBie3785HNVn9Wbl7iIionG3a8+36BrGBefNA5ieg0b4vHJ6RyWInIDahMlyXOpTWIG1A+tXc65kS8NUZgeqikrLMSLQ+vWFNMmuwe0CkXso5PQMjjIYo7QgtvG6xK0S85fwD0LXlA1HGKsrKAQezd8jWEP3l9/5tmloZKLRUV17TGTO1RaUAhNa3Wz3Bzdc2OtJ6Mxejp0gaWZMgu2SvloDXrfcrNhPaqCQrQMCQaE5eR0Zw5LmsOcGwsY3JA7UptbUXmxHP4tWzRCi8iRnJEHpX3YJH/4Ca4b8696Ccr67w0Ayas/Qb/bbrE5f6g0v8AhvUj6FazNBVNqAqbS/AKzMxCdlTxtbejOGdPwTdVmejH5a7QMDnbYz5Kp/C+LSfGXAuW1r7xhMRfNWb08DG4sYHBD7kjtrJgFt41Hl77XYODo2zHgzttc0FJyFyUX8nEm4zf0iB5c769sY0JRAEmyep7J17pZgnrKR2tw0/13m+39sZQoa0/virVZSckfflLXG+XAafhRscMwbu4zBkFlVXk5/Fo45g8bIYTu39Wef9u6WlXmf5aclbTM4MYCBjfkrmwpK2/LbAlj1h5W7vYwI9KnLaz4wJvzDZKLrS1xYa53Zc/aJJw/c87sUJO1Pzp0QUIDe5K0gVd03D3oET3Y5Gw6R/x/aetSKuauYe31zuhFs+X57RaVxaZOnYpTp06hvLwce/bswcCBA82eGxcXp4s6tVt5eXkjtpbIOTJ2pCBxZjyKcvMM9hfm5Nb7C0i7SCMg2fyLw9ovJau/tC79f0fUmIQQKMjKwcm09HqzfErz8/H53Fd1i7TKvr7oOqAv+t4ai64D+iIqdhjiFicguG2YwTWDw9ti5BOP4P7XX8bU1Sswd8t6RMVE645rV4U39weEJMuQfXwsHtcm7loSFRONuVvWY+rqFeg5dIjZ2XSOYG9vja1tUfvZncXls6UmTJiAxYsXY8qUKdi7dy+mT5+OLVu24KqrrkJeXp7J1xQVFeGqq67Sfc9ftOQt1Ewb1z83cWa8xSm5zmBqBXDyLrYkPzcWSZKwd10Seg0bgjHxT8GnWTPdsYDQUDz2/tu675XaWoNeHaW29tIwllzvmvqMZ4k5atbVNbHDdL07XQf0NahK3Ssmuq7kgJdy1cw1lw9L7dmzB7/++iuefPLJugZJEs6ePYtly5Zh4cKF9c6Pi4vD0qVLERpqX4Ibh6XI22i7s6+4bgBipzzk6uYQOc1vO3ehx02Dra7z1RD6wynDH3kQIx+f7JDrmlJRWga/li0MKmp7m81vv4dt/1vtkGt5zLBUs2bN0L9/f2zfvl23TwiB7du3Y9CgQWZfFxgYiNOnT+PMmTPYuHEjevToYfZcPz8/aDQag43Im2hr6Gx5530UZueYHaYSigKlttbqMBZ7Zchd9YiuH9gAji1cqR1OuS9hHkZMfdjiubr/p+z8f8Y/oKVXBzZCCFw3dpRLegFdelfbtGkDX19f5OTkGOzPyclBRESEydccO3YMDz30EEaNGoX7778fsixj9+7diIyMNHl+fHw8iouLdVtmZqbDPweRO7CUh6NNSt6Z+JnJ47rzLv2SbuwARygMqMg6R+SLqNXn1liLx4UQgCTV5dzY2SZvT9zXFqUcPjmu0d/b40LGPXv24OOPP0Z6ejp+/PFH3HXXXcjLy8Ojjz5q8vyEhAQEBQXpNnNBEJE3sJaU/O2SFSaP687LzkHy6k9QlJNr8X0cFfzUFWYrQFGu5fcjamzWAilvD0wcacTjkw0StRuDSxOKz58/j5qaGoSHGyZDhoeHIzs7W9U1ampqcODAAXTr1s3k8aqqKlRVVTW4rUSewlpSsv7x4LZhCAgNQWlBIYpz83TnfffWuxg+OQ4jHp8MCBhUvtXWTKm3IJ+NU0y1vUlrX1qIw8m7zL6fO8o8cgyRV19l/UTyOLbk7zDAUUnA6lpWjubS4Ka6uhr79+9HTEwMkpKSANT9sMTExODtt9+28uo6siwjKioK3333nTObSuRRrK1lpeb4tv+tRvbxk/Vrg+Tk4sD32+pVuy0rLIQEyaBSrqUHRWFOrkFdEnPv545++3E3AkJD7KozRO6NAYvjSbKkmxbeWAuaunwq+OLFi5GYmIh9+/bhl19+wfTp0xEQEIDVq+uyqxMTE5GZmYk5c+YAAJ5//nns2bMHx48fR0hICJ555hl06tQJq1atcuXHIPJKlnqBvnvr3Xr7Aej2tenYHtePG2UQqJRcyEfat+ZXi9Z/v2tih2HwveMd/pksBVxq/2o/8ct+/H3k2KWii87FoorkLRpzWrjLg5svv/wSYWFhePnllxEREYGDBw9i5MiRyL00Bt+xY0coer8AQ0NDsXLlSkRERKCgoAD79+/HDTfcgCNHjrjqIxB5NXO9POb26+/bvjLR5nL3+tdVG9zYMyRmqgr0zsTPMDTuHvNl5YVAWWERTuw7AKEo+OipOfj3olcNaqoQkWmmVqZ3FpfXuWlsrHND5Bl0pe/D21oNWhRFUT2TZvPb79XrUdIv3R8VO0xXVM1UTlHiDMNq0dbOt7fXRVtvZdOiZfj36y+7VQBlbRFMcg/aoN0dctjKCgoxb+jtDcq54dpSFjC4IfIc1tbb+vGTz1HwdzZGz55h9VpCESjMycH8kWMBwGKPkql1iAqycpC00PRKx6bOLy0oQEBISL0FBs2txGxpEUhzAZSrFGTl4NzvR9Dr5mib10BSamshybJbfA5PpgswLSxgWVpQAB/fZmihCWzElpnmiGJ+DG4sYHBD5FlMBxqXe1r63hqL+19/2ep1hBD1el0ssXUFaVPn9xo2xGTQY5x4bbxcgKlFIE3dB+PP5+yAQQiBLctXYvvKRAhFMbl6tcng7VKwtmXFSmSfPI24RfNRL2B149wiV7XN3PvWrcoNJH/4Sb3Efm1eW3lxCUZMnQzj+2zyWrAcNFs7x9LxuqHcQrw49I4Gz5RicGMBgxsiz2Mp0Og6oC+mrl5h9RqOLANvC1NtBwx7jk4dzEDnPlFWAyn9a5WcvwBIEjStW5lM3lZL+6BUM3SR8tEabHrjvxY/X8vQEIx+dprZYBQwH7DuXbcJPs2aIfbRSTZ/Dm+kKIrJCsb699Pcz5el1cy11K4hZm0Y0lwvknEPZEMxuLGAwQ2Rd9Hl5pj5Ra6/VlBj1dhwBf2HXJuO7TH43vEGPSrmlBYU4qdPv8TIJx6xeu6KSVNVTeVV0+tl7hy1PXHebtv/VmPrux+gc58os/WozFEb8JeXlKoaskr5aA1OpaVj/LzZBj2O2mt8+cJ8CCEs9rA6gi3Pb5fPliIiagjtshNxixPMzoJKWrjUqwMboP7sNe1MtZ7DhuDaMf+q9xCrKC3Dzg8/xfaViQCA68eNshogansFbG2LLefYMqPGFcNFjfWef+75FUpNjV11YdROuf7ps69U9ZL9lrwLJ/YdwOHkXejavw+6XdvfYGVz7f9bloqHNjYGN0Tk8bTLTpgqOOjIvxw9iTZ4OLHvAL5+822LDyUAbhMgnkxLR2F2jsVZckIIFGbnYtOi/2LCi/Fmex8aOmPNlMYIbEou5KsOJE1RGyAe37sPA0fdpjqoFYqC47+m4fivaSavpyaobSwcliIir2FrEjAZspa83ZjtiFucYHIFcOMp+ZIsY/jkOAyddB+aBwQYnFteWobmAS0tzijSLifi7KCltKAIP332JYbcNx4tg4Mt5q4kPvUcMrYl2/1etgzV9ho2xOKMREflyzgCc24sYHBDRGSeuwSIUTHRJnM8SgsKsfal1+o9cCVZNtk7ZWrGmr6CrGx8/cZ/cdfcZxAQGuLQIEd/lphuhpmuvEH9JG4hBJJXf4Jvl1jPl7HGWhkF/aDFXYJaaxjcWMDghojIM5gLWGwNtszNMtMP3m6fMRXDJt3foOCmqqICfs2b6743FyCYCiZKLuRj/atv4ND2nXa/vzFbghZ3CWotYXBjAYMbIiLSZ2kYzBbvPPQ4hBCqAoTGCiY8IWhRi7OliIiIVJBk+VKFawFJMl/zxVJlZW0Oy4n9B1UHDo2VfOtOSb6NyXr1HiIiIi/VpV9vhESEWy1mt/Wd9wGBesFLUyo34EkY3BARUZOltibM+TPnkDgzHkW5eQb7C3Ny3WpGEdXhsBQRETVZamvCFOed1xWy85YcFm/G4IaIiJosXdFAGwrZNcUcFk/DYSkiImqytMt3ABLzabwIgxsiImrStMt3MJ/Ge7DODREREbyrJow3Yp0bIiIiGzGfxntwWIqIiIi8CoMbIiIi8ioMboiIiMirMLghIiIir8LghoiIiLwKgxsiIiLyKgxuiIiIyKswuCEiIiKvwuCGiIiIvEqTrVCs0Whc3QQiIiJSyZbndpMLbrQ3JzMz08UtISIiIltpNBqra0s1uYUzAeAf//iHUxbN1Gg0yMzMRGRkJBfldALeX+fi/XUu3l/n4v11Lne5vxqNBn///bfV85pczw0AVTemIUpKSvg/lxPx/joX769z8f46F++vc7n6/qp9byYUExERkVdhcENERERehcGNA1VWVuLFF19EZWWlq5vilXh/nYv317l4f52L99e5PO3+NsmEYiIiIvJe7LkhIiIir8LghoiIiLwKgxsiIiLyKgxuiIiIyKswuHGQqVOn4tSpUygvL8eePXswcOBAVzfJLQ0ZMgSbNm1CZmYmhBAYNWpUvXNeeukl/P3337h48SK2bduGbt26GRwPDQ3FJ598gqKiIhQUFGDVqlUICAgwOCcqKgo//vgjysvLcebMGTzzzDNO/VzuYPbs2fjll19QXFyMnJwcbNiwAVdeeaXBOf7+/nj77bdx/vx5lJSUYO3atWjbtq3BOR06dMA333yDsrIy5OTk4PXXX4ePj4/BOdHR0di/fz8qKirw559/Ii4uzumfzx1MmTIF6enpKCoqQlFREXbv3o2RI0fqjvP+Os6sWbMghMCSJUt0+3h/G2bevHkQQhhsR44c0R33tvsruDVsmzBhgqioqBAPPviguPrqq8X//vc/kZ+fL8LCwlzeNnfbRo4cKV555RUxevRoIYQQo0aNMjj+7LPPioKCAnHnnXeKqKgosXHjRnHixAnh7++vO+e7774TBw4cENdee6248cYbxR9//CE+/fRT3XGNRiOysrLExx9/LHr06CEmTpwoysrKxOTJk13++Z25ff/99yIuLk706NFDXHPNNeKbb74Rp0+fFi1bttSds2LFCvHXX3+JYcOGiX79+ondu3eLn376SXdclmVx6NAhsXXrVtG7d28xcuRIkZubK+bPn68755///KcoLS0VixYtEt27dxePP/64qK6uFrfccovL74GztzvuuEPceuutolu3buKKK64Qr776qqisrBQ9evTg/XXgNmDAAHHy5Elx8OBBsWTJEv78OmibN2+eyMjIEOHh4bqtdevW3np/XX/DPX3bs2ePWLZsme57SZLEuXPnxKxZs1zeNnfeTAU3f//9t3jqqad03wcFBYny8nIxceJEAUB0795dCCFE//79deeMGDFC1NbWinbt2gkAYsqUKeLChQuiWbNmunMSEhLEkSNHXP6ZG3Nr06aNEEKIIUOG6O5lZWWlGDt2rO6cq666SgghxHXXXSeAuuCzpqZGtG3bVnfOo48+KgoLC3X387XXXhMZGRkG77VmzRrx/fffu/wzu2K7cOGCeOihh3h/HbQFBASIY8eOiZiYGJGcnKwLbnh/G77NmzdPHDhwwOQxb7u/HJZqoGbNmqF///7Yvn27bp8QAtu3b8egQYNc2DLP07lzZ7Rr187gXhYXF2Pv3r26ezlo0CAUFBRg//79unO2b98ORVFw3XXX6c758ccfUV1drTtny5Yt6N69O0JCQhrnw7iB4OBgAEB+fj4AoH///vDz8zO4v8eOHcNff/1lcH8zMjKQm5urO2fLli0IDg5Gz549defoX0N7TlP7eZdlGRMnTkRAQABSU1N5fx1k+fLl+Pbbb7Fjxw6D/by/jnHFFVcgMzMTJ06cwCeffIIOHToA8L772yQXznSkNm3awNfXFzk5OQb7c3Jy0L17dxe1yjNFREQAgMl7qT0WERFh8D8WANTW1iI/P9/gnFOnTtW7hvZYYWGhM5rvViRJwtKlS/HTTz/ht99+A1D32SsrK1FUVGRwrvH9NXX/tccsnRMcHIzmzZujoqLCKZ/JXfTq1Qupqalo3rw5SktLMWbMGBw5cgR9+vTh/W2giRMnol+/fiZzFvnz23B79+7Fgw8+iGPHjqFdu3aYN28edu3ahV69ennd/WVwQ+SFli9fjl69emHw4MGuborXOXbsGPr06YPg4GCMGzcOiYmJiI6OdnWzPF779u3x1ltvITY21mNK/HuazZs3677OyMjA3r178ddff2HChAkoLy93Ycscj8NSDXT+/HnU1NQgPDzcYH94eDiys7Nd1CrPpL1flu5ldnZ2vex9Hx8ftGrVyuAcU9fQfw9vtmzZMtxxxx0YNmwYMjMzdfuzs7Ph7++vG67SMr6/1u6duXOKioq8+q9ererqapw4cQJpaWmYM2cO0tPTMW3aNN7fBurfvz/Cw8ORlpaG6upqVFdXY+jQofi///s/VFdXIycnh/fXwYqKivDHH3+gW7duXvfzy+Cmgaqrq7F//37ExMTo9kmShJiYGKSmprqwZZ7n1KlTyMrKMriXGo0G1113ne5epqamIjQ0FP369dOdc/PNN0OWZezdu1d3zk033QRf38sdk7GxsTh69KjXD0ktW7YMY8aMwc0334zTp08bHNu/fz+qqqoM7u+VV16JTp06GdzfqKgohIWF6c6JjY1FUVERfv/9d905+tfQntNUf95lWYa/vz/vbwPt2LEDvXr1Qp8+fXTbr7/+ik8//RR9+vTBvn37eH8dLCAgAF27dkVWVpZX/vy6PIPb07cJEyaI8vJy8cADD4ju3buLd999V+Tn5xtklHOr2wICAkTv3r1F7969hRBCTJ8+XfTu3Vt06NBBAHVTwfPz88W//vUv0atXL7FhwwaTU8H3798vBg4cKG644QZx7Ngxg6ngQUFBIisrSyQmJooePXqICRMmiNLSUq+fCr58+XJRUFAgbrrpJoOpns2bN9eds2LFCnH69GkxdOhQ0a9fP/Hzzz+Ln3/+WXdcO9Vz8+bN4pprrhG33HKLyMnJMTnVc+HCheKqq64Sjz32WJOZSrtgwQIxZMgQ0alTJ9GrVy+xYMECUVtbK4YPH87764RNf7YU72/DtzfeeEPcdNNNolOnTmLQoEFi69atIjc3V7Rp08Yb76/rb7g3bI8//rg4ffq0qKioEHv27BHXXnuty9vkjlt0dLQwZfXq1bpzXnrpJZGVlSXKy8vFtm3bxBVXXGFwjdDQUPHpp5+K4uJiUVhYKN5//30REBBgcE5UVJT48ccfRXl5uTh79qx49tlnXf7Znb2ZExcXpzvH399fvP322+LChQuitLRUrFu3ToSHhxtcp2PHjuLbb78VZWVlIjc3V7zxxhvCx8en3r9jWlqaqKioEMePHzd4D2/eVq1aJU6dOiUqKipETk6O2LZtmy6w4f11/GYc3PD+Nmxbs2aNyMzMFBUVFeLs2bNizZo1okuXLl55f6VLXxARERF5BebcEBERkVdhcENERERehcENEREReRUGN0RERORVGNwQERGRV2FwQ0RERF6FwQ0RERF5FQY3RERE5FUY3BBRk3Pq1ClMmzbN1c0gIidhcENETrV69Wps2LABAJCcnIwlS5Y02nvHxcWhoKCg3v6BAwfivffea7R2EFHj8rV+ChGRe2nWrBmqq6vtfv358+cd2BoicjfsuSGiRrF69WoMHToU06dPhxACQgh06tQJANCzZ0989913KCkpQXZ2Nj766CO0bt1a99rk5GQsW7YMS5YsQV5eHrZs2QIAmDFjBg4dOoTS0lKcOXMGy5cvR0BAAAAgOjoaH374IUJCQnTvN2/ePAD1h6U6dOiAjRs3oqSkBEVFRfjiiy/Qtm1b3fF58+bhwIEDuP/++3Hq1CkUFhZizZo1CAwMdPp9IyLbMbghokYxbdo07N69G++99x4iIiIQERGBs2fPIjg4GD/88AMOHDiAAQMGYOTIkQgPD8eXX35p8Pq4uDhUVVXhxhtvxJQpUwAAiqLg//7v/9CzZ0/ExcXh5ptvxuuvvw4A2L17N6ZNm4aioiLd+y1atKheuyRJQlJSElq1aoXo6GjExsaiS5cu+OKLLwzO69q1K0aPHo077rgDd9xxB6KjozF79mwn3S0iaiiXL8POjRs3791Wr14tNmzYIACI5ORksWTJEoPjzz33nNi8ebPBvsjISCGEEFdccYXudfv377f6XmPHjhV5eXm67+Pi4kRBQUG9806dOiWmTZsmAIjhw4eL6upq0b59e93xq6++WgghxIABAwQAMW/ePFFaWioCAwN15yxcuFCkpqa6/P5y48at/sacGyJyqd69e2PYsGEoKSmpd6xr1674888/AQD79++vdzwmJgbx8fHo3r07goKC4OvrixYtWqBFixYoLy9X9f5XX301zp49i3Pnzun2HTlyBAUFBbj66quxb98+AMDp06dRWlqqOycrK8tg6IqI3AeDGyJyqcDAQHz99deYNWtWvWNZWVm6r8vKygyOderUCd988w3eeecdPPfcc8jPz8fgwYPxwQcfwM/PT3Vwo5ZxArMQArLMkX0id8TghogaTVVVFXx8fAz2paWlYezYsTh9+jRqa2tVX6t///6QZRlPPfUUhBAAgAkTJlh9P2NHjhxBhw4d0L59e13vzdVXX43Q0FD8/vvvqttDRO6Df3YQUaM5ffo0rrvuOnTq1AmtW7eGJElYvnw5WrVqhTVr1mDAgAHo0qULbrnlFnzwwQcWe0aOHz8OPz8/PPnkk+jcuTPuv/9+XaKx/vtpNBrcfPPNaN26NVq0aFHvOtu3b0dGRgY+/fRT9O3bFwMHDsRHH32EnTt3mhwKIyL3x+CGiBrNokWLUFtbi99//x3nz59Hx44dkZWVhRtvvBE+Pj7YunUrMjIysHTpUhQWFkJRFLPXOnToEGbMmIFZs2bh8OHDuO+++xAfH29wTmpqKt555x188cUXOH/+PJ599lmT1xo1ahQKCgrw448/Yvv27Th58iQmTpzo0M9ORI1HQl1mMREREZFXYM8NEREReRUGN0RERORVGNwQERGRV2FwQ0RERF6FwQ0RERF5FQY3RERE5FUY3BAREZFXYXBDREREXoXBDREREXkVBjdERETkVRjcEBERkVf5fzPjzncv+LqHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_its, train_losses = zip(*metrics.train_losses)\n",
    "val_its, val_losses = zip(*metrics.val_losses)\n",
    "plt.plot(train_its, train_losses, '-o')\n",
    "plt.plot(val_its, val_losses, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', \"Valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "The adapted weights are: $\\tilde{W} = W + c \\cdot \\mathbf{b}^\\top \\mathbf{a}$. Note, this process can be destructive if the inputs are in low precision and they have very different magnitudes. Tuning the `scale` parameter, $c$, prior to fine-tuning can improve the model performance after fusion.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlx_lm.fuse --model {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `lora_fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = load(\"lora_fused_model\")\n",
    "# num_correct = 0\n",
    "# for prompt, answer in tqdm.tqdm(test_set[:num_test]):\n",
    "#     response = generate(model, tokenizer, prompt, max_tokens=2)\n",
    "#     num_correct += (response==answer)\n",
    "# test_acc = num_correct / num_test\n",
    "# print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your LoRA adapters are not working well it's critical to plot both the trianing loss and validation loss over the duration of fine-tuning. There are really only two cases to consider: underfitting or overfitting. And you can figure out which regime you are in based on the above plot.\n",
    "\n",
    "**Underfitting**: The trianing loss is not low enough and the validation loss closely matches the training loss. You could also measure the accuracy on the training set itself for question-answering style tasks like HellaSwag. If you are in this regime you have a few options to improve the results:\n",
    "\n",
    "- Use more adapters. Increase `lora_layers` or adapt more of the linear layers within a given block by setting `lora_parameters[\"keys\"]`.\n",
    "- Use a higher rank. A higher rank means more parameters per adapter.\n",
    "- If you are using dropout, decrease the droupout rate or turn it off entirely.\n",
    "- Sometimes, underfitting issues are really optimization issues. In these cases it can be helpful to tune the learning rate or learning rate schedule.\n",
    "- If none of the above works, try a bigger model. For example, try Phi-3 medium instead of Phi-3 tiny.\n",
    "\n",
    "**Overfitting**: The trianing loss keeps going down but the validation loss stops going down and even starts to go up. If you are in this regime you also have a few options:\n",
    "\n",
    "- The best thing to do is to use more trianing data if you have it.\n",
    "- Contrary to the underfitting regime decreasing the capacity of the model can help. For example, use fewer adapters, a lower LoRA rank, or a smaller model size.\n",
    "- If you are not using dropout, use it.\n",
    "\n",
    "If you find your adapters work well pre-fusion but stop working post-fusion, try tuning the `scale` parameter, $c$, prior to fine-tuning. Typically the adapters have a smaller magnitude than the weights, so using a larger scale helps.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "Fine-tuning a large LM with LoRA requires a machine with a decent amount of memory. Here are some tips to reduce memory use should you need to do so. \n",
    "\n",
    "- Try quantization (QLoRA). You can use QLoRA by generating a quantized model with `mlx_lm.convert` and the `-q` flag or by using an already quantized model from HuggingFace.\n",
    "\n",
    "- Try using a smaller batch size. You can set the `batch_size` parameter in the `TrainingArgs` or pass `--batch-size` if you are using the CLI. The default is 4 so setting this to 2 or 1 will reduce memory consumption. Note, this may slow things down a little..\n",
    "\n",
    "- Reduce the number of layers to fine-tune with by setting `lora_layers` to a smaller value or passing `--lora-layers` if you are using the CLI. The default is `16`, so you can try `8` or `4`. This reduces the amount of memory needed for back propagation. It may also reduce the quality of the fine-tuned model and you may need to compensate with a larger `rank`.\n",
    "\n",
    "- Longer examples require more memory. If it makes sense for your data, one thing you can do is break your examples into smaller sequences when making the `train`, `valid`, and `test` data sets.\n",
    "\n",
    "- Gradient checkpointing lets you trade-off memory use (less) for computation (more) by recomputing instead of storing intermediate values needed by the backward pass. You can use gradient checkpointing by passing `grad_checkpoint=True` to the `TrainingArgs` or the `--grad-checkpoint` flag if using the CLI. Gradient checkpointing will be more helpful for larger batch sizes or sequence lengths with smaller or quantized models.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project.\n",
    "- We also have an example of [LoRA fine-tuning in MLX Swift](https://github.com/ml-explore/mlx-swift-examples/tree/main/Applications/LoRATrainingExample)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
