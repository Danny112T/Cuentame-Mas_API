{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to [LoRA fine-tune](https://arxiv.org/abs/2106.09685) an LLM with MLX LM. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it in the right format for MLX LM.\n",
    "2. Setup and run LoRA training. We'll show how to capture the training logs and plot some statistics to visualize the performance.\n",
    "3. Evaluate on the test set. We'll compute the final question-answer accuracy of the fine-tuned model.\n",
    "4. Fuse the resulting adapters into the base model and upload to Hugging Face.\n",
    "5. Discuss tips for debugging accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "664272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install mlx-lm\n",
    "# pip install matplotlib\n",
    "# pip install rouge-score\n",
    "# pip install scikit-learn\n",
    "# pip install tqdm\n",
    "# pip install numpy\n",
    "# pip install json\n",
    "# pip install pathlib\n",
    "# pip install transformers\n",
    "# pip install sentencepiece\n",
    "# pip install datasets\n",
    "# pip install torch\n",
    "# pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1131315",
   "metadata": {},
   "source": [
    "### MLFOW CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4d1cf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae62a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1d438bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_name=\"finetuning_comparativa\"\n",
    "exp_name=\"MLX1.0\"\n",
    "corrida_name=\"MLX-20_2_4e4\"\n",
    "# base_model = \"Llama-3.2-1B-Instruct\"\n",
    "# four_bits = True \n",
    "# dataset_path = \"FAQ_All.jsonl\"\n",
    "# dataset_type = \"alpaca_chat.load_qa\"\n",
    "# output_dir = \"../trained_models/adapters/adapters.safetensors\"\n",
    "output_path = \"../trained_models/adapters_20_2_4e4/\"\n",
    "output_dir = output_path + 'adapters.safetensors'\n",
    "sequence_len = 2048\n",
    "lora_layers = 8\n",
    "lora_layers_scale = 20.0\n",
    "grad_checkpoint_value = True\n",
    "\n",
    "\n",
    "\n",
    "lora_r = 8\n",
    "# lora_alpha = 16\n",
    "lora_dropout = 0.0\n",
    "# gradient_accumulation_steps = 4\n",
    "batch_size = 2\n",
    "optimizer = \"adam\"\n",
    "learning_rate_value = 2e-4\n",
    "# weight_decay_value = 0.02\n",
    "lr_scheduler = \"linear\"\n",
    "\n",
    "epochs = 20\n",
    "steps = 10240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0c0069b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/880645134898555871', creation_time=1733877767863, experiment_id='880645134898555871', last_update_time=1733877767863, lifecycle_stage='active', name='MLX1.0', tags={}>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"https://4z0r6nts-5000.usw3.devtunnels.ms/\") \n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(experiment_name=exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61698208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello stats: 1024 lenght trainging dataset\n",
      "An example:\n",
      "\n",
      "{\n",
      "  \"pregunta\": \"\\u00bfPuedo presentar una p\\u00f3liza de bonos en formato electr\\u00f3nico?\",\n",
      "  \"respuesta\": \"Este tipo de garant\\u00eda es v\\u00e1lida, \\u00bfno?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# jsonl file path\n",
    "\"\"\" save_path = \"./dataset/FAQ_All.jsonl\"\n",
    "with open(save_path, 'r') as file:\n",
    "\tdataset = [json.loads(line) for line in file]\n",
    "\"\"\"\n",
    "\n",
    "# csv file path\n",
    "save_path = \"./datasets/Parph_Data/FAQs_1000.csv\"\n",
    "dataset = []\n",
    "with open(save_path, 'r', encoding='utf-8') as file:\n",
    "\treader = csv.DictReader(file)\n",
    "\tfor row in reader:\n",
    "\t\tdataset.append(row)\n",
    "\n",
    "print(f\"Hello stats: {len(dataset)} lenght trainging dataset\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9b607237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "valid_set = [dataset[i] for i in perm[:valid_size]]\n",
    "train_set = [dataset[i] for i in perm[valid_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c38c4e",
   "metadata": {},
   "source": [
    "Finally, put the data splits in the MLX LM training format. The format simply expects the data to be in a container which supports random access to the individual examples (e.g. a Python `list`):\n",
    "```\n",
    "[\"An example for the model.\", \"Another example for the model.\", ...]\n",
    "```\n",
    "For more details, see the [documentation on supported formats](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ea738f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\"\"\"def preprocess(dataset):\n",
    "    return [\n",
    "        f\"Question: {clean_text(t['question'])}\\n\"\n",
    "        f\"Answer: {clean_text(t['answer'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\"\"\"\n",
    "def preprocess(dataset):\n",
    "    return [\n",
    "        f\"pregunta: {clean_text(t['pregunta'])}\\n\"\n",
    "        f\"respuesta: {clean_text(t['respuesta'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\n",
    "\n",
    "train_set, valid_set = map(preprocess, (train_set, valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [Phi-3 mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). At 3.8 billion parameters, Phi-3 mini is a high-quality model that is also fast to fine-tune on most Apple silicon machines. Also, it has a [permissive MIT License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE).\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c3ff309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm.utils import load, generate\n",
    "from mlx_lm.tuner.trainer import train, evaluate, TrainingArgs\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters and make the training arguments. See the [training argument class](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/trainer.py#L31-L63) for a more detailed list of training parameters. \n",
    "\n",
    "Recall the LoRA update is $W^\\top \\mathbf{x} + c \\cdot \\mathbf{a} \\mathbf{b}^\\top \\mathbf{x}$ where $\\mathbf{a}$ has shape `(D, rank)`.\n",
    "\n",
    "With that in mind, the LoRA parameters to attend to are:\n",
    "- `lora_layers`: The number of Transformer blocks from the top of the model to adapt.\n",
    "- `rank`: The rank of the low-rank adapters. A larger rank implies more adapter parameters per linear layer.\n",
    "- `scale`: This is the constant $c$ that scales the low-rank update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f0851dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save the adapter config and weights\n",
    "adapter_path = Path(\"../trained_models/adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"lora_layers\": lora_layers,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": lora_r,\n",
    "    \"scale\": lora_layers_scale,\n",
    "    \"dropout\": lora_dropout,\n",
    "    \"epochs\": epochs\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)    \n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=output_dir,\n",
    "    iters=steps,\n",
    "    steps_per_eval=50,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_length=sequence_len,\n",
    "    grad_checkpoint=grad_checkpoint_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the Phi-3 mini model. Note this may take a few minutes to download from HuggingFace if you haven't downloaded it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fb0b16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../original_models/Llama-3.2-1B-Instruct-bf16\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the model, freeze it's parameters so we don't train them. Then convert linear layers to LoRA layers using the MLX LM utility `linear_to_lora_layers`. The adapters in the `LoRA` layers are not frozen, so they will be included in the model's `trainable_parameters`. Check-out the [LoRA layer implementation](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/lora.py#L72-L104) to see how it all works.\n",
    "\n",
    "By default, MLX LM only adapts the query, key, and value projection matrices for Phi-3. You can specify the layers to adapt by setting `lora_parameters[\"keys\"]` to a list of layer names. In this case it defaults to `[\"attn.qkv_proj\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "50e1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 425984\n"
     ]
    }
   ],
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"lora_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34ee27",
   "metadata": {},
   "source": [
    "### Evaluate Functions\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each example in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37e55f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > 0.8\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    # Adjust generation parameters\n",
    "    generation_params = {\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    for example in tqdm(dataset):\n",
    "        # Get prompt and true answer\n",
    "        prompt = example[\"prompt\"]\n",
    "        true_answer = example[\"response\"]\n",
    "        \n",
    "        # Generate prediction\n",
    "        response = generate(model, tokenizer, prompt, **generation_params)\n",
    "        \n",
    "        # Store prediction and true label\n",
    "        all_preds.append(response)\n",
    "        all_labels.append(true_answer)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(true_answer, response)\n",
    "        for key, score in scores.items():\n",
    "            rouge_scores[key].append(score.fmeasure)\n",
    "\n",
    "        # Calculate loss/perplexity\n",
    "        tokens = tokenizer.encode(prompt + true_answer)\n",
    "        tokens = mx.array(tokens)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(tokens[None])[0]\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        targets = tokens[1:]\n",
    "        logits = logits[:-1]\n",
    "        loss = mx.mean(nn.losses.cross_entropy(logits, targets))\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = sum(1 for x,y in zip(all_preds, all_labels) if similar(x.strip(), y.strip())) / len(all_preds)\n",
    "    \n",
    "    # Convert predictions and labels to match format for F1\n",
    "    pred_labels = [1 if similar(p.strip(), l.strip()) else 0 for p,l in zip(all_preds, all_labels)]\n",
    "    true_labels = [1] * len(all_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='binary')\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge_scores = {key: np.mean(scores) for key, scores in rouge_scores.items()}\n",
    "\n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    perplexity = float(mx.exp(mx.array(avg_loss)))\n",
    "\n",
    "    return accuracy, f1, perplexity, avg_rouge_scores\n",
    "\n",
    "# Load test dataset\n",
    "test_set_path = \"./datasets/Parph_Data/FAQs_200_testing.jsonl\" \n",
    "with open(test_set_path, 'r') as file:\n",
    "    test_set = [json.loads(line) for line in file]\n",
    "\n",
    "# Define number of test samples\n",
    "num_test = len(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer, but you can specify any [optimizer](https://ml-explore.github.io/mlx/build/html/python/optimizers/common_optimizers.html) with any [scheduler](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html). We also added a custom class to capture the training and validation loss to plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "984516d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder ../trained_models/adapters_20_2_2e4/ created\n",
      "Starting training..., iters: 10240\n",
      "Iter 1: Val loss 3.567, Val took 3.917s\n",
      "Iter 10: Train loss 3.013, Learning Rate 2.000e-04, It/sec 2.324, Tokens/sec 345.385, Trained Tokens 1486, Peak mem 9.324 GB\n",
      "Iter 20: Train loss 2.945, Learning Rate 2.000e-04, It/sec 2.135, Tokens/sec 367.566, Trained Tokens 3208, Peak mem 9.324 GB\n",
      "Iter 30: Train loss 2.691, Learning Rate 2.000e-04, It/sec 2.079, Tokens/sec 359.380, Trained Tokens 4937, Peak mem 9.324 GB\n",
      "Iter 40: Train loss 2.504, Learning Rate 2.000e-04, It/sec 2.186, Tokens/sec 347.715, Trained Tokens 6528, Peak mem 9.324 GB\n",
      "Iter 50: Val loss 2.758, Val took 3.351s\n",
      "Iter 50: Train loss 2.681, Learning Rate 2.000e-04, It/sec 39.587, Tokens/sec 6013.204, Trained Tokens 8047, Peak mem 9.324 GB\n",
      "Iter 60: Train loss 2.744, Learning Rate 2.000e-04, It/sec 2.173, Tokens/sec 350.879, Trained Tokens 9662, Peak mem 9.324 GB\n",
      "Iter 70: Train loss 2.563, Learning Rate 2.000e-04, It/sec 2.432, Tokens/sec 342.142, Trained Tokens 11069, Peak mem 9.324 GB\n",
      "Iter 80: Train loss 2.753, Learning Rate 2.000e-04, It/sec 2.036, Tokens/sec 350.947, Trained Tokens 12793, Peak mem 9.324 GB\n",
      "Iter 90: Train loss 2.718, Learning Rate 2.000e-04, It/sec 2.499, Tokens/sec 343.179, Trained Tokens 14166, Peak mem 9.324 GB\n",
      "Iter 100: Val loss 2.693, Val took 3.035s\n",
      "Iter 100: Train loss 2.743, Learning Rate 2.000e-04, It/sec 16.651, Tokens/sec 2720.779, Trained Tokens 15800, Peak mem 9.324 GB\n",
      "Iter 100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 2.699, Learning Rate 2.000e-04, It/sec 2.474, Tokens/sec 351.052, Trained Tokens 17219, Peak mem 9.324 GB\n",
      "Iter 120: Train loss 2.711, Learning Rate 2.000e-04, It/sec 2.057, Tokens/sec 355.033, Trained Tokens 18945, Peak mem 9.324 GB\n",
      "Iter 130: Train loss 2.420, Learning Rate 2.000e-04, It/sec 1.449, Tokens/sec 377.936, Trained Tokens 21553, Peak mem 9.324 GB\n",
      "Iter 140: Train loss 2.346, Learning Rate 2.000e-04, It/sec 1.717, Tokens/sec 377.734, Trained Tokens 23753, Peak mem 9.324 GB\n",
      "Iter 150: Val loss 2.551, Val took 3.302s\n",
      "Iter 150: Train loss 2.657, Learning Rate 2.000e-04, It/sec 24.133, Tokens/sec 5820.925, Trained Tokens 26165, Peak mem 9.324 GB\n",
      "Iter 160: Train loss 2.719, Learning Rate 2.000e-04, It/sec 2.450, Tokens/sec 348.146, Trained Tokens 27586, Peak mem 9.324 GB\n",
      "Iter 170: Train loss 2.557, Learning Rate 2.000e-04, It/sec 1.934, Tokens/sec 365.221, Trained Tokens 29474, Peak mem 9.324 GB\n",
      "Iter 180: Train loss 2.506, Learning Rate 2.000e-04, It/sec 2.840, Tokens/sec 331.147, Trained Tokens 30640, Peak mem 9.324 GB\n",
      "Iter 190: Train loss 2.454, Learning Rate 2.000e-04, It/sec 2.465, Tokens/sec 343.062, Trained Tokens 32032, Peak mem 9.324 GB\n",
      "Iter 200: Val loss 2.640, Val took 3.205s\n",
      "Iter 200: Train loss 2.370, Learning Rate 2.000e-04, It/sec 16.627, Tokens/sec 2889.750, Trained Tokens 33770, Peak mem 9.324 GB\n",
      "Iter 200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 2.430, Learning Rate 2.000e-04, It/sec 2.601, Tokens/sec 348.058, Trained Tokens 35108, Peak mem 9.324 GB\n",
      "Iter 220: Train loss 2.565, Learning Rate 2.000e-04, It/sec 2.541, Tokens/sec 341.555, Trained Tokens 36452, Peak mem 9.324 GB\n",
      "Iter 230: Train loss 2.258, Learning Rate 2.000e-04, It/sec 2.833, Tokens/sec 331.996, Trained Tokens 37624, Peak mem 9.324 GB\n",
      "Iter 240: Train loss 2.416, Learning Rate 2.000e-04, It/sec 2.125, Tokens/sec 355.266, Trained Tokens 39296, Peak mem 9.324 GB\n",
      "Iter 250: Val loss 2.458, Val took 2.902s\n",
      "Iter 250: Train loss 2.387, Learning Rate 2.000e-04, It/sec 11.846, Tokens/sec 1874.083, Trained Tokens 40878, Peak mem 9.324 GB\n",
      "Iter 260: Train loss 2.466, Learning Rate 2.000e-04, It/sec 2.680, Tokens/sec 345.198, Trained Tokens 42166, Peak mem 9.324 GB\n",
      "Iter 270: Train loss 2.313, Learning Rate 2.000e-04, It/sec 1.954, Tokens/sec 364.016, Trained Tokens 44029, Peak mem 9.324 GB\n",
      "Iter 280: Train loss 2.459, Learning Rate 2.000e-04, It/sec 2.240, Tokens/sec 369.147, Trained Tokens 45677, Peak mem 9.324 GB\n",
      "Iter 290: Train loss 2.477, Learning Rate 2.000e-04, It/sec 1.955, Tokens/sec 372.005, Trained Tokens 47580, Peak mem 9.324 GB\n",
      "Iter 300: Val loss 2.513, Val took 3.794s\n",
      "Iter 300: Train loss 2.284, Learning Rate 2.000e-04, It/sec 31.727, Tokens/sec 3753.304, Trained Tokens 48763, Peak mem 9.324 GB\n",
      "Iter 300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 2.288, Learning Rate 2.000e-04, It/sec 2.250, Tokens/sec 333.401, Trained Tokens 50245, Peak mem 9.324 GB\n",
      "Iter 320: Train loss 2.355, Learning Rate 2.000e-04, It/sec 2.530, Tokens/sec 345.829, Trained Tokens 51612, Peak mem 9.324 GB\n",
      "Iter 330: Train loss 2.333, Learning Rate 2.000e-04, It/sec 2.561, Tokens/sec 360.337, Trained Tokens 53019, Peak mem 9.324 GB\n",
      "Iter 340: Train loss 2.365, Learning Rate 2.000e-04, It/sec 1.177, Tokens/sec 378.535, Trained Tokens 56236, Peak mem 9.324 GB\n",
      "Iter 350: Val loss 2.347, Val took 3.160s\n",
      "Iter 350: Train loss 2.309, Learning Rate 2.000e-04, It/sec 17.970, Tokens/sec 2857.253, Trained Tokens 57826, Peak mem 9.324 GB\n",
      "Iter 360: Train loss 2.359, Learning Rate 2.000e-04, It/sec 2.541, Tokens/sec 338.010, Trained Tokens 59156, Peak mem 9.324 GB\n",
      "Iter 370: Train loss 2.252, Learning Rate 2.000e-04, It/sec 1.458, Tokens/sec 367.562, Trained Tokens 61677, Peak mem 9.324 GB\n",
      "Iter 380: Train loss 2.519, Learning Rate 2.000e-04, It/sec 1.881, Tokens/sec 359.828, Trained Tokens 63590, Peak mem 9.324 GB\n",
      "Iter 390: Train loss 2.416, Learning Rate 2.000e-04, It/sec 2.021, Tokens/sec 351.731, Trained Tokens 65330, Peak mem 9.324 GB\n",
      "Iter 400: Val loss 2.380, Val took 2.936s\n",
      "Iter 400: Train loss 2.080, Learning Rate 2.000e-04, It/sec 38.852, Tokens/sec 5109.099, Trained Tokens 66645, Peak mem 9.324 GB\n",
      "Iter 400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 2.142, Learning Rate 2.000e-04, It/sec 2.493, Tokens/sec 316.617, Trained Tokens 67915, Peak mem 9.324 GB\n",
      "Iter 420: Train loss 2.039, Learning Rate 2.000e-04, It/sec 2.245, Tokens/sec 340.339, Trained Tokens 69431, Peak mem 9.324 GB\n",
      "Iter 430: Train loss 1.957, Learning Rate 2.000e-04, It/sec 2.129, Tokens/sec 338.928, Trained Tokens 71023, Peak mem 9.324 GB\n",
      "Iter 440: Train loss 1.863, Learning Rate 2.000e-04, It/sec 2.009, Tokens/sec 335.541, Trained Tokens 72693, Peak mem 9.324 GB\n",
      "Iter 450: Val loss 2.451, Val took 3.630s\n",
      "Iter 450: Train loss 2.024, Learning Rate 2.000e-04, It/sec 7.113, Tokens/sec 1584.718, Trained Tokens 74921, Peak mem 9.324 GB\n",
      "Iter 460: Train loss 1.911, Learning Rate 2.000e-04, It/sec 2.167, Tokens/sec 334.204, Trained Tokens 76463, Peak mem 9.324 GB\n",
      "Iter 470: Train loss 2.030, Learning Rate 2.000e-04, It/sec 1.806, Tokens/sec 364.518, Trained Tokens 78481, Peak mem 9.324 GB\n",
      "Iter 480: Train loss 2.082, Learning Rate 2.000e-04, It/sec 2.643, Tokens/sec 333.288, Trained Tokens 79742, Peak mem 9.324 GB\n",
      "Iter 490: Train loss 2.035, Learning Rate 2.000e-04, It/sec 2.273, Tokens/sec 349.529, Trained Tokens 81280, Peak mem 9.324 GB\n",
      "Iter 500: Val loss 2.339, Val took 3.302s\n",
      "Iter 500: Train loss 1.957, Learning Rate 2.000e-04, It/sec 30.741, Tokens/sec 4011.708, Trained Tokens 82585, Peak mem 9.324 GB\n",
      "Iter 500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 1.976, Learning Rate 2.000e-04, It/sec 2.525, Tokens/sec 330.760, Trained Tokens 83895, Peak mem 9.324 GB\n",
      "Iter 520: Train loss 1.928, Learning Rate 2.000e-04, It/sec 2.484, Tokens/sec 326.639, Trained Tokens 85210, Peak mem 9.324 GB\n",
      "Iter 530: Train loss 2.125, Learning Rate 2.000e-04, It/sec 2.345, Tokens/sec 354.303, Trained Tokens 86721, Peak mem 9.324 GB\n",
      "Iter 540: Train loss 2.335, Learning Rate 2.000e-04, It/sec 1.503, Tokens/sec 368.428, Trained Tokens 89172, Peak mem 9.324 GB\n",
      "Iter 550: Val loss 2.305, Val took 2.993s\n",
      "Iter 550: Train loss 1.978, Learning Rate 2.000e-04, It/sec 24.567, Tokens/sec 3574.562, Trained Tokens 90627, Peak mem 9.324 GB\n",
      "Iter 560: Train loss 1.753, Learning Rate 2.000e-04, It/sec 2.248, Tokens/sec 326.116, Trained Tokens 92078, Peak mem 9.324 GB\n",
      "Iter 570: Train loss 2.224, Learning Rate 2.000e-04, It/sec 1.504, Tokens/sec 361.606, Trained Tokens 94483, Peak mem 9.324 GB\n",
      "Iter 580: Train loss 2.139, Learning Rate 2.000e-04, It/sec 2.118, Tokens/sec 343.401, Trained Tokens 96104, Peak mem 9.324 GB\n",
      "Iter 590: Train loss 1.987, Learning Rate 2.000e-04, It/sec 2.119, Tokens/sec 352.771, Trained Tokens 97769, Peak mem 9.324 GB\n",
      "Iter 600: Val loss 2.211, Val took 3.002s\n",
      "Iter 600: Train loss 2.165, Learning Rate 2.000e-04, It/sec 4.794, Tokens/sec 838.947, Trained Tokens 99519, Peak mem 9.324 GB\n",
      "Iter 600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 1.992, Learning Rate 2.000e-04, It/sec 2.270, Tokens/sec 362.361, Trained Tokens 101115, Peak mem 9.324 GB\n",
      "Iter 620: Train loss 2.251, Learning Rate 2.000e-04, It/sec 2.115, Tokens/sec 344.170, Trained Tokens 102742, Peak mem 9.324 GB\n",
      "Iter 630: Train loss 2.108, Learning Rate 2.000e-04, It/sec 2.015, Tokens/sec 330.693, Trained Tokens 104383, Peak mem 9.324 GB\n",
      "Iter 640: Train loss 2.054, Learning Rate 2.000e-04, It/sec 2.034, Tokens/sec 331.730, Trained Tokens 106014, Peak mem 9.324 GB\n",
      "Iter 650: Val loss 2.307, Val took 2.898s\n",
      "Iter 650: Train loss 2.023, Learning Rate 2.000e-04, It/sec 16.467, Tokens/sec 2351.531, Trained Tokens 107442, Peak mem 9.324 GB\n",
      "Iter 660: Train loss 1.834, Learning Rate 2.000e-04, It/sec 2.652, Tokens/sec 315.644, Trained Tokens 108632, Peak mem 9.324 GB\n",
      "Iter 670: Train loss 1.969, Learning Rate 2.000e-04, It/sec 2.358, Tokens/sec 341.643, Trained Tokens 110081, Peak mem 9.324 GB\n",
      "Iter 680: Train loss 1.893, Learning Rate 2.000e-04, It/sec 2.102, Tokens/sec 333.993, Trained Tokens 111670, Peak mem 9.324 GB\n",
      "Iter 690: Train loss 2.138, Learning Rate 2.000e-04, It/sec 2.249, Tokens/sec 342.051, Trained Tokens 113191, Peak mem 9.324 GB\n",
      "Iter 700: Val loss 2.527, Val took 3.879s\n",
      "Iter 700: Train loss 2.010, Learning Rate 2.000e-04, It/sec 28.239, Tokens/sec 4537.999, Trained Tokens 114798, Peak mem 9.324 GB\n",
      "Iter 700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 2.079, Learning Rate 2.000e-04, It/sec 1.611, Tokens/sec 347.860, Trained Tokens 116957, Peak mem 9.324 GB\n",
      "Iter 720: Train loss 1.982, Learning Rate 2.000e-04, It/sec 2.379, Tokens/sec 347.811, Trained Tokens 118419, Peak mem 9.324 GB\n",
      "Iter 730: Train loss 1.811, Learning Rate 2.000e-04, It/sec 3.627, Tokens/sec 326.753, Trained Tokens 119320, Peak mem 9.324 GB\n",
      "Iter 740: Train loss 2.104, Learning Rate 2.000e-04, It/sec 2.229, Tokens/sec 354.590, Trained Tokens 120911, Peak mem 9.324 GB\n",
      "Iter 750: Val loss 2.447, Val took 3.562s\n",
      "Iter 750: Train loss 1.955, Learning Rate 2.000e-04, It/sec 21.897, Tokens/sec 3553.834, Trained Tokens 122534, Peak mem 9.324 GB\n",
      "Iter 760: Train loss 1.994, Learning Rate 2.000e-04, It/sec 1.522, Tokens/sec 356.591, Trained Tokens 124877, Peak mem 9.324 GB\n",
      "Iter 770: Train loss 2.104, Learning Rate 2.000e-04, It/sec 2.182, Tokens/sec 352.994, Trained Tokens 126495, Peak mem 9.324 GB\n",
      "Iter 780: Train loss 1.893, Learning Rate 2.000e-04, It/sec 2.546, Tokens/sec 334.580, Trained Tokens 127809, Peak mem 9.324 GB\n",
      "Iter 790: Train loss 2.036, Learning Rate 2.000e-04, It/sec 1.307, Tokens/sec 373.916, Trained Tokens 130670, Peak mem 9.324 GB\n",
      "Iter 800: Val loss 2.264, Val took 2.783s\n",
      "Iter 800: Train loss 2.204, Learning Rate 2.000e-04, It/sec 22.217, Tokens/sec 4021.301, Trained Tokens 132480, Peak mem 9.324 GB\n",
      "Iter 800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 1.862, Learning Rate 2.000e-04, It/sec 1.838, Tokens/sec 355.796, Trained Tokens 134416, Peak mem 9.324 GB\n",
      "Iter 820: Train loss 1.871, Learning Rate 2.000e-04, It/sec 2.475, Tokens/sec 350.025, Trained Tokens 135830, Peak mem 9.324 GB\n",
      "Iter 830: Train loss 1.645, Learning Rate 2.000e-04, It/sec 2.330, Tokens/sec 338.274, Trained Tokens 137282, Peak mem 9.324 GB\n",
      "Iter 840: Train loss 1.832, Learning Rate 2.000e-04, It/sec 1.263, Tokens/sec 375.913, Trained Tokens 140259, Peak mem 9.324 GB\n",
      "Iter 850: Val loss 2.132, Val took 3.717s\n",
      "Iter 850: Train loss 1.756, Learning Rate 2.000e-04, It/sec 31.293, Tokens/sec 3767.667, Trained Tokens 141463, Peak mem 9.324 GB\n",
      "Iter 860: Train loss 1.626, Learning Rate 2.000e-04, It/sec 1.683, Tokens/sec 335.448, Trained Tokens 143456, Peak mem 9.324 GB\n",
      "Iter 870: Train loss 1.626, Learning Rate 2.000e-04, It/sec 1.914, Tokens/sec 363.925, Trained Tokens 145357, Peak mem 9.324 GB\n",
      "Iter 880: Train loss 1.719, Learning Rate 2.000e-04, It/sec 2.483, Tokens/sec 357.572, Trained Tokens 146797, Peak mem 9.324 GB\n",
      "Iter 890: Train loss 1.639, Learning Rate 2.000e-04, It/sec 2.322, Tokens/sec 352.506, Trained Tokens 148315, Peak mem 9.324 GB\n",
      "Iter 900: Val loss 2.252, Val took 2.757s\n",
      "Iter 900: Train loss 1.746, Learning Rate 2.000e-04, It/sec 22.581, Tokens/sec 3798.069, Trained Tokens 149997, Peak mem 9.324 GB\n",
      "Iter 900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 1.513, Learning Rate 2.000e-04, It/sec 2.390, Tokens/sec 331.070, Trained Tokens 151382, Peak mem 9.324 GB\n",
      "Iter 920: Train loss 1.710, Learning Rate 2.000e-04, It/sec 1.383, Tokens/sec 352.365, Trained Tokens 153930, Peak mem 9.324 GB\n",
      "Iter 930: Train loss 1.566, Learning Rate 2.000e-04, It/sec 2.443, Tokens/sec 315.409, Trained Tokens 155221, Peak mem 9.324 GB\n",
      "Iter 940: Train loss 1.728, Learning Rate 2.000e-04, It/sec 1.449, Tokens/sec 301.961, Trained Tokens 157305, Peak mem 9.324 GB\n",
      "Iter 950: Val loss 2.373, Val took 3.380s\n",
      "Iter 950: Train loss 1.885, Learning Rate 2.000e-04, It/sec 38.368, Tokens/sec 6019.944, Trained Tokens 158874, Peak mem 9.324 GB\n",
      "Iter 960: Train loss 1.906, Learning Rate 2.000e-04, It/sec 1.395, Tokens/sec 357.161, Trained Tokens 161434, Peak mem 9.324 GB\n",
      "Iter 970: Train loss 1.736, Learning Rate 2.000e-04, It/sec 1.972, Tokens/sec 355.947, Trained Tokens 163239, Peak mem 9.324 GB\n",
      "Iter 980: Train loss 1.748, Learning Rate 2.000e-04, It/sec 1.813, Tokens/sec 319.310, Trained Tokens 165000, Peak mem 9.324 GB\n",
      "Iter 990: Train loss 1.448, Learning Rate 2.000e-04, It/sec 2.369, Tokens/sec 332.811, Trained Tokens 166405, Peak mem 9.324 GB\n",
      "Iter 1000: Val loss 2.380, Val took 3.971s\n",
      "Iter 1000: Train loss 1.758, Learning Rate 2.000e-04, It/sec 22.776, Tokens/sec 3190.927, Trained Tokens 167806, Peak mem 9.324 GB\n",
      "Iter 1000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001000_adapters.safetensors.\n",
      "Iter 1010: Train loss 1.668, Learning Rate 2.000e-04, It/sec 2.340, Tokens/sec 339.356, Trained Tokens 169256, Peak mem 9.324 GB\n",
      "Iter 1020: Train loss 1.707, Learning Rate 2.000e-04, It/sec 2.851, Tokens/sec 338.966, Trained Tokens 170445, Peak mem 9.324 GB\n",
      "Iter 1030: Train loss 1.711, Learning Rate 2.000e-04, It/sec 2.398, Tokens/sec 357.115, Trained Tokens 171934, Peak mem 9.324 GB\n",
      "Iter 1040: Train loss 1.807, Learning Rate 2.000e-04, It/sec 1.844, Tokens/sec 341.601, Trained Tokens 173787, Peak mem 9.324 GB\n",
      "Iter 1050: Val loss 2.304, Val took 5.522s\n",
      "Iter 1050: Train loss 1.814, Learning Rate 2.000e-04, It/sec 23.632, Tokens/sec 3651.214, Trained Tokens 175332, Peak mem 9.324 GB\n",
      "Iter 1060: Train loss 1.782, Learning Rate 2.000e-04, It/sec 1.742, Tokens/sec 360.975, Trained Tokens 177404, Peak mem 9.324 GB\n",
      "Iter 1070: Train loss 1.724, Learning Rate 2.000e-04, It/sec 2.138, Tokens/sec 360.194, Trained Tokens 179089, Peak mem 9.324 GB\n",
      "Iter 1080: Train loss 1.627, Learning Rate 2.000e-04, It/sec 2.181, Tokens/sec 341.385, Trained Tokens 180654, Peak mem 9.324 GB\n",
      "Iter 1090: Train loss 1.917, Learning Rate 2.000e-04, It/sec 2.632, Tokens/sec 359.021, Trained Tokens 182018, Peak mem 9.324 GB\n",
      "Iter 1100: Val loss 2.125, Val took 4.116s\n",
      "Iter 1100: Train loss 1.908, Learning Rate 2.000e-04, It/sec 7.265, Tokens/sec 1442.123, Trained Tokens 184003, Peak mem 9.324 GB\n",
      "Iter 1100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001100_adapters.safetensors.\n",
      "Iter 1110: Train loss 1.676, Learning Rate 2.000e-04, It/sec 2.348, Tokens/sec 346.082, Trained Tokens 185477, Peak mem 9.324 GB\n",
      "Iter 1120: Train loss 1.644, Learning Rate 2.000e-04, It/sec 2.870, Tokens/sec 337.176, Trained Tokens 186652, Peak mem 9.324 GB\n",
      "Iter 1130: Train loss 2.189, Learning Rate 2.000e-04, It/sec 1.724, Tokens/sec 367.424, Trained Tokens 188783, Peak mem 9.324 GB\n",
      "Iter 1140: Train loss 1.651, Learning Rate 2.000e-04, It/sec 2.390, Tokens/sec 352.227, Trained Tokens 190257, Peak mem 9.324 GB\n",
      "Iter 1150: Val loss 2.365, Val took 3.438s\n",
      "Iter 1150: Train loss 1.876, Learning Rate 2.000e-04, It/sec 24.387, Tokens/sec 3828.746, Trained Tokens 191827, Peak mem 9.324 GB\n",
      "Iter 1160: Train loss 1.625, Learning Rate 2.000e-04, It/sec 2.254, Tokens/sec 326.866, Trained Tokens 193277, Peak mem 9.324 GB\n",
      "Iter 1170: Train loss 1.786, Learning Rate 2.000e-04, It/sec 2.483, Tokens/sec 331.761, Trained Tokens 194613, Peak mem 9.324 GB\n",
      "Iter 1180: Train loss 1.578, Learning Rate 2.000e-04, It/sec 2.580, Tokens/sec 328.419, Trained Tokens 195886, Peak mem 9.324 GB\n",
      "Iter 1190: Train loss 1.808, Learning Rate 2.000e-04, It/sec 2.171, Tokens/sec 348.884, Trained Tokens 197493, Peak mem 9.324 GB\n",
      "Iter 1200: Val loss 2.291, Val took 3.568s\n",
      "Iter 1200: Train loss 1.767, Learning Rate 2.000e-04, It/sec 24.397, Tokens/sec 4462.290, Trained Tokens 199322, Peak mem 9.324 GB\n",
      "Iter 1200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001200_adapters.safetensors.\n",
      "Iter 1210: Train loss 1.671, Learning Rate 2.000e-04, It/sec 2.151, Tokens/sec 322.856, Trained Tokens 200823, Peak mem 9.324 GB\n",
      "Iter 1220: Train loss 1.484, Learning Rate 2.000e-04, It/sec 2.225, Tokens/sec 339.355, Trained Tokens 202348, Peak mem 9.324 GB\n",
      "Iter 1230: Train loss 1.816, Learning Rate 2.000e-04, It/sec 2.522, Tokens/sec 352.383, Trained Tokens 203745, Peak mem 9.324 GB\n",
      "Iter 1240: Train loss 1.557, Learning Rate 2.000e-04, It/sec 2.277, Tokens/sec 357.012, Trained Tokens 205313, Peak mem 9.324 GB\n",
      "Iter 1250: Val loss 2.146, Val took 3.229s\n",
      "Iter 1250: Train loss 1.449, Learning Rate 2.000e-04, It/sec 14.006, Tokens/sec 2227.031, Trained Tokens 206903, Peak mem 9.324 GB\n",
      "Iter 1260: Train loss 1.566, Learning Rate 2.000e-04, It/sec 1.871, Tokens/sec 345.200, Trained Tokens 208748, Peak mem 9.324 GB\n",
      "Iter 1270: Train loss 1.555, Learning Rate 2.000e-04, It/sec 1.995, Tokens/sec 346.768, Trained Tokens 210486, Peak mem 9.324 GB\n",
      "Iter 1280: Train loss 1.386, Learning Rate 2.000e-04, It/sec 2.130, Tokens/sec 354.822, Trained Tokens 212152, Peak mem 9.324 GB\n",
      "Iter 1290: Train loss 1.302, Learning Rate 2.000e-04, It/sec 2.863, Tokens/sec 333.289, Trained Tokens 213316, Peak mem 9.324 GB\n",
      "Iter 1300: Val loss 2.344, Val took 3.612s\n",
      "Iter 1300: Train loss 1.753, Learning Rate 2.000e-04, It/sec 22.438, Tokens/sec 5620.756, Trained Tokens 215821, Peak mem 9.324 GB\n",
      "Iter 1300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001300_adapters.safetensors.\n",
      "Iter 1310: Train loss 1.641, Learning Rate 2.000e-04, It/sec 1.680, Tokens/sec 294.214, Trained Tokens 217572, Peak mem 9.324 GB\n",
      "Iter 1320: Train loss 1.543, Learning Rate 2.000e-04, It/sec 1.765, Tokens/sec 356.803, Trained Tokens 219594, Peak mem 9.324 GB\n",
      "Iter 1330: Train loss 1.309, Learning Rate 2.000e-04, It/sec 2.829, Tokens/sec 338.928, Trained Tokens 220792, Peak mem 9.324 GB\n",
      "Iter 1340: Train loss 1.591, Learning Rate 2.000e-04, It/sec 1.820, Tokens/sec 343.403, Trained Tokens 222679, Peak mem 9.324 GB\n",
      "Iter 1350: Val loss 2.044, Val took 3.218s\n",
      "Iter 1350: Train loss 1.388, Learning Rate 2.000e-04, It/sec 24.537, Tokens/sec 3057.333, Trained Tokens 223925, Peak mem 9.324 GB\n",
      "Iter 1360: Train loss 1.401, Learning Rate 2.000e-04, It/sec 2.242, Tokens/sec 326.609, Trained Tokens 225382, Peak mem 9.324 GB\n",
      "Iter 1370: Train loss 1.327, Learning Rate 2.000e-04, It/sec 2.969, Tokens/sec 319.806, Trained Tokens 226459, Peak mem 9.324 GB\n",
      "Iter 1380: Train loss 1.517, Learning Rate 2.000e-04, It/sec 2.122, Tokens/sec 321.102, Trained Tokens 227972, Peak mem 9.324 GB\n",
      "Iter 1390: Train loss 1.520, Learning Rate 2.000e-04, It/sec 1.849, Tokens/sec 341.620, Trained Tokens 229820, Peak mem 9.324 GB\n",
      "Iter 1400: Val loss 2.183, Val took 2.711s\n",
      "Iter 1400: Train loss 1.800, Learning Rate 2.000e-04, It/sec 14.287, Tokens/sec 4043.331, Trained Tokens 232650, Peak mem 9.324 GB\n",
      "Iter 1400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001400_adapters.safetensors.\n",
      "Iter 1410: Train loss 1.418, Learning Rate 2.000e-04, It/sec 2.125, Tokens/sec 343.791, Trained Tokens 234268, Peak mem 9.324 GB\n",
      "Iter 1420: Train loss 1.593, Learning Rate 2.000e-04, It/sec 1.847, Tokens/sec 357.052, Trained Tokens 236201, Peak mem 9.324 GB\n",
      "Iter 1430: Train loss 1.396, Learning Rate 2.000e-04, It/sec 1.938, Tokens/sec 326.512, Trained Tokens 237886, Peak mem 9.324 GB\n",
      "Iter 1440: Train loss 1.581, Learning Rate 2.000e-04, It/sec 1.620, Tokens/sec 367.735, Trained Tokens 240156, Peak mem 9.324 GB\n",
      "Iter 1450: Val loss 2.499, Val took 3.541s\n",
      "Iter 1450: Train loss 1.314, Learning Rate 2.000e-04, It/sec 37.227, Tokens/sec 4500.760, Trained Tokens 241365, Peak mem 9.324 GB\n",
      "Iter 1460: Train loss 1.578, Learning Rate 2.000e-04, It/sec 1.974, Tokens/sec 354.401, Trained Tokens 243160, Peak mem 9.324 GB\n",
      "Iter 1470: Train loss 1.516, Learning Rate 2.000e-04, It/sec 1.955, Tokens/sec 362.758, Trained Tokens 245016, Peak mem 9.324 GB\n",
      "Iter 1480: Train loss 1.292, Learning Rate 2.000e-04, It/sec 2.754, Tokens/sec 340.653, Trained Tokens 246253, Peak mem 9.324 GB\n",
      "Iter 1490: Train loss 1.423, Learning Rate 2.000e-04, It/sec 2.294, Tokens/sec 359.441, Trained Tokens 247820, Peak mem 9.324 GB\n",
      "Iter 1500: Val loss 2.245, Val took 3.113s\n",
      "Iter 1500: Train loss 1.324, Learning Rate 2.000e-04, It/sec 31.017, Tokens/sec 3802.694, Trained Tokens 249046, Peak mem 9.324 GB\n",
      "Iter 1500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001500_adapters.safetensors.\n",
      "Iter 1510: Train loss 1.447, Learning Rate 2.000e-04, It/sec 2.330, Tokens/sec 319.910, Trained Tokens 250419, Peak mem 9.324 GB\n",
      "Iter 1520: Train loss 1.721, Learning Rate 2.000e-04, It/sec 2.408, Tokens/sec 357.372, Trained Tokens 251903, Peak mem 9.324 GB\n",
      "Iter 1530: Train loss 1.643, Learning Rate 2.000e-04, It/sec 2.228, Tokens/sec 349.614, Trained Tokens 253472, Peak mem 9.324 GB\n",
      "Iter 1540: Train loss 1.323, Learning Rate 2.000e-04, It/sec 2.779, Tokens/sec 344.000, Trained Tokens 254710, Peak mem 9.324 GB\n",
      "Iter 1550: Val loss 1.974, Val took 3.039s\n",
      "Iter 1550: Train loss 1.448, Learning Rate 2.000e-04, It/sec 40.140, Tokens/sec 6795.690, Trained Tokens 256403, Peak mem 9.324 GB\n",
      "Iter 1560: Train loss 1.507, Learning Rate 2.000e-04, It/sec 2.718, Tokens/sec 351.650, Trained Tokens 257697, Peak mem 9.324 GB\n",
      "Iter 1570: Train loss 1.511, Learning Rate 2.000e-04, It/sec 2.026, Tokens/sec 347.066, Trained Tokens 259410, Peak mem 9.324 GB\n",
      "Iter 1580: Train loss 1.609, Learning Rate 2.000e-04, It/sec 1.286, Tokens/sec 369.633, Trained Tokens 262284, Peak mem 9.324 GB\n",
      "Iter 1590: Train loss 1.671, Learning Rate 2.000e-04, It/sec 1.928, Tokens/sec 350.270, Trained Tokens 264101, Peak mem 9.324 GB\n",
      "Iter 1600: Val loss 1.981, Val took 2.733s\n",
      "Iter 1600: Train loss 1.599, Learning Rate 2.000e-04, It/sec 16.270, Tokens/sec 2678.089, Trained Tokens 265747, Peak mem 9.324 GB\n",
      "Iter 1600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001600_adapters.safetensors.\n",
      "Iter 1610: Train loss 1.597, Learning Rate 2.000e-04, It/sec 2.574, Tokens/sec 334.311, Trained Tokens 267046, Peak mem 9.324 GB\n",
      "Iter 1620: Train loss 1.704, Learning Rate 2.000e-04, It/sec 2.031, Tokens/sec 346.438, Trained Tokens 268752, Peak mem 9.324 GB\n",
      "Iter 1630: Train loss 1.466, Learning Rate 2.000e-04, It/sec 2.139, Tokens/sec 367.497, Trained Tokens 270470, Peak mem 9.324 GB\n",
      "Iter 1640: Train loss 1.484, Learning Rate 2.000e-04, It/sec 2.643, Tokens/sec 314.501, Trained Tokens 271660, Peak mem 9.324 GB\n",
      "Iter 1650: Val loss 2.045, Val took 2.877s\n",
      "Iter 1650: Train loss 1.327, Learning Rate 2.000e-04, It/sec 24.632, Tokens/sec 4465.859, Trained Tokens 273473, Peak mem 9.324 GB\n",
      "Iter 1660: Train loss 1.119, Learning Rate 2.000e-04, It/sec 2.823, Tokens/sec 348.302, Trained Tokens 274707, Peak mem 9.324 GB\n",
      "Iter 1670: Train loss 1.483, Learning Rate 2.000e-04, It/sec 1.717, Tokens/sec 368.521, Trained Tokens 276853, Peak mem 9.324 GB\n",
      "Iter 1680: Train loss 1.280, Learning Rate 2.000e-04, It/sec 1.721, Tokens/sec 356.958, Trained Tokens 278927, Peak mem 9.324 GB\n",
      "Iter 1690: Train loss 1.232, Learning Rate 2.000e-04, It/sec 2.323, Tokens/sec 348.618, Trained Tokens 280428, Peak mem 9.324 GB\n",
      "Iter 1700: Val loss 2.371, Val took 3.197s\n",
      "Iter 1700: Train loss 1.029, Learning Rate 2.000e-04, It/sec 21.133, Tokens/sec 2563.380, Trained Tokens 281641, Peak mem 9.324 GB\n",
      "Iter 1700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001700_adapters.safetensors.\n",
      "Iter 1710: Train loss 1.259, Learning Rate 2.000e-04, It/sec 1.819, Tokens/sec 335.054, Trained Tokens 283483, Peak mem 9.324 GB\n",
      "Iter 1720: Train loss 1.389, Learning Rate 2.000e-04, It/sec 2.089, Tokens/sec 350.395, Trained Tokens 285160, Peak mem 9.324 GB\n",
      "Iter 1730: Train loss 1.293, Learning Rate 2.000e-04, It/sec 2.682, Tokens/sec 340.099, Trained Tokens 286428, Peak mem 9.324 GB\n",
      "Iter 1740: Train loss 1.208, Learning Rate 2.000e-04, It/sec 2.416, Tokens/sec 353.226, Trained Tokens 287890, Peak mem 9.324 GB\n",
      "Iter 1750: Val loss 2.247, Val took 3.071s\n",
      "Iter 1750: Train loss 1.321, Learning Rate 2.000e-04, It/sec 39.233, Tokens/sec 7344.342, Trained Tokens 289762, Peak mem 9.324 GB\n",
      "Iter 1760: Train loss 1.495, Learning Rate 2.000e-04, It/sec 1.497, Tokens/sec 380.654, Trained Tokens 292305, Peak mem 9.324 GB\n",
      "Iter 1770: Train loss 1.234, Learning Rate 2.000e-04, It/sec 2.482, Tokens/sec 348.965, Trained Tokens 293711, Peak mem 9.324 GB\n",
      "Iter 1780: Train loss 1.208, Learning Rate 2.000e-04, It/sec 2.261, Tokens/sec 350.151, Trained Tokens 295260, Peak mem 9.324 GB\n",
      "Iter 1790: Train loss 1.158, Learning Rate 2.000e-04, It/sec 2.964, Tokens/sec 332.589, Trained Tokens 296382, Peak mem 9.324 GB\n",
      "Iter 1800: Val loss 2.204, Val took 3.685s\n",
      "Iter 1800: Train loss 1.332, Learning Rate 2.000e-04, It/sec 36.455, Tokens/sec 5920.366, Trained Tokens 298006, Peak mem 9.324 GB\n",
      "Iter 1800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001800_adapters.safetensors.\n",
      "Iter 1810: Train loss 1.149, Learning Rate 2.000e-04, It/sec 2.728, Tokens/sec 346.464, Trained Tokens 299276, Peak mem 9.324 GB\n",
      "Iter 1820: Train loss 1.311, Learning Rate 2.000e-04, It/sec 2.162, Tokens/sec 344.586, Trained Tokens 300870, Peak mem 9.324 GB\n",
      "Iter 1830: Train loss 1.331, Learning Rate 2.000e-04, It/sec 1.716, Tokens/sec 356.352, Trained Tokens 302947, Peak mem 9.324 GB\n",
      "Iter 1840: Train loss 1.296, Learning Rate 2.000e-04, It/sec 1.875, Tokens/sec 353.331, Trained Tokens 304831, Peak mem 9.324 GB\n",
      "Iter 1850: Val loss 2.166, Val took 3.524s\n",
      "Iter 1850: Train loss 1.441, Learning Rate 2.000e-04, It/sec 46.468, Tokens/sec 11630.881, Trained Tokens 307334, Peak mem 9.324 GB\n",
      "Iter 1860: Train loss 1.562, Learning Rate 2.000e-04, It/sec 1.264, Tokens/sec 364.807, Trained Tokens 310220, Peak mem 9.324 GB\n",
      "Iter 1870: Train loss 1.279, Learning Rate 2.000e-04, It/sec 2.240, Tokens/sec 357.557, Trained Tokens 311816, Peak mem 9.324 GB\n",
      "Iter 1880: Train loss 1.266, Learning Rate 2.000e-04, It/sec 2.112, Tokens/sec 350.103, Trained Tokens 313474, Peak mem 9.324 GB\n",
      "Iter 1890: Train loss 1.411, Learning Rate 2.000e-04, It/sec 2.339, Tokens/sec 351.600, Trained Tokens 314977, Peak mem 9.324 GB\n",
      "Iter 1900: Val loss 2.103, Val took 2.525s\n",
      "Iter 1900: Train loss 1.126, Learning Rate 2.000e-04, It/sec 39.414, Tokens/sec 3610.332, Trained Tokens 315893, Peak mem 9.324 GB\n",
      "Iter 1900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0001900_adapters.safetensors.\n",
      "Iter 1910: Train loss 1.543, Learning Rate 2.000e-04, It/sec 1.919, Tokens/sec 339.641, Trained Tokens 317663, Peak mem 9.324 GB\n",
      "Iter 1920: Train loss 1.461, Learning Rate 2.000e-04, It/sec 2.295, Tokens/sec 350.496, Trained Tokens 319190, Peak mem 9.324 GB\n",
      "Iter 1930: Train loss 1.253, Learning Rate 2.000e-04, It/sec 2.303, Tokens/sec 362.481, Trained Tokens 320764, Peak mem 9.324 GB\n",
      "Iter 1940: Train loss 1.247, Learning Rate 2.000e-04, It/sec 2.556, Tokens/sec 346.568, Trained Tokens 322120, Peak mem 9.324 GB\n",
      "Iter 1950: Val loss 1.880, Val took 2.933s\n",
      "Iter 1950: Train loss 1.543, Learning Rate 2.000e-04, It/sec 16.543, Tokens/sec 3030.688, Trained Tokens 323952, Peak mem 9.324 GB\n",
      "Iter 1960: Train loss 1.357, Learning Rate 2.000e-04, It/sec 2.189, Tokens/sec 319.141, Trained Tokens 325410, Peak mem 9.324 GB\n",
      "Iter 1970: Train loss 1.402, Learning Rate 2.000e-04, It/sec 2.660, Tokens/sec 343.402, Trained Tokens 326701, Peak mem 9.324 GB\n",
      "Iter 1980: Train loss 1.588, Learning Rate 2.000e-04, It/sec 1.595, Tokens/sec 358.292, Trained Tokens 328947, Peak mem 9.324 GB\n",
      "Iter 1990: Train loss 1.385, Learning Rate 2.000e-04, It/sec 2.553, Tokens/sec 349.029, Trained Tokens 330314, Peak mem 9.324 GB\n",
      "Iter 2000: Val loss 2.165, Val took 3.084s\n",
      "Iter 2000: Train loss 1.447, Learning Rate 2.000e-04, It/sec 40.147, Tokens/sec 5821.256, Trained Tokens 331764, Peak mem 9.324 GB\n",
      "Iter 2000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002000_adapters.safetensors.\n",
      "Iter 2010: Train loss 1.327, Learning Rate 2.000e-04, It/sec 2.435, Tokens/sec 350.174, Trained Tokens 333202, Peak mem 9.324 GB\n",
      "Iter 2020: Train loss 1.276, Learning Rate 2.000e-04, It/sec 2.108, Tokens/sec 362.097, Trained Tokens 334920, Peak mem 9.324 GB\n",
      "Iter 2030: Train loss 1.461, Learning Rate 2.000e-04, It/sec 2.065, Tokens/sec 348.367, Trained Tokens 336607, Peak mem 9.324 GB\n",
      "Iter 2040: Train loss 1.314, Learning Rate 2.000e-04, It/sec 2.159, Tokens/sec 358.144, Trained Tokens 338266, Peak mem 9.324 GB\n",
      "Iter 2050: Val loss 2.193, Val took 4.647s\n",
      "Iter 2050: Train loss 1.217, Learning Rate 2.000e-04, It/sec 39.937, Tokens/sec 5227.693, Trained Tokens 339575, Peak mem 9.324 GB\n",
      "Iter 2060: Train loss 1.148, Learning Rate 2.000e-04, It/sec 1.553, Tokens/sec 359.093, Trained Tokens 341888, Peak mem 9.324 GB\n",
      "Iter 2070: Train loss 1.144, Learning Rate 2.000e-04, It/sec 2.334, Tokens/sec 346.401, Trained Tokens 343372, Peak mem 9.324 GB\n",
      "Iter 2080: Train loss 1.045, Learning Rate 2.000e-04, It/sec 2.224, Tokens/sec 350.010, Trained Tokens 344946, Peak mem 9.324 GB\n",
      "Iter 2090: Train loss 1.257, Learning Rate 2.000e-04, It/sec 1.692, Tokens/sec 364.047, Trained Tokens 347098, Peak mem 9.324 GB\n",
      "Iter 2100: Val loss 2.242, Val took 3.061s\n",
      "Iter 2100: Train loss 1.156, Learning Rate 2.000e-04, It/sec 17.609, Tokens/sec 2916.003, Trained Tokens 348754, Peak mem 9.324 GB\n",
      "Iter 2100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002100_adapters.safetensors.\n",
      "Iter 2110: Train loss 1.252, Learning Rate 2.000e-04, It/sec 1.758, Tokens/sec 342.041, Trained Tokens 350700, Peak mem 9.324 GB\n",
      "Iter 2120: Train loss 1.089, Learning Rate 2.000e-04, It/sec 2.457, Tokens/sec 357.292, Trained Tokens 352154, Peak mem 9.324 GB\n",
      "Iter 2130: Train loss 1.173, Learning Rate 2.000e-04, It/sec 2.064, Tokens/sec 346.534, Trained Tokens 353833, Peak mem 9.324 GB\n",
      "Iter 2140: Train loss 1.133, Learning Rate 2.000e-04, It/sec 2.327, Tokens/sec 326.506, Trained Tokens 355236, Peak mem 9.324 GB\n",
      "Iter 2150: Val loss 2.248, Val took 3.526s\n",
      "Iter 2150: Train loss 1.165, Learning Rate 2.000e-04, It/sec 22.874, Tokens/sec 4254.536, Trained Tokens 357096, Peak mem 9.324 GB\n",
      "Iter 2160: Train loss 1.134, Learning Rate 2.000e-04, It/sec 2.161, Tokens/sec 312.501, Trained Tokens 358542, Peak mem 9.324 GB\n",
      "Iter 2170: Train loss 1.131, Learning Rate 2.000e-04, It/sec 2.597, Tokens/sec 329.004, Trained Tokens 359809, Peak mem 9.324 GB\n",
      "Iter 2180: Train loss 1.268, Learning Rate 2.000e-04, It/sec 2.188, Tokens/sec 358.905, Trained Tokens 361449, Peak mem 9.324 GB\n",
      "Iter 2190: Train loss 1.412, Learning Rate 2.000e-04, It/sec 2.049, Tokens/sec 363.094, Trained Tokens 363221, Peak mem 9.324 GB\n",
      "Iter 2200: Val loss 2.342, Val took 3.193s\n",
      "Iter 2200: Train loss 1.249, Learning Rate 2.000e-04, It/sec 55.159, Tokens/sec 9663.830, Trained Tokens 364973, Peak mem 9.324 GB\n",
      "Iter 2200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002200_adapters.safetensors.\n",
      "Iter 2210: Train loss 1.213, Learning Rate 2.000e-04, It/sec 2.061, Tokens/sec 346.628, Trained Tokens 366655, Peak mem 9.324 GB\n",
      "Iter 2220: Train loss 1.236, Learning Rate 2.000e-04, It/sec 1.893, Tokens/sec 354.245, Trained Tokens 368526, Peak mem 9.324 GB\n",
      "Iter 2230: Train loss 1.142, Learning Rate 2.000e-04, It/sec 2.297, Tokens/sec 351.643, Trained Tokens 370057, Peak mem 9.324 GB\n",
      "Iter 2240: Train loss 1.087, Learning Rate 2.000e-04, It/sec 1.575, Tokens/sec 366.572, Trained Tokens 372384, Peak mem 9.324 GB\n",
      "Iter 2250: Val loss 2.280, Val took 3.412s\n",
      "Iter 2250: Train loss 1.143, Learning Rate 2.000e-04, It/sec 24.923, Tokens/sec 3377.091, Trained Tokens 373739, Peak mem 9.324 GB\n",
      "Iter 2260: Train loss 1.184, Learning Rate 2.000e-04, It/sec 1.922, Tokens/sec 282.606, Trained Tokens 375209, Peak mem 9.324 GB\n",
      "Iter 2270: Train loss 0.953, Learning Rate 2.000e-04, It/sec 2.944, Tokens/sec 338.873, Trained Tokens 376360, Peak mem 9.324 GB\n",
      "Iter 2280: Train loss 1.180, Learning Rate 2.000e-04, It/sec 2.016, Tokens/sec 358.658, Trained Tokens 378139, Peak mem 9.324 GB\n",
      "Iter 2290: Train loss 1.346, Learning Rate 2.000e-04, It/sec 1.942, Tokens/sec 370.508, Trained Tokens 380047, Peak mem 9.324 GB\n",
      "Iter 2300: Val loss 2.136, Val took 3.261s\n",
      "Iter 2300: Train loss 1.141, Learning Rate 2.000e-04, It/sec 23.979, Tokens/sec 3249.132, Trained Tokens 381402, Peak mem 9.324 GB\n",
      "Iter 2300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002300_adapters.safetensors.\n",
      "Iter 2310: Train loss 1.293, Learning Rate 2.000e-04, It/sec 2.106, Tokens/sec 336.795, Trained Tokens 383001, Peak mem 9.324 GB\n",
      "Iter 2320: Train loss 1.024, Learning Rate 2.000e-04, It/sec 2.957, Tokens/sec 330.647, Trained Tokens 384119, Peak mem 9.324 GB\n",
      "Iter 2330: Train loss 1.065, Learning Rate 2.000e-04, It/sec 2.086, Tokens/sec 341.856, Trained Tokens 385758, Peak mem 9.324 GB\n",
      "Iter 2340: Train loss 1.065, Learning Rate 2.000e-04, It/sec 2.489, Tokens/sec 338.204, Trained Tokens 387117, Peak mem 9.324 GB\n",
      "Iter 2350: Val loss 2.263, Val took 3.113s\n",
      "Iter 2350: Train loss 1.450, Learning Rate 2.000e-04, It/sec 24.345, Tokens/sec 5467.824, Trained Tokens 389363, Peak mem 9.324 GB\n",
      "Iter 2360: Train loss 1.249, Learning Rate 2.000e-04, It/sec 2.211, Tokens/sec 326.596, Trained Tokens 390840, Peak mem 9.324 GB\n",
      "Iter 2370: Train loss 1.338, Learning Rate 2.000e-04, It/sec 1.602, Tokens/sec 350.218, Trained Tokens 393026, Peak mem 9.324 GB\n",
      "Iter 2380: Train loss 1.319, Learning Rate 2.000e-04, It/sec 1.857, Tokens/sec 363.744, Trained Tokens 394985, Peak mem 9.324 GB\n",
      "Iter 2390: Train loss 1.204, Learning Rate 2.000e-04, It/sec 1.772, Tokens/sec 338.426, Trained Tokens 396895, Peak mem 9.324 GB\n",
      "Iter 2400: Val loss 2.189, Val took 3.332s\n",
      "Iter 2400: Train loss 1.167, Learning Rate 2.000e-04, It/sec 16.826, Tokens/sec 2666.941, Trained Tokens 398480, Peak mem 9.324 GB\n",
      "Iter 2400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002400_adapters.safetensors.\n",
      "Iter 2410: Train loss 1.025, Learning Rate 2.000e-04, It/sec 2.332, Tokens/sec 290.333, Trained Tokens 399725, Peak mem 9.324 GB\n",
      "Iter 2420: Train loss 1.297, Learning Rate 2.000e-04, It/sec 1.938, Tokens/sec 338.112, Trained Tokens 401470, Peak mem 9.324 GB\n",
      "Iter 2430: Train loss 1.126, Learning Rate 2.000e-04, It/sec 2.796, Tokens/sec 333.016, Trained Tokens 402661, Peak mem 9.324 GB\n",
      "Iter 2440: Train loss 1.104, Learning Rate 2.000e-04, It/sec 2.519, Tokens/sec 325.140, Trained Tokens 403952, Peak mem 9.324 GB\n",
      "Iter 2450: Val loss 2.154, Val took 3.232s\n",
      "Iter 2450: Train loss 1.230, Learning Rate 2.000e-04, It/sec 31.550, Tokens/sec 5679.054, Trained Tokens 405752, Peak mem 9.324 GB\n",
      "Iter 2460: Train loss 1.219, Learning Rate 2.000e-04, It/sec 1.738, Tokens/sec 302.115, Trained Tokens 407490, Peak mem 9.324 GB\n",
      "Iter 2470: Train loss 1.059, Learning Rate 2.000e-04, It/sec 2.245, Tokens/sec 335.217, Trained Tokens 408983, Peak mem 9.324 GB\n",
      "Iter 2480: Train loss 1.135, Learning Rate 2.000e-04, It/sec 1.936, Tokens/sec 369.117, Trained Tokens 410890, Peak mem 9.324 GB\n",
      "Iter 2490: Train loss 0.954, Learning Rate 2.000e-04, It/sec 2.170, Tokens/sec 347.201, Trained Tokens 412490, Peak mem 9.324 GB\n",
      "Iter 2500: Val loss 2.295, Val took 3.642s\n",
      "Iter 2500: Train loss 0.970, Learning Rate 2.000e-04, It/sec 34.244, Tokens/sec 5379.791, Trained Tokens 414061, Peak mem 9.324 GB\n",
      "Iter 2500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002500_adapters.safetensors.\n",
      "Iter 2510: Train loss 1.131, Learning Rate 2.000e-04, It/sec 1.759, Tokens/sec 348.746, Trained Tokens 416044, Peak mem 9.324 GB\n",
      "Iter 2520: Train loss 1.024, Learning Rate 2.000e-04, It/sec 2.010, Tokens/sec 357.857, Trained Tokens 417824, Peak mem 9.324 GB\n",
      "Iter 2530: Train loss 0.966, Learning Rate 2.000e-04, It/sec 2.431, Tokens/sec 338.187, Trained Tokens 419215, Peak mem 9.324 GB\n",
      "Iter 2540: Train loss 1.152, Learning Rate 2.000e-04, It/sec 1.585, Tokens/sec 372.425, Trained Tokens 421564, Peak mem 9.324 GB\n",
      "Iter 2550: Val loss 2.128, Val took 3.534s\n",
      "Iter 2550: Train loss 0.968, Learning Rate 2.000e-04, It/sec 54.330, Tokens/sec 5921.985, Trained Tokens 422654, Peak mem 9.324 GB\n",
      "Iter 2560: Train loss 1.028, Learning Rate 2.000e-04, It/sec 1.826, Tokens/sec 339.739, Trained Tokens 424515, Peak mem 9.324 GB\n",
      "Iter 2570: Train loss 0.950, Learning Rate 2.000e-04, It/sec 2.134, Tokens/sec 360.873, Trained Tokens 426206, Peak mem 9.324 GB\n",
      "Iter 2580: Train loss 1.371, Learning Rate 2.000e-04, It/sec 1.426, Tokens/sec 378.584, Trained Tokens 428861, Peak mem 9.324 GB\n",
      "Iter 2590: Train loss 1.083, Learning Rate 2.000e-04, It/sec 2.453, Tokens/sec 330.614, Trained Tokens 430209, Peak mem 9.324 GB\n",
      "Iter 2600: Val loss 2.376, Val took 3.458s\n",
      "Iter 2600: Train loss 1.003, Learning Rate 2.000e-04, It/sec 4.112, Tokens/sec 647.178, Trained Tokens 431783, Peak mem 9.324 GB\n",
      "Iter 2600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002600_adapters.safetensors.\n",
      "Iter 2610: Train loss 1.005, Learning Rate 2.000e-04, It/sec 2.271, Tokens/sec 337.052, Trained Tokens 433267, Peak mem 9.324 GB\n",
      "Iter 2620: Train loss 1.071, Learning Rate 2.000e-04, It/sec 2.179, Tokens/sec 343.402, Trained Tokens 434843, Peak mem 9.324 GB\n",
      "Iter 2630: Train loss 1.181, Learning Rate 2.000e-04, It/sec 2.040, Tokens/sec 350.824, Trained Tokens 436563, Peak mem 9.324 GB\n",
      "Iter 2640: Train loss 1.112, Learning Rate 2.000e-04, It/sec 2.627, Tokens/sec 352.270, Trained Tokens 437904, Peak mem 9.324 GB\n",
      "Iter 2650: Val loss 2.301, Val took 3.152s\n",
      "Iter 2650: Train loss 1.026, Learning Rate 2.000e-04, It/sec 31.401, Tokens/sec 4609.632, Trained Tokens 439372, Peak mem 9.324 GB\n",
      "Iter 2660: Train loss 1.203, Learning Rate 2.000e-04, It/sec 1.965, Tokens/sec 362.433, Trained Tokens 441216, Peak mem 9.324 GB\n",
      "Iter 2670: Train loss 0.939, Learning Rate 2.000e-04, It/sec 2.336, Tokens/sec 332.440, Trained Tokens 442639, Peak mem 9.324 GB\n",
      "Iter 2680: Train loss 0.972, Learning Rate 2.000e-04, It/sec 2.640, Tokens/sec 345.535, Trained Tokens 443948, Peak mem 9.324 GB\n",
      "Iter 2690: Train loss 1.124, Learning Rate 2.000e-04, It/sec 1.691, Tokens/sec 379.245, Trained Tokens 446191, Peak mem 9.324 GB\n",
      "Iter 2700: Val loss 2.227, Val took 3.118s\n",
      "Iter 2700: Train loss 0.967, Learning Rate 2.000e-04, It/sec 38.973, Tokens/sec 5713.452, Trained Tokens 447657, Peak mem 9.324 GB\n",
      "Iter 2700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002700_adapters.safetensors.\n",
      "Iter 2710: Train loss 0.926, Learning Rate 2.000e-04, It/sec 2.708, Tokens/sec 335.491, Trained Tokens 448896, Peak mem 9.324 GB\n",
      "Iter 2720: Train loss 0.942, Learning Rate 2.000e-04, It/sec 3.009, Tokens/sec 335.246, Trained Tokens 450010, Peak mem 9.324 GB\n",
      "Iter 2730: Train loss 1.191, Learning Rate 2.000e-04, It/sec 1.696, Tokens/sec 361.331, Trained Tokens 452141, Peak mem 9.324 GB\n",
      "Iter 2740: Train loss 1.045, Learning Rate 2.000e-04, It/sec 1.747, Tokens/sec 356.673, Trained Tokens 454183, Peak mem 9.324 GB\n",
      "Iter 2750: Val loss 1.998, Val took 2.846s\n",
      "Iter 2750: Train loss 1.020, Learning Rate 2.000e-04, It/sec 39.024, Tokens/sec 5240.969, Trained Tokens 455526, Peak mem 9.324 GB\n",
      "Iter 2760: Train loss 1.036, Learning Rate 2.000e-04, It/sec 2.737, Tokens/sec 346.011, Trained Tokens 456790, Peak mem 9.324 GB\n",
      "Iter 2770: Train loss 1.076, Learning Rate 2.000e-04, It/sec 2.549, Tokens/sec 339.209, Trained Tokens 458121, Peak mem 9.324 GB\n",
      "Iter 2780: Train loss 1.129, Learning Rate 2.000e-04, It/sec 1.777, Tokens/sec 364.750, Trained Tokens 460174, Peak mem 9.324 GB\n",
      "Iter 2790: Train loss 1.028, Learning Rate 2.000e-04, It/sec 1.462, Tokens/sec 370.586, Trained Tokens 462709, Peak mem 9.324 GB\n",
      "Iter 2800: Val loss 2.173, Val took 2.941s\n",
      "Iter 2800: Train loss 1.121, Learning Rate 2.000e-04, It/sec 16.822, Tokens/sec 2960.686, Trained Tokens 464469, Peak mem 9.324 GB\n",
      "Iter 2800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002800_adapters.safetensors.\n",
      "Iter 2810: Train loss 0.950, Learning Rate 2.000e-04, It/sec 2.530, Tokens/sec 343.126, Trained Tokens 465825, Peak mem 9.324 GB\n",
      "Iter 2820: Train loss 1.239, Learning Rate 2.000e-04, It/sec 2.134, Tokens/sec 351.187, Trained Tokens 467471, Peak mem 9.324 GB\n",
      "Iter 2830: Train loss 1.053, Learning Rate 2.000e-04, It/sec 1.953, Tokens/sec 369.594, Trained Tokens 469363, Peak mem 9.324 GB\n",
      "Iter 2840: Train loss 1.099, Learning Rate 2.000e-04, It/sec 2.192, Tokens/sec 347.248, Trained Tokens 470947, Peak mem 9.324 GB\n",
      "Iter 2850: Val loss 2.092, Val took 3.378s\n",
      "Iter 2850: Train loss 1.291, Learning Rate 2.000e-04, It/sec 21.589, Tokens/sec 4218.451, Trained Tokens 472901, Peak mem 9.324 GB\n",
      "Iter 2860: Train loss 1.111, Learning Rate 2.000e-04, It/sec 2.253, Tokens/sec 331.398, Trained Tokens 474372, Peak mem 9.324 GB\n",
      "Iter 2870: Train loss 0.942, Learning Rate 2.000e-04, It/sec 3.065, Tokens/sec 316.581, Trained Tokens 475405, Peak mem 9.324 GB\n",
      "Iter 2880: Train loss 0.761, Learning Rate 2.000e-04, It/sec 2.643, Tokens/sec 328.775, Trained Tokens 476649, Peak mem 9.324 GB\n",
      "Iter 2890: Train loss 0.794, Learning Rate 2.000e-04, It/sec 2.661, Tokens/sec 323.098, Trained Tokens 477863, Peak mem 9.324 GB\n",
      "Iter 2900: Val loss 2.487, Val took 3.826s\n",
      "Iter 2900: Train loss 0.866, Learning Rate 2.000e-04, It/sec 13.799, Tokens/sec 2443.775, Trained Tokens 479634, Peak mem 9.324 GB\n",
      "Iter 2900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0002900_adapters.safetensors.\n",
      "Iter 2910: Train loss 0.934, Learning Rate 2.000e-04, It/sec 1.993, Tokens/sec 358.672, Trained Tokens 481434, Peak mem 9.324 GB\n",
      "Iter 2920: Train loss 0.862, Learning Rate 2.000e-04, It/sec 2.687, Tokens/sec 315.973, Trained Tokens 482610, Peak mem 9.324 GB\n",
      "Iter 2930: Train loss 0.869, Learning Rate 2.000e-04, It/sec 2.326, Tokens/sec 344.961, Trained Tokens 484093, Peak mem 9.324 GB\n",
      "Iter 2940: Train loss 1.084, Learning Rate 2.000e-04, It/sec 1.820, Tokens/sec 359.191, Trained Tokens 486067, Peak mem 9.324 GB\n",
      "Iter 2950: Val loss 2.113, Val took 3.510s\n",
      "Iter 2950: Train loss 0.974, Learning Rate 2.000e-04, It/sec 28.195, Tokens/sec 6673.788, Trained Tokens 488434, Peak mem 9.324 GB\n",
      "Iter 2960: Train loss 0.976, Learning Rate 2.000e-04, It/sec 1.972, Tokens/sec 334.581, Trained Tokens 490131, Peak mem 9.324 GB\n",
      "Iter 2970: Train loss 0.981, Learning Rate 2.000e-04, It/sec 1.856, Tokens/sec 360.446, Trained Tokens 492073, Peak mem 9.324 GB\n",
      "Iter 2980: Train loss 0.914, Learning Rate 2.000e-04, It/sec 2.235, Tokens/sec 343.565, Trained Tokens 493610, Peak mem 9.324 GB\n",
      "Iter 2990: Train loss 0.800, Learning Rate 2.000e-04, It/sec 2.119, Tokens/sec 335.916, Trained Tokens 495195, Peak mem 9.324 GB\n",
      "Iter 3000: Val loss 2.152, Val took 3.587s\n",
      "Iter 3000: Train loss 0.817, Learning Rate 2.000e-04, It/sec 23.630, Tokens/sec 2752.942, Trained Tokens 496360, Peak mem 9.324 GB\n",
      "Iter 3000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003000_adapters.safetensors.\n",
      "Iter 3010: Train loss 1.078, Learning Rate 2.000e-04, It/sec 1.573, Tokens/sec 358.857, Trained Tokens 498642, Peak mem 9.324 GB\n",
      "Iter 3020: Train loss 0.803, Learning Rate 2.000e-04, It/sec 2.668, Tokens/sec 345.218, Trained Tokens 499936, Peak mem 9.324 GB\n",
      "Iter 3030: Train loss 0.954, Learning Rate 2.000e-04, It/sec 2.080, Tokens/sec 330.106, Trained Tokens 501523, Peak mem 9.324 GB\n",
      "Iter 3040: Train loss 0.996, Learning Rate 2.000e-04, It/sec 1.268, Tokens/sec 367.077, Trained Tokens 504418, Peak mem 9.324 GB\n",
      "Iter 3050: Val loss 2.257, Val took 3.012s\n",
      "Iter 3050: Train loss 1.137, Learning Rate 2.000e-04, It/sec 8.465, Tokens/sec 1801.450, Trained Tokens 506546, Peak mem 9.324 GB\n",
      "Iter 3060: Train loss 0.987, Learning Rate 2.000e-04, It/sec 1.844, Tokens/sec 333.866, Trained Tokens 508357, Peak mem 9.324 GB\n",
      "Iter 3070: Train loss 0.970, Learning Rate 2.000e-04, It/sec 1.784, Tokens/sec 330.412, Trained Tokens 510209, Peak mem 9.324 GB\n",
      "Iter 3080: Train loss 0.985, Learning Rate 2.000e-04, It/sec 2.282, Tokens/sec 345.915, Trained Tokens 511725, Peak mem 9.324 GB\n",
      "Iter 3090: Train loss 1.072, Learning Rate 2.000e-04, It/sec 2.242, Tokens/sec 355.649, Trained Tokens 513311, Peak mem 9.324 GB\n",
      "Iter 3100: Val loss 2.176, Val took 2.987s\n",
      "Iter 3100: Train loss 0.926, Learning Rate 2.000e-04, It/sec 40.388, Tokens/sec 5113.071, Trained Tokens 514577, Peak mem 9.324 GB\n",
      "Iter 3100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003100_adapters.safetensors.\n",
      "Iter 3110: Train loss 0.943, Learning Rate 2.000e-04, It/sec 2.381, Tokens/sec 342.214, Trained Tokens 516014, Peak mem 9.324 GB\n",
      "Iter 3120: Train loss 0.962, Learning Rate 2.000e-04, It/sec 2.152, Tokens/sec 337.926, Trained Tokens 517584, Peak mem 9.324 GB\n",
      "Iter 3130: Train loss 1.123, Learning Rate 2.000e-04, It/sec 2.041, Tokens/sec 333.566, Trained Tokens 519218, Peak mem 9.324 GB\n",
      "Iter 3140: Train loss 0.945, Learning Rate 2.000e-04, It/sec 2.578, Tokens/sec 330.447, Trained Tokens 520500, Peak mem 9.324 GB\n",
      "Iter 3150: Val loss 2.066, Val took 2.938s\n",
      "Iter 3150: Train loss 1.019, Learning Rate 2.000e-04, It/sec 30.277, Tokens/sec 4838.201, Trained Tokens 522098, Peak mem 9.324 GB\n",
      "Iter 3160: Train loss 1.162, Learning Rate 2.000e-04, It/sec 1.756, Tokens/sec 342.165, Trained Tokens 524047, Peak mem 9.324 GB\n",
      "Iter 3170: Train loss 1.028, Learning Rate 2.000e-04, It/sec 2.342, Tokens/sec 346.210, Trained Tokens 525525, Peak mem 9.324 GB\n",
      "Iter 3180: Train loss 0.958, Learning Rate 2.000e-04, It/sec 2.184, Tokens/sec 312.364, Trained Tokens 526955, Peak mem 9.324 GB\n",
      "Iter 3190: Train loss 0.910, Learning Rate 2.000e-04, It/sec 2.746, Tokens/sec 332.228, Trained Tokens 528165, Peak mem 9.324 GB\n",
      "Iter 3200: Val loss 2.410, Val took 3.830s\n",
      "Iter 3200: Train loss 1.063, Learning Rate 2.000e-04, It/sec 14.500, Tokens/sec 2460.607, Trained Tokens 529862, Peak mem 9.324 GB\n",
      "Iter 3200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003200_adapters.safetensors.\n",
      "Iter 3210: Train loss 0.968, Learning Rate 2.000e-04, It/sec 2.288, Tokens/sec 338.460, Trained Tokens 531341, Peak mem 9.324 GB\n",
      "Iter 3220: Train loss 1.037, Learning Rate 2.000e-04, It/sec 1.904, Tokens/sec 362.544, Trained Tokens 533245, Peak mem 9.324 GB\n",
      "Iter 3230: Train loss 0.980, Learning Rate 2.000e-04, It/sec 2.072, Tokens/sec 315.819, Trained Tokens 534769, Peak mem 9.324 GB\n",
      "Iter 3240: Train loss 1.090, Learning Rate 2.000e-04, It/sec 2.153, Tokens/sec 326.667, Trained Tokens 536286, Peak mem 9.324 GB\n",
      "Iter 3250: Val loss 2.407, Val took 3.225s\n",
      "Iter 3250: Train loss 0.915, Learning Rate 2.000e-04, It/sec 37.139, Tokens/sec 4631.214, Trained Tokens 537533, Peak mem 9.324 GB\n",
      "Iter 3260: Train loss 0.902, Learning Rate 2.000e-04, It/sec 2.455, Tokens/sec 319.927, Trained Tokens 538836, Peak mem 9.324 GB\n",
      "Iter 3270: Train loss 1.408, Learning Rate 2.000e-04, It/sec 1.500, Tokens/sec 363.328, Trained Tokens 541258, Peak mem 9.324 GB\n",
      "Iter 3280: Train loss 1.069, Learning Rate 2.000e-04, It/sec 1.723, Tokens/sec 355.335, Trained Tokens 543320, Peak mem 9.324 GB\n",
      "Iter 3290: Train loss 0.763, Learning Rate 2.000e-04, It/sec 2.120, Tokens/sec 357.203, Trained Tokens 545005, Peak mem 9.324 GB\n",
      "Iter 3300: Val loss 2.218, Val took 3.069s\n",
      "Iter 3300: Train loss 0.935, Learning Rate 2.000e-04, It/sec 23.658, Tokens/sec 4551.769, Trained Tokens 546929, Peak mem 9.324 GB\n",
      "Iter 3300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003300_adapters.safetensors.\n",
      "Iter 3310: Train loss 0.932, Learning Rate 2.000e-04, It/sec 1.783, Tokens/sec 300.285, Trained Tokens 548613, Peak mem 9.324 GB\n",
      "Iter 3320: Train loss 0.885, Learning Rate 2.000e-04, It/sec 2.331, Tokens/sec 352.030, Trained Tokens 550123, Peak mem 9.324 GB\n",
      "Iter 3330: Train loss 0.763, Learning Rate 2.000e-04, It/sec 2.511, Tokens/sec 338.194, Trained Tokens 551470, Peak mem 9.324 GB\n",
      "Iter 3340: Train loss 0.935, Learning Rate 2.000e-04, It/sec 1.076, Tokens/sec 350.423, Trained Tokens 554728, Peak mem 9.324 GB\n",
      "Iter 3350: Val loss 2.380, Val took 3.762s\n",
      "Iter 3350: Train loss 0.824, Learning Rate 2.000e-04, It/sec 8.507, Tokens/sec 1292.186, Trained Tokens 556247, Peak mem 9.324 GB\n",
      "Iter 3360: Train loss 0.690, Learning Rate 2.000e-04, It/sec 2.482, Tokens/sec 348.526, Trained Tokens 557651, Peak mem 9.324 GB\n",
      "Iter 3370: Train loss 0.834, Learning Rate 2.000e-04, It/sec 2.196, Tokens/sec 339.941, Trained Tokens 559199, Peak mem 9.324 GB\n",
      "Iter 3380: Train loss 0.842, Learning Rate 2.000e-04, It/sec 2.159, Tokens/sec 346.243, Trained Tokens 560803, Peak mem 9.324 GB\n",
      "Iter 3390: Train loss 0.844, Learning Rate 2.000e-04, It/sec 2.365, Tokens/sec 342.441, Trained Tokens 562251, Peak mem 9.324 GB\n",
      "Iter 3400: Val loss 2.133, Val took 3.348s\n",
      "Iter 3400: Train loss 0.961, Learning Rate 2.000e-04, It/sec 8.525, Tokens/sec 1282.088, Trained Tokens 563755, Peak mem 9.324 GB\n",
      "Iter 3400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003400_adapters.safetensors.\n",
      "Iter 3410: Train loss 0.876, Learning Rate 2.000e-04, It/sec 2.138, Tokens/sec 340.952, Trained Tokens 565350, Peak mem 9.324 GB\n",
      "Iter 3420: Train loss 1.068, Learning Rate 2.000e-04, It/sec 1.837, Tokens/sec 369.025, Trained Tokens 567359, Peak mem 9.324 GB\n",
      "Iter 3430: Train loss 0.913, Learning Rate 2.000e-04, It/sec 2.270, Tokens/sec 330.726, Trained Tokens 568816, Peak mem 9.324 GB\n",
      "Iter 3440: Train loss 1.012, Learning Rate 2.000e-04, It/sec 2.037, Tokens/sec 343.451, Trained Tokens 570502, Peak mem 9.324 GB\n",
      "Iter 3450: Val loss 1.982, Val took 3.073s\n",
      "Iter 3450: Train loss 1.052, Learning Rate 2.000e-04, It/sec 4.115, Tokens/sec 920.557, Trained Tokens 572739, Peak mem 9.324 GB\n",
      "Iter 3460: Train loss 0.786, Learning Rate 2.000e-04, It/sec 2.074, Tokens/sec 337.860, Trained Tokens 574368, Peak mem 9.324 GB\n",
      "Iter 3470: Train loss 0.897, Learning Rate 2.000e-04, It/sec 2.193, Tokens/sec 358.723, Trained Tokens 576004, Peak mem 9.324 GB\n",
      "Iter 3480: Train loss 0.940, Learning Rate 2.000e-04, It/sec 2.119, Tokens/sec 354.519, Trained Tokens 577677, Peak mem 9.324 GB\n",
      "Iter 3490: Train loss 0.809, Learning Rate 2.000e-04, It/sec 2.412, Tokens/sec 350.407, Trained Tokens 579130, Peak mem 9.324 GB\n",
      "Iter 3500: Val loss 2.380, Val took 3.777s\n",
      "Iter 3500: Train loss 0.946, Learning Rate 2.000e-04, It/sec 14.007, Tokens/sec 2224.351, Trained Tokens 580718, Peak mem 9.324 GB\n",
      "Iter 3500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003500_adapters.safetensors.\n",
      "Iter 3510: Train loss 0.902, Learning Rate 2.000e-04, It/sec 2.121, Tokens/sec 303.453, Trained Tokens 582149, Peak mem 9.324 GB\n",
      "Iter 3520: Train loss 1.002, Learning Rate 2.000e-04, It/sec 2.187, Tokens/sec 340.515, Trained Tokens 583706, Peak mem 9.324 GB\n",
      "Iter 3530: Train loss 0.762, Learning Rate 2.000e-04, It/sec 3.539, Tokens/sec 303.992, Trained Tokens 584565, Peak mem 9.324 GB\n",
      "Iter 3540: Train loss 1.009, Learning Rate 2.000e-04, It/sec 1.302, Tokens/sec 366.968, Trained Tokens 587383, Peak mem 9.324 GB\n",
      "Iter 3550: Val loss 1.971, Val took 3.482s\n",
      "Iter 3550: Train loss 0.989, Learning Rate 2.000e-04, It/sec 5.014, Tokens/sec 964.642, Trained Tokens 589307, Peak mem 9.324 GB\n",
      "Iter 3560: Train loss 0.884, Learning Rate 2.000e-04, It/sec 2.462, Tokens/sec 326.413, Trained Tokens 590633, Peak mem 9.324 GB\n",
      "Iter 3570: Train loss 0.721, Learning Rate 2.000e-04, It/sec 2.600, Tokens/sec 339.253, Trained Tokens 591938, Peak mem 9.324 GB\n",
      "Iter 3580: Train loss 0.933, Learning Rate 2.000e-04, It/sec 2.127, Tokens/sec 348.995, Trained Tokens 593579, Peak mem 9.324 GB\n",
      "Iter 3590: Train loss 1.066, Learning Rate 2.000e-04, It/sec 1.966, Tokens/sec 365.102, Trained Tokens 595436, Peak mem 9.324 GB\n",
      "Iter 3600: Val loss 2.267, Val took 3.327s\n",
      "Iter 3600: Train loss 0.845, Learning Rate 2.000e-04, It/sec 24.513, Tokens/sec 3358.241, Trained Tokens 596806, Peak mem 9.324 GB\n",
      "Iter 3600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003600_adapters.safetensors.\n",
      "Iter 3610: Train loss 0.919, Learning Rate 2.000e-04, It/sec 1.926, Tokens/sec 298.881, Trained Tokens 598358, Peak mem 9.324 GB\n",
      "Iter 3620: Train loss 0.780, Learning Rate 2.000e-04, It/sec 2.443, Tokens/sec 320.778, Trained Tokens 599671, Peak mem 9.324 GB\n",
      "Iter 3630: Train loss 1.070, Learning Rate 2.000e-04, It/sec 1.436, Tokens/sec 369.867, Trained Tokens 602247, Peak mem 9.324 GB\n",
      "Iter 3640: Train loss 1.019, Learning Rate 2.000e-04, It/sec 1.903, Tokens/sec 351.865, Trained Tokens 604096, Peak mem 9.324 GB\n",
      "Iter 3650: Val loss 2.260, Val took 3.522s\n",
      "Iter 3650: Train loss 0.874, Learning Rate 2.000e-04, It/sec 31.093, Tokens/sec 3958.167, Trained Tokens 605369, Peak mem 9.324 GB\n",
      "Iter 3660: Train loss 0.846, Learning Rate 2.000e-04, It/sec 2.480, Tokens/sec 337.761, Trained Tokens 606731, Peak mem 9.324 GB\n",
      "Iter 3670: Train loss 0.947, Learning Rate 2.000e-04, It/sec 2.099, Tokens/sec 332.626, Trained Tokens 608316, Peak mem 9.324 GB\n",
      "Iter 3680: Train loss 0.938, Learning Rate 2.000e-04, It/sec 2.107, Tokens/sec 334.153, Trained Tokens 609902, Peak mem 9.324 GB\n",
      "Iter 3690: Train loss 0.963, Learning Rate 2.000e-04, It/sec 2.477, Tokens/sec 330.241, Trained Tokens 611235, Peak mem 9.324 GB\n",
      "Iter 3700: Val loss 1.959, Val took 2.892s\n",
      "Iter 3700: Train loss 0.916, Learning Rate 2.000e-04, It/sec 17.707, Tokens/sec 3861.944, Trained Tokens 613416, Peak mem 9.324 GB\n",
      "Iter 3700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003700_adapters.safetensors.\n",
      "Iter 3710: Train loss 0.875, Learning Rate 2.000e-04, It/sec 1.904, Tokens/sec 338.419, Trained Tokens 615193, Peak mem 9.324 GB\n",
      "Iter 3720: Train loss 0.676, Learning Rate 2.000e-04, It/sec 2.513, Tokens/sec 327.234, Trained Tokens 616495, Peak mem 9.324 GB\n",
      "Iter 3730: Train loss 0.770, Learning Rate 2.000e-04, It/sec 2.374, Tokens/sec 334.082, Trained Tokens 617902, Peak mem 9.324 GB\n",
      "Iter 3740: Train loss 0.904, Learning Rate 2.000e-04, It/sec 1.427, Tokens/sec 361.585, Trained Tokens 620436, Peak mem 9.324 GB\n",
      "Iter 3750: Val loss 2.148, Val took 3.206s\n",
      "Iter 3750: Train loss 0.926, Learning Rate 2.000e-04, It/sec 9.868, Tokens/sec 2213.456, Trained Tokens 622679, Peak mem 9.324 GB\n",
      "Iter 3760: Train loss 0.891, Learning Rate 2.000e-04, It/sec 1.966, Tokens/sec 345.286, Trained Tokens 624435, Peak mem 9.324 GB\n",
      "Iter 3770: Train loss 0.854, Learning Rate 2.000e-04, It/sec 2.183, Tokens/sec 317.856, Trained Tokens 625891, Peak mem 9.324 GB\n",
      "Iter 3780: Train loss 0.756, Learning Rate 2.000e-04, It/sec 2.208, Tokens/sec 335.359, Trained Tokens 627410, Peak mem 9.324 GB\n",
      "Iter 3790: Train loss 0.835, Learning Rate 2.000e-04, It/sec 1.362, Tokens/sec 354.079, Trained Tokens 630010, Peak mem 9.324 GB\n",
      "Iter 3800: Val loss 1.974, Val took 2.964s\n",
      "Iter 3800: Train loss 0.909, Learning Rate 2.000e-04, It/sec 30.656, Tokens/sec 6033.068, Trained Tokens 631978, Peak mem 9.324 GB\n",
      "Iter 3800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003800_adapters.safetensors.\n",
      "Iter 3810: Train loss 0.852, Learning Rate 2.000e-04, It/sec 2.121, Tokens/sec 345.903, Trained Tokens 633609, Peak mem 9.324 GB\n",
      "Iter 3820: Train loss 0.727, Learning Rate 2.000e-04, It/sec 3.177, Tokens/sec 335.148, Trained Tokens 634664, Peak mem 9.324 GB\n",
      "Iter 3830: Train loss 0.803, Learning Rate 2.000e-04, It/sec 2.556, Tokens/sec 359.434, Trained Tokens 636070, Peak mem 9.324 GB\n",
      "Iter 3840: Train loss 0.800, Learning Rate 2.000e-04, It/sec 2.020, Tokens/sec 333.517, Trained Tokens 637721, Peak mem 9.324 GB\n",
      "Iter 3850: Val loss 2.139, Val took 2.940s\n",
      "Iter 3850: Train loss 0.761, Learning Rate 2.000e-04, It/sec 31.110, Tokens/sec 3646.072, Trained Tokens 638893, Peak mem 9.324 GB\n",
      "Iter 3860: Train loss 0.788, Learning Rate 2.000e-04, It/sec 2.119, Tokens/sec 344.050, Trained Tokens 640517, Peak mem 9.324 GB\n",
      "Iter 3870: Train loss 0.824, Learning Rate 2.000e-04, It/sec 2.289, Tokens/sec 337.242, Trained Tokens 641990, Peak mem 9.324 GB\n",
      "Iter 3880: Train loss 0.779, Learning Rate 2.000e-04, It/sec 2.887, Tokens/sec 323.580, Trained Tokens 643111, Peak mem 9.324 GB\n",
      "Iter 3890: Train loss 0.833, Learning Rate 2.000e-04, It/sec 2.672, Tokens/sec 322.769, Trained Tokens 644319, Peak mem 9.324 GB\n",
      "Iter 3900: Val loss 2.060, Val took 3.048s\n",
      "Iter 3900: Train loss 0.752, Learning Rate 2.000e-04, It/sec 21.949, Tokens/sec 3459.192, Trained Tokens 645895, Peak mem 9.324 GB\n",
      "Iter 3900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0003900_adapters.safetensors.\n",
      "Iter 3910: Train loss 0.790, Learning Rate 2.000e-04, It/sec 1.819, Tokens/sec 339.406, Trained Tokens 647761, Peak mem 9.324 GB\n",
      "Iter 3920: Train loss 0.862, Learning Rate 2.000e-04, It/sec 2.088, Tokens/sec 357.626, Trained Tokens 649474, Peak mem 9.324 GB\n",
      "Iter 3930: Train loss 0.801, Learning Rate 2.000e-04, It/sec 2.212, Tokens/sec 350.123, Trained Tokens 651057, Peak mem 9.324 GB\n",
      "Iter 3940: Train loss 0.913, Learning Rate 2.000e-04, It/sec 1.659, Tokens/sec 357.706, Trained Tokens 653213, Peak mem 9.324 GB\n",
      "Iter 3950: Val loss 1.986, Val took 3.025s\n",
      "Iter 3950: Train loss 0.686, Learning Rate 2.000e-04, It/sec 17.612, Tokens/sec 2402.321, Trained Tokens 654577, Peak mem 9.324 GB\n",
      "Iter 3960: Train loss 0.791, Learning Rate 2.000e-04, It/sec 2.027, Tokens/sec 302.845, Trained Tokens 656071, Peak mem 9.324 GB\n",
      "Iter 3970: Train loss 0.875, Learning Rate 2.000e-04, It/sec 1.647, Tokens/sec 340.580, Trained Tokens 658139, Peak mem 9.324 GB\n",
      "Iter 3980: Train loss 0.940, Learning Rate 2.000e-04, It/sec 1.851, Tokens/sec 361.724, Trained Tokens 660093, Peak mem 9.324 GB\n",
      "Iter 3990: Train loss 0.881, Learning Rate 2.000e-04, It/sec 2.263, Tokens/sec 351.601, Trained Tokens 661647, Peak mem 9.324 GB\n",
      "Iter 4000: Val loss 2.333, Val took 4.431s\n",
      "Iter 4000: Train loss 0.887, Learning Rate 2.000e-04, It/sec 23.322, Tokens/sec 3335.110, Trained Tokens 663077, Peak mem 9.324 GB\n",
      "Iter 4000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004000_adapters.safetensors.\n",
      "Iter 4010: Train loss 0.787, Learning Rate 2.000e-04, It/sec 2.407, Tokens/sec 327.559, Trained Tokens 664438, Peak mem 9.324 GB\n",
      "Iter 4020: Train loss 0.908, Learning Rate 2.000e-04, It/sec 1.890, Tokens/sec 287.607, Trained Tokens 665960, Peak mem 9.324 GB\n",
      "Iter 4030: Train loss 0.992, Learning Rate 2.000e-04, It/sec 1.418, Tokens/sec 338.954, Trained Tokens 668351, Peak mem 9.324 GB\n",
      "Iter 4040: Train loss 0.780, Learning Rate 2.000e-04, It/sec 2.253, Tokens/sec 308.257, Trained Tokens 669719, Peak mem 9.324 GB\n",
      "Iter 4050: Val loss 1.942, Val took 3.517s\n",
      "Iter 4050: Train loss 0.879, Learning Rate 2.000e-04, It/sec 27.882, Tokens/sec 3524.345, Trained Tokens 670983, Peak mem 9.324 GB\n",
      "Iter 4060: Train loss 0.960, Learning Rate 2.000e-04, It/sec 2.004, Tokens/sec 329.408, Trained Tokens 672627, Peak mem 9.324 GB\n",
      "Iter 4070: Train loss 0.897, Learning Rate 2.000e-04, It/sec 2.252, Tokens/sec 359.403, Trained Tokens 674223, Peak mem 9.324 GB\n",
      "Iter 4080: Train loss 0.804, Learning Rate 2.000e-04, It/sec 2.025, Tokens/sec 308.028, Trained Tokens 675744, Peak mem 9.324 GB\n",
      "Iter 4090: Train loss 0.968, Learning Rate 2.000e-04, It/sec 2.490, Tokens/sec 333.227, Trained Tokens 677082, Peak mem 9.324 GB\n",
      "Iter 4100: Val loss 2.494, Val took 3.454s\n",
      "Iter 4100: Train loss 0.912, Learning Rate 2.000e-04, It/sec 24.079, Tokens/sec 4979.592, Trained Tokens 679150, Peak mem 9.324 GB\n",
      "Iter 4100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004100_adapters.safetensors.\n",
      "Iter 4110: Train loss 0.629, Learning Rate 2.000e-04, It/sec 2.386, Tokens/sec 334.266, Trained Tokens 680551, Peak mem 9.324 GB\n",
      "Iter 4120: Train loss 0.720, Learning Rate 2.000e-04, It/sec 2.146, Tokens/sec 339.114, Trained Tokens 682131, Peak mem 9.324 GB\n",
      "Iter 4130: Train loss 0.678, Learning Rate 2.000e-04, It/sec 2.025, Tokens/sec 349.282, Trained Tokens 683856, Peak mem 9.324 GB\n",
      "Iter 4140: Train loss 0.819, Learning Rate 2.000e-04, It/sec 1.647, Tokens/sec 374.907, Trained Tokens 686132, Peak mem 9.324 GB\n",
      "Iter 4150: Val loss 2.383, Val took 2.818s\n",
      "Iter 4150: Train loss 0.806, Learning Rate 2.000e-04, It/sec 15.849, Tokens/sec 3367.825, Trained Tokens 688257, Peak mem 9.324 GB\n",
      "Iter 4160: Train loss 0.722, Learning Rate 2.000e-04, It/sec 2.109, Tokens/sec 318.740, Trained Tokens 689768, Peak mem 9.324 GB\n",
      "Iter 4170: Train loss 0.849, Learning Rate 2.000e-04, It/sec 1.891, Tokens/sec 339.766, Trained Tokens 691565, Peak mem 9.324 GB\n",
      "Iter 4180: Train loss 0.787, Learning Rate 2.000e-04, It/sec 2.101, Tokens/sec 349.177, Trained Tokens 693227, Peak mem 9.324 GB\n",
      "Iter 4190: Train loss 0.682, Learning Rate 2.000e-04, It/sec 2.011, Tokens/sec 320.748, Trained Tokens 694822, Peak mem 9.324 GB\n",
      "Iter 4200: Val loss 2.439, Val took 3.633s\n",
      "Iter 4200: Train loss 0.676, Learning Rate 2.000e-04, It/sec 22.373, Tokens/sec 3384.999, Trained Tokens 696335, Peak mem 9.324 GB\n",
      "Iter 4200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004200_adapters.safetensors.\n",
      "Iter 4210: Train loss 0.720, Learning Rate 2.000e-04, It/sec 2.350, Tokens/sec 290.679, Trained Tokens 697572, Peak mem 9.324 GB\n",
      "Iter 4220: Train loss 0.670, Learning Rate 2.000e-04, It/sec 2.648, Tokens/sec 316.947, Trained Tokens 698769, Peak mem 9.324 GB\n",
      "Iter 4230: Train loss 0.699, Learning Rate 2.000e-04, It/sec 2.790, Tokens/sec 325.557, Trained Tokens 699936, Peak mem 9.324 GB\n",
      "Iter 4240: Train loss 0.836, Learning Rate 2.000e-04, It/sec 1.932, Tokens/sec 362.047, Trained Tokens 701810, Peak mem 9.324 GB\n",
      "Iter 4250: Val loss 2.452, Val took 3.589s\n",
      "Iter 4250: Train loss 0.720, Learning Rate 2.000e-04, It/sec 23.576, Tokens/sec 2652.264, Trained Tokens 702935, Peak mem 9.324 GB\n",
      "Iter 4260: Train loss 0.746, Learning Rate 2.000e-04, It/sec 2.246, Tokens/sec 329.269, Trained Tokens 704401, Peak mem 9.324 GB\n",
      "Iter 4270: Train loss 0.771, Learning Rate 2.000e-04, It/sec 2.045, Tokens/sec 366.381, Trained Tokens 706193, Peak mem 9.324 GB\n",
      "Iter 4280: Train loss 0.854, Learning Rate 2.000e-04, It/sec 2.367, Tokens/sec 347.506, Trained Tokens 707661, Peak mem 9.324 GB\n",
      "Iter 4290: Train loss 0.796, Learning Rate 2.000e-04, It/sec 2.080, Tokens/sec 355.481, Trained Tokens 709370, Peak mem 9.324 GB\n",
      "Iter 4300: Val loss 2.456, Val took 2.767s\n",
      "Iter 4300: Train loss 0.787, Learning Rate 2.000e-04, It/sec 24.494, Tokens/sec 4710.207, Trained Tokens 711293, Peak mem 9.324 GB\n",
      "Iter 4300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004300_adapters.safetensors.\n",
      "Iter 4310: Train loss 0.846, Learning Rate 2.000e-04, It/sec 1.778, Tokens/sec 360.231, Trained Tokens 713319, Peak mem 9.324 GB\n",
      "Iter 4320: Train loss 0.897, Learning Rate 2.000e-04, It/sec 2.459, Tokens/sec 353.633, Trained Tokens 714757, Peak mem 9.324 GB\n",
      "Iter 4330: Train loss 0.795, Learning Rate 2.000e-04, It/sec 2.474, Tokens/sec 353.081, Trained Tokens 716184, Peak mem 9.324 GB\n",
      "Iter 4340: Train loss 0.683, Learning Rate 2.000e-04, It/sec 3.265, Tokens/sec 334.621, Trained Tokens 717209, Peak mem 9.324 GB\n",
      "Iter 4350: Val loss 2.379, Val took 3.528s\n",
      "Iter 4350: Train loss 0.642, Learning Rate 2.000e-04, It/sec 22.489, Tokens/sec 3018.045, Trained Tokens 718551, Peak mem 9.324 GB\n",
      "Iter 4360: Train loss 0.909, Learning Rate 2.000e-04, It/sec 1.624, Tokens/sec 365.255, Trained Tokens 720800, Peak mem 9.324 GB\n",
      "Iter 4370: Train loss 0.935, Learning Rate 2.000e-04, It/sec 1.615, Tokens/sec 370.233, Trained Tokens 723093, Peak mem 9.324 GB\n",
      "Iter 4380: Train loss 0.760, Learning Rate 2.000e-04, It/sec 2.426, Tokens/sec 337.426, Trained Tokens 724484, Peak mem 9.324 GB\n",
      "Iter 4390: Train loss 0.765, Learning Rate 2.000e-04, It/sec 1.850, Tokens/sec 354.569, Trained Tokens 726401, Peak mem 9.324 GB\n",
      "Iter 4400: Val loss 2.191, Val took 3.538s\n",
      "Iter 4400: Train loss 0.967, Learning Rate 2.000e-04, It/sec 24.101, Tokens/sec 5890.286, Trained Tokens 728845, Peak mem 9.324 GB\n",
      "Iter 4400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004400_adapters.safetensors.\n",
      "Iter 4410: Train loss 0.783, Learning Rate 2.000e-04, It/sec 2.140, Tokens/sec 348.887, Trained Tokens 730475, Peak mem 9.324 GB\n",
      "Iter 4420: Train loss 0.815, Learning Rate 2.000e-04, It/sec 2.330, Tokens/sec 360.516, Trained Tokens 732022, Peak mem 9.324 GB\n",
      "Iter 4430: Train loss 0.680, Learning Rate 2.000e-04, It/sec 3.289, Tokens/sec 317.722, Trained Tokens 732988, Peak mem 9.324 GB\n",
      "Iter 4440: Train loss 0.973, Learning Rate 2.000e-04, It/sec 1.310, Tokens/sec 364.726, Trained Tokens 735773, Peak mem 9.324 GB\n",
      "Iter 4450: Val loss 2.178, Val took 3.623s\n",
      "Iter 4450: Train loss 0.740, Learning Rate 2.000e-04, It/sec 16.247, Tokens/sec 2326.552, Trained Tokens 737205, Peak mem 9.324 GB\n",
      "Iter 4460: Train loss 0.763, Learning Rate 2.000e-04, It/sec 2.082, Tokens/sec 353.300, Trained Tokens 738902, Peak mem 9.324 GB\n",
      "Iter 4470: Train loss 0.954, Learning Rate 2.000e-04, It/sec 1.827, Tokens/sec 363.991, Trained Tokens 740894, Peak mem 9.324 GB\n",
      "Iter 4480: Train loss 0.834, Learning Rate 2.000e-04, It/sec 2.771, Tokens/sec 333.060, Trained Tokens 742096, Peak mem 9.324 GB\n",
      "Iter 4490: Train loss 0.785, Learning Rate 2.000e-04, It/sec 2.258, Tokens/sec 348.219, Trained Tokens 743638, Peak mem 9.324 GB\n",
      "Iter 4500: Val loss 2.102, Val took 3.122s\n",
      "Iter 4500: Train loss 0.912, Learning Rate 2.000e-04, It/sec 30.850, Tokens/sec 5293.779, Trained Tokens 745354, Peak mem 9.324 GB\n",
      "Iter 4500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004500_adapters.safetensors.\n",
      "Iter 4510: Train loss 0.882, Learning Rate 2.000e-04, It/sec 1.900, Tokens/sec 325.080, Trained Tokens 747065, Peak mem 9.324 GB\n",
      "Iter 4520: Train loss 0.646, Learning Rate 2.000e-04, It/sec 2.024, Tokens/sec 361.255, Trained Tokens 748850, Peak mem 9.324 GB\n",
      "Iter 4530: Train loss 0.622, Learning Rate 2.000e-04, It/sec 2.471, Tokens/sec 350.157, Trained Tokens 750267, Peak mem 9.324 GB\n",
      "Iter 4540: Train loss 0.726, Learning Rate 2.000e-04, It/sec 1.312, Tokens/sec 370.559, Trained Tokens 753091, Peak mem 9.324 GB\n",
      "Iter 4550: Val loss 2.473, Val took 3.824s\n",
      "Iter 4550: Train loss 0.572, Learning Rate 2.000e-04, It/sec 14.188, Tokens/sec 2189.208, Trained Tokens 754634, Peak mem 9.324 GB\n",
      "Iter 4560: Train loss 0.644, Learning Rate 2.000e-04, It/sec 2.792, Tokens/sec 346.771, Trained Tokens 755876, Peak mem 9.324 GB\n",
      "Iter 4570: Train loss 0.723, Learning Rate 2.000e-04, It/sec 1.961, Tokens/sec 346.746, Trained Tokens 757644, Peak mem 9.324 GB\n",
      "Iter 4580: Train loss 0.734, Learning Rate 2.000e-04, It/sec 1.956, Tokens/sec 351.867, Trained Tokens 759443, Peak mem 9.324 GB\n",
      "Iter 4590: Train loss 0.854, Learning Rate 2.000e-04, It/sec 1.596, Tokens/sec 375.095, Trained Tokens 761793, Peak mem 9.324 GB\n",
      "Iter 4600: Val loss 2.254, Val took 3.448s\n",
      "Iter 4600: Train loss 0.778, Learning Rate 2.000e-04, It/sec 11.557, Tokens/sec 2322.889, Trained Tokens 763803, Peak mem 9.324 GB\n",
      "Iter 4600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004600_adapters.safetensors.\n",
      "Iter 4610: Train loss 0.641, Learning Rate 2.000e-04, It/sec 3.177, Tokens/sec 284.020, Trained Tokens 764697, Peak mem 9.324 GB\n",
      "Iter 4620: Train loss 0.595, Learning Rate 2.000e-04, It/sec 2.712, Tokens/sec 333.902, Trained Tokens 765928, Peak mem 9.324 GB\n",
      "Iter 4630: Train loss 0.793, Learning Rate 2.000e-04, It/sec 1.771, Tokens/sec 371.989, Trained Tokens 768029, Peak mem 9.324 GB\n",
      "Iter 4640: Train loss 0.631, Learning Rate 2.000e-04, It/sec 3.289, Tokens/sec 320.395, Trained Tokens 769003, Peak mem 9.324 GB\n",
      "Iter 4650: Val loss 2.551, Val took 3.415s\n",
      "Iter 4650: Train loss 0.630, Learning Rate 2.000e-04, It/sec 40.229, Tokens/sec 4956.217, Trained Tokens 770235, Peak mem 9.324 GB\n",
      "Iter 4660: Train loss 0.693, Learning Rate 2.000e-04, It/sec 2.496, Tokens/sec 346.173, Trained Tokens 771622, Peak mem 9.324 GB\n",
      "Iter 4670: Train loss 0.917, Learning Rate 2.000e-04, It/sec 1.406, Tokens/sec 360.794, Trained Tokens 774188, Peak mem 9.324 GB\n",
      "Iter 4680: Train loss 0.661, Learning Rate 2.000e-04, It/sec 2.305, Tokens/sec 327.712, Trained Tokens 775610, Peak mem 9.324 GB\n",
      "Iter 4690: Train loss 0.668, Learning Rate 2.000e-04, It/sec 2.185, Tokens/sec 361.019, Trained Tokens 777262, Peak mem 9.324 GB\n",
      "Iter 4700: Val loss 1.945, Val took 3.158s\n",
      "Iter 4700: Train loss 0.665, Learning Rate 2.000e-04, It/sec 21.139, Tokens/sec 2805.080, Trained Tokens 778589, Peak mem 9.324 GB\n",
      "Iter 4700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004700_adapters.safetensors.\n",
      "Iter 4710: Train loss 0.693, Learning Rate 2.000e-04, It/sec 1.722, Tokens/sec 313.754, Trained Tokens 780411, Peak mem 9.324 GB\n",
      "Iter 4720: Train loss 0.802, Learning Rate 2.000e-04, It/sec 1.889, Tokens/sec 353.948, Trained Tokens 782285, Peak mem 9.324 GB\n",
      "Iter 4730: Train loss 0.841, Learning Rate 2.000e-04, It/sec 1.808, Tokens/sec 368.798, Trained Tokens 784325, Peak mem 9.324 GB\n",
      "Iter 4740: Train loss 0.637, Learning Rate 2.000e-04, It/sec 2.788, Tokens/sec 358.855, Trained Tokens 785612, Peak mem 9.324 GB\n",
      "Iter 4750: Val loss 2.149, Val took 3.056s\n",
      "Iter 4750: Train loss 0.684, Learning Rate 2.000e-04, It/sec 24.011, Tokens/sec 3284.753, Trained Tokens 786980, Peak mem 9.324 GB\n",
      "Iter 4760: Train loss 0.841, Learning Rate 2.000e-04, It/sec 1.939, Tokens/sec 348.277, Trained Tokens 788776, Peak mem 9.324 GB\n",
      "Iter 4770: Train loss 0.777, Learning Rate 2.000e-04, It/sec 1.906, Tokens/sec 349.691, Trained Tokens 790611, Peak mem 9.324 GB\n",
      "Iter 4780: Train loss 0.715, Learning Rate 2.000e-04, It/sec 2.488, Tokens/sec 344.846, Trained Tokens 791997, Peak mem 9.324 GB\n",
      "Iter 4790: Train loss 0.689, Learning Rate 2.000e-04, It/sec 2.264, Tokens/sec 295.626, Trained Tokens 793303, Peak mem 9.324 GB\n",
      "Iter 4800: Val loss 2.262, Val took 3.169s\n",
      "Iter 4800: Train loss 0.761, Learning Rate 2.000e-04, It/sec 37.381, Tokens/sec 5382.916, Trained Tokens 794743, Peak mem 9.324 GB\n",
      "Iter 4800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004800_adapters.safetensors.\n",
      "Iter 4810: Train loss 0.957, Learning Rate 2.000e-04, It/sec 1.408, Tokens/sec 357.685, Trained Tokens 797283, Peak mem 9.324 GB\n",
      "Iter 4820: Train loss 0.761, Learning Rate 2.000e-04, It/sec 2.101, Tokens/sec 288.872, Trained Tokens 798658, Peak mem 9.324 GB\n",
      "Iter 4830: Train loss 0.712, Learning Rate 2.000e-04, It/sec 2.086, Tokens/sec 348.935, Trained Tokens 800331, Peak mem 9.324 GB\n",
      "Iter 4840: Train loss 0.753, Learning Rate 2.000e-04, It/sec 2.010, Tokens/sec 351.299, Trained Tokens 802079, Peak mem 9.324 GB\n",
      "Iter 4850: Val loss 2.428, Val took 3.057s\n",
      "Iter 4850: Train loss 0.729, Learning Rate 2.000e-04, It/sec 38.238, Tokens/sec 6091.235, Trained Tokens 803672, Peak mem 9.324 GB\n",
      "Iter 4860: Train loss 0.743, Learning Rate 2.000e-04, It/sec 2.228, Tokens/sec 310.825, Trained Tokens 805067, Peak mem 9.324 GB\n",
      "Iter 4870: Train loss 0.770, Learning Rate 2.000e-04, It/sec 1.892, Tokens/sec 359.339, Trained Tokens 806966, Peak mem 9.324 GB\n",
      "Iter 4880: Train loss 0.818, Learning Rate 2.000e-04, It/sec 2.732, Tokens/sec 322.158, Trained Tokens 808145, Peak mem 9.324 GB\n",
      "Iter 4890: Train loss 0.766, Learning Rate 2.000e-04, It/sec 1.930, Tokens/sec 327.257, Trained Tokens 809841, Peak mem 9.324 GB\n",
      "Iter 4900: Val loss 2.070, Val took 3.138s\n",
      "Iter 4900: Train loss 0.952, Learning Rate 2.000e-04, It/sec 55.433, Tokens/sec 10521.273, Trained Tokens 811739, Peak mem 9.324 GB\n",
      "Iter 4900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0004900_adapters.safetensors.\n",
      "Iter 4910: Train loss 0.786, Learning Rate 2.000e-04, It/sec 1.862, Tokens/sec 325.826, Trained Tokens 813489, Peak mem 9.324 GB\n",
      "Iter 4920: Train loss 0.733, Learning Rate 2.000e-04, It/sec 2.245, Tokens/sec 334.733, Trained Tokens 814980, Peak mem 9.324 GB\n",
      "Iter 4930: Train loss 0.632, Learning Rate 2.000e-04, It/sec 2.695, Tokens/sec 343.606, Trained Tokens 816255, Peak mem 9.324 GB\n",
      "Iter 4940: Train loss 0.628, Learning Rate 2.000e-04, It/sec 1.989, Tokens/sec 325.370, Trained Tokens 817891, Peak mem 9.324 GB\n",
      "Iter 4950: Val loss 2.388, Val took 3.431s\n",
      "Iter 4950: Train loss 0.626, Learning Rate 2.000e-04, It/sec 38.332, Tokens/sec 5765.104, Trained Tokens 819395, Peak mem 9.324 GB\n",
      "Iter 4960: Train loss 0.645, Learning Rate 2.000e-04, It/sec 2.554, Tokens/sec 318.708, Trained Tokens 820643, Peak mem 9.324 GB\n",
      "Iter 4970: Train loss 0.963, Learning Rate 2.000e-04, It/sec 1.289, Tokens/sec 365.896, Trained Tokens 823482, Peak mem 9.324 GB\n",
      "Iter 4980: Train loss 0.667, Learning Rate 2.000e-04, It/sec 1.788, Tokens/sec 357.917, Trained Tokens 825484, Peak mem 9.324 GB\n",
      "Iter 4990: Train loss 0.584, Learning Rate 2.000e-04, It/sec 2.307, Tokens/sec 358.064, Trained Tokens 827036, Peak mem 9.324 GB\n",
      "Iter 5000: Val loss 2.283, Val took 3.013s\n",
      "Iter 5000: Train loss 0.625, Learning Rate 2.000e-04, It/sec 39.374, Tokens/sec 6571.508, Trained Tokens 828705, Peak mem 9.324 GB\n",
      "Iter 5000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005000_adapters.safetensors.\n",
      "Iter 5010: Train loss 0.558, Learning Rate 2.000e-04, It/sec 2.164, Tokens/sec 363.400, Trained Tokens 830384, Peak mem 9.324 GB\n",
      "Iter 5020: Train loss 0.588, Learning Rate 2.000e-04, It/sec 2.273, Tokens/sec 342.492, Trained Tokens 831891, Peak mem 9.324 GB\n",
      "Iter 5030: Train loss 0.806, Learning Rate 2.000e-04, It/sec 1.865, Tokens/sec 378.806, Trained Tokens 833922, Peak mem 9.324 GB\n",
      "Iter 5040: Train loss 0.619, Learning Rate 2.000e-04, It/sec 1.719, Tokens/sec 344.911, Trained Tokens 835929, Peak mem 9.324 GB\n",
      "Iter 5050: Val loss 2.261, Val took 3.609s\n",
      "Iter 5050: Train loss 0.641, Learning Rate 2.000e-04, It/sec 21.804, Tokens/sec 2941.398, Trained Tokens 837278, Peak mem 9.324 GB\n",
      "Iter 5060: Train loss 0.724, Learning Rate 2.000e-04, It/sec 2.008, Tokens/sec 364.375, Trained Tokens 839093, Peak mem 9.324 GB\n",
      "Iter 5070: Train loss 0.767, Learning Rate 2.000e-04, It/sec 1.906, Tokens/sec 361.383, Trained Tokens 840989, Peak mem 9.324 GB\n",
      "Iter 5080: Train loss 0.680, Learning Rate 2.000e-04, It/sec 2.036, Tokens/sec 344.761, Trained Tokens 842682, Peak mem 9.324 GB\n",
      "Iter 5090: Train loss 0.731, Learning Rate 2.000e-04, It/sec 2.120, Tokens/sec 342.725, Trained Tokens 844299, Peak mem 9.324 GB\n",
      "Iter 5100: Val loss 2.260, Val took 3.481s\n",
      "Iter 5100: Train loss 0.641, Learning Rate 2.000e-04, It/sec 39.450, Tokens/sec 5286.327, Trained Tokens 845639, Peak mem 9.324 GB\n",
      "Iter 5100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005100_adapters.safetensors.\n",
      "Iter 5110: Train loss 0.628, Learning Rate 2.000e-04, It/sec 2.451, Tokens/sec 301.235, Trained Tokens 846868, Peak mem 9.324 GB\n",
      "Iter 5120: Train loss 0.731, Learning Rate 2.000e-04, It/sec 1.875, Tokens/sec 357.471, Trained Tokens 848775, Peak mem 9.324 GB\n",
      "Iter 5130: Train loss 0.706, Learning Rate 2.000e-04, It/sec 2.173, Tokens/sec 295.551, Trained Tokens 850135, Peak mem 9.324 GB\n",
      "Iter 5140: Train loss 0.689, Learning Rate 2.000e-04, It/sec 1.678, Tokens/sec 299.703, Trained Tokens 851921, Peak mem 9.324 GB\n",
      "Iter 5150: Val loss 2.614, Val took 3.721s\n",
      "Iter 5150: Train loss 0.606, Learning Rate 2.000e-04, It/sec 23.847, Tokens/sec 3791.623, Trained Tokens 853511, Peak mem 9.324 GB\n",
      "Iter 5160: Train loss 0.721, Learning Rate 2.000e-04, It/sec 2.744, Tokens/sec 295.248, Trained Tokens 854587, Peak mem 9.324 GB\n",
      "Iter 5170: Train loss 0.704, Learning Rate 2.000e-04, It/sec 2.069, Tokens/sec 299.971, Trained Tokens 856037, Peak mem 9.324 GB\n",
      "Iter 5180: Train loss 0.654, Learning Rate 2.000e-04, It/sec 2.765, Tokens/sec 318.482, Trained Tokens 857189, Peak mem 9.324 GB\n",
      "Iter 5190: Train loss 0.866, Learning Rate 2.000e-04, It/sec 1.028, Tokens/sec 327.414, Trained Tokens 860375, Peak mem 9.324 GB\n",
      "Iter 5200: Val loss 2.541, Val took 3.589s\n",
      "Iter 5200: Train loss 0.647, Learning Rate 2.000e-04, It/sec 26.631, Tokens/sec 3648.458, Trained Tokens 861745, Peak mem 9.324 GB\n",
      "Iter 5200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005200_adapters.safetensors.\n",
      "Iter 5210: Train loss 0.856, Learning Rate 2.000e-04, It/sec 1.603, Tokens/sec 314.717, Trained Tokens 863708, Peak mem 9.324 GB\n",
      "Iter 5220: Train loss 0.788, Learning Rate 2.000e-04, It/sec 1.655, Tokens/sec 313.876, Trained Tokens 865605, Peak mem 9.324 GB\n",
      "Iter 5230: Train loss 0.649, Learning Rate 2.000e-04, It/sec 2.629, Tokens/sec 327.579, Trained Tokens 866851, Peak mem 9.324 GB\n",
      "Iter 5240: Train loss 0.783, Learning Rate 2.000e-04, It/sec 1.675, Tokens/sec 294.437, Trained Tokens 868609, Peak mem 9.324 GB\n",
      "Iter 5250: Val loss 2.437, Val took 2.821s\n",
      "Iter 5250: Train loss 0.806, Learning Rate 2.000e-04, It/sec 32.173, Tokens/sec 5607.725, Trained Tokens 870352, Peak mem 9.324 GB\n",
      "Iter 5260: Train loss 0.785, Learning Rate 2.000e-04, It/sec 2.146, Tokens/sec 333.731, Trained Tokens 871907, Peak mem 9.324 GB\n",
      "Iter 5270: Train loss 0.649, Learning Rate 2.000e-04, It/sec 2.543, Tokens/sec 293.756, Trained Tokens 873062, Peak mem 9.324 GB\n",
      "Iter 5280: Train loss 0.668, Learning Rate 2.000e-04, It/sec 2.263, Tokens/sec 314.267, Trained Tokens 874451, Peak mem 9.324 GB\n",
      "Iter 5290: Train loss 0.741, Learning Rate 2.000e-04, It/sec 2.188, Tokens/sec 312.813, Trained Tokens 875881, Peak mem 9.324 GB\n",
      "Iter 5300: Val loss 2.250, Val took 3.673s\n",
      "Iter 5300: Train loss 0.877, Learning Rate 2.000e-04, It/sec 16.974, Tokens/sec 2712.388, Trained Tokens 877479, Peak mem 9.324 GB\n",
      "Iter 5300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005300_adapters.safetensors.\n",
      "Iter 5310: Train loss 0.965, Learning Rate 2.000e-04, It/sec 1.368, Tokens/sec 337.499, Trained Tokens 879947, Peak mem 9.324 GB\n",
      "Iter 5320: Train loss 0.788, Learning Rate 2.000e-04, It/sec 2.192, Tokens/sec 299.244, Trained Tokens 881312, Peak mem 9.324 GB\n",
      "Iter 5330: Train loss 0.791, Learning Rate 2.000e-04, It/sec 2.007, Tokens/sec 317.665, Trained Tokens 882895, Peak mem 9.324 GB\n",
      "Iter 5340: Train loss 0.669, Learning Rate 2.000e-04, It/sec 1.718, Tokens/sec 315.107, Trained Tokens 884729, Peak mem 9.324 GB\n",
      "Iter 5350: Val loss 2.509, Val took 3.909s\n",
      "Iter 5350: Train loss 0.610, Learning Rate 2.000e-04, It/sec 22.323, Tokens/sec 2832.817, Trained Tokens 885998, Peak mem 9.324 GB\n",
      "Iter 5360: Train loss 0.635, Learning Rate 2.000e-04, It/sec 1.502, Tokens/sec 252.274, Trained Tokens 887678, Peak mem 9.324 GB\n",
      "Iter 5370: Train loss 0.669, Learning Rate 2.000e-04, It/sec 1.533, Tokens/sec 322.474, Trained Tokens 889782, Peak mem 9.324 GB\n",
      "Iter 5380: Train loss 0.640, Learning Rate 2.000e-04, It/sec 1.952, Tokens/sec 288.903, Trained Tokens 891262, Peak mem 9.324 GB\n",
      "Iter 5390: Train loss 0.631, Learning Rate 2.000e-04, It/sec 1.926, Tokens/sec 312.229, Trained Tokens 892883, Peak mem 9.324 GB\n",
      "Iter 5400: Val loss 2.278, Val took 4.162s\n",
      "Iter 5400: Train loss 0.600, Learning Rate 2.000e-04, It/sec 8.793, Tokens/sec 1720.825, Trained Tokens 894840, Peak mem 9.324 GB\n",
      "Iter 5400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005400_adapters.safetensors.\n",
      "Iter 5410: Train loss 0.633, Learning Rate 2.000e-04, It/sec 1.694, Tokens/sec 272.488, Trained Tokens 896449, Peak mem 9.324 GB\n",
      "Iter 5420: Train loss 0.609, Learning Rate 2.000e-04, It/sec 1.636, Tokens/sec 292.886, Trained Tokens 898239, Peak mem 9.324 GB\n",
      "Iter 5430: Train loss 0.647, Learning Rate 2.000e-04, It/sec 3.273, Tokens/sec 307.684, Trained Tokens 899179, Peak mem 9.324 GB\n",
      "Iter 5440: Train loss 0.660, Learning Rate 2.000e-04, It/sec 1.615, Tokens/sec 308.961, Trained Tokens 901092, Peak mem 9.324 GB\n",
      "Iter 5450: Val loss 2.303, Val took 3.492s\n",
      "Iter 5450: Train loss 0.628, Learning Rate 2.000e-04, It/sec 11.642, Tokens/sec 1822.010, Trained Tokens 902657, Peak mem 9.324 GB\n",
      "Iter 5460: Train loss 0.689, Learning Rate 2.000e-04, It/sec 1.718, Tokens/sec 271.546, Trained Tokens 904238, Peak mem 9.324 GB\n",
      "Iter 5470: Train loss 0.580, Learning Rate 2.000e-04, It/sec 1.751, Tokens/sec 293.363, Trained Tokens 905913, Peak mem 9.324 GB\n",
      "Iter 5480: Train loss 0.560, Learning Rate 2.000e-04, It/sec 2.410, Tokens/sec 298.578, Trained Tokens 907152, Peak mem 9.324 GB\n",
      "Iter 5490: Train loss 0.630, Learning Rate 2.000e-04, It/sec 2.089, Tokens/sec 316.019, Trained Tokens 908665, Peak mem 9.324 GB\n",
      "Iter 5500: Val loss 2.059, Val took 2.968s\n",
      "Iter 5500: Train loss 0.651, Learning Rate 2.000e-04, It/sec 24.068, Tokens/sec 3436.891, Trained Tokens 910093, Peak mem 9.324 GB\n",
      "Iter 5500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005500_adapters.safetensors.\n",
      "Iter 5510: Train loss 0.620, Learning Rate 2.000e-04, It/sec 2.136, Tokens/sec 343.736, Trained Tokens 911702, Peak mem 9.324 GB\n",
      "Iter 5520: Train loss 0.591, Learning Rate 2.000e-04, It/sec 2.454, Tokens/sec 355.310, Trained Tokens 913150, Peak mem 9.324 GB\n",
      "Iter 5530: Train loss 0.634, Learning Rate 2.000e-04, It/sec 2.262, Tokens/sec 348.983, Trained Tokens 914693, Peak mem 9.324 GB\n",
      "Iter 5540: Train loss 0.773, Learning Rate 2.000e-04, It/sec 1.668, Tokens/sec 373.532, Trained Tokens 916932, Peak mem 9.324 GB\n",
      "Iter 5550: Val loss 2.291, Val took 2.774s\n",
      "Iter 5550: Train loss 0.770, Learning Rate 2.000e-04, It/sec 11.197, Tokens/sec 1846.308, Trained Tokens 918581, Peak mem 9.324 GB\n",
      "Iter 5560: Train loss 0.606, Learning Rate 2.000e-04, It/sec 2.808, Tokens/sec 328.808, Trained Tokens 919752, Peak mem 9.324 GB\n",
      "Iter 5570: Train loss 0.647, Learning Rate 2.000e-04, It/sec 2.243, Tokens/sec 342.756, Trained Tokens 921280, Peak mem 9.324 GB\n",
      "Iter 5580: Train loss 0.625, Learning Rate 2.000e-04, It/sec 2.406, Tokens/sec 315.640, Trained Tokens 922592, Peak mem 9.324 GB\n",
      "Iter 5590: Train loss 0.645, Learning Rate 2.000e-04, It/sec 2.236, Tokens/sec 339.886, Trained Tokens 924112, Peak mem 9.324 GB\n",
      "Iter 5600: Val loss 2.430, Val took 3.896s\n",
      "Iter 5600: Train loss 0.801, Learning Rate 2.000e-04, It/sec 19.735, Tokens/sec 3731.920, Trained Tokens 926003, Peak mem 9.324 GB\n",
      "Iter 5600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005600_adapters.safetensors.\n",
      "Iter 5610: Train loss 0.687, Learning Rate 2.000e-04, It/sec 2.526, Tokens/sec 315.729, Trained Tokens 927253, Peak mem 9.324 GB\n",
      "Iter 5620: Train loss 0.840, Learning Rate 2.000e-04, It/sec 1.785, Tokens/sec 347.135, Trained Tokens 929198, Peak mem 9.324 GB\n",
      "Iter 5630: Train loss 0.670, Learning Rate 2.000e-04, It/sec 2.761, Tokens/sec 332.979, Trained Tokens 930404, Peak mem 9.324 GB\n",
      "Iter 5640: Train loss 0.678, Learning Rate 2.000e-04, It/sec 1.925, Tokens/sec 343.216, Trained Tokens 932187, Peak mem 9.324 GB\n",
      "Iter 5650: Val loss 2.490, Val took 3.343s\n",
      "Iter 5650: Train loss 0.817, Learning Rate 2.000e-04, It/sec 38.793, Tokens/sec 7626.679, Trained Tokens 934153, Peak mem 9.324 GB\n",
      "Iter 5660: Train loss 0.717, Learning Rate 2.000e-04, It/sec 2.553, Tokens/sec 316.053, Trained Tokens 935391, Peak mem 9.324 GB\n",
      "Iter 5670: Train loss 0.666, Learning Rate 2.000e-04, It/sec 2.630, Tokens/sec 332.455, Trained Tokens 936655, Peak mem 9.324 GB\n",
      "Iter 5680: Train loss 0.848, Learning Rate 2.000e-04, It/sec 1.446, Tokens/sec 366.364, Trained Tokens 939188, Peak mem 9.324 GB\n",
      "Iter 5690: Train loss 0.777, Learning Rate 2.000e-04, It/sec 2.576, Tokens/sec 350.320, Trained Tokens 940548, Peak mem 9.324 GB\n",
      "Iter 5700: Val loss 2.338, Val took 2.828s\n",
      "Iter 5700: Train loss 0.836, Learning Rate 2.000e-04, It/sec 39.063, Tokens/sec 6379.024, Trained Tokens 942181, Peak mem 9.324 GB\n",
      "Iter 5700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005700_adapters.safetensors.\n",
      "Iter 5710: Train loss 0.700, Learning Rate 2.000e-04, It/sec 2.297, Tokens/sec 329.375, Trained Tokens 943615, Peak mem 9.324 GB\n",
      "Iter 5720: Train loss 0.897, Learning Rate 2.000e-04, It/sec 1.449, Tokens/sec 370.852, Trained Tokens 946175, Peak mem 9.324 GB\n",
      "Iter 5730: Train loss 0.872, Learning Rate 2.000e-04, It/sec 1.959, Tokens/sec 360.455, Trained Tokens 948015, Peak mem 9.324 GB\n",
      "Iter 5740: Train loss 0.948, Learning Rate 2.000e-04, It/sec 1.317, Tokens/sec 368.083, Trained Tokens 950810, Peak mem 9.324 GB\n",
      "Iter 5750: Val loss 2.661, Val took 3.168s\n",
      "Iter 5750: Train loss 0.557, Learning Rate 2.000e-04, It/sec 38.743, Tokens/sec 3963.425, Trained Tokens 951833, Peak mem 9.324 GB\n",
      "Iter 5760: Train loss 0.568, Learning Rate 2.000e-04, It/sec 2.643, Tokens/sec 337.517, Trained Tokens 953110, Peak mem 9.324 GB\n",
      "Iter 5770: Train loss 0.653, Learning Rate 2.000e-04, It/sec 2.676, Tokens/sec 312.020, Trained Tokens 954276, Peak mem 9.324 GB\n",
      "Iter 5780: Train loss 0.648, Learning Rate 2.000e-04, It/sec 2.378, Tokens/sec 344.545, Trained Tokens 955725, Peak mem 9.324 GB\n",
      "Iter 5790: Train loss 0.705, Learning Rate 2.000e-04, It/sec 1.490, Tokens/sec 362.004, Trained Tokens 958155, Peak mem 9.324 GB\n",
      "Iter 5800: Val loss 2.296, Val took 3.312s\n",
      "Iter 5800: Train loss 0.555, Learning Rate 2.000e-04, It/sec 31.425, Tokens/sec 4258.107, Trained Tokens 959510, Peak mem 9.324 GB\n",
      "Iter 5800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005800_adapters.safetensors.\n",
      "Iter 5810: Train loss 0.637, Learning Rate 2.000e-04, It/sec 2.247, Tokens/sec 315.097, Trained Tokens 960912, Peak mem 9.324 GB\n",
      "Iter 5820: Train loss 0.581, Learning Rate 2.000e-04, It/sec 2.055, Tokens/sec 348.405, Trained Tokens 962607, Peak mem 9.324 GB\n",
      "Iter 5830: Train loss 0.702, Learning Rate 2.000e-04, It/sec 1.673, Tokens/sec 371.157, Trained Tokens 964825, Peak mem 9.324 GB\n",
      "Iter 5840: Train loss 0.554, Learning Rate 2.000e-04, It/sec 2.743, Tokens/sec 322.896, Trained Tokens 966002, Peak mem 9.324 GB\n",
      "Iter 5850: Val loss 2.445, Val took 3.213s\n",
      "Iter 5850: Train loss 0.607, Learning Rate 2.000e-04, It/sec 21.924, Tokens/sec 3841.111, Trained Tokens 967754, Peak mem 9.324 GB\n",
      "Iter 5860: Train loss 0.595, Learning Rate 2.000e-04, It/sec 1.848, Tokens/sec 318.977, Trained Tokens 969480, Peak mem 9.324 GB\n",
      "Iter 5870: Train loss 0.610, Learning Rate 2.000e-04, It/sec 2.613, Tokens/sec 325.270, Trained Tokens 970725, Peak mem 9.324 GB\n",
      "Iter 5880: Train loss 0.636, Learning Rate 2.000e-04, It/sec 2.709, Tokens/sec 317.200, Trained Tokens 971896, Peak mem 9.324 GB\n",
      "Iter 5890: Train loss 0.655, Learning Rate 2.000e-04, It/sec 1.965, Tokens/sec 347.946, Trained Tokens 973667, Peak mem 9.324 GB\n",
      "Iter 5900: Val loss 2.637, Val took 3.430s\n",
      "Iter 5900: Train loss 0.643, Learning Rate 2.000e-04, It/sec 24.501, Tokens/sec 3636.011, Trained Tokens 975151, Peak mem 9.324 GB\n",
      "Iter 5900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0005900_adapters.safetensors.\n",
      "Iter 5910: Train loss 0.610, Learning Rate 2.000e-04, It/sec 2.191, Tokens/sec 336.769, Trained Tokens 976688, Peak mem 9.324 GB\n",
      "Iter 5920: Train loss 0.713, Learning Rate 2.000e-04, It/sec 1.916, Tokens/sec 367.394, Trained Tokens 978606, Peak mem 9.324 GB\n",
      "Iter 5930: Train loss 0.637, Learning Rate 2.000e-04, It/sec 2.417, Tokens/sec 340.282, Trained Tokens 980014, Peak mem 9.324 GB\n",
      "Iter 5940: Train loss 0.610, Learning Rate 2.000e-04, It/sec 1.990, Tokens/sec 358.161, Trained Tokens 981814, Peak mem 9.324 GB\n",
      "Iter 5950: Val loss 2.422, Val took 2.965s\n",
      "Iter 5950: Train loss 0.775, Learning Rate 2.000e-04, It/sec 38.112, Tokens/sec 6318.975, Trained Tokens 983472, Peak mem 9.324 GB\n",
      "Iter 5960: Train loss 0.624, Learning Rate 2.000e-04, It/sec 2.434, Tokens/sec 341.959, Trained Tokens 984877, Peak mem 9.324 GB\n",
      "Iter 5970: Train loss 0.729, Learning Rate 2.000e-04, It/sec 2.140, Tokens/sec 331.455, Trained Tokens 986426, Peak mem 9.324 GB\n",
      "Iter 5980: Train loss 0.890, Learning Rate 2.000e-04, It/sec 1.302, Tokens/sec 342.930, Trained Tokens 989060, Peak mem 9.324 GB\n",
      "Iter 5990: Train loss 0.659, Learning Rate 2.000e-04, It/sec 2.237, Tokens/sec 323.303, Trained Tokens 990505, Peak mem 9.324 GB\n",
      "Iter 6000: Val loss 2.606, Val took 3.175s\n",
      "Iter 6000: Train loss 0.622, Learning Rate 2.000e-04, It/sec 24.204, Tokens/sec 3841.171, Trained Tokens 992092, Peak mem 9.324 GB\n",
      "Iter 6000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006000_adapters.safetensors.\n",
      "Iter 6010: Train loss 0.657, Learning Rate 2.000e-04, It/sec 2.382, Tokens/sec 341.514, Trained Tokens 993526, Peak mem 9.324 GB\n",
      "Iter 6020: Train loss 0.579, Learning Rate 2.000e-04, It/sec 2.206, Tokens/sec 325.319, Trained Tokens 995001, Peak mem 9.324 GB\n",
      "Iter 6030: Train loss 0.666, Learning Rate 2.000e-04, It/sec 1.867, Tokens/sec 372.199, Trained Tokens 996995, Peak mem 9.324 GB\n",
      "Iter 6040: Train loss 0.763, Learning Rate 2.000e-04, It/sec 1.579, Tokens/sec 352.680, Trained Tokens 999228, Peak mem 9.324 GB\n",
      "Iter 6050: Val loss 2.148, Val took 3.029s\n",
      "Iter 6050: Train loss 0.627, Learning Rate 2.000e-04, It/sec 9.210, Tokens/sec 1328.075, Trained Tokens 1000670, Peak mem 9.324 GB\n",
      "Iter 6060: Train loss 0.653, Learning Rate 2.000e-04, It/sec 2.461, Tokens/sec 328.508, Trained Tokens 1002005, Peak mem 9.324 GB\n",
      "Iter 6070: Train loss 0.680, Learning Rate 2.000e-04, It/sec 2.014, Tokens/sec 360.856, Trained Tokens 1003797, Peak mem 9.324 GB\n",
      "Iter 6080: Train loss 0.809, Learning Rate 2.000e-04, It/sec 1.474, Tokens/sec 362.412, Trained Tokens 1006255, Peak mem 9.324 GB\n",
      "Iter 6090: Train loss 0.629, Learning Rate 2.000e-04, It/sec 2.488, Tokens/sec 335.093, Trained Tokens 1007602, Peak mem 9.324 GB\n",
      "Iter 6100: Val loss 2.230, Val took 2.846s\n",
      "Iter 6100: Train loss 0.626, Learning Rate 2.000e-04, It/sec 38.705, Tokens/sec 5383.928, Trained Tokens 1008993, Peak mem 9.324 GB\n",
      "Iter 6100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006100_adapters.safetensors.\n",
      "Iter 6110: Train loss 0.825, Learning Rate 2.000e-04, It/sec 1.183, Tokens/sec 363.905, Trained Tokens 1012070, Peak mem 9.324 GB\n",
      "Iter 6120: Train loss 0.637, Learning Rate 2.000e-04, It/sec 2.137, Tokens/sec 323.130, Trained Tokens 1013582, Peak mem 9.324 GB\n",
      "Iter 6130: Train loss 0.703, Learning Rate 2.000e-04, It/sec 2.118, Tokens/sec 314.487, Trained Tokens 1015067, Peak mem 9.324 GB\n",
      "Iter 6140: Train loss 0.670, Learning Rate 2.000e-04, It/sec 1.730, Tokens/sec 333.564, Trained Tokens 1016995, Peak mem 9.324 GB\n",
      "Iter 6150: Val loss 2.187, Val took 3.493s\n",
      "Iter 6150: Train loss 0.709, Learning Rate 2.000e-04, It/sec 26.782, Tokens/sec 4633.355, Trained Tokens 1018725, Peak mem 9.324 GB\n",
      "Iter 6160: Train loss 0.525, Learning Rate 2.000e-04, It/sec 3.079, Tokens/sec 332.219, Trained Tokens 1019804, Peak mem 9.324 GB\n",
      "Iter 6170: Train loss 0.648, Learning Rate 2.000e-04, It/sec 1.744, Tokens/sec 356.740, Trained Tokens 1021850, Peak mem 9.324 GB\n",
      "Iter 6180: Train loss 0.588, Learning Rate 2.000e-04, It/sec 2.247, Tokens/sec 341.945, Trained Tokens 1023372, Peak mem 9.324 GB\n",
      "Iter 6190: Train loss 0.551, Learning Rate 2.000e-04, It/sec 2.064, Tokens/sec 327.184, Trained Tokens 1024957, Peak mem 9.324 GB\n",
      "Iter 6200: Val loss 2.099, Val took 2.871s\n",
      "Iter 6200: Train loss 0.544, Learning Rate 2.000e-04, It/sec 30.844, Tokens/sec 5255.808, Trained Tokens 1026661, Peak mem 9.324 GB\n",
      "Iter 6200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006200_adapters.safetensors.\n",
      "Iter 6210: Train loss 0.581, Learning Rate 2.000e-04, It/sec 2.349, Tokens/sec 331.882, Trained Tokens 1028074, Peak mem 9.324 GB\n",
      "Iter 6220: Train loss 0.592, Learning Rate 2.000e-04, It/sec 1.909, Tokens/sec 338.888, Trained Tokens 1029849, Peak mem 9.324 GB\n",
      "Iter 6230: Train loss 0.582, Learning Rate 2.000e-04, It/sec 2.979, Tokens/sec 322.911, Trained Tokens 1030933, Peak mem 9.324 GB\n",
      "Iter 6240: Train loss 0.836, Learning Rate 2.000e-04, It/sec 1.415, Tokens/sec 333.812, Trained Tokens 1033292, Peak mem 9.324 GB\n",
      "Iter 6250: Val loss 1.971, Val took 3.256s\n",
      "Iter 6250: Train loss 0.491, Learning Rate 2.000e-04, It/sec 21.987, Tokens/sec 2908.908, Trained Tokens 1034615, Peak mem 9.324 GB\n",
      "Iter 6260: Train loss 0.534, Learning Rate 2.000e-04, It/sec 1.746, Tokens/sec 340.852, Trained Tokens 1036567, Peak mem 9.324 GB\n",
      "Iter 6270: Train loss 0.588, Learning Rate 2.000e-04, It/sec 2.198, Tokens/sec 331.608, Trained Tokens 1038076, Peak mem 9.324 GB\n",
      "Iter 6280: Train loss 0.527, Learning Rate 2.000e-04, It/sec 2.635, Tokens/sec 339.336, Trained Tokens 1039364, Peak mem 9.324 GB\n",
      "Iter 6290: Train loss 0.647, Learning Rate 2.000e-04, It/sec 1.987, Tokens/sec 349.432, Trained Tokens 1041123, Peak mem 9.324 GB\n",
      "Iter 6300: Val loss 2.233, Val took 2.742s\n",
      "Iter 6300: Train loss 0.575, Learning Rate 2.000e-04, It/sec 39.169, Tokens/sec 6122.076, Trained Tokens 1042686, Peak mem 9.324 GB\n",
      "Iter 6300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006300_adapters.safetensors.\n",
      "Iter 6310: Train loss 0.797, Learning Rate 2.000e-04, It/sec 1.035, Tokens/sec 346.431, Trained Tokens 1046032, Peak mem 9.324 GB\n",
      "Iter 6320: Train loss 0.527, Learning Rate 2.000e-04, It/sec 2.326, Tokens/sec 328.389, Trained Tokens 1047444, Peak mem 9.324 GB\n",
      "Iter 6330: Train loss 0.703, Learning Rate 2.000e-04, It/sec 1.746, Tokens/sec 351.314, Trained Tokens 1049456, Peak mem 9.324 GB\n",
      "Iter 6340: Train loss 0.697, Learning Rate 2.000e-04, It/sec 1.687, Tokens/sec 350.455, Trained Tokens 1051533, Peak mem 9.324 GB\n",
      "Iter 6350: Val loss 2.519, Val took 3.346s\n",
      "Iter 6350: Train loss 0.566, Learning Rate 2.000e-04, It/sec 40.036, Tokens/sec 6609.992, Trained Tokens 1053184, Peak mem 9.324 GB\n",
      "Iter 6360: Train loss 0.581, Learning Rate 2.000e-04, It/sec 1.942, Tokens/sec 270.471, Trained Tokens 1054577, Peak mem 9.324 GB\n",
      "Iter 6370: Train loss 0.615, Learning Rate 2.000e-04, It/sec 2.090, Tokens/sec 368.479, Trained Tokens 1056340, Peak mem 9.324 GB\n",
      "Iter 6380: Train loss 0.598, Learning Rate 2.000e-04, It/sec 1.826, Tokens/sec 342.853, Trained Tokens 1058218, Peak mem 9.324 GB\n",
      "Iter 6390: Train loss 0.611, Learning Rate 2.000e-04, It/sec 2.247, Tokens/sec 323.282, Trained Tokens 1059657, Peak mem 9.324 GB\n",
      "Iter 6400: Val loss 2.107, Val took 2.889s\n",
      "Iter 6400: Train loss 0.626, Learning Rate 2.000e-04, It/sec 22.014, Tokens/sec 2518.352, Trained Tokens 1060801, Peak mem 9.324 GB\n",
      "Iter 6400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006400_adapters.safetensors.\n",
      "Iter 6410: Train loss 0.679, Learning Rate 2.000e-04, It/sec 2.011, Tokens/sec 325.144, Trained Tokens 1062418, Peak mem 9.324 GB\n",
      "Iter 6420: Train loss 0.643, Learning Rate 2.000e-04, It/sec 1.525, Tokens/sec 366.719, Trained Tokens 1064823, Peak mem 9.324 GB\n",
      "Iter 6430: Train loss 0.569, Learning Rate 2.000e-04, It/sec 1.782, Tokens/sec 329.015, Trained Tokens 1066669, Peak mem 9.324 GB\n",
      "Iter 6440: Train loss 0.625, Learning Rate 2.000e-04, It/sec 1.949, Tokens/sec 349.097, Trained Tokens 1068460, Peak mem 9.324 GB\n",
      "Iter 6450: Val loss 2.357, Val took 3.056s\n",
      "Iter 6450: Train loss 0.564, Learning Rate 2.000e-04, It/sec 39.462, Tokens/sec 4289.469, Trained Tokens 1069547, Peak mem 9.324 GB\n",
      "Iter 6460: Train loss 0.753, Learning Rate 2.000e-04, It/sec 1.335, Tokens/sec 314.567, Trained Tokens 1071904, Peak mem 9.324 GB\n",
      "Iter 6470: Train loss 0.703, Learning Rate 2.000e-04, It/sec 2.106, Tokens/sec 364.255, Trained Tokens 1073634, Peak mem 9.324 GB\n",
      "Iter 6480: Train loss 0.626, Learning Rate 2.000e-04, It/sec 2.770, Tokens/sec 345.704, Trained Tokens 1074882, Peak mem 9.324 GB\n",
      "Iter 6490: Train loss 0.569, Learning Rate 2.000e-04, It/sec 2.372, Tokens/sec 339.740, Trained Tokens 1076314, Peak mem 9.324 GB\n",
      "Iter 6500: Val loss 2.164, Val took 3.232s\n",
      "Iter 6500: Train loss 0.594, Learning Rate 2.000e-04, It/sec 5.453, Tokens/sec 966.206, Trained Tokens 1078086, Peak mem 9.324 GB\n",
      "Iter 6500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006500_adapters.safetensors.\n",
      "Iter 6510: Train loss 0.676, Learning Rate 2.000e-04, It/sec 2.062, Tokens/sec 349.344, Trained Tokens 1079780, Peak mem 9.324 GB\n",
      "Iter 6520: Train loss 0.599, Learning Rate 2.000e-04, It/sec 2.754, Tokens/sec 343.370, Trained Tokens 1081027, Peak mem 9.324 GB\n",
      "Iter 6530: Train loss 0.613, Learning Rate 2.000e-04, It/sec 2.393, Tokens/sec 359.377, Trained Tokens 1082529, Peak mem 9.324 GB\n",
      "Iter 6540: Train loss 0.633, Learning Rate 2.000e-04, It/sec 2.137, Tokens/sec 349.126, Trained Tokens 1084163, Peak mem 9.324 GB\n",
      "Iter 6550: Val loss 2.384, Val took 3.029s\n",
      "Iter 6550: Train loss 0.590, Learning Rate 2.000e-04, It/sec 31.346, Tokens/sec 3708.261, Trained Tokens 1085346, Peak mem 9.324 GB\n",
      "Iter 6560: Train loss 0.660, Learning Rate 2.000e-04, It/sec 2.716, Tokens/sec 351.444, Trained Tokens 1086640, Peak mem 9.324 GB\n",
      "Iter 6570: Train loss 0.516, Learning Rate 2.000e-04, It/sec 2.669, Tokens/sec 331.206, Trained Tokens 1087881, Peak mem 9.324 GB\n",
      "Iter 6580: Train loss 0.715, Learning Rate 2.000e-04, It/sec 1.457, Tokens/sec 369.448, Trained Tokens 1090416, Peak mem 9.324 GB\n",
      "Iter 6590: Train loss 0.543, Learning Rate 2.000e-04, It/sec 2.344, Tokens/sec 347.200, Trained Tokens 1091897, Peak mem 9.324 GB\n",
      "Iter 6600: Val loss 2.316, Val took 3.095s\n",
      "Iter 6600: Train loss 0.653, Learning Rate 2.000e-04, It/sec 24.828, Tokens/sec 5216.327, Trained Tokens 1093998, Peak mem 9.324 GB\n",
      "Iter 6600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006600_adapters.safetensors.\n",
      "Iter 6610: Train loss 0.558, Learning Rate 2.000e-04, It/sec 1.725, Tokens/sec 349.891, Trained Tokens 1096026, Peak mem 9.324 GB\n",
      "Iter 6620: Train loss 0.547, Learning Rate 2.000e-04, It/sec 2.154, Tokens/sec 348.524, Trained Tokens 1097644, Peak mem 9.324 GB\n",
      "Iter 6630: Train loss 0.643, Learning Rate 2.000e-04, It/sec 1.514, Tokens/sec 361.788, Trained Tokens 1100033, Peak mem 9.324 GB\n",
      "Iter 6640: Train loss 0.532, Learning Rate 2.000e-04, It/sec 2.046, Tokens/sec 342.579, Trained Tokens 1101707, Peak mem 9.324 GB\n",
      "Iter 6650: Val loss 2.747, Val took 2.826s\n",
      "Iter 6650: Train loss 0.528, Learning Rate 2.000e-04, It/sec 22.309, Tokens/sec 3190.175, Trained Tokens 1103137, Peak mem 9.324 GB\n",
      "Iter 6660: Train loss 0.602, Learning Rate 2.000e-04, It/sec 2.328, Tokens/sec 333.877, Trained Tokens 1104571, Peak mem 9.324 GB\n",
      "Iter 6670: Train loss 0.513, Learning Rate 2.000e-04, It/sec 2.262, Tokens/sec 349.237, Trained Tokens 1106115, Peak mem 9.324 GB\n",
      "Iter 6680: Train loss 0.534, Learning Rate 2.000e-04, It/sec 1.834, Tokens/sec 358.763, Trained Tokens 1108071, Peak mem 9.324 GB\n",
      "Iter 6690: Train loss 0.543, Learning Rate 2.000e-04, It/sec 2.533, Tokens/sec 356.355, Trained Tokens 1109478, Peak mem 9.324 GB\n",
      "Iter 6700: Val loss 2.659, Val took 2.825s\n",
      "Iter 6700: Train loss 0.498, Learning Rate 2.000e-04, It/sec 24.506, Tokens/sec 3403.845, Trained Tokens 1110867, Peak mem 9.324 GB\n",
      "Iter 6700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006700_adapters.safetensors.\n",
      "Iter 6710: Train loss 0.658, Learning Rate 2.000e-04, It/sec 1.783, Tokens/sec 353.934, Trained Tokens 1112852, Peak mem 9.324 GB\n",
      "Iter 6720: Train loss 0.599, Learning Rate 2.000e-04, It/sec 2.028, Tokens/sec 355.638, Trained Tokens 1114606, Peak mem 9.324 GB\n",
      "Iter 6730: Train loss 0.600, Learning Rate 2.000e-04, It/sec 2.053, Tokens/sec 345.564, Trained Tokens 1116289, Peak mem 9.324 GB\n",
      "Iter 6740: Train loss 0.593, Learning Rate 2.000e-04, It/sec 2.300, Tokens/sec 345.033, Trained Tokens 1117789, Peak mem 9.324 GB\n",
      "Iter 6750: Val loss 2.512, Val took 3.807s\n",
      "Iter 6750: Train loss 0.739, Learning Rate 2.000e-04, It/sec 53.616, Tokens/sec 9731.380, Trained Tokens 1119604, Peak mem 9.324 GB\n",
      "Iter 6760: Train loss 0.587, Learning Rate 2.000e-04, It/sec 1.774, Tokens/sec 332.621, Trained Tokens 1121479, Peak mem 9.324 GB\n",
      "Iter 6770: Train loss 0.535, Learning Rate 2.000e-04, It/sec 2.652, Tokens/sec 361.162, Trained Tokens 1122841, Peak mem 9.324 GB\n",
      "Iter 6780: Train loss 0.610, Learning Rate 2.000e-04, It/sec 2.851, Tokens/sec 335.579, Trained Tokens 1124018, Peak mem 9.324 GB\n",
      "Iter 6790: Train loss 0.713, Learning Rate 2.000e-04, It/sec 2.234, Tokens/sec 359.610, Trained Tokens 1125628, Peak mem 9.324 GB\n",
      "Iter 6800: Val loss 2.400, Val took 3.024s\n",
      "Iter 6800: Train loss 0.579, Learning Rate 2.000e-04, It/sec 12.581, Tokens/sec 1746.273, Trained Tokens 1127016, Peak mem 9.324 GB\n",
      "Iter 6800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006800_adapters.safetensors.\n",
      "Iter 6810: Train loss 0.727, Learning Rate 2.000e-04, It/sec 1.390, Tokens/sec 367.556, Trained Tokens 1129660, Peak mem 9.324 GB\n",
      "Iter 6820: Train loss 0.669, Learning Rate 2.000e-04, It/sec 1.817, Tokens/sec 328.608, Trained Tokens 1131469, Peak mem 9.324 GB\n",
      "Iter 6830: Train loss 0.661, Learning Rate 2.000e-04, It/sec 2.078, Tokens/sec 326.424, Trained Tokens 1133040, Peak mem 9.324 GB\n",
      "Iter 6840: Train loss 0.620, Learning Rate 2.000e-04, It/sec 1.855, Tokens/sec 334.751, Trained Tokens 1134845, Peak mem 9.324 GB\n",
      "Iter 6850: Val loss 2.375, Val took 3.109s\n",
      "Iter 6850: Train loss 0.590, Learning Rate 2.000e-04, It/sec 21.478, Tokens/sec 3303.375, Trained Tokens 1136383, Peak mem 9.324 GB\n",
      "Iter 6860: Train loss 0.601, Learning Rate 2.000e-04, It/sec 2.150, Tokens/sec 325.665, Trained Tokens 1137898, Peak mem 9.324 GB\n",
      "Iter 6870: Train loss 0.583, Learning Rate 2.000e-04, It/sec 2.183, Tokens/sec 354.908, Trained Tokens 1139524, Peak mem 9.324 GB\n",
      "Iter 6880: Train loss 0.600, Learning Rate 2.000e-04, It/sec 2.952, Tokens/sec 348.390, Trained Tokens 1140704, Peak mem 9.324 GB\n",
      "Iter 6890: Train loss 0.554, Learning Rate 2.000e-04, It/sec 2.419, Tokens/sec 343.448, Trained Tokens 1142124, Peak mem 9.324 GB\n",
      "Iter 6900: Val loss 2.208, Val took 2.942s\n",
      "Iter 6900: Train loss 0.650, Learning Rate 2.000e-04, It/sec 14.286, Tokens/sec 2281.404, Trained Tokens 1143721, Peak mem 9.324 GB\n",
      "Iter 6900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0006900_adapters.safetensors.\n",
      "Iter 6910: Train loss 0.614, Learning Rate 2.000e-04, It/sec 1.490, Tokens/sec 309.273, Trained Tokens 1145797, Peak mem 9.324 GB\n",
      "Iter 6920: Train loss 0.604, Learning Rate 2.000e-04, It/sec 1.965, Tokens/sec 349.407, Trained Tokens 1147575, Peak mem 9.324 GB\n",
      "Iter 6930: Train loss 0.604, Learning Rate 2.000e-04, It/sec 2.343, Tokens/sec 336.405, Trained Tokens 1149011, Peak mem 9.324 GB\n",
      "Iter 6940: Train loss 0.643, Learning Rate 2.000e-04, It/sec 2.270, Tokens/sec 347.235, Trained Tokens 1150541, Peak mem 9.324 GB\n",
      "Iter 6950: Val loss 2.220, Val took 3.321s\n",
      "Iter 6950: Train loss 0.727, Learning Rate 2.000e-04, It/sec 54.750, Tokens/sec 8081.164, Trained Tokens 1152017, Peak mem 9.324 GB\n",
      "Iter 6960: Train loss 0.608, Learning Rate 2.000e-04, It/sec 2.077, Tokens/sec 317.590, Trained Tokens 1153546, Peak mem 9.324 GB\n",
      "Iter 6970: Train loss 0.638, Learning Rate 2.000e-04, It/sec 3.276, Tokens/sec 330.579, Trained Tokens 1154555, Peak mem 9.324 GB\n",
      "Iter 6980: Train loss 0.598, Learning Rate 2.000e-04, It/sec 2.121, Tokens/sec 358.205, Trained Tokens 1156244, Peak mem 9.324 GB\n",
      "Iter 6990: Train loss 0.553, Learning Rate 2.000e-04, It/sec 2.293, Tokens/sec 339.802, Trained Tokens 1157726, Peak mem 9.324 GB\n",
      "Iter 7000: Val loss 2.717, Val took 3.032s\n",
      "Iter 7000: Train loss 0.635, Learning Rate 2.000e-04, It/sec 39.196, Tokens/sec 5561.920, Trained Tokens 1159145, Peak mem 9.324 GB\n",
      "Iter 7000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007000_adapters.safetensors.\n",
      "Iter 7010: Train loss 0.576, Learning Rate 2.000e-04, It/sec 2.157, Tokens/sec 314.755, Trained Tokens 1160604, Peak mem 9.324 GB\n",
      "Iter 7020: Train loss 0.497, Learning Rate 2.000e-04, It/sec 1.875, Tokens/sec 357.769, Trained Tokens 1162512, Peak mem 9.324 GB\n",
      "Iter 7030: Train loss 0.565, Learning Rate 2.000e-04, It/sec 2.208, Tokens/sec 341.592, Trained Tokens 1164059, Peak mem 9.324 GB\n",
      "Iter 7040: Train loss 0.556, Learning Rate 2.000e-04, It/sec 2.493, Tokens/sec 333.747, Trained Tokens 1165398, Peak mem 9.324 GB\n",
      "Iter 7050: Val loss 2.572, Val took 2.995s\n",
      "Iter 7050: Train loss 0.502, Learning Rate 2.000e-04, It/sec 38.775, Tokens/sec 5789.050, Trained Tokens 1166891, Peak mem 9.324 GB\n",
      "Iter 7060: Train loss 0.801, Learning Rate 2.000e-04, It/sec 1.364, Tokens/sec 344.744, Trained Tokens 1169419, Peak mem 9.324 GB\n",
      "Iter 7070: Train loss 0.519, Learning Rate 2.000e-04, It/sec 2.271, Tokens/sec 362.758, Trained Tokens 1171016, Peak mem 9.324 GB\n",
      "Iter 7080: Train loss 0.560, Learning Rate 2.000e-04, It/sec 2.384, Tokens/sec 357.665, Trained Tokens 1172516, Peak mem 9.324 GB\n",
      "Iter 7090: Train loss 0.637, Learning Rate 2.000e-04, It/sec 2.765, Tokens/sec 355.016, Trained Tokens 1173800, Peak mem 9.324 GB\n",
      "Iter 7100: Val loss 2.447, Val took 3.099s\n",
      "Iter 7100: Train loss 0.712, Learning Rate 2.000e-04, It/sec 7.162, Tokens/sec 1820.593, Trained Tokens 1176342, Peak mem 9.324 GB\n",
      "Iter 7100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007100_adapters.safetensors.\n",
      "Iter 7110: Train loss 0.626, Learning Rate 2.000e-04, It/sec 1.981, Tokens/sec 323.262, Trained Tokens 1177974, Peak mem 9.324 GB\n",
      "Iter 7120: Train loss 0.638, Learning Rate 2.000e-04, It/sec 1.993, Tokens/sec 356.919, Trained Tokens 1179765, Peak mem 9.324 GB\n",
      "Iter 7130: Train loss 0.535, Learning Rate 2.000e-04, It/sec 2.721, Tokens/sec 338.748, Trained Tokens 1181010, Peak mem 9.324 GB\n",
      "Iter 7140: Train loss 0.564, Learning Rate 2.000e-04, It/sec 2.895, Tokens/sec 346.833, Trained Tokens 1182208, Peak mem 9.324 GB\n",
      "Iter 7150: Val loss 2.055, Val took 3.173s\n",
      "Iter 7150: Train loss 0.526, Learning Rate 2.000e-04, It/sec 23.911, Tokens/sec 2950.588, Trained Tokens 1183442, Peak mem 9.324 GB\n",
      "Iter 7160: Train loss 0.576, Learning Rate 2.000e-04, It/sec 2.138, Tokens/sec 351.561, Trained Tokens 1185086, Peak mem 9.324 GB\n",
      "Iter 7170: Train loss 0.604, Learning Rate 2.000e-04, It/sec 2.308, Tokens/sec 342.538, Trained Tokens 1186570, Peak mem 9.324 GB\n",
      "Iter 7180: Train loss 0.575, Learning Rate 2.000e-04, It/sec 2.475, Tokens/sec 351.977, Trained Tokens 1187992, Peak mem 9.324 GB\n",
      "Iter 7190: Train loss 0.601, Learning Rate 2.000e-04, It/sec 2.064, Tokens/sec 351.237, Trained Tokens 1189694, Peak mem 9.324 GB\n",
      "Iter 7200: Val loss 2.134, Val took 2.781s\n",
      "Iter 7200: Train loss 0.546, Learning Rate 2.000e-04, It/sec 22.346, Tokens/sec 3128.407, Trained Tokens 1191094, Peak mem 9.324 GB\n",
      "Iter 7200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007200_adapters.safetensors.\n",
      "Iter 7210: Train loss 0.643, Learning Rate 2.000e-04, It/sec 2.253, Tokens/sec 334.733, Trained Tokens 1192580, Peak mem 9.324 GB\n",
      "Iter 7220: Train loss 0.626, Learning Rate 2.000e-04, It/sec 1.907, Tokens/sec 350.569, Trained Tokens 1194418, Peak mem 9.324 GB\n",
      "Iter 7230: Train loss 0.577, Learning Rate 2.000e-04, It/sec 2.142, Tokens/sec 342.551, Trained Tokens 1196017, Peak mem 9.324 GB\n",
      "Iter 7240: Train loss 0.701, Learning Rate 2.000e-04, It/sec 2.167, Tokens/sec 350.544, Trained Tokens 1197635, Peak mem 9.324 GB\n",
      "Iter 7250: Val loss 2.445, Val took 3.164s\n",
      "Iter 7250: Train loss 0.555, Learning Rate 2.000e-04, It/sec 38.794, Tokens/sec 5854.062, Trained Tokens 1199144, Peak mem 9.324 GB\n",
      "Iter 7260: Train loss 0.517, Learning Rate 2.000e-04, It/sec 2.295, Tokens/sec 347.752, Trained Tokens 1200659, Peak mem 9.324 GB\n",
      "Iter 7270: Train loss 0.546, Learning Rate 2.000e-04, It/sec 2.448, Tokens/sec 362.128, Trained Tokens 1202138, Peak mem 9.324 GB\n",
      "Iter 7280: Train loss 0.620, Learning Rate 2.000e-04, It/sec 2.001, Tokens/sec 331.710, Trained Tokens 1203796, Peak mem 9.324 GB\n",
      "Iter 7290: Train loss 0.623, Learning Rate 2.000e-04, It/sec 2.663, Tokens/sec 325.905, Trained Tokens 1205020, Peak mem 9.324 GB\n",
      "Iter 7300: Val loss 2.342, Val took 3.305s\n",
      "Iter 7300: Train loss 0.653, Learning Rate 2.000e-04, It/sec 38.908, Tokens/sec 8322.482, Trained Tokens 1207159, Peak mem 9.324 GB\n",
      "Iter 7300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007300_adapters.safetensors.\n",
      "Iter 7310: Train loss 0.639, Learning Rate 2.000e-04, It/sec 1.978, Tokens/sec 322.595, Trained Tokens 1208790, Peak mem 9.324 GB\n",
      "Iter 7320: Train loss 0.621, Learning Rate 2.000e-04, It/sec 2.354, Tokens/sec 336.835, Trained Tokens 1210221, Peak mem 9.324 GB\n",
      "Iter 7330: Train loss 0.937, Learning Rate 2.000e-04, It/sec 0.759, Tokens/sec 353.951, Trained Tokens 1214887, Peak mem 9.324 GB\n",
      "Iter 7340: Train loss 0.605, Learning Rate 2.000e-04, It/sec 2.137, Tokens/sec 345.202, Trained Tokens 1216502, Peak mem 9.324 GB\n",
      "Iter 7350: Val loss 2.405, Val took 3.307s\n",
      "Iter 7350: Train loss 0.583, Learning Rate 2.000e-04, It/sec 40.178, Tokens/sec 5223.161, Trained Tokens 1217802, Peak mem 9.324 GB\n",
      "Iter 7360: Train loss 0.657, Learning Rate 2.000e-04, It/sec 2.674, Tokens/sec 327.563, Trained Tokens 1219027, Peak mem 9.324 GB\n",
      "Iter 7370: Train loss 0.586, Learning Rate 2.000e-04, It/sec 2.586, Tokens/sec 336.448, Trained Tokens 1220328, Peak mem 9.324 GB\n",
      "Iter 7380: Train loss 0.747, Learning Rate 2.000e-04, It/sec 1.586, Tokens/sec 339.639, Trained Tokens 1222470, Peak mem 9.324 GB\n",
      "Iter 7390: Train loss 0.603, Learning Rate 2.000e-04, It/sec 1.943, Tokens/sec 332.503, Trained Tokens 1224181, Peak mem 9.324 GB\n",
      "Iter 7400: Val loss 2.531, Val took 4.380s\n",
      "Iter 7400: Train loss 0.524, Learning Rate 2.000e-04, It/sec 36.714, Tokens/sec 5011.426, Trained Tokens 1225546, Peak mem 9.324 GB\n",
      "Iter 7400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007400_adapters.safetensors.\n",
      "Iter 7410: Train loss 0.477, Learning Rate 2.000e-04, It/sec 2.550, Tokens/sec 331.214, Trained Tokens 1226845, Peak mem 9.324 GB\n",
      "Iter 7420: Train loss 0.663, Learning Rate 2.000e-04, It/sec 1.318, Tokens/sec 356.013, Trained Tokens 1229546, Peak mem 9.324 GB\n",
      "Iter 7430: Train loss 0.521, Learning Rate 2.000e-04, It/sec 1.916, Tokens/sec 350.512, Trained Tokens 1231375, Peak mem 9.324 GB\n",
      "Iter 7440: Train loss 0.543, Learning Rate 2.000e-04, It/sec 1.562, Tokens/sec 323.859, Trained Tokens 1233449, Peak mem 9.324 GB\n",
      "Iter 7450: Val loss 2.526, Val took 4.203s\n",
      "Iter 7450: Train loss 0.472, Learning Rate 2.000e-04, It/sec 21.432, Tokens/sec 2781.938, Trained Tokens 1234747, Peak mem 9.324 GB\n",
      "Iter 7460: Train loss 0.531, Learning Rate 2.000e-04, It/sec 2.490, Tokens/sec 344.323, Trained Tokens 1236130, Peak mem 9.324 GB\n",
      "Iter 7470: Train loss 0.643, Learning Rate 2.000e-04, It/sec 1.735, Tokens/sec 352.540, Trained Tokens 1238162, Peak mem 9.324 GB\n",
      "Iter 7480: Train loss 0.500, Learning Rate 2.000e-04, It/sec 2.881, Tokens/sec 331.013, Trained Tokens 1239311, Peak mem 9.324 GB\n",
      "Iter 7490: Train loss 0.570, Learning Rate 2.000e-04, It/sec 2.473, Tokens/sec 320.285, Trained Tokens 1240606, Peak mem 9.324 GB\n",
      "Iter 7500: Val loss 2.306, Val took 3.081s\n",
      "Iter 7500: Train loss 0.549, Learning Rate 2.000e-04, It/sec 18.146, Tokens/sec 2761.807, Trained Tokens 1242128, Peak mem 9.324 GB\n",
      "Iter 7500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007500_adapters.safetensors.\n",
      "Iter 7510: Train loss 0.560, Learning Rate 2.000e-04, It/sec 2.057, Tokens/sec 322.481, Trained Tokens 1243696, Peak mem 9.324 GB\n",
      "Iter 7520: Train loss 0.573, Learning Rate 2.000e-04, It/sec 1.872, Tokens/sec 348.933, Trained Tokens 1245560, Peak mem 9.324 GB\n",
      "Iter 7530: Train loss 0.493, Learning Rate 2.000e-04, It/sec 2.084, Tokens/sec 362.551, Trained Tokens 1247300, Peak mem 9.324 GB\n",
      "Iter 7540: Train loss 0.561, Learning Rate 2.000e-04, It/sec 2.508, Tokens/sec 343.819, Trained Tokens 1248671, Peak mem 9.324 GB\n",
      "Iter 7550: Val loss 2.291, Val took 3.776s\n",
      "Iter 7550: Train loss 0.522, Learning Rate 2.000e-04, It/sec 23.757, Tokens/sec 4435.374, Trained Tokens 1250538, Peak mem 9.324 GB\n",
      "Iter 7560: Train loss 0.554, Learning Rate 2.000e-04, It/sec 1.800, Tokens/sec 363.605, Trained Tokens 1252558, Peak mem 9.324 GB\n",
      "Iter 7570: Train loss 0.540, Learning Rate 2.000e-04, It/sec 2.096, Tokens/sec 316.922, Trained Tokens 1254070, Peak mem 9.324 GB\n",
      "Iter 7580: Train loss 0.583, Learning Rate 2.000e-04, It/sec 3.286, Tokens/sec 344.423, Trained Tokens 1255118, Peak mem 9.324 GB\n",
      "Iter 7590: Train loss 0.669, Learning Rate 2.000e-04, It/sec 1.143, Tokens/sec 356.516, Trained Tokens 1258236, Peak mem 9.324 GB\n",
      "Iter 7600: Val loss 2.478, Val took 3.285s\n",
      "Iter 7600: Train loss 0.590, Learning Rate 2.000e-04, It/sec 39.121, Tokens/sec 6591.818, Trained Tokens 1259921, Peak mem 9.324 GB\n",
      "Iter 7600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007600_adapters.safetensors.\n",
      "Iter 7610: Train loss 0.721, Learning Rate 2.000e-04, It/sec 1.372, Tokens/sec 330.619, Trained Tokens 1262330, Peak mem 9.324 GB\n",
      "Iter 7620: Train loss 0.622, Learning Rate 2.000e-04, It/sec 2.720, Tokens/sec 339.411, Trained Tokens 1263578, Peak mem 9.324 GB\n",
      "Iter 7630: Train loss 0.632, Learning Rate 2.000e-04, It/sec 1.666, Tokens/sec 323.308, Trained Tokens 1265519, Peak mem 9.324 GB\n",
      "Iter 7640: Train loss 0.662, Learning Rate 2.000e-04, It/sec 2.780, Tokens/sec 297.984, Trained Tokens 1266591, Peak mem 9.324 GB\n",
      "Iter 7650: Val loss 2.669, Val took 3.873s\n",
      "Iter 7650: Train loss 0.551, Learning Rate 2.000e-04, It/sec 13.377, Tokens/sec 2179.060, Trained Tokens 1268220, Peak mem 9.324 GB\n",
      "Iter 7660: Train loss 0.589, Learning Rate 2.000e-04, It/sec 2.175, Tokens/sec 319.342, Trained Tokens 1269688, Peak mem 9.324 GB\n",
      "Iter 7670: Train loss 0.619, Learning Rate 2.000e-04, It/sec 1.926, Tokens/sec 345.197, Trained Tokens 1271480, Peak mem 9.324 GB\n",
      "Iter 7680: Train loss 0.582, Learning Rate 2.000e-04, It/sec 2.684, Tokens/sec 334.634, Trained Tokens 1272727, Peak mem 9.324 GB\n",
      "Iter 7690: Train loss 0.632, Learning Rate 2.000e-04, It/sec 2.550, Tokens/sec 339.095, Trained Tokens 1274057, Peak mem 9.324 GB\n",
      "Iter 7700: Val loss 2.185, Val took 2.727s\n",
      "Iter 7700: Train loss 0.584, Learning Rate 2.000e-04, It/sec 20.808, Tokens/sec 2332.537, Trained Tokens 1275178, Peak mem 9.324 GB\n",
      "Iter 7700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007700_adapters.safetensors.\n",
      "Iter 7710: Train loss 0.638, Learning Rate 2.000e-04, It/sec 1.694, Tokens/sec 330.173, Trained Tokens 1277127, Peak mem 9.324 GB\n",
      "Iter 7720: Train loss 0.552, Learning Rate 2.000e-04, It/sec 2.442, Tokens/sec 323.793, Trained Tokens 1278453, Peak mem 9.324 GB\n",
      "Iter 7730: Train loss 0.611, Learning Rate 2.000e-04, It/sec 2.275, Tokens/sec 354.225, Trained Tokens 1280010, Peak mem 9.324 GB\n",
      "Iter 7740: Train loss 0.770, Learning Rate 2.000e-04, It/sec 1.598, Tokens/sec 357.169, Trained Tokens 1282245, Peak mem 9.324 GB\n",
      "Iter 7750: Val loss 2.384, Val took 3.203s\n",
      "Iter 7750: Train loss 0.650, Learning Rate 2.000e-04, It/sec 14.158, Tokens/sec 2235.508, Trained Tokens 1283824, Peak mem 9.324 GB\n",
      "Iter 7760: Train loss 0.656, Learning Rate 2.000e-04, It/sec 2.440, Tokens/sec 353.501, Trained Tokens 1285273, Peak mem 9.324 GB\n",
      "Iter 7770: Train loss 0.552, Learning Rate 2.000e-04, It/sec 2.142, Tokens/sec 327.520, Trained Tokens 1286802, Peak mem 9.324 GB\n",
      "Iter 7780: Train loss 0.620, Learning Rate 2.000e-04, It/sec 1.645, Tokens/sec 355.982, Trained Tokens 1288966, Peak mem 9.324 GB\n",
      "Iter 7790: Train loss 0.594, Learning Rate 2.000e-04, It/sec 2.414, Tokens/sec 342.479, Trained Tokens 1290385, Peak mem 9.324 GB\n",
      "Iter 7800: Val loss 2.261, Val took 3.466s\n",
      "Iter 7800: Train loss 0.488, Learning Rate 2.000e-04, It/sec 55.271, Tokens/sec 7107.890, Trained Tokens 1291671, Peak mem 9.324 GB\n",
      "Iter 7800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007800_adapters.safetensors.\n",
      "Iter 7810: Train loss 0.537, Learning Rate 2.000e-04, It/sec 2.824, Tokens/sec 328.962, Trained Tokens 1292836, Peak mem 9.324 GB\n",
      "Iter 7820: Train loss 0.537, Learning Rate 2.000e-04, It/sec 2.158, Tokens/sec 364.646, Trained Tokens 1294526, Peak mem 9.324 GB\n",
      "Iter 7830: Train loss 0.495, Learning Rate 2.000e-04, It/sec 2.355, Tokens/sec 349.910, Trained Tokens 1296012, Peak mem 9.324 GB\n",
      "Iter 7840: Train loss 0.503, Learning Rate 2.000e-04, It/sec 1.899, Tokens/sec 356.719, Trained Tokens 1297890, Peak mem 9.324 GB\n",
      "Iter 7850: Val loss 2.781, Val took 3.549s\n",
      "Iter 7850: Train loss 0.576, Learning Rate 2.000e-04, It/sec 40.183, Tokens/sec 5255.935, Trained Tokens 1299198, Peak mem 9.324 GB\n",
      "Iter 7860: Train loss 0.498, Learning Rate 2.000e-04, It/sec 2.610, Tokens/sec 351.109, Trained Tokens 1300543, Peak mem 9.324 GB\n",
      "Iter 7870: Train loss 0.609, Learning Rate 2.000e-04, It/sec 1.838, Tokens/sec 352.890, Trained Tokens 1302463, Peak mem 9.324 GB\n",
      "Iter 7880: Train loss 0.560, Learning Rate 2.000e-04, It/sec 2.396, Tokens/sec 342.365, Trained Tokens 1303892, Peak mem 9.324 GB\n",
      "Iter 7890: Train loss 0.479, Learning Rate 2.000e-04, It/sec 2.483, Tokens/sec 338.888, Trained Tokens 1305257, Peak mem 9.324 GB\n",
      "Iter 7900: Val loss 2.531, Val took 3.433s\n",
      "Iter 7900: Train loss 0.468, Learning Rate 2.000e-04, It/sec 7.721, Tokens/sec 1516.342, Trained Tokens 1307221, Peak mem 9.324 GB\n",
      "Iter 7900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0007900_adapters.safetensors.\n",
      "Iter 7910: Train loss 0.503, Learning Rate 2.000e-04, It/sec 2.299, Tokens/sec 290.868, Trained Tokens 1308486, Peak mem 9.324 GB\n",
      "Iter 7920: Train loss 0.495, Learning Rate 2.000e-04, It/sec 2.618, Tokens/sec 338.205, Trained Tokens 1309778, Peak mem 9.324 GB\n",
      "Iter 7930: Train loss 0.546, Learning Rate 2.000e-04, It/sec 2.502, Tokens/sec 336.576, Trained Tokens 1311123, Peak mem 9.324 GB\n",
      "Iter 7940: Train loss 0.629, Learning Rate 2.000e-04, It/sec 1.778, Tokens/sec 361.913, Trained Tokens 1313159, Peak mem 9.324 GB\n",
      "Iter 7950: Val loss 2.324, Val took 3.353s\n",
      "Iter 7950: Train loss 0.522, Learning Rate 2.000e-04, It/sec 39.606, Tokens/sec 5568.663, Trained Tokens 1314565, Peak mem 9.324 GB\n",
      "Iter 7960: Train loss 0.526, Learning Rate 2.000e-04, It/sec 2.056, Tokens/sec 358.526, Trained Tokens 1316309, Peak mem 9.324 GB\n",
      "Iter 7970: Train loss 0.544, Learning Rate 2.000e-04, It/sec 2.075, Tokens/sec 354.957, Trained Tokens 1318020, Peak mem 9.324 GB\n",
      "Iter 7980: Train loss 0.564, Learning Rate 2.000e-04, It/sec 2.419, Tokens/sec 336.174, Trained Tokens 1319410, Peak mem 9.324 GB\n",
      "Iter 7990: Train loss 0.602, Learning Rate 2.000e-04, It/sec 2.139, Tokens/sec 357.395, Trained Tokens 1321081, Peak mem 9.324 GB\n",
      "Iter 8000: Val loss 2.003, Val took 3.347s\n",
      "Iter 8000: Train loss 0.490, Learning Rate 2.000e-04, It/sec 17.597, Tokens/sec 2880.649, Trained Tokens 1322718, Peak mem 9.324 GB\n",
      "Iter 8000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008000_adapters.safetensors.\n",
      "Iter 8010: Train loss 0.578, Learning Rate 2.000e-04, It/sec 2.176, Tokens/sec 316.672, Trained Tokens 1324173, Peak mem 9.324 GB\n",
      "Iter 8020: Train loss 0.726, Learning Rate 2.000e-04, It/sec 1.837, Tokens/sec 354.935, Trained Tokens 1326105, Peak mem 9.324 GB\n",
      "Iter 8030: Train loss 0.544, Learning Rate 2.000e-04, It/sec 2.146, Tokens/sec 342.648, Trained Tokens 1327702, Peak mem 9.324 GB\n",
      "Iter 8040: Train loss 0.538, Learning Rate 2.000e-04, It/sec 2.235, Tokens/sec 337.493, Trained Tokens 1329212, Peak mem 9.324 GB\n",
      "Iter 8050: Val loss 2.666, Val took 2.969s\n",
      "Iter 8050: Train loss 0.489, Learning Rate 2.000e-04, It/sec 17.768, Tokens/sec 2638.478, Trained Tokens 1330697, Peak mem 9.324 GB\n",
      "Iter 8060: Train loss 0.601, Learning Rate 2.000e-04, It/sec 3.358, Tokens/sec 326.060, Trained Tokens 1331668, Peak mem 9.324 GB\n",
      "Iter 8070: Train loss 0.594, Learning Rate 2.000e-04, It/sec 1.995, Tokens/sec 355.301, Trained Tokens 1333449, Peak mem 9.324 GB\n",
      "Iter 8080: Train loss 0.634, Learning Rate 2.000e-04, It/sec 1.741, Tokens/sec 367.682, Trained Tokens 1335561, Peak mem 9.324 GB\n",
      "Iter 8090: Train loss 0.568, Learning Rate 2.000e-04, It/sec 2.523, Tokens/sec 341.656, Trained Tokens 1336915, Peak mem 9.324 GB\n",
      "Iter 8100: Val loss 2.727, Val took 3.079s\n",
      "Iter 8100: Train loss 0.575, Learning Rate 2.000e-04, It/sec 22.452, Tokens/sec 4414.133, Trained Tokens 1338881, Peak mem 9.324 GB\n",
      "Iter 8100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008100_adapters.safetensors.\n",
      "Iter 8110: Train loss 0.591, Learning Rate 2.000e-04, It/sec 2.467, Tokens/sec 326.604, Trained Tokens 1340205, Peak mem 9.324 GB\n",
      "Iter 8120: Train loss 0.540, Learning Rate 2.000e-04, It/sec 2.724, Tokens/sec 341.258, Trained Tokens 1341458, Peak mem 9.324 GB\n",
      "Iter 8130: Train loss 0.624, Learning Rate 2.000e-04, It/sec 2.024, Tokens/sec 349.356, Trained Tokens 1343184, Peak mem 9.324 GB\n",
      "Iter 8140: Train loss 0.723, Learning Rate 2.000e-04, It/sec 1.669, Tokens/sec 370.163, Trained Tokens 1345402, Peak mem 9.324 GB\n",
      "Iter 8150: Val loss 2.472, Val took 3.286s\n",
      "Iter 8150: Train loss 0.752, Learning Rate 2.000e-04, It/sec 53.508, Tokens/sec 13644.448, Trained Tokens 1347952, Peak mem 9.324 GB\n",
      "Iter 8160: Train loss 0.685, Learning Rate 2.000e-04, It/sec 1.842, Tokens/sec 324.906, Trained Tokens 1349716, Peak mem 9.324 GB\n",
      "Iter 8170: Train loss 0.608, Learning Rate 2.000e-04, It/sec 1.128, Tokens/sec 367.519, Trained Tokens 1352975, Peak mem 9.324 GB\n",
      "Iter 8180: Train loss 0.543, Learning Rate 2.000e-04, It/sec 2.321, Tokens/sec 335.904, Trained Tokens 1354422, Peak mem 9.324 GB\n",
      "Iter 8190: Train loss 0.615, Learning Rate 2.000e-04, It/sec 1.787, Tokens/sec 364.827, Trained Tokens 1356464, Peak mem 9.324 GB\n",
      "Iter 8200: Val loss 2.659, Val took 3.396s\n",
      "Iter 8200: Train loss 0.666, Learning Rate 2.000e-04, It/sec 23.241, Tokens/sec 4267.091, Trained Tokens 1358300, Peak mem 9.324 GB\n",
      "Iter 8200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008200_adapters.safetensors.\n",
      "Iter 8210: Train loss 0.526, Learning Rate 2.000e-04, It/sec 2.095, Tokens/sec 349.437, Trained Tokens 1359968, Peak mem 9.324 GB\n",
      "Iter 8220: Train loss 0.500, Learning Rate 2.000e-04, It/sec 1.792, Tokens/sec 362.110, Trained Tokens 1361989, Peak mem 9.324 GB\n",
      "Iter 8230: Train loss 0.471, Learning Rate 2.000e-04, It/sec 2.374, Tokens/sec 354.958, Trained Tokens 1363484, Peak mem 9.324 GB\n",
      "Iter 8240: Train loss 0.517, Learning Rate 2.000e-04, It/sec 2.299, Tokens/sec 334.999, Trained Tokens 1364941, Peak mem 9.324 GB\n",
      "Iter 8250: Val loss 2.603, Val took 3.546s\n",
      "Iter 8250: Train loss 0.640, Learning Rate 2.000e-04, It/sec 2.714, Tokens/sec 678.125, Trained Tokens 1367440, Peak mem 9.324 GB\n",
      "Iter 8260: Train loss 0.560, Learning Rate 2.000e-04, It/sec 2.423, Tokens/sec 337.466, Trained Tokens 1368833, Peak mem 9.324 GB\n",
      "Iter 8270: Train loss 0.486, Learning Rate 2.000e-04, It/sec 2.420, Tokens/sec 339.556, Trained Tokens 1370236, Peak mem 9.324 GB\n",
      "Iter 8280: Train loss 0.521, Learning Rate 2.000e-04, It/sec 2.500, Tokens/sec 339.464, Trained Tokens 1371594, Peak mem 9.324 GB\n",
      "Iter 8290: Train loss 0.532, Learning Rate 2.000e-04, It/sec 2.470, Tokens/sec 311.434, Trained Tokens 1372855, Peak mem 9.324 GB\n",
      "Iter 8300: Val loss 2.439, Val took 2.595s\n",
      "Iter 8300: Train loss 0.663, Learning Rate 2.000e-04, It/sec 31.601, Tokens/sec 8535.362, Trained Tokens 1375556, Peak mem 9.324 GB\n",
      "Iter 8300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008300_adapters.safetensors.\n",
      "Iter 8310: Train loss 0.488, Learning Rate 2.000e-04, It/sec 2.443, Tokens/sec 337.878, Trained Tokens 1376939, Peak mem 9.324 GB\n",
      "Iter 8320: Train loss 0.495, Learning Rate 2.000e-04, It/sec 2.174, Tokens/sec 335.826, Trained Tokens 1378484, Peak mem 9.324 GB\n",
      "Iter 8330: Train loss 0.466, Learning Rate 2.000e-04, It/sec 2.355, Tokens/sec 351.603, Trained Tokens 1379977, Peak mem 9.324 GB\n",
      "Iter 8340: Train loss 0.611, Learning Rate 2.000e-04, It/sec 1.697, Tokens/sec 370.882, Trained Tokens 1382162, Peak mem 9.324 GB\n",
      "Iter 8350: Val loss 2.339, Val took 3.518s\n",
      "Iter 8350: Train loss 0.566, Learning Rate 2.000e-04, It/sec 4.322, Tokens/sec 961.317, Trained Tokens 1384386, Peak mem 9.324 GB\n",
      "Iter 8360: Train loss 0.516, Learning Rate 2.000e-04, It/sec 2.819, Tokens/sec 326.773, Trained Tokens 1385545, Peak mem 9.324 GB\n",
      "Iter 8370: Train loss 0.471, Learning Rate 2.000e-04, It/sec 2.214, Tokens/sec 326.132, Trained Tokens 1387018, Peak mem 9.324 GB\n",
      "Iter 8380: Train loss 0.511, Learning Rate 2.000e-04, It/sec 2.556, Tokens/sec 354.823, Trained Tokens 1388406, Peak mem 9.324 GB\n",
      "Iter 8390: Train loss 0.551, Learning Rate 2.000e-04, It/sec 2.451, Tokens/sec 337.449, Trained Tokens 1389783, Peak mem 9.324 GB\n",
      "Iter 8400: Val loss 2.443, Val took 3.004s\n",
      "Iter 8400: Train loss 0.611, Learning Rate 2.000e-04, It/sec 51.802, Tokens/sec 8588.845, Trained Tokens 1391441, Peak mem 9.324 GB\n",
      "Iter 8400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008400_adapters.safetensors.\n",
      "Iter 8410: Train loss 0.555, Learning Rate 2.000e-04, It/sec 1.508, Tokens/sec 288.848, Trained Tokens 1393356, Peak mem 9.324 GB\n",
      "Iter 8420: Train loss 0.496, Learning Rate 2.000e-04, It/sec 1.876, Tokens/sec 341.676, Trained Tokens 1395177, Peak mem 9.324 GB\n",
      "Iter 8430: Train loss 0.539, Learning Rate 2.000e-04, It/sec 2.484, Tokens/sec 342.286, Trained Tokens 1396555, Peak mem 9.324 GB\n",
      "Iter 8440: Train loss 0.555, Learning Rate 2.000e-04, It/sec 2.167, Tokens/sec 335.684, Trained Tokens 1398104, Peak mem 9.324 GB\n",
      "Iter 8450: Val loss 2.731, Val took 2.642s\n",
      "Iter 8450: Train loss 0.554, Learning Rate 2.000e-04, It/sec 22.322, Tokens/sec 2964.307, Trained Tokens 1399432, Peak mem 9.324 GB\n",
      "Iter 8460: Train loss 0.515, Learning Rate 2.000e-04, It/sec 2.599, Tokens/sec 358.605, Trained Tokens 1400812, Peak mem 9.324 GB\n",
      "Iter 8470: Train loss 0.599, Learning Rate 2.000e-04, It/sec 2.205, Tokens/sec 333.343, Trained Tokens 1402324, Peak mem 9.324 GB\n",
      "Iter 8480: Train loss 0.601, Learning Rate 2.000e-04, It/sec 2.274, Tokens/sec 331.741, Trained Tokens 1403783, Peak mem 9.324 GB\n",
      "Iter 8490: Train loss 0.761, Learning Rate 2.000e-04, It/sec 1.330, Tokens/sec 347.474, Trained Tokens 1406396, Peak mem 9.324 GB\n",
      "Iter 8500: Val loss 2.278, Val took 3.057s\n",
      "Iter 8500: Train loss 0.627, Learning Rate 2.000e-04, It/sec 28.903, Tokens/sec 5292.096, Trained Tokens 1408227, Peak mem 9.324 GB\n",
      "Iter 8500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008500_adapters.safetensors.\n",
      "Iter 8510: Train loss 0.577, Learning Rate 2.000e-04, It/sec 2.648, Tokens/sec 292.856, Trained Tokens 1409333, Peak mem 9.324 GB\n",
      "Iter 8520: Train loss 0.545, Learning Rate 2.000e-04, It/sec 1.992, Tokens/sec 342.025, Trained Tokens 1411050, Peak mem 9.324 GB\n",
      "Iter 8530: Train loss 0.572, Learning Rate 2.000e-04, It/sec 2.545, Tokens/sec 338.976, Trained Tokens 1412382, Peak mem 9.324 GB\n",
      "Iter 8540: Train loss 0.685, Learning Rate 2.000e-04, It/sec 1.437, Tokens/sec 361.726, Trained Tokens 1414899, Peak mem 9.324 GB\n",
      "Iter 8550: Val loss 1.991, Val took 2.889s\n",
      "Iter 8550: Train loss 0.627, Learning Rate 2.000e-04, It/sec 22.229, Tokens/sec 3481.098, Trained Tokens 1416465, Peak mem 9.324 GB\n",
      "Iter 8560: Train loss 0.700, Learning Rate 2.000e-04, It/sec 1.235, Tokens/sec 335.058, Trained Tokens 1419178, Peak mem 9.324 GB\n",
      "Iter 8570: Train loss 0.531, Learning Rate 2.000e-04, It/sec 1.881, Tokens/sec 346.487, Trained Tokens 1421020, Peak mem 9.324 GB\n",
      "Iter 8580: Train loss 0.610, Learning Rate 2.000e-04, It/sec 2.315, Tokens/sec 319.963, Trained Tokens 1422402, Peak mem 9.324 GB\n",
      "Iter 8590: Train loss 0.591, Learning Rate 2.000e-04, It/sec 3.198, Tokens/sec 320.733, Trained Tokens 1423405, Peak mem 9.324 GB\n",
      "Iter 8600: Val loss 2.290, Val took 4.287s\n",
      "Iter 8600: Train loss 0.622, Learning Rate 2.000e-04, It/sec 28.915, Tokens/sec 3122.804, Trained Tokens 1424485, Peak mem 9.324 GB\n",
      "Iter 8600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008600_adapters.safetensors.\n",
      "Iter 8610: Train loss 0.668, Learning Rate 2.000e-04, It/sec 1.942, Tokens/sec 335.886, Trained Tokens 1426215, Peak mem 9.324 GB\n",
      "Iter 8620: Train loss 0.553, Learning Rate 2.000e-04, It/sec 2.165, Tokens/sec 329.552, Trained Tokens 1427737, Peak mem 9.324 GB\n",
      "Iter 8630: Train loss 0.489, Learning Rate 2.000e-04, It/sec 2.157, Tokens/sec 345.999, Trained Tokens 1429341, Peak mem 9.324 GB\n",
      "Iter 8640: Train loss 0.540, Learning Rate 2.000e-04, It/sec 2.056, Tokens/sec 325.387, Trained Tokens 1430924, Peak mem 9.324 GB\n",
      "Iter 8650: Val loss 2.022, Val took 3.026s\n",
      "Iter 8650: Train loss 0.537, Learning Rate 2.000e-04, It/sec 20.654, Tokens/sec 3085.697, Trained Tokens 1432418, Peak mem 9.324 GB\n",
      "Iter 8660: Train loss 0.530, Learning Rate 2.000e-04, It/sec 2.642, Tokens/sec 323.946, Trained Tokens 1433644, Peak mem 9.324 GB\n",
      "Iter 8670: Train loss 0.518, Learning Rate 2.000e-04, It/sec 2.262, Tokens/sec 328.650, Trained Tokens 1435097, Peak mem 9.324 GB\n",
      "Iter 8680: Train loss 0.520, Learning Rate 2.000e-04, It/sec 2.384, Tokens/sec 334.724, Trained Tokens 1436501, Peak mem 9.324 GB\n",
      "Iter 8690: Train loss 0.631, Learning Rate 2.000e-04, It/sec 1.859, Tokens/sec 352.288, Trained Tokens 1438396, Peak mem 9.324 GB\n",
      "Iter 8700: Val loss 2.555, Val took 2.859s\n",
      "Iter 8700: Train loss 0.648, Learning Rate 2.000e-04, It/sec 21.049, Tokens/sec 3715.128, Trained Tokens 1440161, Peak mem 9.324 GB\n",
      "Iter 8700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008700_adapters.safetensors.\n",
      "Iter 8710: Train loss 0.464, Learning Rate 2.000e-04, It/sec 2.079, Tokens/sec 337.629, Trained Tokens 1441785, Peak mem 9.324 GB\n",
      "Iter 8720: Train loss 0.565, Learning Rate 2.000e-04, It/sec 1.609, Tokens/sec 354.424, Trained Tokens 1443988, Peak mem 9.324 GB\n",
      "Iter 8730: Train loss 0.557, Learning Rate 2.000e-04, It/sec 1.871, Tokens/sec 337.189, Trained Tokens 1445790, Peak mem 9.324 GB\n",
      "Iter 8740: Train loss 0.470, Learning Rate 2.000e-04, It/sec 1.918, Tokens/sec 329.521, Trained Tokens 1447508, Peak mem 9.324 GB\n",
      "Iter 8750: Val loss 2.398, Val took 3.321s\n",
      "Iter 8750: Train loss 0.527, Learning Rate 2.000e-04, It/sec 17.761, Tokens/sec 2887.985, Trained Tokens 1449134, Peak mem 9.324 GB\n",
      "Iter 8760: Train loss 0.522, Learning Rate 2.000e-04, It/sec 1.805, Tokens/sec 340.625, Trained Tokens 1451021, Peak mem 9.324 GB\n",
      "Iter 8770: Train loss 0.491, Learning Rate 2.000e-04, It/sec 2.454, Tokens/sec 331.495, Trained Tokens 1452372, Peak mem 9.324 GB\n",
      "Iter 8780: Train loss 0.480, Learning Rate 2.000e-04, It/sec 2.314, Tokens/sec 355.148, Trained Tokens 1453907, Peak mem 9.324 GB\n",
      "Iter 8790: Train loss 0.519, Learning Rate 2.000e-04, It/sec 2.893, Tokens/sec 334.444, Trained Tokens 1455063, Peak mem 9.324 GB\n",
      "Iter 8800: Val loss 2.616, Val took 3.688s\n",
      "Iter 8800: Train loss 0.534, Learning Rate 2.000e-04, It/sec 40.154, Tokens/sec 6665.531, Trained Tokens 1456723, Peak mem 9.324 GB\n",
      "Iter 8800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008800_adapters.safetensors.\n",
      "Iter 8810: Train loss 0.605, Learning Rate 2.000e-04, It/sec 1.788, Tokens/sec 329.407, Trained Tokens 1458565, Peak mem 9.324 GB\n",
      "Iter 8820: Train loss 0.500, Learning Rate 2.000e-04, It/sec 2.697, Tokens/sec 327.911, Trained Tokens 1459781, Peak mem 9.324 GB\n",
      "Iter 8830: Train loss 0.518, Learning Rate 2.000e-04, It/sec 2.105, Tokens/sec 345.898, Trained Tokens 1461424, Peak mem 9.324 GB\n",
      "Iter 8840: Train loss 0.613, Learning Rate 2.000e-04, It/sec 2.240, Tokens/sec 330.790, Trained Tokens 1462901, Peak mem 9.324 GB\n",
      "Iter 8850: Val loss 2.524, Val took 3.237s\n",
      "Iter 8850: Train loss 0.606, Learning Rate 2.000e-04, It/sec 3.982, Tokens/sec 868.966, Trained Tokens 1465083, Peak mem 9.324 GB\n",
      "Iter 8860: Train loss 0.574, Learning Rate 2.000e-04, It/sec 1.732, Tokens/sec 349.760, Trained Tokens 1467102, Peak mem 9.324 GB\n",
      "Iter 8870: Train loss 0.530, Learning Rate 2.000e-04, It/sec 2.223, Tokens/sec 358.733, Trained Tokens 1468716, Peak mem 9.324 GB\n",
      "Iter 8880: Train loss 0.506, Learning Rate 2.000e-04, It/sec 2.425, Tokens/sec 349.001, Trained Tokens 1470155, Peak mem 9.324 GB\n",
      "Iter 8890: Train loss 0.524, Learning Rate 2.000e-04, It/sec 1.973, Tokens/sec 355.973, Trained Tokens 1471959, Peak mem 9.324 GB\n",
      "Iter 8900: Val loss 2.468, Val took 3.064s\n",
      "Iter 8900: Train loss 0.695, Learning Rate 2.000e-04, It/sec 4.027, Tokens/sec 769.228, Trained Tokens 1473869, Peak mem 9.324 GB\n",
      "Iter 8900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0008900_adapters.safetensors.\n",
      "Iter 8910: Train loss 0.535, Learning Rate 2.000e-04, It/sec 2.648, Tokens/sec 324.905, Trained Tokens 1475096, Peak mem 9.324 GB\n",
      "Iter 8920: Train loss 0.577, Learning Rate 2.000e-04, It/sec 2.110, Tokens/sec 308.240, Trained Tokens 1476557, Peak mem 9.324 GB\n",
      "Iter 8930: Train loss 0.723, Learning Rate 2.000e-04, It/sec 1.297, Tokens/sec 355.422, Trained Tokens 1479297, Peak mem 9.324 GB\n",
      "Iter 8940: Train loss 0.586, Learning Rate 2.000e-04, It/sec 2.705, Tokens/sec 317.886, Trained Tokens 1480472, Peak mem 9.324 GB\n",
      "Iter 8950: Val loss 1.933, Val took 3.489s\n",
      "Iter 8950: Train loss 0.565, Learning Rate 2.000e-04, It/sec 27.671, Tokens/sec 3010.654, Trained Tokens 1481560, Peak mem 9.324 GB\n",
      "Iter 8960: Train loss 0.569, Learning Rate 2.000e-04, It/sec 1.910, Tokens/sec 326.023, Trained Tokens 1483267, Peak mem 9.324 GB\n",
      "Iter 8970: Train loss 0.550, Learning Rate 2.000e-04, It/sec 2.137, Tokens/sec 317.112, Trained Tokens 1484751, Peak mem 9.324 GB\n",
      "Iter 8980: Train loss 0.593, Learning Rate 2.000e-04, It/sec 1.821, Tokens/sec 330.776, Trained Tokens 1486567, Peak mem 9.324 GB\n",
      "Iter 8990: Train loss 0.553, Learning Rate 2.000e-04, It/sec 2.067, Tokens/sec 335.334, Trained Tokens 1488189, Peak mem 9.324 GB\n",
      "Iter 9000: Val loss 2.173, Val took 2.952s\n",
      "Iter 9000: Train loss 0.684, Learning Rate 2.000e-04, It/sec 23.453, Tokens/sec 5628.667, Trained Tokens 1490589, Peak mem 9.324 GB\n",
      "Iter 9000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009000_adapters.safetensors.\n",
      "Iter 9010: Train loss 0.550, Learning Rate 2.000e-04, It/sec 2.189, Tokens/sec 306.935, Trained Tokens 1491991, Peak mem 9.324 GB\n",
      "Iter 9020: Train loss 0.709, Learning Rate 2.000e-04, It/sec 1.691, Tokens/sec 361.644, Trained Tokens 1494130, Peak mem 9.324 GB\n",
      "Iter 9030: Train loss 0.448, Learning Rate 2.000e-04, It/sec 2.414, Tokens/sec 342.529, Trained Tokens 1495549, Peak mem 9.324 GB\n",
      "Iter 9040: Train loss 0.718, Learning Rate 2.000e-04, It/sec 1.372, Tokens/sec 366.548, Trained Tokens 1498220, Peak mem 9.324 GB\n",
      "Iter 9050: Val loss 2.668, Val took 3.470s\n",
      "Iter 9050: Train loss 0.548, Learning Rate 2.000e-04, It/sec 39.100, Tokens/sec 6756.411, Trained Tokens 1499948, Peak mem 9.324 GB\n",
      "Iter 9060: Train loss 0.554, Learning Rate 2.000e-04, It/sec 2.011, Tokens/sec 277.302, Trained Tokens 1501327, Peak mem 9.324 GB\n",
      "Iter 9070: Train loss 0.463, Learning Rate 2.000e-04, It/sec 2.677, Tokens/sec 332.166, Trained Tokens 1502568, Peak mem 9.324 GB\n",
      "Iter 9080: Train loss 0.483, Learning Rate 2.000e-04, It/sec 2.686, Tokens/sec 360.756, Trained Tokens 1503911, Peak mem 9.324 GB\n",
      "Iter 9090: Train loss 0.492, Learning Rate 2.000e-04, It/sec 2.700, Tokens/sec 329.663, Trained Tokens 1505132, Peak mem 9.324 GB\n",
      "Iter 9100: Val loss 2.718, Val took 3.349s\n",
      "Iter 9100: Train loss 0.494, Learning Rate 2.000e-04, It/sec 23.480, Tokens/sec 3554.806, Trained Tokens 1506646, Peak mem 9.324 GB\n",
      "Iter 9100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009100_adapters.safetensors.\n",
      "Iter 9110: Train loss 0.486, Learning Rate 2.000e-04, It/sec 2.239, Tokens/sec 338.598, Trained Tokens 1508158, Peak mem 9.324 GB\n",
      "Iter 9120: Train loss 0.600, Learning Rate 2.000e-04, It/sec 2.006, Tokens/sec 363.962, Trained Tokens 1509972, Peak mem 9.324 GB\n",
      "Iter 9130: Train loss 0.529, Learning Rate 2.000e-04, It/sec 2.186, Tokens/sec 361.641, Trained Tokens 1511626, Peak mem 9.324 GB\n",
      "Iter 9140: Train loss 0.531, Learning Rate 2.000e-04, It/sec 2.051, Tokens/sec 353.727, Trained Tokens 1513351, Peak mem 9.324 GB\n",
      "Iter 9150: Val loss 2.424, Val took 3.230s\n",
      "Iter 9150: Train loss 0.508, Learning Rate 2.000e-04, It/sec 31.269, Tokens/sec 4261.914, Trained Tokens 1514714, Peak mem 9.324 GB\n",
      "Iter 9160: Train loss 0.498, Learning Rate 2.000e-04, It/sec 2.139, Tokens/sec 327.707, Trained Tokens 1516246, Peak mem 9.324 GB\n",
      "Iter 9170: Train loss 0.477, Learning Rate 2.000e-04, It/sec 1.919, Tokens/sec 343.040, Trained Tokens 1518034, Peak mem 9.324 GB\n",
      "Iter 9180: Train loss 0.532, Learning Rate 2.000e-04, It/sec 2.706, Tokens/sec 327.401, Trained Tokens 1519244, Peak mem 9.324 GB\n",
      "Iter 9190: Train loss 0.471, Learning Rate 2.000e-04, It/sec 2.262, Tokens/sec 358.290, Trained Tokens 1520828, Peak mem 9.324 GB\n",
      "Iter 9200: Val loss 2.569, Val took 3.498s\n",
      "Iter 9200: Train loss 0.459, Learning Rate 2.000e-04, It/sec 31.196, Tokens/sec 5259.684, Trained Tokens 1522514, Peak mem 9.324 GB\n",
      "Iter 9200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009200_adapters.safetensors.\n",
      "Iter 9210: Train loss 0.468, Learning Rate 2.000e-04, It/sec 1.753, Tokens/sec 286.451, Trained Tokens 1524148, Peak mem 9.324 GB\n",
      "Iter 9220: Train loss 0.611, Learning Rate 2.000e-04, It/sec 2.361, Tokens/sec 346.611, Trained Tokens 1525616, Peak mem 9.324 GB\n",
      "Iter 9230: Train loss 0.554, Learning Rate 2.000e-04, It/sec 2.298, Tokens/sec 339.344, Trained Tokens 1527093, Peak mem 9.324 GB\n",
      "Iter 9240: Train loss 0.515, Learning Rate 2.000e-04, It/sec 1.755, Tokens/sec 343.520, Trained Tokens 1529050, Peak mem 9.324 GB\n",
      "Iter 9250: Val loss 2.442, Val took 3.219s\n",
      "Iter 9250: Train loss 0.523, Learning Rate 2.000e-04, It/sec 16.570, Tokens/sec 2744.050, Trained Tokens 1530706, Peak mem 9.324 GB\n",
      "Iter 9260: Train loss 0.493, Learning Rate 2.000e-04, It/sec 2.833, Tokens/sec 351.022, Trained Tokens 1531945, Peak mem 9.324 GB\n",
      "Iter 9270: Train loss 0.622, Learning Rate 2.000e-04, It/sec 1.598, Tokens/sec 358.342, Trained Tokens 1534188, Peak mem 9.324 GB\n",
      "Iter 9280: Train loss 0.615, Learning Rate 2.000e-04, It/sec 1.116, Tokens/sec 378.039, Trained Tokens 1537575, Peak mem 9.324 GB\n",
      "Iter 9290: Train loss 0.544, Learning Rate 2.000e-04, It/sec 3.336, Tokens/sec 328.303, Trained Tokens 1538559, Peak mem 9.324 GB\n",
      "Iter 9300: Val loss 2.320, Val took 2.623s\n",
      "Iter 9300: Train loss 0.449, Learning Rate 2.000e-04, It/sec 9.453, Tokens/sec 1824.455, Trained Tokens 1540489, Peak mem 9.324 GB\n",
      "Iter 9300: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009300_adapters.safetensors.\n",
      "Iter 9310: Train loss 0.579, Learning Rate 2.000e-04, It/sec 1.474, Tokens/sec 349.473, Trained Tokens 1542860, Peak mem 9.324 GB\n",
      "Iter 9320: Train loss 0.576, Learning Rate 2.000e-04, It/sec 2.355, Tokens/sec 335.994, Trained Tokens 1544287, Peak mem 9.324 GB\n",
      "Iter 9330: Train loss 0.517, Learning Rate 2.000e-04, It/sec 2.151, Tokens/sec 357.067, Trained Tokens 1545947, Peak mem 9.324 GB\n",
      "Iter 9340: Train loss 0.553, Learning Rate 2.000e-04, It/sec 2.920, Tokens/sec 336.692, Trained Tokens 1547100, Peak mem 9.324 GB\n",
      "Iter 9350: Val loss 2.227, Val took 2.987s\n",
      "Iter 9350: Train loss 0.590, Learning Rate 2.000e-04, It/sec 14.081, Tokens/sec 2023.442, Trained Tokens 1548537, Peak mem 9.324 GB\n",
      "Iter 9360: Train loss 0.518, Learning Rate 2.000e-04, It/sec 2.202, Tokens/sec 322.791, Trained Tokens 1550003, Peak mem 9.324 GB\n",
      "Iter 9370: Train loss 0.496, Learning Rate 2.000e-04, It/sec 2.231, Tokens/sec 350.225, Trained Tokens 1551573, Peak mem 9.324 GB\n",
      "Iter 9380: Train loss 0.499, Learning Rate 2.000e-04, It/sec 1.845, Tokens/sec 341.762, Trained Tokens 1553425, Peak mem 9.324 GB\n",
      "Iter 9390: Train loss 0.549, Learning Rate 2.000e-04, It/sec 1.742, Tokens/sec 350.915, Trained Tokens 1555440, Peak mem 9.324 GB\n",
      "Iter 9400: Val loss 2.569, Val took 3.171s\n",
      "Iter 9400: Train loss 0.591, Learning Rate 2.000e-04, It/sec 4.100, Tokens/sec 792.078, Trained Tokens 1557372, Peak mem 9.324 GB\n",
      "Iter 9400: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009400_adapters.safetensors.\n",
      "Iter 9410: Train loss 0.607, Learning Rate 2.000e-04, It/sec 2.229, Tokens/sec 326.559, Trained Tokens 1558837, Peak mem 9.324 GB\n",
      "Iter 9420: Train loss 0.588, Learning Rate 2.000e-04, It/sec 1.885, Tokens/sec 356.146, Trained Tokens 1560726, Peak mem 9.324 GB\n",
      "Iter 9430: Train loss 0.589, Learning Rate 2.000e-04, It/sec 2.490, Tokens/sec 328.369, Trained Tokens 1562045, Peak mem 9.324 GB\n",
      "Iter 9440: Train loss 0.524, Learning Rate 2.000e-04, It/sec 2.501, Tokens/sec 339.859, Trained Tokens 1563404, Peak mem 9.324 GB\n",
      "Iter 9450: Val loss 2.689, Val took 3.177s\n",
      "Iter 9450: Train loss 0.685, Learning Rate 2.000e-04, It/sec 31.280, Tokens/sec 6987.981, Trained Tokens 1565638, Peak mem 9.324 GB\n",
      "Iter 9460: Train loss 0.442, Learning Rate 2.000e-04, It/sec 1.710, Tokens/sec 359.525, Trained Tokens 1567740, Peak mem 9.324 GB\n",
      "Iter 9470: Train loss 0.558, Learning Rate 2.000e-04, It/sec 2.039, Tokens/sec 365.808, Trained Tokens 1569534, Peak mem 9.324 GB\n",
      "Iter 9480: Train loss 0.442, Learning Rate 2.000e-04, It/sec 2.232, Tokens/sec 355.350, Trained Tokens 1571126, Peak mem 9.324 GB\n",
      "Iter 9490: Train loss 0.487, Learning Rate 2.000e-04, It/sec 2.644, Tokens/sec 326.502, Trained Tokens 1572361, Peak mem 9.324 GB\n",
      "Iter 9500: Val loss 2.508, Val took 3.229s\n",
      "Iter 9500: Train loss 0.484, Learning Rate 2.000e-04, It/sec 23.263, Tokens/sec 3840.719, Trained Tokens 1574012, Peak mem 9.324 GB\n",
      "Iter 9500: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009500_adapters.safetensors.\n",
      "Iter 9510: Train loss 0.515, Learning Rate 2.000e-04, It/sec 2.521, Tokens/sec 319.205, Trained Tokens 1575278, Peak mem 9.324 GB\n",
      "Iter 9520: Train loss 0.550, Learning Rate 2.000e-04, It/sec 2.275, Tokens/sec 349.198, Trained Tokens 1576813, Peak mem 9.324 GB\n",
      "Iter 9530: Train loss 0.524, Learning Rate 2.000e-04, It/sec 2.767, Tokens/sec 333.926, Trained Tokens 1578020, Peak mem 9.324 GB\n",
      "Iter 9540: Train loss 0.494, Learning Rate 2.000e-04, It/sec 2.429, Tokens/sec 350.715, Trained Tokens 1579464, Peak mem 9.324 GB\n",
      "Iter 9550: Val loss 2.942, Val took 3.120s\n",
      "Iter 9550: Train loss 0.471, Learning Rate 2.000e-04, It/sec 14.019, Tokens/sec 2463.216, Trained Tokens 1581221, Peak mem 9.324 GB\n",
      "Iter 9560: Train loss 0.524, Learning Rate 2.000e-04, It/sec 1.978, Tokens/sec 336.238, Trained Tokens 1582921, Peak mem 9.324 GB\n",
      "Iter 9570: Train loss 0.491, Learning Rate 2.000e-04, It/sec 2.626, Tokens/sec 341.152, Trained Tokens 1584220, Peak mem 9.324 GB\n",
      "Iter 9580: Train loss 0.517, Learning Rate 2.000e-04, It/sec 2.496, Tokens/sec 336.749, Trained Tokens 1585569, Peak mem 9.324 GB\n",
      "Iter 9590: Train loss 0.551, Learning Rate 2.000e-04, It/sec 1.941, Tokens/sec 345.177, Trained Tokens 1587347, Peak mem 9.324 GB\n",
      "Iter 9600: Val loss 2.228, Val took 3.197s\n",
      "Iter 9600: Train loss 0.538, Learning Rate 2.000e-04, It/sec 53.007, Tokens/sec 6663.019, Trained Tokens 1588604, Peak mem 9.324 GB\n",
      "Iter 9600: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009600_adapters.safetensors.\n",
      "Iter 9610: Train loss 0.558, Learning Rate 2.000e-04, It/sec 1.189, Tokens/sec 364.688, Trained Tokens 1591671, Peak mem 9.324 GB\n",
      "Iter 9620: Train loss 0.470, Learning Rate 2.000e-04, It/sec 1.833, Tokens/sec 340.853, Trained Tokens 1593531, Peak mem 9.324 GB\n",
      "Iter 9630: Train loss 0.569, Learning Rate 2.000e-04, It/sec 2.182, Tokens/sec 320.535, Trained Tokens 1595000, Peak mem 9.324 GB\n",
      "Iter 9640: Train loss 0.548, Learning Rate 2.000e-04, It/sec 2.718, Tokens/sec 305.757, Trained Tokens 1596125, Peak mem 9.324 GB\n",
      "Iter 9650: Val loss 2.349, Val took 3.337s\n",
      "Iter 9650: Train loss 0.520, Learning Rate 2.000e-04, It/sec 13.491, Tokens/sec 2618.569, Trained Tokens 1598066, Peak mem 9.324 GB\n",
      "Iter 9660: Train loss 0.507, Learning Rate 2.000e-04, It/sec 2.173, Tokens/sec 323.979, Trained Tokens 1599557, Peak mem 9.324 GB\n",
      "Iter 9670: Train loss 0.498, Learning Rate 2.000e-04, It/sec 2.251, Tokens/sec 345.038, Trained Tokens 1601090, Peak mem 9.324 GB\n",
      "Iter 9680: Train loss 0.540, Learning Rate 2.000e-04, It/sec 2.029, Tokens/sec 332.683, Trained Tokens 1602730, Peak mem 9.324 GB\n",
      "Iter 9690: Train loss 0.515, Learning Rate 2.000e-04, It/sec 2.000, Tokens/sec 330.722, Trained Tokens 1604384, Peak mem 9.324 GB\n",
      "Iter 9700: Val loss 2.346, Val took 3.269s\n",
      "Iter 9700: Train loss 0.664, Learning Rate 2.000e-04, It/sec 39.120, Tokens/sec 10476.263, Trained Tokens 1607062, Peak mem 9.324 GB\n",
      "Iter 9700: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009700_adapters.safetensors.\n",
      "Iter 9710: Train loss 0.595, Learning Rate 2.000e-04, It/sec 1.738, Tokens/sec 339.504, Trained Tokens 1609015, Peak mem 9.324 GB\n",
      "Iter 9720: Train loss 0.586, Learning Rate 2.000e-04, It/sec 2.350, Tokens/sec 356.979, Trained Tokens 1610534, Peak mem 9.324 GB\n",
      "Iter 9730: Train loss 0.518, Learning Rate 2.000e-04, It/sec 2.717, Tokens/sec 340.698, Trained Tokens 1611788, Peak mem 9.324 GB\n",
      "Iter 9740: Train loss 0.478, Learning Rate 2.000e-04, It/sec 2.510, Tokens/sec 354.432, Trained Tokens 1613200, Peak mem 9.324 GB\n",
      "Iter 9750: Val loss 2.357, Val took 3.046s\n",
      "Iter 9750: Train loss 0.477, Learning Rate 2.000e-04, It/sec 30.795, Tokens/sec 4206.559, Trained Tokens 1614566, Peak mem 9.324 GB\n",
      "Iter 9760: Train loss 0.703, Learning Rate 2.000e-04, It/sec 1.170, Tokens/sec 318.195, Trained Tokens 1617286, Peak mem 9.324 GB\n",
      "Iter 9770: Train loss 0.566, Learning Rate 2.000e-04, It/sec 1.652, Tokens/sec 363.426, Trained Tokens 1619486, Peak mem 9.324 GB\n",
      "Iter 9780: Train loss 0.474, Learning Rate 2.000e-04, It/sec 2.041, Tokens/sec 332.509, Trained Tokens 1621115, Peak mem 9.324 GB\n",
      "Iter 9790: Train loss 0.544, Learning Rate 2.000e-04, It/sec 2.084, Tokens/sec 332.962, Trained Tokens 1622713, Peak mem 9.324 GB\n",
      "Iter 9800: Val loss 2.266, Val took 3.717s\n",
      "Iter 9800: Train loss 0.520, Learning Rate 2.000e-04, It/sec 29.842, Tokens/sec 4106.322, Trained Tokens 1624089, Peak mem 9.324 GB\n",
      "Iter 9800: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009800_adapters.safetensors.\n",
      "Iter 9810: Train loss 0.545, Learning Rate 2.000e-04, It/sec 1.922, Tokens/sec 298.474, Trained Tokens 1625642, Peak mem 9.324 GB\n",
      "Iter 9820: Train loss 0.562, Learning Rate 2.000e-04, It/sec 2.475, Tokens/sec 345.705, Trained Tokens 1627039, Peak mem 9.324 GB\n",
      "Iter 9830: Train loss 0.574, Learning Rate 2.000e-04, It/sec 2.274, Tokens/sec 349.034, Trained Tokens 1628574, Peak mem 9.324 GB\n",
      "Iter 9840: Train loss 0.567, Learning Rate 2.000e-04, It/sec 2.490, Tokens/sec 345.142, Trained Tokens 1629960, Peak mem 9.324 GB\n",
      "Iter 9850: Val loss 2.690, Val took 3.620s\n",
      "Iter 9850: Train loss 0.537, Learning Rate 2.000e-04, It/sec 39.079, Tokens/sec 7573.553, Trained Tokens 1631898, Peak mem 9.324 GB\n",
      "Iter 9860: Train loss 0.468, Learning Rate 2.000e-04, It/sec 2.522, Tokens/sec 328.896, Trained Tokens 1633202, Peak mem 9.324 GB\n",
      "Iter 9870: Train loss 0.571, Learning Rate 2.000e-04, It/sec 2.869, Tokens/sec 327.623, Trained Tokens 1634344, Peak mem 9.324 GB\n",
      "Iter 9880: Train loss 0.550, Learning Rate 2.000e-04, It/sec 2.318, Tokens/sec 336.992, Trained Tokens 1635798, Peak mem 9.324 GB\n",
      "Iter 9890: Train loss 0.529, Learning Rate 2.000e-04, It/sec 2.171, Tokens/sec 347.431, Trained Tokens 1637398, Peak mem 9.324 GB\n",
      "Iter 9900: Val loss 2.416, Val took 3.234s\n",
      "Iter 9900: Train loss 0.493, Learning Rate 2.000e-04, It/sec 16.035, Tokens/sec 2406.804, Trained Tokens 1638899, Peak mem 9.324 GB\n",
      "Iter 9900: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0009900_adapters.safetensors.\n",
      "Iter 9910: Train loss 0.556, Learning Rate 2.000e-04, It/sec 1.541, Tokens/sec 306.131, Trained Tokens 1640886, Peak mem 9.324 GB\n",
      "Iter 9920: Train loss 0.519, Learning Rate 2.000e-04, It/sec 1.725, Tokens/sec 358.633, Trained Tokens 1642965, Peak mem 9.324 GB\n",
      "Iter 9930: Train loss 0.484, Learning Rate 2.000e-04, It/sec 2.511, Tokens/sec 337.206, Trained Tokens 1644308, Peak mem 9.324 GB\n",
      "Iter 9940: Train loss 0.485, Learning Rate 2.000e-04, It/sec 2.456, Tokens/sec 333.243, Trained Tokens 1645665, Peak mem 9.324 GB\n",
      "Iter 9950: Val loss 2.389, Val took 2.744s\n",
      "Iter 9950: Train loss 0.479, Learning Rate 2.000e-04, It/sec 22.508, Tokens/sec 4015.478, Trained Tokens 1647449, Peak mem 9.324 GB\n",
      "Iter 9960: Train loss 0.531, Learning Rate 2.000e-04, It/sec 2.770, Tokens/sec 334.857, Trained Tokens 1648658, Peak mem 9.324 GB\n",
      "Iter 9970: Train loss 0.542, Learning Rate 2.000e-04, It/sec 2.170, Tokens/sec 339.980, Trained Tokens 1650225, Peak mem 9.324 GB\n",
      "Iter 9980: Train loss 0.563, Learning Rate 2.000e-04, It/sec 2.176, Tokens/sec 343.116, Trained Tokens 1651802, Peak mem 9.324 GB\n",
      "Iter 9990: Train loss 0.557, Learning Rate 2.000e-04, It/sec 2.473, Tokens/sec 317.006, Trained Tokens 1653084, Peak mem 9.324 GB\n",
      "Iter 10000: Val loss 2.437, Val took 3.258s\n",
      "Iter 10000: Train loss 0.492, Learning Rate 2.000e-04, It/sec 7.512, Tokens/sec 1229.785, Trained Tokens 1654721, Peak mem 9.324 GB\n",
      "Iter 10000: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0010000_adapters.safetensors.\n",
      "Iter 10010: Train loss 0.473, Learning Rate 2.000e-04, It/sec 1.963, Tokens/sec 347.868, Trained Tokens 1656493, Peak mem 9.324 GB\n",
      "Iter 10020: Train loss 0.479, Learning Rate 2.000e-04, It/sec 2.136, Tokens/sec 344.689, Trained Tokens 1658107, Peak mem 9.324 GB\n",
      "Iter 10030: Train loss 0.556, Learning Rate 2.000e-04, It/sec 2.426, Tokens/sec 343.721, Trained Tokens 1659524, Peak mem 9.324 GB\n",
      "Iter 10040: Train loss 0.530, Learning Rate 2.000e-04, It/sec 2.403, Tokens/sec 339.555, Trained Tokens 1660937, Peak mem 9.324 GB\n",
      "Iter 10050: Val loss 1.982, Val took 2.893s\n",
      "Iter 10050: Train loss 0.483, Learning Rate 2.000e-04, It/sec 30.047, Tokens/sec 5886.202, Trained Tokens 1662896, Peak mem 9.324 GB\n",
      "Iter 10060: Train loss 0.552, Learning Rate 2.000e-04, It/sec 2.154, Tokens/sec 348.467, Trained Tokens 1664514, Peak mem 9.324 GB\n",
      "Iter 10070: Train loss 0.484, Learning Rate 2.000e-04, It/sec 2.652, Tokens/sec 349.325, Trained Tokens 1665831, Peak mem 9.324 GB\n",
      "Iter 10080: Train loss 0.619, Learning Rate 2.000e-04, It/sec 2.235, Tokens/sec 349.164, Trained Tokens 1667393, Peak mem 9.324 GB\n",
      "Iter 10090: Train loss 0.553, Learning Rate 2.000e-04, It/sec 1.203, Tokens/sec 334.927, Trained Tokens 1670177, Peak mem 9.324 GB\n",
      "Iter 10100: Val loss 2.306, Val took 3.428s\n",
      "Iter 10100: Train loss 0.523, Learning Rate 2.000e-04, It/sec 21.001, Tokens/sec 2284.891, Trained Tokens 1671265, Peak mem 9.324 GB\n",
      "Iter 10100: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0010100_adapters.safetensors.\n",
      "Iter 10110: Train loss 0.567, Learning Rate 2.000e-04, It/sec 2.057, Tokens/sec 343.689, Trained Tokens 1672936, Peak mem 9.324 GB\n",
      "Iter 10120: Train loss 0.803, Learning Rate 2.000e-04, It/sec 1.475, Tokens/sec 372.223, Trained Tokens 1675460, Peak mem 9.324 GB\n",
      "Iter 10130: Train loss 0.583, Learning Rate 2.000e-04, It/sec 2.060, Tokens/sec 357.662, Trained Tokens 1677196, Peak mem 9.324 GB\n",
      "Iter 10140: Train loss 0.549, Learning Rate 2.000e-04, It/sec 2.458, Tokens/sec 337.541, Trained Tokens 1678569, Peak mem 9.324 GB\n",
      "Iter 10150: Val loss 2.374, Val took 3.076s\n",
      "Iter 10150: Train loss 0.508, Learning Rate 2.000e-04, It/sec 39.441, Tokens/sec 5525.622, Trained Tokens 1679970, Peak mem 9.324 GB\n",
      "Iter 10160: Train loss 0.540, Learning Rate 2.000e-04, It/sec 2.126, Tokens/sec 336.792, Trained Tokens 1681554, Peak mem 9.324 GB\n",
      "Iter 10170: Train loss 0.594, Learning Rate 2.000e-04, It/sec 2.290, Tokens/sec 315.499, Trained Tokens 1682932, Peak mem 9.324 GB\n",
      "Iter 10180: Train loss 0.673, Learning Rate 2.000e-04, It/sec 1.773, Tokens/sec 322.549, Trained Tokens 1684751, Peak mem 9.324 GB\n",
      "Iter 10190: Train loss 0.537, Learning Rate 2.000e-04, It/sec 1.647, Tokens/sec 354.655, Trained Tokens 1686904, Peak mem 9.324 GB\n",
      "Iter 10200: Val loss 2.584, Val took 3.879s\n",
      "Iter 10200: Train loss 0.549, Learning Rate 2.000e-04, It/sec 23.625, Tokens/sec 3621.752, Trained Tokens 1688437, Peak mem 9.324 GB\n",
      "Iter 10200: Saved adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors and ../trained_models/adapters_20_2_2e4/0010200_adapters.safetensors.\n",
      "Iter 10210: Train loss 0.504, Learning Rate 2.000e-04, It/sec 1.924, Tokens/sec 320.310, Trained Tokens 1690102, Peak mem 9.324 GB\n",
      "Iter 10220: Train loss 0.717, Learning Rate 2.000e-04, It/sec 1.610, Tokens/sec 362.350, Trained Tokens 1692352, Peak mem 9.324 GB\n",
      "Iter 10230: Train loss 0.564, Learning Rate 2.000e-04, It/sec 3.123, Tokens/sec 335.759, Trained Tokens 1693427, Peak mem 9.324 GB\n",
      "Iter 10240: Val loss 2.531, Val took 3.328s\n",
      "Iter 10240: Train loss 0.592, Learning Rate 2.000e-04, It/sec 21.861, Tokens/sec 3843.157, Trained Tokens 1695185, Peak mem 9.324 GB\n",
      "Saved final adapter weights to ../trained_models/adapters_20_2_2e4/adapters.safetensors.\n",
      "\n",
      " Starting Evaluation\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [38:10<00:00, 11.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on 205 test samples:\n",
      "Accuracy: 0.000\n",
      "F1 Score: 0.000\n",
      "Perplexity: 8.766\n",
      "\n",
      "ROUGE Scores:\n",
      "rouge1: 0.097\n",
      "rouge2: 0.037\n",
      "rougeL: 0.079\n",
      "🏃 View run MLX-20_2_2e4 at: http://127.0.0.1:5000/#/experiments/880645134898555871/runs/b917148e0c8341ccbc7cc8bbb500b20d\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/880645134898555871\n"
     ]
    }
   ],
   "source": [
    "# Checl if the output folder exists, if not, create it:\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Output folder {output_path} created\")\n",
    "else:\n",
    "    print(f\"Output folder {output_path} already exists\")\n",
    "\n",
    "\n",
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "if optimizer == \"adam\":\n",
    "    opt = optim.Adam(learning_rate=learning_rate_value)\n",
    "else:\n",
    "    opt = optim.AdamW(learning_rate=learning_rate_value, weight_decay=weight_decay_value)\n",
    "\n",
    "# Make a class to record the training stats:\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "        try:\n",
    "            mlflow.log_metric(\"train_loss\", info[\"train_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log train metric to MLflow: {e}\")\n",
    "            \n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        self.val_accuracies.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        # log validation loss\n",
    "        try:\n",
    "            mlflow.log_metric(\"val_loss\", info[\"val_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log validation metric to MLflow: {e}\")\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "with mlflow.start_run(run_name=corrida_name):\n",
    "    mlflow.log_params({\n",
    "        \"num_train_epoch\": lora_config[\"lora_parameters\"][\"epochs\"],\n",
    "        \"max_steps\": training_args.iters,\n",
    "        \"lora_r\": lora_config[\"lora_parameters\"][\"rank\"],\n",
    "        \"lora_dropout\":lora_config[\"lora_parameters\"][\"dropout\"],\n",
    "        \"lora_layers\":lora_config[\"lora_layers\"],\n",
    "        \"lora_layeres_scale\":lora_config[\"lora_parameters\"][\"scale\"],\n",
    "        \"batch_size\":training_args.batch_size,\n",
    "        \"optimizer\":optimizer,\n",
    "        \"learning_rate\":learning_rate_value,\n",
    "        # \"weight_decay\": weight_decay_value,\n",
    "        \"scheduler\":lr_scheduler\n",
    "    })\n",
    "\n",
    "    # Train model:\n",
    "    train(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        optimizer=opt,\n",
    "        train_dataset=train_set,\n",
    "        val_dataset=valid_set,\n",
    "        training_callback=metrics,\n",
    "    )\n",
    "\n",
    "    print(\"\\n Starting Evaluation\")\n",
    "    # Evaluate model and log metrics in the same run\n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, f1, perplexity, rouge_scores = evaluate_model(model, tokenizer, test_set[:num_test])\n",
    "\n",
    "    # Log final metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": metrics.train_losses[-1][1],\n",
    "        \"final_val_loss\": metrics.val_losses[-1][1],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"rouge1\": rouge_scores['rouge1'],\n",
    "        \"rouge2\": rouge_scores['rouge2'],\n",
    "        \"rougeL\": rouge_scores['rougeL']\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nResults on {num_test} test samples:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\") \n",
    "    print(f\"Perplexity: {perplexity:.3f}\")\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for key, score in rouge_scores.items():\n",
    "        print(f\"{key}: {score:.3f}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved every 100 iterations along with the final adapters in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac329358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(88213) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000100_adapters.safetensors 0001200_adapters.safetensors\n",
      "0000200_adapters.safetensors 0001300_adapters.safetensors\n",
      "0000300_adapters.safetensors 0001400_adapters.safetensors\n",
      "0000400_adapters.safetensors 0001500_adapters.safetensors\n",
      "0000500_adapters.safetensors 0001600_adapters.safetensors\n",
      "0000600_adapters.safetensors 0001700_adapters.safetensors\n",
      "0000700_adapters.safetensors 0001800_adapters.safetensors\n",
      "0000800_adapters.safetensors 0001900_adapters.safetensors\n",
      "0000900_adapters.safetensors 0002000_adapters.safetensors\n",
      "0001000_adapters.safetensors adapters.safetensors\n",
      "0001100_adapters.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls ../trained_models/adapters2k/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training and validation losses to see how well the adapters fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3d2744ec0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC1oElEQVR4nOydd3gU5drG75mEJKQnQAqhwwFEkK5GxRARwXKEKGAX8aAi6qFYEMSuFAvlqHg+wYJiOYIGsKIgRMQgIr2I0iGkkt7LzPfH5p3MzM7MzmzJbpLnd125YGdnZ9+dnZ33fp/KARBBEARBEATRTOC9PQCCIAiCIAh3QuKGIAiCIIhmBYkbgiAIgiCaFSRuCIIgCIJoVpC4IQiCIAiiWUHihiAIgiCIZgWJG4IgCIIgmhX+3h6AN2jfvj1KSkq8PQyCIAiCICwQFhaGc+fOOdyvxYmb9u3bIyMjw9vDIAiCIAjCCRISEhwKnBYnbpjFJiEhgaw3BEEQBNFECAsLQ0ZGhqm5u8WJG0ZJSQmJG4IgCIJohlBAMUEQBEEQzQoSNwRBEARBNCtI3BAEQRAE0axosTE3BEEQBOFO/P39ER8fD54nu4EzCIKAzMxM1NbWunwsEjcEQRAE4SIxMTF46aWXEBQU5O2hNGkqKyvx1FNPITc316XjkLghCIIgCBfgOA6TJ09GaWkpXnvtNVRVVXl7SE2SwMBATJkyBffddx/mz58PURSdPhaJG4IgCIJwgcjISPTu3RvLli3DX3/95e3hNGk+//xzTJ06FRERESgsLHT6OOQYJAiCIAgXCAsLAwDk5OR4eSRNH3YOw8PDXToOiRuCIAiCcAGO4wAAdXV1Xh5J04edQ3ZOnYXcUm6C53kMG9YH8fHRyMzMx9athyAIgreHRRAEQRAtDhI3biAlJRFLlt6Hjh3bSdvOnMnF9GnLkZqa7sWREQRBEETLg9xSLpKSkojVa2YjIaGtYntCQhusXjMbKSmJXhoZQRAE0ZTgeB7dhwzEwGtHovuQgeCaYL2cEydOYNq0ad4eBlluXIHneSxZel/9/zm75wRBwOIl92Hdut/IRUUQBEHo0m9EEsY+OQORcbHStsKsbKxdsBj7N6W5/f0cpVk/99xzeP755y0fd+jQoSgrK3N2WG6j6clCH2LYsD7o2LGdnbBh8DyPTp3aYdiwPo08MoIgCKKp0G9EEiYumo+ImHaK7REx7TBx0Xz0G5Hk9veMi4uT/qZNm4aioiLFttdee02xv5+fn6nj5uXloaKiwu3jtQqJGxeIj492634EQRBE8yCgdZCpv8CQYIydPRMA7NxQtscixj45A4EhwaaOZ5bs7Gzpr6ioCKIoSo979+6N0tJSjB49Gjt37kRVVRWuuOIKdOvWDWvXrkVWVhZKSkqwY8cOjBgxQnFctVtKFEX861//wpdffomysjL89ddf+Oc//+n8iTUJuaVcIDMz3637EQRBEE2fgNZBmL9js1uOxfE8IuNiMW/7JlP7z744GdUVlW557wULFuCxxx7D8ePHUVBQgI4dO+Lbb7/FU089haqqKtx999346quv0KtXL5w5c0b3OM8++yyeeOIJPP7443jkkUfw8ccfo3PnzigoKHDLOLUgy40LbN16CGfO5OrG0wiCgNOnc7F166FGHhlBEARBuMYzzzyDjRs3SuJm3759eOedd3Dw4EEcPXoUzzzzDI4dO4Ybb7zR8DgffPABPvvsMxw7dgxz5sxBWFgYLr74Yo+OnSw3LiAIAqZPW47Va2ZDEARFJ1ib4OEwY/pyCiYmCIJoQVRXVGL2xcmm9u06qD/u/+8Sh/u9M2U6Tuzaa+q93cXOnTsVj0NCQvDcc8/h+uuvR3x8PPz9/dG6dWt06tTJ8Dj79u2T/l9eXo6ioiLExMS4bZxaeNVyM2XKFOzduxdFRUUoKirCr7/+itGjR+vuP3HiRIiiqPjzduBSamo6xo+bj6wspXnt7NnzGD9uPtW5IQiCaIFUV1Sa+vsr/XcUZmVD1FkEi4KAgsws/JX+u6njuRN11tNrr72GlJQUzJkzB8OGDcOAAQOwf/9+BAQEGB6npqZG+ZlEUWEM8ARetdycPXsWTz75JP7++29wHIeJEydi3bp1GDhwIA4d0nblFBUVoVevXtJjV7qGuovU1HRs23YIWdmrAABXJc/Bzz8fJIsNQRAEYYgoCFi7YDEmLpoPURAUQcVivQdg3cIluuKnMbn88svxwQcfYO3atQBslpwuXbp4dUx6eNVy8/XXX+O7777D0aNH8ffff2Pu3LkoLS3FpZdeqvsaeUR3dna2w0ZlAQEBCAsLU/x5goqKaun/v/56mIQNQRAEYYr9m9KwcuZsFOXkKrYXZudg5czZHqlz4wx///03brrpJvTv3x8XXXQRPvnkE49bYJzFZ2JueJ7H+PHjERISgvR0fVdOaGgoTp48CZ7nsWvXLsyZM0fXygMAs2fPxnPPPeeBESuprq6V/h8Y2ErxmCAIgiCM2L8pDQc2b0W3Qf0R3q4tinPzcHzXXp+w2DBmzpyJ9957D7/++ivy8vKwcOFCl7t3exLRm399+/YVS0pKxJqaGrGgoEC89tprdfe99NJLxbvuukvs37+/eOWVV4rr168XCwsLxYSEBN3XBAQEiGFhYdJf+/btRVEUxbCwMLd+Do7jREH8ShTEr8Q2bcK9ek7pj/7oj/7or/H+OnfuLH744Ydi586dvT6Wpv5ndC7DwsJMz99et9wcOXIEAwYMQEREBMaNG4eVK1ciKSkJhw8fttt3+/bt2L59u/T4119/xeHDh/HAAw/gmWee0Tx+dXU1qqurNZ9zJ6IooqamFq1a+SMgwOunlSAIgiBaLF6fhWtqanDs2DEAwK5duzB06FBMmzYNU6ZMcfja2tpa7N69Gz169PD0ME1RXW0TN4GBrbw9FIIgCIJosfhcJBDP8wgMDDS9b79+/ZCZmenhUZmDxdmQ5YYgCIIgvIdXZ+F58+bhu+++w+nTpxEWFobbb78dw4cPx6hRowAAK1euREZGBubMmQMAePrpp7F9+3YcPXoUkZGRePzxx9G5c2esWLHCmx9DoqrKlstP4oYgCIIgvIdXZ+GYmBh8+OGHiI+PR1FREfbt24dRo0Zh48aNAIBOnTopUqqjoqKwfPlyxMXFoaCgAH/88Qcuu+wyzfgcb8AsN+SWIgiCIAjv4VVxM3nyZMPnk5OV5atnzpyJmTNnenJILlFdTZYbgiAIgvA2Phdz05SpqqKYG4IgCILwNiRu3Aiz3JBbiiAIgiC8B4kbN0LZUgRBEERLYfPmzVi8eLH0+MSJE5g2bZrha0RRxJgxYzw9NO/XuWlONGRLkeWGIAiCsAbP8xg2rA/i46ORmZmPrVsPeaxP4fr169GqVStce+21ds9dccUV2Lp1Ky666CLs37/f9DGHDh1q10ncW5C4cSOULUUQBEE4Q0pKIpYsvQ8dO7aTtp05k4vp05YjNVW/36KzvPvuu/jiiy+QkJCAjIwMxXOTJk3C77//bknYAEBeXp47h+gS5JZyI+SWIgiCIKySkpKI1WtmIyGhrWJ7QkIbrF4zGykpiW5/z6+//hq5ubm45557FNtDQkIwfvx4rF27Fp988gnOnj2LsrIy7Nu3D7feeqvhMdVuqR49eiAtLQ0VFRU4ePAgrr76ard/Dj1oFnYjVMSPIAiCAIDgYPOV9pf+5/76/3N2zwmCgCVL78ePP+4x5aIqL68y9b51dXX48MMPcc899+Dll1+Wto8fPx5+fn5YtWoVxo8fj4ULF6K4uBjXX389PvroIxw7dgy///67w+NzHIcvv/wS2dnZuOSSSxAREYElS5aYGps7oFnYjZBbiiAIgggODkRp2Rq3HIvneXTs2BbFJZ+b2j80ZJxpgfPee+/hiSeeQFJSEtLS0gDYXFJffPEFTp8+jddff13a980338SoUaMwYcIEU+Lm6quvRu/evTFq1CipRdKcOXPw/fffmxqbq5C4cSPkliIIgiCaCkeOHMG2bdtw7733Ii0tDd27d8eVV16J4cOHg+d5zJkzBxMmTEBCQgICAgIQGBiI8vJyU8e+4IILcObMGUXvx/R098cO6UGzsBuprqI6NwRBEC2d8vIqhIaMM7XvsGEX4rvvn3e437Wjn8XWrQdNvbcV3n33Xbzxxht46KGHMGnSJBw9ehRpaWmYNWsWpk2bhunTp2P//v0oKyvDkiVLEBAQYOn43oLEjRshyw1BEAQBmBcZP/64B2fO5CIhoQ143j7HRxAEnD173nTMjVU+//xzLF26FLfffjvuvvtuvP322wCAyy+/HOvWrcPHH38MwBZD07NnTxw6dMjUcQ8fPoyOHTsiLi4OWVlZAIBLL73U7ePXg7Kl3AgFFBMEQRBWEAQB06ctB8DZiRfbYw4zpi/3WL2bsrIy/O9//8P8+fMRHx+PDz74AADw999/Y+TIkUhMTETv3r3xf//3f4iNjTV93I0bN+Kvv/7CypUrcdFFF+GKK65QBC57GhI3boTaLxAEQRBWSU1Nx/hx85GRcV6x/ezZ8xg/br5H6tzIeffddxEdHY0NGzZIMTIvvfQSdu3ahQ0bNmDLli3IysrC2rVrTR9TFEWkpKSgdevW2LFjB1asWIGnnnrKQ5/AHjIxuBFySxEEQRDOkJqajnXrfmu0CsVytm/fDo5TpqEXFBQgJSXF8HXJycmKx127dlU8/vvvv3HllVcqtqnfx1PQLOxGqP0CQRAE4SyCICAt7YC3h9EsILeUG5EsN+SWIgiCIAivQeLGjZBbiiAIgiC8D4kbN0LZUgRBEAThfUjcuBFqv0AQBNHyEEURAODn5+flkTR92Dlk59RZSNy4EXJLEQRBtDxKSkoAADExMV4eSdOHncPi4mKXjkOzsBshtxRBEETLo7CwEH/++ScmTJiA/Px8VFVZa4FA2AgMDMSECRPw559/oqioyKVj0SzsRsgtRRAE0fIQRRHLly/Hyy+/jLlz53p7OE2ayspKzJ8/32W3FIkbN8IqFJPlhiAIomWRm5uLqVOnIi4ujmJvnKSurg5ZWVmora11+Vg0C7sRcksRBEG0XGpra3H27FlvD4MABRS7FXJLEQRBEIT3IXHjRhqypUjcEARBEIS3IHHjRsgtRRAEQRDeh8SNGyG3FEEQBEF4HxI3boSK+BEEQRCE9yFx40bILUUQBEEQ3ofEjRthlpugoAAvj4QgCIIgWi4kbtwIs9wAgL8/FXEiCIIgCG9A4saNMMsNQEHFBEEQBOEtSNy4EdZ+AaC4G4IgCILwFiRu3EhdnYC6ujoAJG4IgiAIwluQuHEzVOuGIAiCILwLiRs3Q7VuCIIgCMK7kLhxMw21bshyQxAEQRDegMSNmyG3FEEQBEF4FxI3bobcUgRBEAThXWgGdhMcz6PboP7g/G3ViQODyHJDEARBEN6ALDduoN+IJMzd8CWmvr8Moe3aAQAmLXoJ/UYkeXlkBEEQBNHy8Kq4mTJlCvbu3YuioiIUFRXh119/xejRow1fM27cOBw+fBgVFRXYt28frr322kYarTb9RiRh4qL5iIixiZo6kQMAhEdFYOKi+SRwCIIgCKKR8aq4OXv2LJ588kkMHjwYQ4YMwU8//YR169ahT58+mvsnJibi008/xbvvvouBAwdi7dq1WLt2LS688MJGHrkNjucx9skZAERwvO1U1om25/x4DoCIMbOmS88RBEEQBOF5bDOwD3H+/Hk8/vjjeO+99+ye++yzzxASEoJ//vOf0rb09HTs2bMHDz74oKnjh4WFobi4GOHh4SgpKXFprN2HDMTU95cptt3cpQidQmvwzZkw/FUUCABYNmkqju3c7dJ7EQRBEERLxsr87TMmBZ7nccsttyAkJATp6ema+yQmJmLjxo2KbRs2bEBiYqLucQMCAhAWFqb4cxfh7drabROY5YYTDfcjCIIgCMIzeF3c9O3bFyUlJaiqqsJ///tfpKSk4PDhw5r7xsXFITs7W7EtOzsbcXFxusefPXs2iouLpb+MjAy3jb04N89uG4u58eOM9yMIgiAIwjN4XdwcOXIEAwYMwCWXXIK3334bK1euxAUXXOC248+fPx/h4eHSX0JCgtuOfXzXXhRmZUMUBGlbncxyIwoCCjKzcHzXXre9J0EQBEEYwfM8kpL64tZbr0RSUl/wLTDu0+t1bmpqanDs2DEAwK5duzB06FBMmzYNU6ZMsds3KysLsbGxim2xsbHIysrSPX51dTWqq6vdO+h6REHA2gWLMXHRfIiCAI7nJcsNzwHgOOz+7keF+CEIgiAIT5GSkoglS+9Dx47tpG1nzuRi+rTlSE3VDvlojvicnON5HoGBgZrPpaenY8SIEYptI0eO1I3RaQz2b0rD5g9WAZxN1NSpYm6S77mT0sEJgiAIj5OSkojVa2YjIUEZ55mQ0Aar18xGSop+fGpzw6viZt68eRg2bBg6d+6Mvn37Yt68eRg+fDg+/vhjAMDKlSsxb948af+lS5di9OjRmDlzJnr16oVnn30WQ4YMwZtvvumtjwCO5zHoumsA0SZmBFnMDcdROjhBEATheXiex5Kl99X/n7N7DhCxeMl9LcZF5dVPGRMTgw8//BBHjhzBpk2bMHToUIwaNUrKiOrUqRPi4+Ol/dPT03H77bfj/vvvx969ezFu3DiMHTsWBw8e9NZHQLdB/REZF9tQ56beA8UsNxzPIyo+Dt0G9ffWEAmCIIhmzrBhfdCxYzs7YcPgeR6dOrXDsGHadeSaG16NuZk8ebLh88nJyXbb1qxZgzVr1nhqSJZRp3lrZUtp7UcQBEEQ7iI+Ptqt+zV1WoZ9yoOo07zVMTd6+xEEQRCEu8jMzHfrfk0dEjcuok4HV2RLAZQOThAEQXicrVsP4cyZXAg62bmCIOD06Vxs3XqokUfmHUjcuAhLBwc4iKLYUKGYF+sFD4d1C5dQOjhBEAThMQRBwPRpywFwdgJHqJ+LZkxfrit+mhskbtzA/k1pWDlzNsoKixQxN4XZOVg5czb2b0rz8ggJgiCI5k5qajrGj5uPjIzziu1nz57H+HHzW1SdG8DWOLPF/IWFhYmiKIphYWFuP3afKy8XN2b8JAriV+LXP8wXOZ73+uelP/qjP/qjv5b1x/O8WFa+RhTEr8RPPnlM5JvJXGRl/ibLjRupq62VLDc15aXkiiIIgiAaHVEU0bq1rRhubm5xi3FFySFx40aEujop5iYgoJV3B0MQBEG0SIKDG6r8Bwa2zLmIxI0bqaurQ2295SYgwOttuwiCIIgWSGhokPT/ABI3hKsItQ2Wm5aqlgmCIAjvEhraWvp/UFDLnItI3LgRoa4h5oYsNwRBEIQ3kFtuWupCm8SNG6mrrZV6S5G4IQiCILyB3HJD4oZwGaGuTrLctNQLiiAIgvAucstNS3VLkXnBjchjbshyQxAE4T54nsewYX0QHx+NzMx8bN16qEWmOJuB3FIkbtxKncxyQ+KGIAjCPaSkJGLJ0vvQsWM7aduZM7mYPm15i6u6awZyS5Fbyq3Y6tzYTDeRUWGY/MQ9+MfFg8DxdJoJgiCcISUlEavXzEZCQlvF9oSENli9ZjZSUhK9NDLfRemWCvDiSLwHzbpu5Lpr+uOmLsUAgLDQILyz8Gbs3PoiPt71JfqNSPLy6AiCaEnwPI+kpL649dYrkZTUF3wTXGTxPI8lS++r/z9n9xwgYvGS+5rkZ/MkZLkhceM2UlISsXLFgwj2FxXbQ1sJuOUiP7y44jkSOARBNAopKYk4cXIFNm+Zj08+fRybt8zHiZMrmpyVY9iwPujYsZ2dsGHwPI9Ondph2LA+jTwy34ZibkjcuAWe5/HW/z0CgAOn+g2yx8Pbl2P8s0+Qi4ogCI/SnNw48fHRTu3nqtWqqVu9KFuKAordwpVJfRHXLkz3eY4DwgME9OoQjgnPPYnPn1tATTUJgnA7jtw4giBg8ZL7sG7db4aZRr6SmZSZmW95P1eDj70VvOzOcx4WRm6ppiVHfZT+iQNM7RfiL+LilH9i7gaKwSEIwv24w43jSy6trVsP4cyZXN1JXhAEnD6di61bDwFw3WrlLauXu895CMXckLhxB2W15k5jWa3thhMRE4OJi+aTwCEIwq0468Zh+JpLSxAETJ+2HABnJ3BsjznMmL4cgiC4HHzsreBlT5xzypYiceMW0rbsQ0k1D1HUfl4UgeJqDhllNgXN8RwAEWNmTacYHIIg3IYzbhyGr2YmpaamY/y4+cjIOK/YfvbseYwfN19yFblqtfJG8LK7zrk6RkieLQWYr7vW1GON5FDMjRs4unMPvj8KjLvAJmS0gopb8UD38GocLQ60beN5RMXHodug/ji2c7cXRk0QRHODuXESEtpoTkyCIODs2fOSG0cOm9z1kE/uaWkH3DpuR6SmpmPdut9w8NAy9OqVAADo1XMKqqqqpX1ctVq5+npncMc514oRqq6uUewTGNgK1dW1hmNpboUSm64s8yH6Jg/DmdoofHU6FJW12qo/yE/EDR1L0CO8SrE9vF1bzf0JgiCsInfjiCpTstqNo8Ybk7scR1YDQRAU446ODlU874rVyh2vdwZPuRFbtVLaLRy5pnzNHekOSNy4CMfzGPvkDAAijpcEolbkNN1TzJqTFFcGDg07FOfmNc5ACYJwK75qwmdunPIy5UJK7cZR443JnWE2oDY4OFD6f7t24YrnbFarPAiCdnyAOvhYjdXgZXeQnV1oaj+rbkRO5T4wCir2VXekqzSt0fog3Qb1R2RcLDieR0JIDcICBDu3FIOlhCeE1EAURRRkZuH4rr2NO2CCIFzGlzKKtEhNTce33+2UHldWVuOppz5Efn6J7iTljckdsGY1UIqbCMX+Y8ZcgtatW2nGzDCr1YrlGzBhwhW6liFnrV7OkJKSiA9WTjfcx+icO4oRkmMkbpproUQSNy4idyuF+OtEFKtg+61buITq3RBEE6OpmPDlQaVBQQH46KNHDUWYlcwkd2HVaqAnbth3Eh2ttOYwSksqkJ9fghdevNNQjDKrV3FRuWK7I6uXVfSuITnuciMCxoX8vO2O9BQkblxE7lYq04m3URMVUIvcrd/h4OatnhoWQRAewOxk7O/v73WXVefOtsBQtRXCSISxyT0np0ix3d2TO8Oq1SAkpCHFmYkbo+8EsH3+sPBgREcrC63qnYfU1HS8++4P0uMHHngT3bpOdttndzRehrvciICx5cab7khPQuLGRY7v2ovSfNuXnlHWymFKOAAkxlZg/tSLceLkuz6zyiMIwjFmJ+OzGe971WXF8zx69GgPwD7+wlEcRWpqOlLGviQ9PnEi262TuxwrVgN1UCyLuXH0nXAcB47jLMWTREU1BCsf+TPDrdYqs+6kSfcsNjznjtyIAFBbWwfAWNx4yx3paUjcuIgoCPjixVchiiIEEdiSFWLbrhI4WoLH18zYBNEc8GSgr9nJWB0P0ti/9WHD+hjWNnEUR9G6dYP7Jz4+ys764y6sWA3kLimg4Ry74i7ROw9RMiuPvCCe2WMaXX9mxxsbG2X4vFGMEGCzWJ07Z6sNZJQt1dixRo0FiRs3sG/jFmx+fxUA4GhxIL4+E4bSGvtTqw405uuL+TXFSHSC8EU8HehrdjLWs5b83zsPIzn5Io//3l2No5ALiaCgAHTo4JmSFVasBnKXFAC0rRc37nCXqM9DmzbOiRsz15873UDMjcgsNHKKisqQm1sMwHELBnacyspqxXZPuSMbA5pR3cQ3i5dh5aNPQRRFHC0OxLt/RWHtyYYfiF4GVVONRCcIX6MxAn3NuAL04HkebduGY9NPL3vcTeXqBKoWEj16xLs8Ji2sWA3Ulpu2bW1uKVe+E4b6PMjjc9TnQg+z15+Z8Z47l2/aDfT1178rHh85kgEAOH++RBIrZvpLpaamIz39TwC2c39V8hyPuSMbAxI3bmT/j5ul7CcRHE6UBqLaXlBr0tQi0QnCl2isWh3KjCKtydgcnnZT/fLLYcPnHcVRqIWEp8QN0GA1qKlR3izVVgN7t5RN3BhleTHBZORWq62tQ5u2yiwreYFAdSsDLaxcf2bG+5+l601fTz17tlcU7evQoQ0AoLS0ElVVtkrFRtlScsLCgqUx7917osm5ouSQuPEwxRruKS2aWiQ6QfgSjVmrg03G+fkliu3MBWAGTxdHk09mzqR1h4Q0nrgBbOe0uNiWfl1aWoHk4bPtrAZ6MTfs9Vr9p8rLGwoZ6gkcnufx+edPKoSm0nITqPUyBVavP73x1tTYWiTs23fS4Xsy+vbtDKDhe2aWptLSClRV2Y5ntjO43AUnd82ZwdeKWpK4cTcq/1N2ue0U6y0cmmokOkH4Eo1dqyM1NR2PP/ae9PjZZz9Gxw6TLLlHPOmSZpOUIAgOG05qoRYSScP7eXTCCgxsJbmZ/P39kJZ2wO48sjGxhWCbNmGK8aSmpqNrl8koKakAANx91yKs/GAjAOCLNdt078Hq2Mfg4ECFGDBjuXHm+mPjZa6jW299BT//fBCAzXJkRizwPI/rrh8CAPjzz7OK50pKKqRjm+0MrhQ32jWDtPDFopYkbjxMZl6l7nNNORKdIHwJb9TqiIgIlv6fcfY8amtrDbNX9PCES5pNyKWllejaZTJefeULAMBvvx0xFUfBVv8sUPXii3t6dMJKSGgj/T8oKEBzImfi5swZW20xnuft+ksJggB/f9trf/75gFSvJzAwwDD1Wi401fVwzAQUO3v9iaIoCY+fNu1Ffn4pAGB48kUOxQITFHfddRUAoE+fTopjy91SZi03YWENQs6s5cZXi1qSuHEz6h9ldetIANoBxSXVHB57YX2TDdgiCFdwpxnbG7U65LVQwsNtQoe5G/T6G2nhCZc0m5BLSysgCAK+/dbWiiE6OszUQmrAgG4AAD8/5XfiqQlLnY2l5Qpi4qaoqExyCapT7oGGNPbKyhrk5dlchTGxkabGER8fbTepOwoo5nkePM/h/Pliy32t5Bay8vIqFNR/rn/9a6ShWNATFHJRrRQ3+mUB5MitVGbEjS/3pSJx40b6jUhSPO4RXoWkuDLNfX/JCsZ7f0Wjw0332r2OIJo77jZjm8m6eXTmCrdaSCMjQ6T/h4c3TArff79LIQqcbeToCnLLDQAcPZoJAOjSJcZOsKjheR4jRvQHYL0AoLPILTeAtqBgQqC8vAq5uTaLjFrcyGv7VFZWS3FQYWHmMp4yM/PtLDchBpYbdh1v+mke2rQJB89bqxUjFzcVFdUoKCiVHhuJhSVL79fcR/59lZdVoKqSBRQ7dksFBraCv7+f9NiMW8qX+1KRuHETDd3BbfQIr8INHUsQrOo3xa778FZ1AO8HQMSYWdPBUZ0booXgKTO2Xq0OtrJetHiyWy0OEZENlpuIiAahk5CgdDNxnH0wq6dd0sy9wOJPzp3LR0VFFVq18kenTu0MXztsWB+EhunHmViZsMxa51iGD8OxuLGJlrFjL1UcVz6J28SNTQR16RILQD+oWC401a4uPcuNmf5QAJCfX6ob48SOXV5eBVEUEVEvmNWiksHOfceObR1WOI6MCrPklgpTfedmLDe+3JeKZlQ3wbqDAwAHEcPrLTbqa5Q97hVZBQ4iOJ5HVHwcug3q7/R7+1qUOkHo4WkzdmpqOk6ezNF8zt0ulagoueWmIf5GbYV4842vJZHB8HRxtAa3lM1yI4oijh/PBgCpLYMe7pqwtKxz2TkfYe7cW+y+XzNuKbYtpl0EBg3qDgCYPmOMwuonFzfV1bWSW0puIXGUPWYm5sboOlYLk61bDyAwsJXmvZmNq6ysUvNYruDvz1uqc6MOnG7b1rHlxqxL1dPZdlrQLOgm5N3BE0JqEBYg6BbuA4BAPyAhpBodQqrRK6IKyVf1h5+/P7oPGYiB145E9yEDTVlzfDFKnSD08LQZm+d5dO+ufSN1t0slUma5CY/QFzcZGeexdm2DiHly1geWiqM5s3iRx9wwjh49BwC45dZhhsdxR3C2nlWjTZtwvPDincjK/khxj0pQWW7U2VrybVeN6I/WrZVuFiZcx4y9BABQUWFLAWeWG8aBA6ccZo8xcZOTUwhA23Jjtj8UAKSkXKZ7b2aCjYkbFiztDnJyiizVuVGLuGgTlhszBQlFUcRzz9/R6HMSiRs3Ie8OHuJvLpjwho6lGN+1GNd1LME7C29GXsmXWPTFYtz5yguY+v4yzN3wpWE8jq9GqROEHp42Y5vtqfTwI9e7bOlUxtw0iBu1FSI+PkrxGz1+PMu0K8rZxYs65iYlJRFXXXURAODee0caHmfr1kN2rj05jmKFzHS9btMmTHGPMhNzI7fm6MUCPfvs7QBswcSArUqvnD17TqBrl8mYOvVtAEBWVoGd0GRuKSY0tCw3zl6f6nuz3C0FAL9usxVfdOQ+O3Mmz+E1VFlRg+pq83VunHFLOepvBbDvqvEDi0ncuInju/airKAQAFBWa860GOSnvBjCAznc0LEEPcJtF3pETDtMXDRfU+D4WpQ6ucaaF576Pj2dsm120lmy5H6XLZ164oZN1MxqEhsXhY4dG8SNu8v5a8Em5LLSCuk4areD3nEEQZAmdu0qzMaxQmasGkycsHsUE4TM0qB1jphFzigepX37aMVxamvrFEG6GWdtoiD1y18B2Kocq8fJJvXTp3MBaNe5cfb6VN+bG9xStns+c6MBxnFa06e9Ay1BIX/82OMpmPnoWABAgBNuKbN1blJT0/Hcsx/rfi+AdwKLvToDPfnkk9ixYweKi4uRnZ2N1NRU9OzZ0/A1EydOhCiKir+KigrD1zQGfZOHITjCFrmfUdYKJdW8btEowBZYrBePkxRXJsXj6AUc+1KUekt3jTU3YefJ79PTKdtZWQWWX2PW0qn+nuWp4PKaN8zFsnv3cQA2wSW35pipm+Lq4oWtwsvKKqXjWMl8YuJAPtkCtsBkR7FCZgUmz3Po1Kkdhg/vh/h4WwdsltWlJW4io0LttukhtzzJP8PZszbRlp1diIqKKvj5+dlZ2lhH8DP14kYr/sfVHmPs3qy23LAUd0EQpO+gYewN7jMWPM8sc3qwQHc9V60cdl0yl56VCsXse3NEYwYWe/UunJSUhLfeeguXXnopRo4ciVatWuGHH35AcHCw4euKiooQFxcn/XXu3LmRRqxNQ6ZUfR8TcNiSZbuo1AKHPdYTuRwHhAcISAipkY6tFXDsK1HqLd011tyEnae/TyuNEp1h9+5jhs9rmc7NiAWt71lurdGy3Pyx8ygAoE+fjooYEjOWG1cXL2wVHhkV6tRx2BjHjnkRycNnS5PuLRMWOowVsmrV+OeNF4PnedTW1uHEiez697cXFFZqBzG3FKCMuzl7tiHe5tQpm3jp0iVG8VoWc9PglrK33Bj1GDNbwDE+PtouoJgV8fPz81MIlzNncu3cZ6mp6fiy3gJVXmbLtlILWPZ40KDuDhddTBCzgHwr4sYbRTQd4VVxc+2112LlypU4dOgQ9u3bh3vuuQedO3fG4MGDDV8niiKys7Olv5wc7eyIxoJlSsmtK0eLA/H1mTCUqnpLVdSZc1mp43bkAcuAb1xMvuYaa2yam7Dz5Pcpt3rk55dgwoQFUjwAwx0ZRKzxnyiKmgLJUYqtllgwk/IboRFQvHPn3wDsa7GYsdy4unhh79HK31zxNvVxgoNtAbulpZVISzuAP/6wCbXevTs4PJZVq8a0aTcCsLVdSE7uB0BbADIBYFQ3iAUB61luYmIipev35EmbkLIXNzYLEXNLqeu/MPR6jKkbgOqRmZlvF1BcWVktWU7k2Urt20ejVSv7MTBRHRwSaOgWCg4OdGjFZ9fMqVO2+bR160CpIKIjtm49JPXF0sIbbYZ8ataJqHfr5OcbT8ihoaE4efIkTp8+jbVr16JPH/0vLSAgAGFhYYo/d6MWHoyjxYF4968orD4Rjm/PhEn/mkEdtyMPWAa8U5FVjS+5xhqb5ijsrH6fZt1xWlaPxYsn4/x526Tz118Zmo0SnYHd7EtKKuyyYsygnuTNBMcCNlHF8zz8/HjJxbKz3nKjxozlxtXFS0i9tSG7frK3ehw2RjbpHj50BoB9iX8tzASZApDCCuQwS8aQof+w2589x3H66dzvvvsjgAZxYwukbrB6v7P8YcmyeqreQsFq4DCY5YaJG0D/O0tNTa//rDa2bz8iCSwjSxPrRC6v3cNg1hvAFrdVUFAKPz8/9OyZYHccuah2hCPBzCxUWVmFqK62Wb7MWm8EQZBadTgTp+UJfObOy3EclixZgl9++QUHDx7U3e/IkSO49957MWbMGNx5553geR6//vorEhLsv3gAmD17NoqLi6W/jIwMt49dLTzkiOBwtiwAR4oCcbYsAIF+gsNYnOJqHufK/NEhpBo9wysQUZmNk3v2K/bzRkVWNb7iGvMGzVHYmf2e2rdvg7lzb0F2zkcO3XFG1i32fuXlVZqNEp2B3exzc4vQtctkzJhum3iYkHKEepK3kvIbGhqE2NhI+Pn5oaamFkePZiomLfl+jnB18cLeY++eE04dRz3psqaMV424yFRcWWpqOiZNWqJrTWD3LD03yg03XGz3HkxgzJ+3Wjede099nFNlZY107aldXMyyyo7XWcdyk5VVIFkjjDqDy9PSo6JCECtr9aB33lkn8sFDegCwuZUYckvQ0aOZOHToNACbe1ONvHikIxwJZuaWKi0pl7LMzIqbkJAgycpz7pz1Rq2ewGfEzVtvvYW+ffvi1ltvNdxv+/bt+Oijj7B37178/PPPuOmmm5Cbm4sHHnhAc//58+cjPDxc+tMTQa5wfNdeFGZlQ3Rwc7ZVLS7VfZ5plCNFAbi3ZyHGdy3G9Z3KcO8QPxw//o7dxMHMonV1yvf1VEVWNb7gGvMWzVHYmf2e3ln+CF548U67bAq1O86xdcuG2Y7FZmA3+6KicgiCgA8+2ATAlvmRn19iuRWCle8vKioUN95oq7PCVt/yAGc20QWbsNwoYzqMi85pwcRNSUm55ePIG1eWlVUhJSURzz1vS7EeOLC7nZDVs+AdP5YFACgoKLETlxzHGbpRIiKC7RYGTHBt2rQXXbtMRkaGbVH570f+T7L6sXoulZXVDgOpR19rC38YOrSnNO6QkNbSJH3BBR2luBejzuDyeKp//KM9WrWyuQJvu+1V3euNdSK/9lpbR289y83Ro5n487DNajZ+/BV2wpJl7GVnFxouDqqraxxa8RuumQo7cePISsuslaWlFejS+V9IHj4bt9/2qtssss7gE+LmjTfewA033IDk5GTLlpXa2lrs3r0bPXr00Hy+uroaJSUlij93IwoC1i5YDICzEzjSCsWgarG0L4CdeUEY0rYSoa2Ux9GL41i7druu6dfTsR++4BrzFs1R2Dn6Ptl1preKVbvjHFk92KQjT6lWH89qFhqz3BQXlwMArrrqIikGIjo6zHLvn+zsQofvyUjf/hqWvT0VABAbG4kTJ1coMl5YoKYZyw3QsHhxVHROC2kVXlpp+ThyF8w1owZi9ZrZaNtWGTfE7i0LFtyjG1DPLA3bt/+F2Ji78NNPewEAGzbsMvX57eOAGqxJgiDgWL14kk/sTCiHhrZ2aFllMS19+nTE5i3zkZX9IU6dflfa59vvnpPOo9F3Jhc37BrNyytGTnaBZqyOfF/mRmXuP0BpueE5DjfffDkA4KabL7MTlkzMz5/3ObQFrO1az8oyFj+A8pph4qZt23BTSRPsu8rMLIAgCEhLO4DPPvvZbRZZZ/C6uHnjjTeQkpKCq666CidPnrT8ep7n0a9fP2RmmktF8xT7N6Vh5czZKMrJVT5RfyM1U7WY54ALo2wKXr2fXhxHmzZh0krB7ngejv3wdOaLL9MchZ2RtcBsBojcHWfW6qGuNgs4n4XWYLkpk9wS/v7G177eJJ+SkogPVk43fK0oilKsQUxMpOK5hIQ2igBclrVjts4NYBM4fS98WHq8b98JUythZmlgbR9SU9PRtctkjB3zEgDbd92v78Oax2GTdWVlNRYvngxAP67s8Sdu0g2ov+GfFwMADh86DUEQsH/fSQD2hfX0UC8M1K6yzEybVYxZDYAGceNMG4M2bcIV6f1AQ1d0ZuXRQquaclZWgSWrn1zcFBU2WG5SbkqU+k0x5ItWJua/+OJXTQGbm1sIAFIMjREhsmuGWdquGtHfVNIE+w58aTHnVXHz1ltv4c4778Ttt9+OkpISxMbGIjY2FkFBDT/+lStXYt68edLjp59+GiNHjkTXrl0xcOBArFq1Cp07d8aKFSu88REU7N+UhpdG3YRlk6Zi1RPPYEfqV1IGldmqxcH+oq4A0orjuP76oYbH83TsR0OzQv2aDM0RV90GgG/Wx9Fb5QP6mUZaxMdHm77RqVe3rmShse7cxcXlum4J+ePS0gpNsWC2KaIcI9cbAFxySS8AUBT0M4PcslVXJ5haLGi1XxAEAevX/4bMzHzwPK8ZwwE0WOaqqmocWj84jtMVPldfPQAAcKg+GDkrqxAAUFNT6zCbqry8ymEcUFb99cUK9wENbQaY5c4KWq4y9njav2/U/X3qiRsrEz37TCkpibh53OWGY2Lnd8nS+6WFbWFhmSRg5S6hG65/AYDZ3lIN10xBvfXozjuTwXGOkybi4pi4sV5nylN49W46depUREZGIi0tDVlZWdLfLbfcIu3TqVMnxMc3FCCKiorC8uXLcfjwYXz77bcIDw/HZZddhsOHD3vjI9ghCgKO7dyN2upqDB1zg7TdbNViM8hXBN1NNiTzZOxHamo69u49Lj2+4/ZXveZnbUxccRtYaSrY2LCbpDxOwoqwAWwrOLMpwfJ2Ca5moTHLTWhIkKlA4NDQ1nYptmYzpADgt9+OGLoetM5bz54JllzF8i7V6hL5eqgbZ8rZu/cEAOBfk68xbObILFLOwPO8ZJE7csQWjMzij2JiIg0WBrZF4N9/Z9g9pxY3587ZxEOc7N7GJvEzp3OdLrKnRbuYCN0FopblMSurwJR1l6V9l5VV6laS1oLneUkk19bWydLklS6higpb1piZuDZ2bfXp0wnjJwwDYDvnZsonsPkliyw3NpgqVf+tXLlS2ic5ORmTJk2SHs+cORNdunRBUFAQ4uPjccMNN2DPnj1eGL0+6qJ+gLmqxeWOLYcAVKY/k3WtPG0ulJtzjx/PbpauKC2YEGC1IU6cyHYo7Kw2FfQGgiBIJnlrrxMld5yxdavhwm3Vyl96L1ez0KTUWAtiTJ7dYmYMcjLPOfe7suIqlmesmBE3HMdJri+1uElJScRll10AAJg8+RqdZo4sDdw+08sZPvvfLKSkJEriJi4uUloYqFf6TFCr37tVK39JRJpxS1VUVOtee86it0BsXS+6iorKFOMFoFvoj1l3WdxQRYV+ALQjjKxULObLSlfwRx9LMS2iAdt5iYsny02LQKuon5mqxTtyWxsKIK04jsJ6/6yjRmuejv2Qixt58amWgFwI+PvzDl1RVpsKegstc7sR7BqUu+PYJJaTrezOXFiozBpkk5JZC+NNN1+maXUIr7fcZFtow6COlbFi5XTmZs5xnCVXMau7Auhn7chdnNdcM1A6L3K3FBPV6olL7e5j33teXrFbrB/x8VFYvWY2LrzQViOHuTBSU9OlGKCCghIkD5+Nu+58HYBNYMk/08hrBkrHaxA3zC3V0HRTni1l5GJ1Br0FIjtfcuvILbcMw4mTtlCJ8ePm2wWmM+su+yxdusaaFtRqCgvLdJ9j9X6suKVsrYHMjyMzM18WUOw7lhtz5SsJSxgV9fv6DDA8rgxhAQ03DEEE/DigqMYP+wsCkRhj3yvLpvzt4zjUNRXkN3ut2A+WxdK+fRu0iwlHbm4xzmWcl1baziKPC2jXzljcsDGwuAxX39udODs2NmHIy/BrwawCRnCcLUD7/955GIWFZYqMg8Y6d35+PAICHN8Q5VRUVGPB/NVYt+43xfbU1HRkZRVg26+vArB1Zl79+Va8PO9uaZ/WrQNQVlZp+ub4yCP/xCOP/BNnzuRi+rTlkqWMWW727j2JM2dykZDQxqGFRG25sXKD3rXrGEpKKiytdBlmRZRc3LBJX/6dp6QkYsnS++yuK0EQpMlTLqq1YjgEQcDiJfdh3brfFAX8Xn7pf1i9ZrbmvcV2nTp23bHjz5g5FgAQExMhbWPxMkePZiEt7YBkVYqJicCJkyvsPlNdXZ1Ue4a5pbQsNywGMDU1HevW/Sb9ZrKzC/DByplISIi25P4tLCzTXSB26xYHAHbd6JloHD9uPi5LfAwnTr6L6upajLrmael3+8KLdwIAQiwuJARBQF5eCWJiIhQWIzVyyw27r+jRkPZtTtgIgoCzZ89j27Y/0aM+PKJduwi769NbkOXGAxgV9dOqWny8xDaJjOpQhstiKzQt6jl5pZpxHLH1q6CPPtrsMPZDHufx8SePYcmS+/Hxx48pTNPOBLkGBwcqJkJ1uXk5vtyLyZWxscktLKy14arHSlPBtm3Dsemnl6UxNOa5M1t2HWiw2AQHB+KFF+/UHJPc4hAU1MouK4W9n9l0dIba6sDEZWFhqenML7W4sdI+oKCgFD/8sEvzuI4wK6LkMTeAMi3ZKPCZ4zjpvJh1982ZM14KKC4vrzKMK3v1lS8BmOv5xPM8EhLa1Fs5/STrLmtayRpastiRuLgozc/E87z0mdj5i44OkywTcssNQx6HsnnzfqmjtpmMQLZt44+7Na8HW7aurbehUWNSJjIDAvwVCxJm9Tlnwb3JFq0fffQTAFtNJz3kpQjU4kuNFUstW2x/9unPOHb8HUngvfb6v3zmfk7ixgM4KuqnrlrMfhSBvLoBm+3fvAoeSf98TTOOg5l4N23cg65dJmPzZlsl4zff+EoR++Eo+yMhoS1Wr5mNrGzHVWfVqOuU6LmlfLkXkytjY6tpAPWFwPRTfZ0x27IxrPmi8c6dUUVWRl2doDkhaI1JXiY+Li7SbsJmk5KjdHRHna3lRfz0JuYyVRxKTIxSjBs1RVQ/Ligoxe87/pbGpx6vHmfO5Jl2FauLJYaFtQbP80hO7od3lj+smc3CYOfFrKh+/oU7JOsJExosruzZZz8G0JCO/uSTH2j2VjKipMQ2EcfFRQJo6MOVUS9uWIAt4PgzFRaWSSKGfb5AleVGC73rory82m7fkmKbFV3ecFPOsGF9DIN1mWgcOLC7tI1d60DD72zbNvOCOqN+0fr3X+cAOHJLNZwHtWtKvpAdPryfKdcVQxAEvP76l3jscf1SAN4WOCRuPICiqJ+D1RwHER1YB3DVb5k9jgqsw/Fd+zRfz1adrJDVnvquyFVVtQpXhqM4D57nwHH25bblF6qeVUe9Cm+rYbnx5V5Mro5N7ZJg6chaWG0qyMbAroXGOndJw/s53IfFGTkSHIDSXRcZGYp4WZwEoMw4keJ0cpRxOmayNpiIYqZ6NjGzVe7a1HR8X19EjgViqi038jHkqyrrZmTkoa6uIYuosLBMOg77rFu3HsQzT6+CKNqX32f3g2ef+dj0NaAWgsyCt+mneWjTJlz3vMhje8yKalEEJtxiy5SRV80VBAHb04/U/7+hKWlqajpmPfG+qWMDQG6uTQixRVlCB9t1wMRD375dpLE7+kyAvWuqwS1lL1TkyNOmf//dJk6Z8Prxx91SKvWrr9qsU3pF/MyKRvl9VatDfGlppWFpCUEQpUaz11zzNFJT0+2udS3kzSyDgwORlNQXt92WhBXvPoLsnFXSQvanzfOk69fMdenv74eJE0cA8L37uTQOr71zM2f/pjRs/mCVQ3FzcbtyBPkZ7+PHc3jppTtx2/0pGHz9Neg+ZKAUrMxWQCwT4cwZ2wqI3TQA89kfRjUV/vt/D+HEyXc1rTpqcaPllmqMXkzO1o1xdWz24kY/7sZsU0E1RuXqPVHLqENCG8c7wXyXbfU5ueACZXdptRssNTUdDz/0ttnhArBNNHLLDUMQBPy6zVYqguM56fpkPXtiNMQNG8PChV8AANLT/0Ty8Nno2mUyTp9ucDv36tVBKpTH+GnTXrz00v80rQPsO//55wOmP1dUtHLBsfQ/91uqvxMfH21aVPM8J/2ey1UZS2zyV3+XehWm5bDEhmPHbMVWmaBkbil2nsw2glQHsLLHWm4pozGlpR3AV+ttMWJMcG34fpeUSs0CsvVaZpgVjefOnZesUlripqzM2AU4ftx87NtnS+Hv3dtWn4hd68UGbimg4Vzs/GOJFJJw773XaPaN0rKO6mHrsO67vfUooNhD9BuRhOR77gQMrpMe4VWawcNazJp1MwCgpJrHlqwQ7PyrEF+9skS6UbMCWWfO2CokywuFuVrjhud5tGsXoRvvsHDBasX2tm3tfzSe7sWkFVSpDjZ19T319rMiboAGq8Cqjx+1FNvi7PjMoA5UlmfZuGNMamuWuhOz3FTP6NQpxm6bEZmZ+bqrWWYZ6NChrbS6P3TwNC69tLddtpQcFii5aeMepKUdQEpKoiKA9dPPHpcqDzOOHrVN4Opg1szMfHz62ROIi4uyVKVYbbkBrFXgzczMl0T16jWzTb9O3fSTiTj19c7EkN7EKE9sGDP2UgANQqKDZLmxCcaT9SUVHMFEhTodXB1Q7IiUlEQ88u9/KrY9MetmnDiRjdTUdFlvKe3va+vWQ6itrdOtdcSCbrduPYTy8iq0bh0oiRv5b1/uAlRfMyxGZ8zYSzFkyD9w4YWdsG7ddklUGrmlAEiWRnbOHVFUVIbISPtrzhm82VuPxI0HkNe54Tht64G815QVQlsJuKFjCUQhAu2XvASeL0BdXR3y8mzmc3YTl0/y7krP08uyuP+B0QBsaaehoa01LTee7MXE4mXUyLMVjASOq2NTixlH4gaw3cTWrt2O225LQllZpaXJzur4HKElDHNyCgEYT1hmLGNsTHrnpK6uDn5+fpoir3PnhvFUV9fC35/XfE82gfz++1Gpvoi69gebPDvILJoHD9osN1puKcY/erYHAPz11znpOlOfDnVMDLNOsLGlpTVYadgkZra/lNbxza6sRVFUxPakpqbjuWc/ljJ0HCFvCQAAxcXa4mbQ4O4wIv98CR544C2kpqbj0kttVZolt1SC0i2VtuWAofVA/ZlY0birr+6PAwdOSSJZHkirh959o23bcOm+4ej7shXiq0ZYWGsIgqgQneps1fLyKrRp02C5kVtwWLE99jr5NcP487CtGOLoawdj27ZDiIh07JbieR5BQYH1/3d83XAch8jIUOTkFKJt2wjN14iiiJycIsPfDcObqeHklvIAWnVu1JjpNaUF2394+zJ0C7P98OTKnVlu2rePlsUAWI/zMAvP89LN9++/bQFuWuLGU72Y3BHL4+rYrFpu1Pv9+5H/w6r6mBA9V5Uoih6pZaQXSC1vlKjXXiIvr9j0OdM7J6z+h1aV184y605BQSkc9TALCwuStqmL17HJMzY2Sgp4NyNuevZMAGATLPqp1MrHrDCbFmxczlhurLYU4DgOwcGBGDPmEmnbvHmrceZMnm6Gk+3c2USMWtwwt1RQUIAkIm2u4H7S+2kdr7yiWioPwL7viy/pieuuHypl0TFXjCAI0vvqXe9MLKSkJOLOu64CAIwZm4jNW+Zj6NB/AHDsljJ73ygvd/x9sQrXWaraSupsVWYJY0KeBRNXVFQ5vDenpCRi5qNjAABXXNEHm7fMx003XQbAOFtq2LA+ThXj/HjVFgD68TcPP/S2w+vI2731SNx4AL06N3LM9prSguOA8AARIzvabkJt2oRL8S9ZWYWSmZTF45iJ8zCaPM3yV330fnh4sF3aoaeabLojlsdxJV0Oj85coTs2KwHFcuRm5W3bGtqHaFUzZadMr9KpM+fO+AZve1xbW4eMDOXqi920H7j/TZjtrRVWL27U+7KAUK2Mky5dGtxSrVr5KVbS6rHYAizrYxCKy+2usfPni6UJz8/PNhmxnkes+aw6Xis4OFCKCWnTJsxU3Fp5eaWdm0qOM5YbVufGmVVwdHSoInPFdq2/I/1fDvvedu+2tVLRc0sBDdf8sGF9DCd++e8vJSURs+dMAABcfnkffP31MwBs35dcjLDFmvr9AVvsU2pquiTK1TE6THQNqRc5epi9b3TtaktxNiqeyK7dQQP/rejrpK5Uzj4Ps9iYrQTNPmt0tNKCx963Z6/2uq911i20fv1vugUQt207BEEQ0bp1K83z58o9yZ2QuPEARnVuGO7sNQU0uGDGjLlEmjDkbobU1HRMn/aOYRaCVpEnK4Ln1MlsqR+NVjo4izVR37RcabLparwMC0IODGyF55792K6SKM/bGgMuWjxZN7XRWcuNvIM1sywcP55Vb6VogJ2fcTfPt6vs68q5MxNo3qqVP+6ZuEjzpm2ltxabhE6dypW2CYIgxYppWm46N4ib8PBgpKam46uvdkjbKiqq0KP7/XYF/PRWssw1ZdunDOfO5aOurg48z2vWD1r4yiRp3zCT32lxcbmhldCq5SYkJEhK0T1wwGZpsvKb1LJe6n1vGRn5GD9uvnT9qSfdurqGooDsGjf7+7vxxkvqJ2j7WI6wsNa4+ebLpMdMADKRJefvv88ZFiRkj6dOvc7wezA7biZC5aURFJWTRw6QtpeWVir6OqknduZ6UrultESc/L30FiDss06cOEL3s1oVxKLY0D5F3YjzlYVrAABBQYGaYouRf77EJ5omk7jxAI7q3ADmek1ZQX4TY64peXwB0HChHz58BosXrzNlrXHk35e7APLzS6XYH71Cfqmp6fj554PS449XbXapyaYr8TLqongvvHinboCvUe0GZ2JugIbJuLCwTJp4Q0OD8MZ/vpL2OX06VyEmXnzhM+m56dPfcencmb3Bx8ZG6d602Q3wxn++IG27oPeDdmNi5+SvvzKkbQUFpTJTvVLcRESEKLLw/P39EBwcqAh0bN06EI/8+wbJ0iIXi1rIa5UwlxqzRsTFKc9Fhw5t8dBD10tjWbx4suYx1cTFRRvWhmITt1lxw6w2VVU1ipYSVgWO2nopn7jYdzB61DNITU03nHSZa4oJerO/vzvuHA5A+37CcRw+/ewJ3FQvcJio+sc/7JsCV5RXmRLlsbFRhtZas+Nm10x0dBiSkvrippsvU9wzvvv++YaxVRi7wvQtN/bNTRlmPmtMTKTuZ9269RCqq002LaxHbnGRF0DctctWZqRvX1sbDT2rjdwN6U1I3HgARZ0bg0J+Rr2m2J8V2E2sqspW20Bdupz1dvlt+58YN+4yzR4iRmJGz5106NApALbJiokbo/5S8tTRkNDWLpkuzcQT1dbWoY1qPHqxJnqpqEbxO65bbsqlVXRsbBR6yG7qISGBis8md2ccP5bl0rlzV5C3IAj4+uvfpZs3c4fKYefkb5m4yc8vldwRarcUCybOyyuWrIERESF2acevv/4vydLCVtEs8FWN3HKTl1dc73qynU+9VTGjbdtwiKJoqhqvkRC26pZilo78/BJJiH391Q7DCVEPtZhlExcLgGYZlkaTrjqoeOvWQ6ir078GBUFAdnahYdowYBOvq1c/iZSUROl9Y2Pts3vKy6vcknlpJs4uN7cI8+pbhISHB2PzlvlYvfpJyVUpRxRFjK3PBNNDLeTNWG5c/ayCIBjGgKn55ZdDuosltogNCgrw6RRwaSzeHkBzZf+mNKycORtFObm6+9h6TYWhtEb5NZTU8NiZ53z2TEX9j2XE1f0V9V4u7GsrE15RUe1Ukzb1DZ+5H1jaZWFhmRRvYNSCQT5B9enT0dIY1LB4GSNRxvM8Pv/8Sbz++r+QlNQX/v7+Dk29esfR+uE6I254npf2KyoqQ15esZThwSrEArZ4KnmaqfzYYWHBChN5cnI/JCdfZLrOjxlhWFZWaToosCEjyf7m32C5OSdtKygoRWX9alduueF5HtdfPxSALVaGBdKGhwdLE7BWWYLHHr8JgK24oNZnz5BZbnJzizFnzniHJenlY7Jl8TgucmYkhMssuqWY5SY/v1QK9M3OLsT33++S9lm/brupY+mJVOYqZOn58vYLapjACpfFUMlX+XLY4ueTj7eYGp8o2qoPq0XVn3+elf7frl2EnetYDyNRbhxnZxt327bhUnsbOXr3CEeJC/aWG9u/RkLVHQsQR60d5JllxUVlup/BiqD2Zgo4g1LBPcj+TWk4sHkrug3qj/B2bREaHVWfIt7A0eJAHCsOQEJIDUL8RZTVcsgoawURHHgOGNzW+grtyqS+AIDrrx+K668fijNncrH8nQ1ITOwNwHrWhRZrVv+CW299FYIg4NnnbgNgm6xyc43dUoBS3HTvHofAwFamUjf1SE1Nx6pVm3FXfeaEGiZgZswcixkzxyInp9Cwtokj1D9cJm6ysgoQFxeFMFlAsV6jS3nQMYsROXs2D927x0t9Whht2oRJN3S5kBp25YVYsHCibiNOR3V+5LVP1GmsLB33l60HTVuHzp7NQ8+eCTrixjbuo0fPybaKqKxUrmbVaem9enWQLDdRUSHSudcuNmkjMbE3TpxcYffZmbsWsE0wzz1/h6nPpX6P3Nwih9ePXAi7kgrOCq3JLTehYcpyCydPZqOiokrXpSqvtaLFqZPZABqsZWzy1bbcKN1SrVsHSkG8587lK777s2fPY8b05cjPL5GaZhrB87bqwyfrx8NIkBWUvOvuqzA8uR/y8ooRHR2qOxFnZOh/XgaLPVKXQTh79jyCgwMRHR1mevEjr5yslcYNNCw6rQQUswWIUQPYM2eMs5Lk99aMjPN44vH3pabJ3bvFYeajKVJc1/U3XKz52wFgl4FohC90Bydx42FEQcCxnbsBABddPVyzPgjrNaXYJoo4eK4ag80XIgXr1Ku+cXbo0FZR2+Jfk0dZ/BQa7yU2lGBnsREFBaVSufqkpL7Yt++EZtdqJm5YE71/T7sRO347YqnDtbq7+ZAhxtkRcoxcZmZQ/3BZsGlGxnnExUVJq1qjwoLMf11eXiWVSM/IOI/u3RtcUizrTb5aDZWJmwceGG3oujRT54fd4JeveETRfbqkpALh4cFSRpEZGgrl2Vc3Zufk3XenSdsuvriXVG01KChAt+4IS2V94IFrTae1sl5p8s8uX8GyeivOMGP6CvTu3QFPP3Orw33VQthqQDH7Ts6fl4mb0NZo167hGo5v3wanT+eiV68ODmutaHGqvnBep/oAbjY2M5abqCjbb7m2tg5dOv9LU8jzPG+6QztgLyDU9zOb2OEkK5rWMefMXmnqXqJVNI/neWz66WWHr9XCyGKhttyYcUspFyAac4coYvo046wkeSbazp1/49NP0wDY7k9aAl/vvsHEbl2dUN/PTL/mlDdTwBnklmok+o1Iwt2vzzNdgAsAznORyMopMfUjlcfPOIqjiY4ONR0/oP+GDf9lYuXii3vittuTAAA33XyZZuPNwMBW0gqzttb2uRYuvMdSh2ut7uYXXGDeveVsvxO92g1sFcviZsLDgx034rzJ9jnlwa/ygNeqqhqp0q18la52gRm5Fs3W+UlNTccr9W0GAOCJJ97Hhys3ATC+8arJ0HFLhYa2lt5fbeZnn6dPn44OXYU33Wy+EZ/tGA2fPSUlEW//d6r0fHBwoGW3LOPcufP46ae9pvZVC2EpoNiE5YbneQyuL5AXGOCP0vrXhqksNwkJbaSVtzoN3Uw2XYNbyiZujCw36irF8oWNPPhUHngudwGZQZ4taNQsNS+v2K5MAWPNml9NvRcbn3zcZorT6WFksVBnS5kJKAb0s9sA2/3DUUKB3HJzqL62kzP1wZgwZ4LZnSU9PAGJm0ZAWbHY3A/clpYNbM4Mhu1CUj6vflxea9x/SI5R/AC7YB1lY5TX90nx9/dDWJhtFffGm1Ps4k3UwZVMCImiKBW/0ttXC0fdzd2Fdko8hxXLN9jtK4mbsw39cRzdOGbNGgdAmbacIQt4PXUqR7LWyFfpcnFj9rs2E+An/95OncyRAnwdZYDIYTffBJXlJrK+kqooirrC5cqkfg7jwNh1Zhb22efMGY/Va2ajXbtIS69XIxe3zhZ+lFtujPqgMQF/3/226t/XXjcEb745BYDt+pL3BUpIaCNV/L0s8THDWitaMMtNZzOWG5VbimWvqcsXqElNTcctExZILkYtpP5TMtelUe+ydu0icM/ERVKPsIUL1kjPu+LmdsalIk+h1kMvoLjCxAJCnt22ThZjVVFR7TC+jjXcBFBv2eOdqg/GRFirVv649ZaFdufYlbIUnoDETSNgpmKxFhzPI8svFo88+Zld0DGbeveeD8TqE+H4OctaLxBbp2lOym5ilMn8qnrCB2i4qamzVxx1iJa7Psx0k1Y/76i7uVX06vqomyFynK3ezQsv3mlnYVJbbmJiIhzeONjqUG65ka9Ci4rKpRYI8vgOqxM8w1GAnzxLLCysNVqbMJmrkfdvkpOcfBEAYzGmVedGD6vFJqdNvxGAa9eMelVqJiBVawXLJojrrx+q2YQW0BfwrBJ4jx7xit9H584xkhjNzCwwrLWixcmTNnHTvn00wsKCpQD2AQO72/0O9dxSjvobAcAXX/yKW299RdNqLD9npaXmr7nY2CipWTD7LdXU1Ho0A9NR5WQ9nAkolsMsTHJXcXx8tKHVOyUlUVFD6Km5t+DEyRW48cZL7PbVQn7fkMfc/PjjHhw4YMuSfeWVL0wL6caExE0jYKZisREZaIt3/4rC6hPhKKiy3aDZfXpvfhDOlgWgtNa5r3LG9BVYMN/W+HLnzr/xw4+2+KDl72ywM4PKJyeWoioXN2ZWAUnD9Uu1q/dVY7a7uRXUN1mW1nrNyKfxzNPaXd3VFqZwWcwNoF/NVAs2KaSkJGLu07dI24cO/QeuvXYwAKVbymz1YzWOVqPhEQ3fY3h4sCQ2WCdjM2j1bwIaYjm8RZs24ZauGUEQ7dKbtValVooYMlg5BnWjUHZN3XzzZQ6rRrPrq7i4XDGZFhWVmeqGrSYnpxAVFVXgeR6nT78nbf/662fsJk11QLHcLWWGL7/4FeNuno+MDGWhU/k5s5KVk5mZL703s145cw7kOBau9uzcedThxC5ZbixWKJaTkpIoWX3laFm9mUhWx3clJLTBtOljTL2f/L5RU1MrxQeGhgZJ9/51a7ebFtKNCYmbRsBMxWIjBt8wGiI4BPmJdm0bUjqXoEd4lVQUULC4qj137jx++MEmaEJDW0tWgh9/3I2uXSYjLW0/AOCr9TsUr2M3NXmhNUfEx0ejUydzQk/L0uDO9EJBEHDmTK6it9ePP+6WzOa5uUW47/5RmgG7aguT2nJjxQpRVFQuK6+u7KbObkoXX9wQLC13S5mxYJjt8aK23JgJdlTDLDdxcVGK9HUzZnfANska3SDZjfXYsUxD9wZDEAQ7y6QZOA5YsngtAODPP88YrkrVVVyN9uV5HvdMurr+PbStlm++9aBpAZ+Zma9Ii1b3NjJLSkqilPEUobLEqidNKeYm3DlxAzg+Z/Jrzkw/taL6BUKsJG6cd0nJx6gnXFmV8MrKaqng5vHjjmvJOBNQLEduudZ6Tn5PMqrizPatra1zya3KrPdmrHbegMRNI+CoYrFuvydBQMn5fIRGR6FHeBVu6FiCVqpvjHUJ7xZahe+PApxGfI4W8ouXmXU7dWqHmBiblSCi2z/QdVB/bN/+F4CG7sh1dbZJhU3E8oqxjsjMzFf4f43o0cO+OqmzvnB7E7gtfmbR62sVZvd27SIa+rX0TDDtk2aCg1ku/Pz8cPZsnuGNg90QSorLHAbSDruywaeuDig2EgNWAvwi7Cw3rKmf+VVwbm4RqqtrwPM8HnjgWikW4PRpW8CqkRirrKzG1KlvQytQUf34xIlsXfcGg33H/1m63vT4GZ98kia5ag4cOO1wVaoXSKtm2LA+hiUS5O5KM+TmFismX9bKwgpMWOtloaknzYaAYuaWqp/gLIgbwPicqS03jtx+7LcUH+8eyw2DibC19aJr5Qeb0K3rZOm3ERQUgPz8EgDmBIpd+wWTAcUMK3EyZvb19/cDxznnVpVbbkjctGCMKhazx3Y39Pqb865vNoCDiOFxtgtI7c1p6BJejr/Oc/jqVKh9fI7q/q++eNmkHBwciO49bCLmismTMfX9Zej9zxQAQO/eHQA0dDyOirLVmLj8clvtnOpqfT+3XEhlZRZofl7leEU89/wddj7ktm3DTa/YWdbIsWOZUsl4BusX9dRcmxuIBcb1rS9yWFZWadoi1bFjWwQE2FwMmZkF0jl49pmPYZRRkLbFZhELDW3tcKUeHByIYcP6gOM4RczN8eNZyMzUX61bCfCTuxedtdyMHXupJMLeePMBbN4yH9k5H+G226+U9tFuTGoThl9+8SvGj5tvJ6iYVYBZFwoKynTdG4yzZ/Mwftz8+i7YxoG/2dmFuOOO1/DJJ7YU2dychjo2uTn6TTCt4u7CZrm5RSpxY81yY7S6V+/HJk1X3VJmkF9zn/9vq27jVnZd27ulXLfcMARBkGJLSkrKERjYSlFNmxWVNGOddKZCsRwr1YrN7rtk8TpLblVmuZFbZ0nctHD0KhYXZudg8/urUJSdo9heWlCIlTNn4+DmrUgIqUFYgGAnbBi2LuECenYIwbHSILz7VxR+zLBNVlV1QGWddmVhdvFWVdXgfIHtAvWrn2TL62yXRlWgcqW5f7/thx4cHIiTp97FM8/eDgAICPCvXwUYpwfKLQR6Asd2o1UGFqekJOJ/nz/pMI2brdhfful/AGyCaP/+k5rvxzJOWL8j9mPNzS0ybSXq1KkhnqSkpEJa2f7yyyGMHzffLnbDZjLmMOLq/rbPajIWJD4+2s53XlcnYHjSkwBs3+F9k/8DACgsLLUc4Cd3S4WGtZbF3JhbBetZANq0CUdKii2g8fDhM3Y30rw8m3hgE1JqarpUbG/evM+RPHx2ffxDA0X1bgG5e+OO21/D9Onv4I47XlN8djOBv1MfXIZPP0nDrj+OAgDaxURIFpTsbOdcPVqYvaaMer7Jt+flFuGcbOLPtihurMawxcdHywKK67Ol3CxuUlIS8Z83HpAe33LrlQBEPPP0Kl23H5tc3RVzoyY/3/bZotuESwHUjA71NazMCBS5W4rneSQk2ARIp87tTJWnsFKt2Oy+69f/ZtqtCjRYbljSQHV1jaW4vMaEivg1IuqKxcW5eTi+ay9EQcC3S/+LboP649aXnkZ0Qjw+nfM8jvz6my3DqrQQZnQoi8cRweFIURBGJpQh0A/Yn98K/aKrcbKkFea99Bnee+1DxY2e43lU+jVMbnUiUFUviAprlJfIX0fOoq6uDn5+fmjfXhk4qlWTglUpZT8WZiFI//UwLrtcPz1ZvlrcuvUQliy9HxznOP357Nk8zJi+HN999weWLL0fkZGhuPRSm3VJr/5Pp07tUFlZLa3IcnOLTVUGBYCXXr4LgO2GKggCiovLERERgvDwYPz880FJMKnPDRMqZou5ZWbm2wUTh4W1lsRibm4RfvhhDwBb1Vi9Kql6qN1SVlaVZi0AF1zQERPGL0BeXrFUMK22tg5bf3lFCrANDg7EP/5hsx7+Z+lXyMkpxA03DFUcRz6RMveGEUaVaOXXZk5OQ+sQ9r2YLfVvhq1bD+HcufN2vxs1tjIQot01w4p0MnJzixXik02aZgM7rVqS5G5le8uN66t3JpDVl1BCQhs89/wdGD9uvuZ3za4HFtjvTssNYGsBAtiSKNRxcZ062a4nM4sA9luKiYnAiZMrpGvxmWduw6RJVxtWEwccVytWF9Azu6+Z3xCDWW5Y1WhftdoAZLlpdFjF4t3f/YhjO3c3uKXqt5cV2W6wbEUvCgK+/2C1qWOX1TbcFWoEDiX17qnu4bYf+7GSAGz+aa/dza/boP6oQEPp9opaHqzgVlktjxrZ7jYTcX3GlkYNF1EUpcDPJ2d9YLcKYOImw0G/E8aNN16COXPGo2PHtg6FjbxLdmVltRTrIQ9u1SIiIkRR/Cw3t8hwxa+1qg4MbIWUlESpsWBkZAjuuWeEtL+esLr00l44c0Y/Poe9ftu2P+3SwMPCWst6D5VI/v/AwFaSODGLXDiFKSw3jsWNFQvA64smY+vWQ1KsBcsUad3aNjHfdVcyeJ5Hfn6JFAysboTpzA3VTOBvQ+p9hBR7luNGt5QgCJj1xAem9tWqWXX27HlpogWAuPgozJjZkPUy+b5RpgthAuYtAYLQUMPFPhXcPZYbMwGweiUi1NeDxyw30WF27mrmlrJiuWnfvo1+cU+D785K+QFnSxU4osFyQ+KGsIhY78bocfEQdB8yEBzPY+WST1BSzesGCosikJlTjIPHChQxPQVVtkk9uN6iczKjCMd32VdWDW/XFiXVDZdCea385sKhSPbcwIHdDEvg8zyv6DWj/vGwbIwsg1gROdNnjMHzL5jrAZSTXaR4P2UfI2PkHYiZ0NHLmDBqnMdiEj797Am8+tq9hvsDNrfN8ne+h1HaKcdxiIoKkVbLTHCEhraWWknk55eirKxSEpYsJkpeKM7f31+zcFxoaGv4+cmbc1qLuTFrAZD332GwFW94eDBOnFyBt//7EADbRMImanktIMD5idRR4K+8L1qDW6rQqffS4/PPf7H8muzsQkmMycXWpElX2022ZiZJhpnGqUzIs4nQqEKxKzhTVI7BspcY7hc3tkWDluXGqOChmqqqhnGZrQqsxkr5AWdKFTiCWW7a11tu3Blr5W7ILeVD9BuRhPa9egAAkifdgeRJd6AwKxvrFi7BlqwQ3NCxpL7NQsNrmOD5JS8S6V98gVEPToYoCOB4HvlVPDrJ7n0fLfw/zYyt4tw8ycoDABV1Df/vEV6FiICG1/xr8jWmP49WDydmufnjj6OW+s2YQb0SPX4sE1dd1d/Ua+XVkuVBpKz3TE7uKrsbmxw2cbPgZHkFWUccPZqp6zaxiZpg3D3xKtTVt6s4dy5f6kHFVo7sJlNQUIqYmEiMG3c5Zj46VnE81quKwfpc7djxl2I8YRZjbqxmscnFEJuIQkODNOtxrF4zGw9NfVux3VOrRbnlhn2P7hQ3rNeYVU6dypHcBlrFJeUwt9TiJfdh3brfHGZ5GfUtAoDzecV44IG3pImQiffWrQMRENBKsnB16xbnUq0TK8GyatQTbFWVuYxMszDLTRuNmBuGGXHDEhbM1PgychNp9cPS68tnZV8zqGNufNlyQ+LGR+g3IgkTF823a78SEdMOd78+D0eLOXx9BhgeV4YwmdgoqeGRlhWCs0IgRj90H8oKCsH58bioQyB6Ryonpo8WT8B0ochOsR/ftRfZeaVAfMPNjYOI7uHVuKFjidOfSWuCl6/02I3VCDMtBgRBxNmzeYqaDCkpiRg3fpjpscrfJzw8WBG7IAgC8vNLDcUNg/ngrfQQy8zMR1raAbubUJu24fj008cBAK++eq+0f21tnRT3xErmF9SvLvPzbeJm0eLJdpY+tcWNiYeZM5QBu9HRYVIGmJmbttn4JPnnZTARYXPDKPdj38FTcycotntqtcgsdq1a+UvWRyZ4XEWvKaiWyxKwXXMFBWVo0yZMUatHXiXW1UkS0I9Hyssrxn+Wrse8easVE6FcXB07vlxKbV++4hE88+ytDuNG9LASLKvG824p228rKipUN5XfzO9EHtdmhBmhZyVOxsq+jigrtX3/5JYiTGHUe4rjeUnwHC0OlCoVf3smDKtPhOO9v6JwtLghviI4Ihz9EgJwQ8cSBPLK2U3PZD12zCW4qkPDTatzaA3+1TMfV7e3TSIW5mkIgiC5EbTEjbw2QmpqOpYuWWf+4DpwnLL0OZtI5BlAgPmy/fc/MNoudsHsJMcaGJrl3LmGAEC52yQ6Ogyff/6kZrxQz54J0g28Y72YYqtL+cSvVzen4XmbKXz2nPEAGorkyUWcGcuN3L9vdI7lsRuMQYO6Gx6b53m7dg6euqFWV9cqXGAVFVV2lhJnMGobwoKH5bCYiG+++R0AFOLGynjMWkO04pHiYu/CSy/9z26FX1tbJwlS9fGtuMTUONurC2g8cQMAXbvGSuORY0bcsBhARzhTz6uxYOKaCbUiEjeEEY56T8knJREczpYF4EhRIM6WBUCEesLikByvXRNHy6/LhECUSgiEthLR2l+0LGwADqtXbwMAtDFwS7Eb0vr1v5l/Ax2efeZjabVoFJhoxZqivlH/edjWz8UoRZeZ7K3w5KwP7G6UZrKPAgNtbiNmKVJbM8x+VlvhOFu8kVbXYbMTBbMAyANe1XAc8OjMFYrPGxVtrp6QfBye9PPLA8vdFUzsKJ5EK3B4/Lj5Umr3eYXlxry4sTJJmi1EyIq/2f7vXNyI3vs7GwBbW1unOC9Vbs6WqqtrWLB1ry8uygqfMswE3m/caB/vKMdsNXFvoi44qI538iVI3PgArvaekmOriaMvSuQma+MVpfX3ZjflzT/tA6BnuVGW7LYS1KiG3QzmzWvIJnNX/yn1jVoee6E3HnX1YDN88snPdtvMTIZsguncmVlubKtLVkHaGdQ3bCsF/ACbwImLvRvPPL1KU+RwHIdFiycrVvZnz2gX4VMjv6l60hQuFzTuirexknJdWFiK2bNXIj+/ROoGzyw3KSmJGDv2UofH8OQkOWxYH4cJBWa60GvhSgCs/Jpwt+UGaLCMsli3v/9WJiuY+a3U1dWhutomvBzVA/NV5G5RgNxShANc7T0lR917So/4+Gi3CYGtWw8qUmvZzVgdUBwUFCC5bdjq22jFJkfPdK++GbizCqz8Rs0mul9//VPTNVBdXeuw7YUgCIoKyyUl5Zqf2cpnYGKR3Xxzsp23NuTlFStWoFbFDWD7jC+99D888MBbmsXo1BYxW3CjkSvLNlGzTCag8Sw37hI3Vor3RUaG4uOPH8PmLfPrC9jZxqTXBNHs78JduBL4awYrvbrkKMWNey03QMPigS0mjjohboAGcVCisvK6ksHUmNhbbkjcEAY46j1lhbJac0IlMzPfbUJg4497FKZstmpv00YpbphLqq5OaUZOTU3HLRMW6E5yWi4WvZuBldodZomPj5YmupqaWqxbtx2AcmIJDGzlUCRyHIdt2xpW00VF2m4sZ3zubMI/ePC03diMsPXmKZXGIxduzlYe5Xkei+sDmh3VLBEEQboWjCZq5hYoLa0w1YLDWeSZcrluCiY2E0+i9X2xopI9/hFv2tXq6UnSlcBfs5h1kcmRC17PWG5s4oaVTHDGciPfb/XqrQCAHTv+slxN3JuoLTe+nApO4sYHEAUBu779wTlfkArWHVxvbpObrJ0VAurHrH0Bg1lu1G4po0ZreXnFhsX22E1880/7DG8GZiYSdU8eR8i7L8fGRmLgQOMgWD2WLF6Hb77+XXosiqKi3gzD0WcQRdHuZspuvnJ3kKMGlKxVxaZNtliA4qIyhbhxxnIDWK9ZwoSLelKST9QsnqmqqkbznLkLT1huHMWTsGtbT7hMnjzalIVVXsTSU2zdeshQ9HorbsTTbqnz55VZo66Km06dbYHJu3cdcymFvrFp9pabDh06ICEhQXo8dOhQLF68GPfdZ72GA2FLA0++5077DpdOIILDliybiLBvmGmbzJjJ2lkhIJ8AAOCvv5Q/dHYjCAoKUJjRjcSNWSvSiRNZhjcDM4GJUx54C127/AsjrpqD8+eLda048hu1XNywJqJWApQBW/B0SGhDXE7Hju2wect8u8wsM646dW0adbbUnj0n7DKd1J+zrKwSS5esg1+9UCgqKlcERVvpCC7HquuCuRGOH88GAKxZ/YtCwKakJOLy+lYdbdqEa54zdyF3f4XVlwRwB3rxJLm5xZrViOVEmwy6Vhex9ASCIOCP+h5cWs95K27E026pgnylheLEiWxFfJtVcdOli62Eg/pe6us0+5ibTz75BMnJyQCA2NhY/Pjjj7j44ovx8ssv4+mnn3brAJs7ijRwN91IjxYH4uszYXbdwbNyihQma+MWA4BcCMh94NeMnKvYV72KKSur1Cxmpw4mlmPWisQ6mBthJjBREARs3rwf99/3JgCtbtXKGzUTN9HRYQ7bOahhIqlN23A8/fStds9rpdDqfQYAeOM/X+HwoTOKbfmyOjeALYNkz57j0vNpaQcw8e7FiteEhbXGjJljMbb+fTt2ausWy41V1wWzBLA02x9+2C0JWBZrwooKMlxJO9YjJSVR6hQPAA8/fINbRZRWPMnMmSvccmygcVKIU1ISMWBAN83nvBk3UthIbinG+fMlCmuO2YUA24/Vp3Jne4/GoNlbbvr27YsdO3YAACZMmIADBw7g8ssvxx133IF77rnHneNr9jhKA3cWeU2cb06H4L2ddejccZLdjUdvEi2p4RVCQO4Dl/tZz58v1pwEG1xTtrgbnueRmNhbet4ZVwwAbNt22NTnNxuYaDZDo6CgVKoDYwUmkh6duQKLF0/W3EcvhZZ9hmPHMhX7b968HyUlDRaWuro6SZSw7yYqKkRRHyY4OBCdOtke68XjTJw4AsHBDZY2Zy03VmuWsPdhLR/O1fcdc6XfkFWYiFJbSdwtotS/pXMm3aM5OYVO1YBxJ3pBzbZeRiIenbnCa3Ej8kmWLazciVrcFBSUOiVu2L2SJVa4q0hkY2FvuWlmMTetWrVCVZXtS7r66quxfv16AMCff/6J+Ph4942uBeDONHA1IjicKW2FI4VBWPrkYtTVak/OciEgLw6odaNKSUnEr+mvSY/btAnXXN2y2I+2bcORkpKIEydX4OlnbFaLwYN7WHLFyB/n5ZmvmGw2MNGMEBJF0alVFhNJeXnFTvXOEQQB+/adVGwrKVEG/hYUlEmChd2E27WLQPv2De6hTp3aSpYRPUQRuOCCDtJjZy03VmuWqN0ImfV9x1zpN2SFxhRRasyI+tOnczF16ttwdxNEK5g5R68vmuyxWChHyCdZT7il5EKmvLwKVVU10gKusrLa9LlX/6bIcuM5nLoSDx48iClTpuCKK67AyJEj8f333wMA2rdvj/PnzQdqEu5NA9eC4ziUFzn+ATEhoFccEGhYubVv30axXWt1y374o0cPwuo1s011wTWyojCriVGROFcwI4TM1LphLqw77nhNIZJcSaE9cTxL8bikpEIhbuSrSma5iYgIUbjPYmOjcPElvQAYle3nFKtyZy03gLWaJeoA1XPnbK/xdNoxo7FElBZm4qtmTF+OL7/41e1NEK3gzXNkhsbKlpL/nwme2to600HuTV3cyC03lZXVHrGSuQunxM2sWbPwwAMPYMuWLfj000+xb5+taNuNN94ouasIc0hp4G4IJtYjOCIcExfNR78RSU4fw6jgn9bqlv3w75l0tenXAA1WlFtvWQjAVj+m74UPSb2O8vO9ZwZVZ8/oraKnPrgMn36SphBJrqTQHjtmL27kNxn5jV2dmnn2bJ6Uan3BBR1NjYHhrOWGYdY1KBdRtbV1UlBvY6QdA40novQwiq/688+z0vlytgaMO/D2OXJEYxXxA2y/sZSURFx99QAAQGhoa9NB7vbiptDdQ/UocsuNL6eBA06Km7S0NLRt2xZt27bFv/71L2n7O++8gylTprhtcC0BURCwfc06y5k3VuDqhcSYWdOdju2xunLLP9/QbM4ZV8y6dba2DAEB/lJmQU1NraXy8+5GLm7WpqZbWkW70jtHHXNTXFyustw03GSqq2sVN6AzZ/KknjYBAdb65FY6WedGjhmLmHwyysoqkPZx5ZxZobFElBFMuDzz9CrFdnWZBWdqwLgDXzhHRsiz/Hr2THC7e0xuueF5vj72KFCxj5n4rEqZkGcNeZsSlZXVUpaYL7ukACfFTVBQEAIDA1FYWAgA6NSpE6ZNm4ZevXohN9dcczCigbzTZz3+HhzPIyo+Dt0G9Xfq9VZXbvJmf84cu6qqRvKj9+ljszioa000Nrk5BdL/T53KRfdu95teRbvSO0fLcqPnlgKUK6qzZ/Nw6lTDb1IURUOxIH+tq5Ybs8hv+Czeho3H2XNmhcYSUY4QBAFr1mxTbDtv4XfkSXzlHGmRkpKIVR8/Jj1+7fV/ub1UgHwi71HfX8qZ+Cz5byovr7jJ1LeRU1Zm+wzNUtysW7cOd999NwAgIiICv/32Gx599FGsXbvWkuXmySefxI4dO1BcXIzs7GykpqaiZ8+eDl83btw4HD58GBUVFdi3bx+uvfZaZz6Gz+DpuBs5zgYwW125WREiesdmlpI+fToBsJ/EG5OUlETcd/9o6fH0GWNw7Pg7iI4OM72KdrZ3zunTuYqaGoMH91D2WlKZh+XnKePseZyRdSO2uXz04js4fPZZQ68rV2JurCB/HxZvw3Cl35BZGktEmeHo0UxFVp687o438aVzJIfFAcbERCq2uzPLLSUlEb/tWCQ9NqpG7ij2SC5umlq8DWD7fKw/Fs9zXgsgN4NTIxs0aBC2brWVjx43bhyys7PRuXNn3H333fj3v/9t+jhJSUl46623cOmll2LkyJFo1aoVfvjhBwQHB+u+JjExEZ9++ineffddDBw4EGvXrsXatWtx4YUXOvNRfAJ3tl9whLNCyurKjU2wRrFEjlZ7WVmFAIALvGy5YTfQiIgQxXZnbqDOxE3ccMNQRUHGnzbPw/8+nyU9Vpu2CwoaVlRnz+YpGmJmZuZjwoQFmvEdPM9h/PgrpMeNZrmRuaUyz9kL3caINWkMEWWG2to6Rd2o8Aj3FRJ0FV85RwyrcYDOwH77CQltHO8sQ8/SrRQ3hU6PyxuwrNe2bSMAAJdc0stjxTTdgTUnfD3BwcEoKbFNNNdccw2+/PJLiKKI7du3o3PnzqaPo7a43HPPPcjNzcXgwYMl8aRm2rRp+P777/Haa7Z05GeeeQYjR47Eww8/jAcffNBu/4CAAAQGNvhGw8LsO1V7G1EQsHbBYkxcNN+j71GYnYPju/Y69Xq2clu9ZjYEQVDcMNQrt5SURCz9z/0A9DNz1NWStWCWGxYI6w3LjaMUWEEQsHjJfVi37jfTq1YWN2EGdnNVn8aYmAjp/1HRodJYAOV5at8+GhPvuVp63L9/VyxePBmfffYzHnvsJrvPFR3d8PvwjuVG24pn5Zw5S2pqOtat+w3DhvVBfHw0MjPz6xt7Np41IiUlEV26NKTsP/jgdbjhhqGYPm25T/Qe8oVzxGBxgHrIrSjOXDtG4skRetbopmq5YfchNWyB54tNP52StEePHsXYsWPRoUMHjBo1Cj/88AMAICYmBsXFzptRIyJsN+z8fH0XSGJiIjZu3KjYtmHDBiQmaqvH2bNno7i4WPrLyMjQ3M/b7N+Uhg8fnQNB5n5wKxyHdQuXuGQdMrNyYz+C8HB96xtgm6wmTFhg+IPIzrLFX/Ts2R6AdzKlvJkC67i2iA11Jd0iWc2PGTPHaham0xI2tuM2PPaGuJHH3HgDbwXsAg0TSGNUY3YFb54jOZ7O4HL029fCkTVaXvYgt4mIm8awkHkCp0bzwgsv4LXXXsPJkyexY8cObN9u65J8zTXXYPfu3U4NhOM4LFmyBL/88gsOHjyou19cXByys7MV27KzsxEXF6e5//z58xEeHi79yXti+Rr7Nm7Blg8/80ha+M8ffYb9m9JcPo6Ri8BoMlbj7+/nMFiSWW6kNHAvuKW8mQJr5ebKJsAFC+7BzeMul7Zr9S3ied5hPyMAUkVjTyOvldGmTZjP3SQbA28WEmyqeDqDy+pv2kzskdxy01T6Svl6jSM9nHJLffHFF+jUqRPi4+Oxd2+Dm2PTpk1ITU11aiBvvfUW+vbtiyuuuMLxzhaorq5GdXXjrEBdpd+IJCRPvN0jxz64WdvN5wx6LgJHZmI1jm4eWVnKVbw33FLeTIG1cnNlbqnHn7jJbe//j3+0V7i7PEFKSiLmPDVBerxg4T146OHrfcYN01h42sXSHGFxgAkJbTRFnyAIOHv2vNMZXFZ/02fPnseM6cbXbVN0S/l6jSM9nF4GZGdnY8+ePWjfvr1kDfn9999x5MgRy8d64403cMMNNyA5Odmh2ygrKwuxscoy8rGxscjKytJ5RdNA0UDTjTVvRFGEUFeHE3v2u+2Yeli9uB3dPNRF87wRUOzNFFirN1ezFhmzjBt/hUcDBpkbhnWLZ/iaG6YxaKoTiDfxdAaXmd++VjVyI+TB8+3ahTcJS5yv1zjSw6kzy3Ecnn76aRQWFuLUqVM4deoUCgoKMHfuXMs31jfeeAMpKSm46qqrcPLkSYf7p6enY8SIEYptI0eORHp6017leaqBJsdx4P380HVAP+P9eB7dhwx06b3MXtyCIJoSBGpx4w3LjTdTYB3dXBsDTwkNcsMoaaoTiLfxZAaXmd++VjVyPVJSEvHByhnS4xdfusuns40YvlzjyAin7hwvv/wyHn74YTz55JMYOHAgBg4ciDlz5uCRRx7Biy++aPo4b731Fu68807cfvvtKCkpQWxsLGJjYxEU1NDfZuXKlZg3b570eOnSpRg9ejRmzpyJXr164dlnn8WQIUPw5ptvOvNRfAZPNtCUH5+JmIHXjkT3IQPB8Tz6jUjC3A1fYur7yxSvsdquwcxkzOKJzAgCX3BLAd5LgTXTd8iVYxsV9GN4Smg0VT++p2iqE4gv4MlSAe767TMrJUujZjQFK6Wv1jhyBAfAcvRqRkYGpkyZgq+++kqx/cYbb8SyZcvQoUMHnVcq0Qucveeee7By5UoAwObNm3Hy5ElMmjRJen7cuHF46aWX0KVLF/z999944okn8N1335l6z7CwMBQXFyM8PFxKZ/cFug8ZaCcu3MmySVMRHBGOsU/OQGRcg1uvrKAQwZG2H5x6BS0KIlbOnG0pELkhZVDUnAzzcovwwANvmbopBAT4o7KqIYZr4IB/Y+/eE6bH4m54nvdKCmxKSiKWLL3PUjyTEeyG9NqrX+K22680fdzk4bPdFu9x661X4pNPH3e43+23vaooLNic0fvtsO/LF9NtWwqu/PZ5nseJkyuQkNBWU8yz2KBuXSf7nECQo3UfOn0612GckTuxMn87JW4qKipw0UUX4e+//1Zs79mzJ/bs2WNYhM/b+Kq44Xgeczd8iYiYdm51TYmiiMKsbKx79T+Y+NrLAETF8ZnA1HIniqKIssJCPDf8Bksp5Fo/gry8Yvxn6XrMm7fa0g847/wnUu2Vzp3uxZkzLbO9B7u53njjJZg+YwxE0X4C5DgOoui4Jof8hsTzPJ599jY8/cytDsfgTqGRlNQXm7c4ruvkTkHVFPCFCYRwL83pWvfWAo9hZf52Kltq7969ePjhhzFt2jTF9ocffljqEE5YQxQErFu4BHcvmgdRdG9Q8e7vN2LsE9OgFjaAcco2x3EIjYpC98EDcPT3Xabfz52FvrKzCyVx06dPR2RknPfp1Y2nYBlqaWkH8Msvh+wmwLNnz+OzT3/GY4/fpFtkcemSdVi//jfFdyEIAn76aa8pcePOeA9PZ7o0VXypSB7hHppTsHhjFNN0F06JmyeeeALffPMNrr76aimQNzExER07dsR1113n1gG2JMoKizzSHTx54h3gLFbYlNPj4sGWxA3gnh9BSkoiunZtcKF99/3zOHMmt8WlCasxmgB/++2IpvAxWvl7Q2hYqXjd0mhKEwjhGAoW9w5O+T9+/vln9OzZE6mpqYiMjERkZCS+/PJLXHjhhbjrrrvcPcYWg5WgYlEUTRX74zjOJWEDAO26mm+p4S5Y/EFgYCvF9qYQgNcY6FWJdSa40lsBg77Wq4ggPAEFi3sHp2Ju9Ljooouwa9cu+Ps7ZRBqFHw15gYwH1TsbreVmfdbOcNaYLErNJcAvKaGt+I9vO3HJwhPQ8Hi7sHjMTeEZwiJjDAM8GWIdXXgGlNAiiLGzJqOA5u3NkrncqrW6h28Fe9BbhiiucOslFZdxoTzkLjxETiex5hZ0wHRPugXUFprCrNzEJ3QvlHHFhUfh26D+uPYTud6h1mhOQXgNTVIaBCEZ6Bg8caFxI2PwCoU6yG35NRUVUv1aRrTPeXpQoMMCsAjCKI5QouHxsOSuPniiy8Mn4+MjHRlLC0aK8IhKDQEwRERjnc0iSgIpmrrhEZHgeN5j7umKE2YIAiCcAVL2VJFRUWGf6dOncKHH37oqbE2a4pz80zvGxodBWcbbKozrEQdN5gWY5+cgbkbvrTclsEqTbXcN0EQBOEbuDVbqingq9lSjioUm7WuWMVq5pVYLy5WzpyNA5u3otug/ghv1xbFuXk4vmuvW606VK2VIAiCYHi8/UJTxlfFDWBrVDlx0XzYtUioFxSu1qtR42xKuSgIKCsqQm1VtSJOqDArG2sXLHZryjilCRMEQRAAiRtDfFncADaBo25uWZCZhXULl+CeJQu8ODJ71OJIbtVprJo4BEEQRMuA6tw0YfZvSnPo7hHq6uorD1t3U7mzAKD6OCzYuDFr4hAEQRCEGhI3PogoCIb1ZKrKyxEUEmo5DkcUBMDDqeONXROHIAiCINS4P0KV8AgKESMCKx97CmWFhZaOUZidg1P7Drp3YDo0Vk0cgiAIglBD4qYJ0G9EEuZu+FJ63Do8DGOfmIYvX3oNpfkFphpoAkBA69aIiovx1DAVWEltJwiCIAh3QuLGx2EZVBExyl5LETHtcNdrL2PH2q8BEXbxLVqCJyQyAhGxnhU3oiCgIDMLx3ft9ej7EARBEIQeJG58GI7nMfbJGVCnhrPnABEDrx2JlY89haKcXK+MUQ7Lllr/ylJ0G9QfA68die5DBnqkPg9BEARB6EEBxT6Mw35T9cG75QWFeGnUTeg2qD/+cckQjJxyb6P2nGIUZudg93c/Ysys6R6vf0MQBEEQetCS2ocxG5Qb3q6tlGGVffykZwelQ3VFBda9+h8k33Onpgtt4qL5uPHxf5MlhyAIgvA4NMv4MGaDcuX7eSuQt66mFmOfmAY9FxrHc0i6+zZMfX9Zo/SnIgiCIFouJG58mOO79qIwK1u3GJ5W8K6j13gKURQRGRdryirDLDkkcAiCIAhPQOLGhxEFAWsXLAbA2WdD1Qfvrlu4RPGc/DWeGI/eYyuuJhYMPWbWdHJREQRBEG6HZhYfZ/+mNKycOdsuG6owO0e3h9P+TWn4edVnbh9LaUGh3RgYglBn6VjySsYEQRAE4U4oW6oJYKbflJqDm7ci6e7b3DsQWQbWt0vfxk/vrcJre7cBAOqqa1CYlY2ImHaWrDFUyZggCIJwN2S5aSKwbKjd3/2IYzt3O4yp8UQRvdCoSOn/eWcylO4wUWxwoZmsmAxQJWOCIAjC/ZC4IUwjr52T0LunnYWGudCqysocHksURRRkZlMlY4IgCMLtkFuqGdJvRFJ9ZWPPMWLy3Rh8wyjpsdxaw/F+Dl/PcRx++2Jdo2d1EQRBEM0fEjfNDNaLCjDvGnIWRbE+UWx4b5OJWnmnz3pmYARBEESLhtxSzQijXlSeej+GKIoN722y9QPF2xAEQRCegCw3zQhHvag8SWBwMIIjwk3tKwoCCrNzKN6GIAiC8AhkuWlGeDOt2j8wwNR+oigCnH3xQYIgCIJwF2S5aUZ4080TEBRkaj+O4yAIAmCxorGVGj8EQRBEy4bETTOC9ZWyWkivseF5HhNffxkrZ2hXWJbDMr/k7rbCrGysXbDY4WsJgiCIlonvzoCEZTzZV8oTjJk1Q7ODePchAzHw2pEY+cAkTFw0X5mVBWq8SRAEQRjDoTFyhn2IsLAwFBcXIzw8HCUlJd4ejkfoPzIZd73+sqmsJVE0n93kCZZNmopjO3cD0LbS6I2PBSW/PPpmclERBEG0AKzM32S5aYbs/XEzNn/wCURRNNUKQb2PlfYJrsKCoFmNHLWVRk94UeNNgiAIQg8SN82Ubxa9ic3vrzJl1VDv05iWnOLcPJfq81DjTYIgCEINiZtmzDeLl+H/HphuuA/HceD9/LB2wWL88snqxhkYlL2lWH0eZ4KgqRAgQRAEoYbETTMnLDrK1H6l+QXY9+NmD4/GBnN7rVu4GKIgOGV9sYmjLCoESBAEQdhB4qaZY9aywerHFGZlezxAt6ywSJEGbtX6IomjV/9DwcQEQRCEHSRumjnHd+1FWUGhbpCw3ALSGKnkf/6yHc8Nv15Ro8aqqOI4DhzHobyg0EOjJAiCIJoyJG6aOX2ThyE4IsJwH3krhP2b0vDho3Mg1NV5ZDy9Lr8EfZOHAWioaTNg1AhsX7MOVkUVBRMTBEEQWnhV3AwbNgzr169HRkYGRFHEmDFjDPdPSkqS0pvlf7Gx3mkW6esospB0MqCEujoc2LxVsW3fxi348PGnPZYSfsuLczHywX9h7oYvMfX9ZbjzlRcw+uH7UVlWZuk4FExMEARBaOFVcRMSEoK9e/fioYcesvS6nj17Ii4uTvrLycnx0AibNmaykPz8/TVrxez/cTM2v7/K7WPiOA6tw0IxeupkRMTGKJ4LCgk2V5dHECiYmCAIgtDFq72lvv/+e3z//feWX5eTk4OioiJT+wYEBCAwMFB6HBYWZvn9mipm3TZa+/UbkYTke+5095AUqK1JHM87FDe25zn89sV6D46MIAiCaMo0yZibPXv24Ny5c/jhhx9w2WWXGe47e/ZsFBcXS38ZGRmNNErvYyVTSo7cndXYOCogyHEcOJ7D6Ifvx9wNX1J/KYIgCMKOJiVuMjMz8cADD+Dmm2/GzTffjDNnzmDLli0YOHCg7mvmz5+P8PBw6S8hIaERR+xdzGQh1dXW4cSe/YptrhTVa0zMNtCUN+PsPmSgz38ugiAIwjW86payyl9//YW//vpLepyeno7u3btjxowZuPvuuzVfU11djerq6sYaok/BUrsnLpoPURA0J3U/fz889d0arF2wWErPbipZSBzPQxQEjJk1HQc2b9UUcVrNOAuzshWflyAIgmheNPkl7I4dO9CjRw9vD8Nn2b8pDStnzkZRTq7uPmoLSFPKQjJqoKnXjNOsxYcgCIJomjR5cTNgwABkZmZ6exg+zf5NaXj52nEozS/QDNi1WXREjJk1HRzPN7izGrE7OOBaN3K1tcmoGaf68xIEQRDNC6+6pUJCQhRWl65du6J///7Iz8/HmTNnMG/ePCQkJGDixIkAgGnTpuHEiRM4ePAggoKCMHnyZFx11VW45pprvPURmgxdB/RDqEGfKbkF5NjO3di+Zh1GP3x/I47QtW7kJXnn0X3IQIS3ayt1Gpe7ouzeS/V5vQ3H8+g2qL80flYxmiAIgrCOV8XNkCFDsGXLFunx4sWLAQAffPABJk2ahPj4eHTq1El6PiAgAK+//joSEhJQXl6Offv24eqrr1Ycg9DGalp43umznhwOAJulxhVBw6gsLcNt855RiJmyQnOlAnwhvojiggiCINyLV8VNWlqa4eQ2adIkxeNXX30Vr776qqeH1Syxmhbu6bgbURRtmeZuaGMVGBKMwODWim3B4ebqGXk7vojFBanT7llc0MqZs0ngEARBWIQCDloIjtLC1VV/G6NDOMe7pmxEUYQgCICoHVfD2nNovtYHqhxTXBBBEIRnoLtmC0He8VstWGyPOUUDTcX+bggsNhIa7HmrxwMAnud1J3/WPVwUlMfW+rxA49fDcVRPyCgTjCAIgtCnSdW5IVyDpYXbxXdk52DdwiV27o8Dm7eivKgIwZHGXcXNUJiVA1GoQ3RCewAarRcsxt5Y2b+8uBghss+g9Xm9EffiSnsMgiAIQh8SNy2M/ZvScGDzVlOZOd0G9UdIVKTL7/n9m+9g4/KVeDz1Y5eP5QwfzpyDB997CwBw7I89ePvehxSf11txL862xyAIgiCMIXHTAhEFwVT6s7ssBj/+3/sAXEv11qOytBSBwcGarh1REFBaUIgw2eeoKCqyc0UZxb04qoDsCiyuKSKmne74C7NzqPs5QRCERSjmhtDF3RaD6opKtx4PALas/BS6cUQch7A20bhz4fPS9rge3RXxNN6MezGKa9KLCyIIgiAcQ+KG0MVdGVPdhwwE7++PutoaN43MFlBcWlCIje98oN1eQsdK1LZTB0x9f5nUUdzbcS8sDqqqrEyxvTA7h9LACYIgnITEDaGLUYaVFaa+vwwLd25B54v6um1sHMch88jfAGwC4aVRN0nPVVVU2NLDDdxgLJ6mbacOpt7Pk3Ev+zelIe3Dz6THyyZNxcujbyZhQxAE4SQkbghDzDTeBByncnsirfoflw7F81u+Qb8RSYqaOYGtWzt8P47nAQ644o7xlur/eAr5+Tu2cze5ogiCIFyAxA3hkP2b0rD2laWG+zgKFvZEMDEABEdGYOKi+RhwzQjLr+U4DqFRUTixay/M1v/xFI3dpJQgCKI5Q+KGcEi/EUmY+NrL3h6GJkw0XT/zIaePccGVl2PlY0/ZWaccxb24s+gfWWoIgiDcB6WCE4ZIqdKeMby4BY7nEBkbIz2uLC1DYLBj1xQjKDQEcd264KVRN+G1vdsAAGVFRXh59M26osPtRf/IcEMQBOE2yHJDGCKlSnvIreQJzh05CnDW2kYMu2OC4rFQW2cobCYumo+ImHaK7SxIud+IJMtjJrcUQRCE+yBxQxjSFEv/dxvcX+orZZaQqEhTtWw81exSFMktRRAE4S5I3BCGuCsF2lXLhKPXu8PyIRdyesLIY0X/yHJDEAThNkjcEIZIhfxcnHxddWs1RjaWXMjpHc9TRf9I2xAEQbgPEjeEIVIhP1HbOtIcYkU0a9noiJvGaHbpiZpABEEQLQm6ixIOYYX8yguL7J4rKyxCWUGhYRG8kvP5nh6iS3A8j4CgIPRNHtawTUfchERGQBRFXVEniqJTRf/k548ncUMQBOESlApOmGL/pjQc2LwV3QcPQI+LB0MEcOz3XTi2czf6Jg/DxEXzIQqCwurAiuB9+dKruOXFuQgKDfHa+B3BigEytMQNx/MYM2u6rbWDVhfvesGz7tX/WK5bI8pywTk/P6C21tLrCYIgiAZI3BCmEQUBR3/fhaO/71JsZ5Ydu7ov2TlYt3AJDmzeigkv+Lb7iuM4hcAAx4HjeXQb1B/h7dqiODcPHM8rPp/WMQCgvKDQ+gBkb+3n5weSNgRBEM5D4oZwC8yyIxcDx3fthSgI6D5kIFqHhXp7iA6RW2v8/P0xd8OXCjFTWVpq6jjOpM/L3VycH7mlCIIgXIHEDeE2REHAsZ277bY3xVo5rYICEREUo9gWFGpOoDkTTEwxNwRBEO6DxA3hcdxVK6exsZpeLgoCCrNzXO4gzvv5ufR6giCIlg6JG8LjsFo5EbExTaaNg2VhI4ow20FcHctzfNdexfuRuCEIgnANEjeEx2G1ciYunu94ZxU1VVX4c2s6+l093P0DUyGKotPiq6ayCp/Mfs5h00y9hpt/pf8uPSa3FEEQhGvQXZRoFPZvSsOBn362/Lq62lqsfPQpW5Vki+nVVnC1GOGfv6SbEjZ6DTeHjr1eekwBxQRBEK5Bd1Gi0TiU9oup/RSZQxzXUCUZ1jp9W0EUBI+4zDieR/chAzHoumsw7plZ0G+42QDPk1uKIAjCFcgtRTQaQp05y4sy/sS/YfL3YLiOy3EuGsJIywWl/3LZZ/YncUMQBOEKJG6IxsMJq0urwADM3fAlWgUGAnBPg0xPoB4Xc0EpqvOZhGJuCIIgXIPuokSj0G9EEm58YppTr42IaYeQqEifFTYA0OmiC9F9yEBwPA+O5zH2yRnQckGZgaNsKYIgCJcgyw3hcSQrhpPapCl0yY5o1xZT31+GwqxsbF+zzpQrSo48U4ssNwRBEK5Bd1HCoyisGD5seXEXEbExGDX1PkuvUQdJU50bgiAI1yBxQ3iUboP6IzIutklYX9wBx3GWLVQcxymE3z8uHeLmUREEQbQsyC1FeBR395VypdBeY+Hq+K6fPhV5p844rJujeE+NqseerAtEEAThy5C4ITyKu/pKiYKAsqIicOAQEhXplmP6LCIwZtZ0HNi8FQAMRQvH87j6vokYductCImMkLYXZmVj7YLFlgQSQRBEc4GDM7mqTZiwsDAUFxcjPDwcJSUl3h5Os4fjeczd8CUiYtppuqZYs8nP5r6EPkmX48q7bgVEZZaRbTLnsHLmbFSUlOLBd99sxE/gPXZ/+wO61rv1GGWFRdi66n/YuHwl+iYPw/hnn9QUe/JzRgKHIIjmgJX5m8QN4XHkNV/UAkcURMUErFX4riAzC+sWLsH+TWkYeO1I3PnKC405fK/BAo213FyVpWUIDAnWfR5oEI4vj76ZXFQEQTR5rMzf5JYiPM7+TWlYOXO2ZrVetWVh/6Y0HNi8VdcV07ZTh0Ydu7fREy6OhA1gs5pFxceh26D+OLZzt0fGRxAE4YuQuCEaBbloufWlpxGdEA8AKC8qBsfzCsuCKAiak3G/EUkYNfW+JhFU7A4MhYuFz+/uoG6CIAhfp2Xk5xI+gSgICI4IR3hMw2Q79f1lmLvhS/QbkWT4WjP1cjzVVLOp466gboIgiKYCiRui0WCxN37+SoNhREw7TFw031DgmKmXw3Ge6xreFBEFEQWZWTi+a6+3h0IQBNGokLghGgUjy4tNsIgYM2u6rngx61opLyp2caS+gytCTRRFgAN2f/cjBRMTBNHi8Kq4GTZsGNavX4+MjAyIoogxY8Y4fE1SUhL++OMPVFZW4u+//8bEiRMbYaSEqziyvMiDX7Uw61r5cOYcrF2w2OlxuosTu/a45TjOChwmIAdeO9J0dWiO59F9yEAMvHak1ASUIAiiKeLVgOKQkBDs3bsX7733HlJTUx3u36VLF3zzzTf473//izvuuAMjRozAihUrkJmZiR9++KERRkw4i1nLi95+x3ftRWFWtsN6Ocf+2IOwtm1cGqs7CImOcvkYNZVVaBUU6PTrOY4znS2llYJPhQAJgmiqeFXcfP/99/j+++9N7z9lyhScOHECjz32GADgzz//xBVXXIEZM2aQuPFxzFpe9PYTBQFrFyzGxEXzIQqCZpG/dQuXQBQEnwigjenS2enXiqKI0vwChLWJdstYHAlLeR0iOSwWasOy5di4fKUp9xbH8+g+eAB6XDwYIoBjv+/CsZ27yTVGEESj0qRSwRMTE7Fx40bFtg0bNmDJkiW6rwkICEBgYMPqNywszFPDIwwwa3kxCn7Vq5dTmJ0jFflj71VVXoHA4Nbu/yAGVFdUIqB1kFuOdXr/QVw4fJhbjmUk9hSxUKrvhT0e/fD9uHTcGIdWnH4jkuwrJk+5F2UFhVj9/AK3WYBc6aNFPbgIomXQpMRNXFwcsrOzFduys7MRERGBoKAgVFZW2r1m9uzZeO655xpphIQeViwvRjgq8seOd3LPPvS67BL7cahq5LizZo4rLiQ1nfpd6JbjCHV1CDboxdVN1d5Bj4jYGExcNF+3nYNk/dE4lcGREZi4eD4+nDkH+zZuMTVutQg5sWc/ug7ohwuTh2HwDaMRKnP7mXWfkeuNIFoOPtN+QRRFjB07FuvWrdPd58iRI3j//fexYMECadu1116Lb7/9Fq1bt9YUN1qWm4yMDGq/4CUctVdw13vcNu9ZTcuNUFcH3s/PLe/jaYzaL1g6hmhfCZphpZ2FKIoozMq2a+cg9Q+LjTEcq1BXhw8ffxr7f9xs+D5a14jR92amj5ZeCxBXenCRFYggGpdm234hKysLsbHKVWZsbCyKioo0hQ0AVFdXo7q6ujGGR5jAjOXFFfQsCGwSW/X40+g9LBEXp/zTLe/naVy1Ktlq/whSl3H1ebbSzkIvQNms9Yf388PE11/GyhnmRIjivY3qG9VXuNb7jI5cb0av1YOsQPqQ6CN8gSYlbtLT03Hdddcpto0cORLp6eleGhHhDHrtFVxFWUtHexL75+P/RkDrxo3FcRZ3uctYmn33wQNw9Pdd0nZn21moA5QvTLYWG+SUCHEwPqM+Wo7El9UeXI4CsFtyJ3YSfYSv4NVCFiEhIejfvz/697fVNunatSv69++Pjh07AgDmzZuHlStXSvv/97//Rbdu3bBw4UL06tULDz74ICZMmIDFi71f14TwPmZr6YRERjTyyHyDuxfNk6pAS0KCsy6i5AHK/UYk4co7bzX9Wrn1R11Xp/uQgQ6rUDtCKzPM1TIEchwHYBsXo2zOMNEXEdNOsd1MBXKCcDdetdwMGTIEW7ZskR4zkfLBBx9g0qRJiI+PR6dOnaTnT548ieuvvx6LFy/GtGnTcPbsWUyePJnSwAkA1CDSEcHh4Zi4aD4+fHQOYrt3NeVKUlNyPl/KaJNP9JqRxAZcmDwMt89/VjGGssIiy+NRo5UZ5moZAjnutgI1Fzzh+iMIV/CquElLSzNcNU6aNEnzNYMGDfLksIgmii/Ut/FlON72W7vrtZcsWxZYcPPv67+TJiezsTZaXHnXrYCq+nJwuPNlGoxKCbijDAHDnVag5gSJPsLXaHm2U6LZwiYxvZWhKAgoyMwy3Kcl4Ey2mNTOYfQISSBYjbVhSFlgWit8UbTccsJRKQFWhgCwb6xqpQwB4JwVqCW0tSDRR/gaTSqgmCCMMFtLB4DmPi0JZ4KV5fEywRHhlmJtzL43e85KPSJ1EUct9m9Kw8rHnsLE11+2/Fo5IZERhin6LF2eWYFaSoCtO11/BOEOWuadnWi2sCrGRTm5iu2F2TlSFovePkILtuZYISKmnSzWxjOYtd4smzQVL4++2U4oqK0l/UYmY+wT0xSCpDQ/H+tfWWpaZHA8jzGzpgM6QouNed2r/4EoCC0qwNas1dSM648g3AFZbohmh5laOlr7yKvgXnmXzSqhZy0QRRHVlZXgOA4BQe5pudBUCImKdDrWxiyl+QUIr2+AumzSVExauhCt62Nyso6eQFyPrgCgGb+hZS3REkshkZG4+/V5plO3HcaV1F8r5QWFLS7AVmE1VVvdLLr+CMIdkOWGaJawWjq7v/tRt3Gjeh+hthbHdu7G+lf/g5UzZqNcJ3tHFARABLZ9ugatAgJNWRmsxpH4KhUlpSgrci2rySiuhq3w62pqpG3Hdu6GKLMSVZaV6h5bz1rCcZydULWaum0lrsRsWYJug/qbOmZTgFlE1d+t3GpKEI0FiRuC0GD/pjQ8O/x6fP/mO3YpyoXZOfjw0TkYdN01sBUMdE+xvcbEWbHVOiwUN815zKX31hIaQMMK//S+g/APCLB+XANridFrHIkM5uLqa9KNVJJ3vsUG2O7flIbK0gbxqec2JAhPQ24pgtBBFAT8+H/vY+PylXYuLqtp0D/89z0c2/FHg8tLND8BewJXBFlQaIhbG44y2PnoP2qE/ZMmtJgrqel6IkPLxeUQjnMpwLY5tS+gtG/CW5C4IQgHaLWLsLrizj1xCsd27saxnbtxYtde6xOmD+EJS5WRWOo3IsmUpckVK4iWyNBrs+CIsDbR2LNhk6naOhzPY+C1IyUR0zd5WNPPrmoeHliiiUPihiCcwGpKa0neeen/6mDmkrzz6Dp4AEZNndwkXVzuwOhzj5k1A7y/rDaPzuTpTJqxXgE/Z1xc8nGYKUsQFBqKB999U9peVlCI4MgIu+KGTa5nVcu8hAkfg2JuCMIJju/ai9L8fNP73/Xai7jo6uHSY3kw89Hfd+HH/76H7BOnPDDSpk9UfCxah4ZKjzv06SX9X57ufWLPfkvfiVEWj6OAYM3jiSIKMhtq3EglB1Siq7K8HOBs8Utygut7nvlaz6qWUISQsI6vXxdkuSEIJxAFAV+8+CruXjQPgGNXTWh0NO5eNA+b31+FbxYvs3ue43m06dDeI2Ntbvi1aiX9f+r7DeeyMCsbO9Z9i+R77gDg+DspKyrGLx9/jgObt9o954yLi+M4/PbFOruSA0d37sZLv2wAAPzx1fcYdMMo3dfrHluns7unaSlFCAlrNIXrwrekFkE0IfZt3IIDP1n7ISdPuhP9Ribbbe82qD9aOZEh1BLREwERMe2QPPEO099JaFQkRj98P+Zu+NKuoF7bTh2cGlve6bN22/xkLrU+w6/QzRYzg7yzu6dpSUUICfM0leuCxA1BOEm/EUnom2z+h8wmtZufeszOhNvcUoK9AXPfdOjTGx89/rTp10XExihuyhzP49JxY5xKl1fH/XA8j+6DB0qP1a4oq7DO7p6eQBwVIfSmm4zwHk3puvD+CAiiCaL4kVtchYe1ibarq9JSeu54upghc9/0vvwS86/hOICDdFO++r6JtngbC9+rLd5G2V6g34gkzN3wJe5W9bNyBVtnd89PIL5QhNDXYzpaIr5wXZiFYm4IwglcqakC2FtqWEPG5p4t1Vifb8C1Iy3tz5qCXn3fRIyaep9T7ykPTHY2jRwwTosHGiaQK24bh9L8Ao/UwvF2EUJfieloTjWH3IG3rwsrkLghCCdw9ccrt9T0G5GEu1+f5+qQFLQEoWSEs/2+hk+6w3IqsyiKKCsskgKTnU0jt2rVsr2HDXdP/C4VIZSdwO5DBloWBnrC0NmUeGcFiq8ILF+iKXV/J3FDEE7g7I9XFEUUZjWkC0sTIedeq0ZLFjauEBQSYvk1HMchNCoS3Qb1x7Gdu12y6m1+fxWuuH28ZXFmZuK3MsmzLt+OihA66vKtzmZzJAzc3XDUWYHiboHVXHDXddEYkBOTIJzgxJ79EOrqLK22RVEERKX7QvJhkxjxOlXlFS69nlnznLXqbVi2At8sXgZRML6mtK45R8GcLP5n6vvLcOcrL2Dq+8vw3OavcOPj/1bEs7A4lwGjRmD7mnUAOLv3c9Tlm/fXXjObyaZxJaZDHaPTb2SyU1k9TSlotrFhxSkBzu6797Xu72S5IQgn6DqgH3g/P8c7yijMysa6hUsUKz5f8E23dNjk/ecv6eh/zVVOH4dZ85y16uWdOgNAaXXTci/qCWH5xC9vF6JnhQiNjkbS3bch6e7bUJiVjV3f/oBB112jsHKUFRQiMDQE/rLaQoXZOXbXsXwMrYICdcenZXmRW5RiunXRfK0a9e9Gy0Ij1NXVW0StWYAcWd70zrMjtCxn7P2aUkwPK05pZxHLzsH6V5aivKhY0VLEW5+HxA1BOIFZUbJz/bf4c9tvKM7J1fyh+4JvmgA2f/AJjjgpbtSuxgbTfUx9dpM5Yrp1qbeiNLymrqbGcof0f1wyRJosT+zZbyr+JyKmHZIn3Wm3PTgiHJCJqR/++x6O/b5Ld1LvNqg/eIP3UQsDpxqTwj5mTUu8GS0+jASKJ4JmtT5nWUEheH9/RXmAphLTw1rIvLZ3GwCgrrYO6179D8bOmu4zMUokbgjCCcyKkh2pXxuu7qSJMDZGd0UuiiJEQTBtKWrpwcRW4TgOf/68Dcd37ZX6O1k9f799sR4DR1+NkKhIRLWPR2BIiCVhAwDXTLkX10y5V+EGyjj8Fzr37wsA+H39txh643UOjzNyyr3S/0vz8xEaHe3wNRzPa1uJ6rfLx4gp9yomLWctL3qihL2f1negjulwpQcYG4cas79teb84I/Q+p9Z1xmouNVZMjyvZYIr9OGDiay/Dl2KUSNwQhBO4K7BO0WDRoGbOqsefRky3Lhj1kC1N2dHkSwLHGhEx7dBtUH/s/Oo7XHnXrZbP3+iH7/fIuFpHhEv/Z8JGb2xaoiAkMtL0e+m6uzS2s0lr8wer7FxZZijJO4/b5j0DzbgWzhbno/6cWjEdrpZk0BIyjn7bjNvmPeNagLTGeeU4DmJ9TI/ZoGlncWc2GM/zgOieIHB30fIiogjCDbgzsI75sMsLizSf5zgONz4xDVlHj2PDW8sdTrocx9niDSwiCgJKzptvPNmcGDNrOqa+vwxJd98mTa5mcbeIlB+vXeeOmvvojU/L8uIJWGBt8qQ77QJ2jc6dKAgoyMwCOM44cFijRUVhdo6dBcCVmLWS8/maiw/Fb9vgs7gjQFoLVnPJSiE8qwUP3d1CgeM4nyvsR+KGIJxE6vqck6vYrnUTNnOsZ4dfj+/ffEdatcphN53W4WGmjpd/LtP0ezM4nkerQO1g0OaKKAgQRREh9R25Gb5s9dLrTdXYY+Z4XnNS03WvykR/WBvHrjI5yyZNxcujb7b7TbkSs7brmw26iw/2266trtZ9vZnMqQuThzk9PrPCTSsTTqtfGsNb2WCNnTxBbimCcAEWWOeujIdLx40xNO8Oul67o7Sa6PbxTr1/YHBrp14npym5xKTzbDIjqbHxlXG4A3kXdqureL24NbMuJC0qSssMiwzu35SGouxcwyaqRoHJHM9jsE4HeDOYEW7O1OPpPmSgR7LBHNHYyRMkbgjCRURBcMtNwEwKalibaJTmFyAkMkI31gccZxd8bBe/oBO0KQ8gdXZibU4TckvDk8KUdWG/dNwY7PruR+PAYVFEXW2tlII+6qH7pCwtuQBRxKwJguI34eizjHrwX+CmTpYeq+NNOJ5HZLy5eJ4Lk4fZ3QO6DepvKphbCz2XmRxnCh72G5GE8c/NNjUGK5YWo3PtrcJ+5JYiCB/B7M3kj6+/h26sT/0NxlFtFD3Xhvw5Tze5JBoHs9+j1n5WC1WaISKmHZLvuQOAsRD2kxUDvGbKvXjw3Tfx/JZv7NwtzIVUml+g2G4mNk09Lnm8SbdB/RX1fYy48q5b7QoiOuuGEUURX7z8mkPrr9WCh8zKE2zSte3I0iJ/X6NMT28V9iNxQxA+glmz7cHNWzVjfUoLCg1FC+P3dd+aeh+ywDQPzH6P6munproap/cd1NyXxSo5NR4Wq2PUHFTnueDICExcbB/wun9TGj6Y0WCR+OXjz50alzzexIo44TgOSXffpoh3cdYNcyjtF+z/cbPD/azU47GSMs+Cvh1ZWsy4FjmOw4Zly6nODUG0ZKykl4uCYBfrEx7TDncufN7h+xRmZnli+B6lKcXxNBdaBQSgy8CLNJ9T17/xBLqp0qKIMbNm4GDaNnQd0E+6/uX7DzQZm2Z3fJm1w1lxwixAHz46x6l4oLSVn5raz0oTS7Mp87bWH+YsLWbFVd7ps6b2czckbgjCRzCMH9BIL1fH+nQfMtDU+xzd8QeGjr3eUET5Ut8cco/5Jt4Sm7ZU6Vg8u2k9QqOjpO1yt1SwrD6QM4S3a4u6mhqnRDWLd7nxiWlY+8pSTHztZVPHMYpN4f39cfktN6Ftpw7IO30W2/73paXF0IBRI0yNvbyoCKufX+DQ0sLxvOLcG+GtKuwkbgjChzDq26LXz4dh9mZ37I89DkWUq0HF7sZXxkH4DiFRkbqPXb1e+o5IcqnPGLMAXTj8Chz7Yxe6DrgIfrL4HU2xw9lbTDiexx3zn0X/0Vcr2lrc+Pi/sWXlJzbx9Lq9eFIvhswKjA8ffQpHf99luI/Zlhne7hDOQZ1D1swJCwtDcXExwsPDUVJS4u3hEIQmzpZFl6eGaokWeWqo1k2qIDMLAUFBCI4I9ynrDUE0J8qLixEcrrQurX7xFeQePyn95oOjInHrC08hKDTE7vVs8VFVVq75fEFmlmIxxPE85m740tBFVpCZhZdH32x4n9G7v2ghiiJWznBv2wUr8zeJG4JoZuiJFi3Lj1pEcTyPB999s7GHTBAtitMHDqNT3wsU24pz8xRxLI6sp3rPi6KID2fOwb6NWxTbJWHCaR/zg+lPOmwl4Uggyfkt9Sv8sf47t3YGtzJ/k1uKIJoZVgoLquN2Bl470tx7bNyCroP6G9bbae6WHwpyJpxFLWwAIKxtG7ttzmSUAcCEF5/C/p9+tv/NG1yunS660FDcWO3jdUnKP3FJyj+91hmcxA1BNEOcLSxo1je/9ePP8cfX3xvG7biDliCSCAJwXJvKynFah4bijvnPYtWsZwHYApLHPTPL8LjJk+5EaFQUWoeForKsHDu/+g7Hft8lCSRn6/Z4qzM43TUIgpBgQcl6ZmR5DQyj3lobli13eSxMJFWWlvlcxlRlaRnKi8mtTfgu/UdfDd7fH/1GJuOFtG8RGh3l0BJ0ccoN6Hf1cAwdcx0eXPEGXtr2Ay66eji6DxmI2G5dnBqHJ/tVGb4vKOaGIAgZVoKSAe3gZwCW/PNasDghAJi4eL7tvXzEDeRr2WQEoUXe6bNo0zHB6etU6zp3xR27bNJUl1rVUECxASRuCMIxVoKS9bjo6uG4e9E8AOZFwI7Ur/BX+u92cUL9RiThlhfnonVYqMVPYoNV1NXqYm3pOPUd281UgiYIQsmqJ57B7u9+dPr1FFBMEIRLuKPbeVlhkWkBwGpifP7cAs332L8pDa2CgnDHgudMv7/82ACHLSs/RvI9d7ocx8NTDBDRSDS3oPXGLOhH4oYgCE1c7XZuNgDRbHO9kMgIU8cTBEEhQOQFEE/vO2iqAJnmcevq8Muna3DlnbdYfi1BOENzEjZmOp27ExI3BEF4BLOrtNL8Anzx4isO3V2lBYWmjrf149U4+FOapsVJbpG6MHkYLrnpnwgKdezqEkURHz02FxGxMabGoPX65jRREYRVdn2zoVE7g5O4IQjCI5hpB1FaUIgXrh4DobbW4fGKVVlZehz8Kc3Q4sQsUsd27sahtG2mihZueGs59m3cgkHXXWNqDHrvqw7QpjR3oqVwcPPWRn0/+mURBOERWCNQgLNbsbE4mC9efMWUsAFkaeo6aeGiKEpp6mY5tnO3qWNuXL4SAOzS3s2y4a3ldq/1tfR2gvAU5cXFjd5jyifEzdSpU3HixAlUVFRg+/btGDp0qO6+EydOlDIW2F9FRUUjjpYgCLMY1cKxWtRLEksitMWSCIdxO64e01EdIK3jM3H00qibsGzSVKx64hmsXbAYvJ+f6XESRFNm59pvG9UlBfiAW2rChAlYtGgRpkyZgt9++w3Tp0/Hhg0b0KtXL+Tmaq+SioqK0KtXL+kxrYAIwndxR+aV/FjOdk13xzGZGNKqzKxGK1CaucvMtrnwBlTDh3A3B7c0rksK8IE6N9u3b8fvv/+ORx55xDYgjsOZM2fwxhtvYOHChXb7T5w4EUuWLEFUVJRT70d1bgii6eNs13R3HVOrDpAao7pA3YcMxNT3l7k03rULFqNtpw644vbxLh1HHeyszjYjCFd5+18P4+iOP1w+TpOpc9OqVSsMHjwY8+fPl7aJooiNGzciMTFR93WhoaE4efIkeJ7Hrl27MGfOHBw6dEhz34CAAAQGBkqPw8LC3PcBCILwCq6mqbt6TLk1KiKmHUKiIlFWWISQyAiUFhSiOCfXUBw5CrZ2NM7C7Bz88ukadBvU3yVxIwq2te33b72DvNNnERodhbFPznD6eK5SVV6O1c8vRGneedz56osIiYokC1IzIKxNdKO/p1fFTdu2beHv74/s7GzF9uzsbPTu3VvzNUeOHMG9996Lffv2ISIiAo899hh+/fVXXHjhhcjIyLDbf/bs2Xjuuec8MXyCIFowrggsK+4txetUri5XRBIAlBcVYfXzCyTrkrvcZc66travWYfd3/6A7kMGIjTaOes84Xs0ZvE+RpOzPW7fvh0fffQR9u7di59//hk33XQTcnNz8cADD2juP3/+fISHh0t/CQkJjTxigiAIe/SCrY0oLyxSBGIbZaSZ4cNHn1K4zZyZhNQxj6IgON2egqULO9uBuqkgCgKERg6w9QbyRruNjVfFTV5eHmpraxEbq/Rbx8bGIisry9QxamtrsXv3bvTo0UPz+erqapSUlCj+CIIgfIH9m9Lw0qib6gWKY9RihB1DSyQJ9f20tGCTzrE/9ii2m8kGE0URu777EaueeAbfv/kOirJzFM+XFRWb+ixa42GToDdW+kaIoogKN80doiAALaA3mdnK457Cq26pmpoa/PHHHxgxYgTWrVsHwGbGHDFiBN5803FhLcDW56Vfv3749ttvPTlUgiAIjyAKAn75dA2G33O7YcHDwuwcOzHC0MpIC46KxMTXXrYFDPOc4lh6k44jd5koitj8wcf4ZtFb0raNy1cq3pfjeVOFEeXHVI/HVXeb+jO52iwVIvD5M/MwZtZ0l8fUUhqvFmZlO53B6A68ngq+aNEirFy5Ejt37sSOHTswffp0hISE4P333wcArFy5EhkZGZgzZw4A4Omnn8b27dtx9OhRREZG4vHHH0fnzp2xYsUKb34MgiAIpzESFUZiRH0MdQyQM2nzeqnxJefz8eVLr2Lfxi2G78vxvCVhotV+w9mYJDVMOLmCfHyiKDo1JkEQsPXjz1GQkenVgG1PIYoifv74cxScPWcqoL4x8HoqOAA89NBDePzxxxEXF4c9e/bg3//+N3bs2AEA2Lx5M06ePIlJkyYBsImhm266CXFxcSgoKMAff/yBuXPnYs+ePabei1LBCYLwVbRSzI1Sys3gbNq8K+n2/UYkYeKi+QCnH1Rspv2G1vmw0qerIDML619Z6pTFRRRFlOYX2I2v34gkjHvmCYRGO84AYm7BDx+bi30//ISB147Ena+8YHoMnsKdtYxs1rxP8M0i89Y6Z7Eyf/uEuGlMSNwQBOHLeKKGjzfoNyIJ4599EiFRkXbPMWuUmSrV8vNx4fArMODakXaTMnMd/fzx/1CQkWlnPZDEFkSlVUxnkmfbV87QHh/v749nN613mKouiiI2v78K3yy21TQyW9/Ik41W2bkXIdrVM1K/r6NxaLkpPQmJGwNI3BAEQTQOHM/j6vsmYtidtyAkMkLa7oo1qt/IZIyb+7giVdzM8bSsQKUFBfBvFYCg0BDFvqUFhVgjS5HXO56WYGJUlJTi82deVrjxOJ7H3A1fGlqRPF0hmp2rg2nbcPmtN6P74AGoLCvH+YxzuPSmf9pZDX/7Yj1ah4dh8A2jFedcz03pSUjcGEDihiAIonFxtzXKna42AOg+eAB6XDwYIoBjv+/CsZ27TR1PWzAV4pePP8fG5Ss1j+FIFJUWFOL31K8xdOz1CjFhZ1Wpz7oCDFx/9a61VbOeRVh0lMNzZXRefcGiSOLGABI3BEEQhLtwZtI3I4rkx23bqQMuHTfGzqqy+7sfkXzPnZqxTY5ca00REjcGkLghCIIgvI1VUaS3v15skxnXWlODxI0BJG4IgiCI5gTH80671poSTaZxJkEQBEEQriEKAo7+vgtHf9/l7aH4DE2utxRBEARBEIQRJG4IgiAIgmhWkLghCIIgCKJZQeKGIAiCIIhmBYkbgiAIgiCaFSRuCIIgCIJoVpC4IQiCIAiiWUHihiAIgiCIZgWJG4IgCIIgmhUttkJxWFiYt4dAEARBEIRJrMzbLU7csJOTkZHh5ZEQBEEQBGGVsLAwapypRfv27T3SNDMsLAwZGRlISEigppwegM6vZ6Hz61no/HoWOr+exVfOb1hYGM6dO+dwvxZnuQFg6sS4QklJCf24PAidX89C59ez0Pn1LHR+PYu3z6/Z96aAYoIgCIIgmhUkbgiCIAiCaFaQuHEjVVVVeO6551BVVeXtoTRL6Px6Fjq/noXOr2eh8+tZmtr5bZEBxQRBEARBNF/IckMQBEEQRLOCxA1BEARBEM0KEjcEQRAEQTQrSNwQBEEQBNGsIHHjJqZOnYoTJ06goqIC27dvx9ChQ709JJ/jySefxI4dO1BcXIzs7GykpqaiZ8+ein0CAwPx5ptvIi8vDyUlJVizZg1iYmIU+3Ts2BFff/01ysrKkJ2djVdeeQV+fn6KfZKSkvDHH3+gsrISf//9NyZOnOjxz+drzJo1C6IoYvHixdI2Or+u0b59e3z00UfIy8tDeXk59u3bh8GDByv2ef7553Hu3DmUl5fjxx9/RI8ePRTPR0VFYdWqVSgqKkJBQQFWrFiBkJAQxT79+vXDzz//jIqKCpw+fRqPP/64xz+bL8DzPF544QUcP34c5eXlOHr0KObOnWu3H51jcwwbNgzr169HRkYGRFHEmDFj7PZprHM5btw4HD58GBUVFdi3bx+uvfZa935YDUT6c+1vwoQJYmVlpXjPPfeIF1xwgfh///d/Yn5+vtiuXTuvj82X/r777jtx4sSJYp8+fcSLLrpI/Prrr8WTJ0+KwcHB0j7Lli0TT506JSYnJ4uDBg0Sf/31V/GXX36Rnud5Xty3b5/4ww8/iP379xdHjx4t5uTkiC+//LK0T5cuXcTS0lLxtddeE3v37i0+9NBDYk1NjXjNNdd4/Rw01t+QIUPE48ePi3v27BEXL15M59cNf5GRkeKJEyfE9957Txw6dKjYpUsXceTIkWK3bt2kfZ544gmxoKBAvPHGG8V+/fqJa9euFY8dOyYGBgZK+3z77bfi7t27xYsvvli8/PLLxb/++kv8+OOPpefDwsLEzMxM8aOPPhL79Okj3nLLLWJZWZl43333ef0cePpv9uzZYm5urnjdddeJnTt3Fm+++WaxuLhYfOSRR+gcO/E3evRo8cUXXxTHjh0riqIojhkzRvF8Y53LxMREsaamRnzsscfE3r17iy+88IJYVVUlXnjhhZ78/N7/Apr63/bt28U33nhDesxxnHj27Flx1qxZXh+bL/+1bdtWFEVRHDZsmAhADA8PF6uqqsSbb75Z2qdXr16iKIriJZdcIgK2H2ttba0YExMj7fPAAw+IhYWFYqtWrUQA4oIFC8T9+/cr3uvTTz8Vv/vuO69/5sb4CwkJEY8cOSKOGDFC3Lx5syRu6Py69jd//nzx559/Ntzn3Llz4qOPPio9Dg8PFysqKsRbbrlFBCD27t1bFEVRHDx4sLTPqFGjxLq6OjE+Pl4EIE6ZMkU8f/68dL7Zex8+fNjr58DTf1999ZW4YsUKxbY1a9aIH330EZ1jF/+0xE1jncvPPvtM/OqrrxTvnZ6eLr799tse+7zklnKRVq1aYfDgwdi4caO0TRRFbNy4EYmJiV4cme8TEREBAMjPzwcADB48GAEBAYpzeeTIEZw6dUo6l4mJidi/fz9ycnKkfTZs2ICIiAhceOGF0j7yY7B9Wsr38dZbb+Gbb77Bpk2bFNvp/LrGjTfeiJ07d+Lzzz9HdnY2du3ahcmTJ0vPd+3aFfHx8YpzU1xcjN9++01xfgsKCvDHH39I+2zcuBGCIOCSSy6R9vn5559RU1Mj7bNhwwb07t0bkZGRHv6U3uXXX3/FiBEj8I9//AMAcNFFF+GKK67Ad999B4DOsTtpzHPpjXsGiRsXadu2Lfz9/ZGdna3Ynp2djbi4OC+NyvfhOA5LlizBL7/8goMHDwIA4uLiUFVVhaKiIsW+8nMZFxenea7Zc0b7REREICgoyCOfx1e45ZZbMGjQIMyePdvuOTq/rtGtWzc8+OCD+PvvvzFq1Ci8/fbb+M9//oO7774bQMP5MboXxMXFKYQjANTV1SE/P9/Sd9BcWbBgAT777DP8+eefqK6uxu7du7FkyRJ88sknAOgcu5PGPJd6+3jyXLfIruCE93nrrbfQt29fXHHFFd4eSrOhQ4cOWLp0KUaOHNlkSqQ3JXiex86dO/HUU08BAPbs2YO+fftiypQp+PDDD708uubBhAkTcMcdd+D222/HwYMHMWDAACxZsgTnzp2jc0xYgiw3LpKXl4fa2lrExsYqtsfGxiIrK8tLo/Jt3njjDdxwww1ITk5GRkaGtD0rKwuBgYGSu4ohP5dZWVma55o9Z7RPUVERKisr3f55fIXBgwcjNjYWu3btQk1NDWpqajB8+HD8+9//Rk1NDbKzs+n8ukBmZiYOHTqk2Hb48GF06tQJQMP5MboXZGVl2WWn+fn5ITo62tJ30Fx59dVXsWDBAvzvf//DgQMHsGrVKixevFiyRNI5dh+NeS719vHkuSZx4yI1NTX4448/MGLECGkbx3EYMWIE0tPTvTgy3+SNN95ASkoKrrrqKpw8eVLx3B9//IHq6mrFuezZsyc6d+4sncv09HT069cP7dq1k/YZOXIkioqKpIknPT1dcQy2T3P/PjZt2oS+fftiwIAB0t/vv/+Ojz/+GAMGDMDOnTvp/LrAtm3b0KtXL8W2nj174tSpUwCAEydOIDMzU3FuwsLCcMkllyjOb1RUFAYNGiTtc9VVV4Hnefz222/SPldeeSX8/RsM6yNHjsSff/6JwsJCT308nyA4OBiCICi21dXVgedtUxWdY/fRmOfSW/cMr0dxN/W/CRMmiBUVFeLdd98t9u7dW/zvf/8r5ufnKzJO6A/iW2+9JRYUFIhXXnmlGBsbK/0FBQVJ+yxbtkw8efKkOHz4cHHQoEHitm3bxG3btknPs1Tl77//XrzooovEa665RszOztZMVV64cKHYq1cv8cEHH2wRqcpaf/JsKTq/rv0NGfL/7d1tSFN7HAfwr64McVkqpJAP5JBaFiOaRRms7IEiI8EwKmFvehFETQtm4osRvelBWFAiCLkSRAxiRQ/0IBm+0F6kggstkjZasNDlNI9NJvq7L+5l3DXvzfvg9J77/cAP5vn/z/n/938hX885fzRKKBSS6upq0el0cuzYMVEURY4fPx7uY7VaZWRkRA4dOiQbNmwQp9M569ba7u5uKSgokO3bt8v79+8jttYmJyeLz+eTO3fuyPr166WsrEwURVHdNuXZyuFwiNfrDW8FLykpkaGhIbl8+TLX+G9UUlKSGAwGMRgMIiJSUVEhBoNBsrKyYrqW27Ztk1AoJOfOnZO1a9eKzWbjVvD/Sp0+fVo8Ho9MTk7K69evZcuWLQs+p8VWf8RsNof7LFu2TG7evClfv34VRVHk3r17kp6eHnGd7Oxsefz4sUxMTMjQ0JBcu3ZNNBpNRB+TySQ9PT0yOTkpg4ODEWP8n+rHcMP1/Wd18OBB6evrk2AwKP39/XLy5MmoPhcvXhSfzyfBYFBevHgheXl5Ee0pKSnS3Nws3759k9HRUbl165YkJSVF9Nm4caN0dHRIMBgUr9crVqt1wb97LEqr1YrdbhePxyPfv3+XwcFBuXTpUsQ2Y67x3MtkMs36O9fhcMR8LY8cOSLv3r2TyclJcblccuDAgXn97nG/fSAiIiJSBb5zQ0RERKrCcENERESqwnBDREREqsJwQ0RERKrCcENERESqwnBDREREqsJwQ0RERKrCcENERESqwnBDRP87brcbFotloadBRPOE4YaI5pXD4YDT6QQAtLe3w263x2xss9mMQCAQdbygoAANDQ0xmwcRxdaSn3chIlpcli5diqmpqb99vt/v/xdnQ0SLDe/cEFFMOBwO7Ny5ExUVFRARiAhycnIAAPn5+Xjy5AnGx8fx5csXNDU1IS0tLXxue3s7bty4AbvdjuHhYTx79gwAUFlZib6+PiiKgk+fPqGurg5JSUkAAJPJhNu3b2PlypXh8Ww2G4Dox1JZWVm4f/8+xsfHMTY2htbWVqxatSrcbrPZ0Nvbi/LycrjdboyOjqKlpQVarXbe142I/jqGGyKKCYvFgs7OTjQ0NCAjIwMZGRnwer1YsWIFXr58id7eXhiNRuzfvx/p6em4e/duxPlmsxmhUAiFhYU4deoUAGBmZgZnz55Ffn4+zGYzioqKcPXqVQBAZ2cnLBYLxsbGwuPV1tZGzSsuLg4PHjxAamoqTCYT9u7di9zcXLS2tkb00+l0KCkpQXFxMYqLi2EymXDhwoV5Wi0i+qcW/N+ys1gs9ZbD4RCn0ykApL29Xex2e0R7TU2NPH36NOLY6tWrRUQkLy8vfF53d/dPxyotLZXh4eHwz2azWQKBQFQ/t9stFotFAMiePXtkampKMjMzw+16vV5ERIxGowAQm80miqKIVqsN97ly5Yp0dXUt+PqyWKzo4js3RLSgDAYDdu3ahfHx8ag2nU6HDx8+AAC6u7uj2nfv3o3q6mqsW7cOycnJWLJkCRITE5GYmIhgMDin8fV6PbxeLz5//hw+NjAwgEAgAL1ejzdv3gAAPB4PFEUJ9/H5fBGProho8WC4IaIFpdVq8fDhQ1RVVUW1+Xy+8OeJiYmItpycHDx69Aj19fWoqanByMgIduzYgcbGRiQkJMw53MzVjy8wiwji4/lkn2gxYrghopgJhULQaDQRx3p6elBaWgqPx4Pp6ek5X2vz5s2Ij4/H+fPnISIAgLKysp+O96OBgQFkZWUhMzMzfPdGr9cjJSUF/f39c54PES0e/LODiGLG4/Fg69atyMnJQVpaGuLi4lBXV4fU1FS0tLTAaDQiNzcX+/btQ2Nj45/eGRkcHERCQgLOnDmDNWvWoLy8PPyi8e/HW758OYqKipCWlobExMSo67S1tcHlcqG5uRmbNm1CQUEBmpqa8OrVq1kfhRHR4sdwQ0QxU1tbi+npafT398Pv9yM7Oxs+nw+FhYXQaDR4/vw5XC4Xrl+/jtHRUczMzPzhtfr6+lBZWYmqqiq8ffsWJ06cQHV1dUSfrq4u1NfXo7W1FX6/H1arddZrHT58GIFAAB0dHWhra8PHjx9x9OjRf/W7E1HsxOHXN4uJiIiIVIF3boiIiEhVGG6IiIhIVRhuiIiISFUYboiIiEhVGG6IiIhIVRhuiIiISFUYboiIiEhVGG6IiIhIVRhuiIiISFUYboiIiEhVGG6IiIhIVX4BxWFKwk0jBtwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_its, train_losses = zip(*metrics.train_losses)\n",
    "val_its, val_losses = zip(*metrics.val_losses)\n",
    "plt.plot(train_its, train_losses, '-o')\n",
    "plt.plot(val_its, val_losses, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', \"Valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "The adapted weights are: $\\tilde{W} = W + c \\cdot \\mathbf{b}^\\top \\mathbf{a}$. Note, this process can be destructive if the inputs are in low precision and they have very different magnitudes. Tuning the `scale` parameter, $c$, prior to fine-tuning can improve the model performance after fusion.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlx_lm.fuse --model {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `lora_fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = load(\"lora_fused_model\")\n",
    "# num_correct = 0\n",
    "# for prompt, answer in tqdm.tqdm(test_set[:num_test]):\n",
    "#     response = generate(model, tokenizer, prompt, max_tokens=2)\n",
    "#     num_correct += (response==answer)\n",
    "# test_acc = num_correct / num_test\n",
    "# print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your LoRA adapters are not working well it's critical to plot both the trianing loss and validation loss over the duration of fine-tuning. There are really only two cases to consider: underfitting or overfitting. And you can figure out which regime you are in based on the above plot.\n",
    "\n",
    "**Underfitting**: The trianing loss is not low enough and the validation loss closely matches the training loss. You could also measure the accuracy on the training set itself for question-answering style tasks like HellaSwag. If you are in this regime you have a few options to improve the results:\n",
    "\n",
    "- Use more adapters. Increase `lora_layers` or adapt more of the linear layers within a given block by setting `lora_parameters[\"keys\"]`.\n",
    "- Use a higher rank. A higher rank means more parameters per adapter.\n",
    "- If you are using dropout, decrease the droupout rate or turn it off entirely.\n",
    "- Sometimes, underfitting issues are really optimization issues. In these cases it can be helpful to tune the learning rate or learning rate schedule.\n",
    "- If none of the above works, try a bigger model. For example, try Phi-3 medium instead of Phi-3 tiny.\n",
    "\n",
    "**Overfitting**: The trianing loss keeps going down but the validation loss stops going down and even starts to go up. If you are in this regime you also have a few options:\n",
    "\n",
    "- The best thing to do is to use more trianing data if you have it.\n",
    "- Contrary to the underfitting regime decreasing the capacity of the model can help. For example, use fewer adapters, a lower LoRA rank, or a smaller model size.\n",
    "- If you are not using dropout, use it.\n",
    "\n",
    "If you find your adapters work well pre-fusion but stop working post-fusion, try tuning the `scale` parameter, $c$, prior to fine-tuning. Typically the adapters have a smaller magnitude than the weights, so using a larger scale helps.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "Fine-tuning a large LM with LoRA requires a machine with a decent amount of memory. Here are some tips to reduce memory use should you need to do so. \n",
    "\n",
    "- Try quantization (QLoRA). You can use QLoRA by generating a quantized model with `mlx_lm.convert` and the `-q` flag or by using an already quantized model from HuggingFace.\n",
    "\n",
    "- Try using a smaller batch size. You can set the `batch_size` parameter in the `TrainingArgs` or pass `--batch-size` if you are using the CLI. The default is 4 so setting this to 2 or 1 will reduce memory consumption. Note, this may slow things down a little..\n",
    "\n",
    "- Reduce the number of layers to fine-tune with by setting `lora_layers` to a smaller value or passing `--lora-layers` if you are using the CLI. The default is `16`, so you can try `8` or `4`. This reduces the amount of memory needed for back propagation. It may also reduce the quality of the fine-tuned model and you may need to compensate with a larger `rank`.\n",
    "\n",
    "- Longer examples require more memory. If it makes sense for your data, one thing you can do is break your examples into smaller sequences when making the `train`, `valid`, and `test` data sets.\n",
    "\n",
    "- Gradient checkpointing lets you trade-off memory use (less) for computation (more) by recomputing instead of storing intermediate values needed by the backward pass. You can use gradient checkpointing by passing `grad_checkpoint=True` to the `TrainingArgs` or the `--grad-checkpoint` flag if using the CLI. Gradient checkpointing will be more helpful for larger batch sizes or sequence lengths with smaller or quantized models.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project.\n",
    "- We also have an example of [LoRA fine-tuning in MLX Swift](https://github.com/ml-explore/mlx-swift-examples/tree/main/Applications/LoRATrainingExample)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
