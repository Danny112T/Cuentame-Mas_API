{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to [LoRA fine-tune](https://arxiv.org/abs/2106.09685) an LLM with MLX LM. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it in the right format for MLX LM.\n",
    "2. Setup and run LoRA training. We'll show how to capture the training logs and plot some statistics to visualize the performance.\n",
    "3. Evaluate on the test set. We'll compute the final question-answer accuracy of the fine-tuned model.\n",
    "4. Fuse the resulting adapters into the base model and upload to Hugging Face.\n",
    "5. Discuss tips for debugging accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "664272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install mlx-lm\n",
    "# pip install matplotlib\n",
    "# pip install rouge-score\n",
    "# pip install scikit-learn\n",
    "# pip install tqdm\n",
    "# pip install numpy\n",
    "# pip install json\n",
    "# pip install pathlib\n",
    "# pip install transformers\n",
    "# pip install sentencepiece\n",
    "# pip install datasets\n",
    "# pip install torch\n",
    "# pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1131315",
   "metadata": {},
   "source": [
    "### MLFOW CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d1cf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae62a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d438bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_name=\"finetuning_comparativa\"\n",
    "exp_name=\"MLX1.0\"\n",
    "corrida_name=\"MLX-40_2_4e4\"\n",
    "# base_model = \"Llama-3.2-1B-Instruct\"\n",
    "# four_bits = True \n",
    "# dataset_path = \"FAQ_All.jsonl\"\n",
    "# dataset_type = \"alpaca_chat.load_qa\"\n",
    "# output_dir = \"../trained_models/adapters/adapters.safetensors\"\n",
    "output_path = \"../trained_models/adapters_40_2_4e4/\"\n",
    "output_dir = output_path + 'adapters.safetensors'\n",
    "sequence_len = 2048\n",
    "lora_layers = 8\n",
    "lora_layers_scale = 20.0\n",
    "grad_checkpoint_value = True\n",
    "\n",
    "\n",
    "lora_r = 8\n",
    "# lora_alpha = 16\n",
    "lora_dropout = 0.0\n",
    "# gradient_accumulation_steps = 4\n",
    "optimizer = \"adam\"\n",
    "# weight_decay_value = 0.02\n",
    "lr_scheduler = \"linear\"\n",
    "ds_len = 1024\n",
    "learning_rate_value = 4e-4\n",
    "batch_size = 2\n",
    "\n",
    "epochs = 40\n",
    "steps = ((ds_len // batch_size) * epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c0069b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/880645134898555871', creation_time=1733877767863, experiment_id='880645134898555871', last_update_time=1733877767863, lifecycle_stage='active', name='MLX1.0', tags={'mlflow.sharedViewState.bb15b83a1c8297e8bcab8dc38315c49bda8d8fec1b37088f0195711a12855512': 'deflate;eJzlVktv2kAQ/iuRz6jCJEDNjVCSokJaAY0iRRFZ1oPZsN619uFgEP+9s5iHm+D2UolIPfiw8/zmm/XOrD0NRNH5DeMGlNfyvIonVQjqOvsGGZ6JMYpNrQH9SRuizMSwGI5GbU291oxwDRVvqx87dctr9/toxNkMaEY5HMK3qWGp8w+JIRqM3msenypeLEPg96A0k+LowfnF0AqNPho4UANhR3Ibo6T1WIT3PJJWUXhGw6J04ILqt9IvefpnD9MqjN5dJkSEEHqt9SaX/GBCuPPjzuIrC0MQx/M902zKODPZgCRHt9zMJUXsN73haDzxq5Phz7sRIqAyTogCLKczR65cBWvPWoZpPL95eRlcBrVGsx7Ugvo0qMpIoY/JEhfquj308gwdaYUZy04ey2v5VWQOsDQ6QnaQut67cKvGy5RfpcLxjhwaV9euaUzfggBFtjKjLOyD7dpPqVWEZt6mchrpKo5qs7S2OjvSmT/RVGKaMqSZqV81Y944P1ImCJ8YRZiYcKl1KeJ5VTZq03TxQRCn+P0R70LPUhtwODveBFTCYYn/ZilW2ngJMuPLs2NV0kbgl+KUQZauFurqY+Cslf9dft2P4zj9GDj7pTgtfF6EJjRHnP3eXfccQAsPgJuUAkaxlGYuAAUtTLxsL5keUcJhnAN1NsSNBH1KqIiIIJ9DW888iTaQFCbng9MMCiBQlx2tc3h7Gc5EhWCwbJ2PPRYJfGC/W8MZDulDnVbDLZdTwh/IPtJbTf9QHhPRTl3WId+3zawW++fv0OHB+z/781RcWHb0lq8sBZoF2e6A283rIsfsOAyZTjjJ9kwzPYTtFlloyF8bVLqGvDZN9kLIMfsoQ27jf50eOUkZvA6QyZitCqZ4OftMm/2WeAjQpriUhMjc+2yOfXytEgjvCbege4etMEc4ZyF048Rkv4sj53ONdQjL+e74Zn8l1sghzPB2zLuCTHkhaXTo9zZqR4oZw46vT1zKE3e+/J5uNr8AWwkZ8g=='}>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"https://4z0r6nts-5000.usw3.devtunnels.ms/\") \n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(experiment_name=exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61698208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello stats: 1024 lenght trainging dataset\n",
      "An example:\n",
      "\n",
      "{\n",
      "  \"pregunta\": \"\\u00bfPuedo presentar una p\\u00f3liza de bonos en formato electr\\u00f3nico?\",\n",
      "  \"respuesta\": \"Este tipo de garant\\u00eda es v\\u00e1lida, \\u00bfno?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# jsonl file path\n",
    "\"\"\" save_path = \"./dataset/FAQ_All.jsonl\"\n",
    "with open(save_path, 'r') as file:\n",
    "\tdataset = [json.loads(line) for line in file]\n",
    "\"\"\"\n",
    "\n",
    "# csv file path\n",
    "save_path = \"./datasets/Parph_Data/FAQs_1000.csv\"\n",
    "dataset = []\n",
    "with open(save_path, 'r', encoding='utf-8') as file:\n",
    "\treader = csv.DictReader(file)\n",
    "\tfor row in reader:\n",
    "\t\tdataset.append(row)\n",
    "\n",
    "print(f\"Hello stats: {len(dataset)} lenght trainging dataset\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b607237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "valid_set = [dataset[i] for i in perm[:valid_size]]\n",
    "train_set = [dataset[i] for i in perm[valid_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c38c4e",
   "metadata": {},
   "source": [
    "Finally, put the data splits in the MLX LM training format. The format simply expects the data to be in a container which supports random access to the individual examples (e.g. a Python `list`):\n",
    "```\n",
    "[\"An example for the model.\", \"Another example for the model.\", ...]\n",
    "```\n",
    "For more details, see the [documentation on supported formats](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea738f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\"\"\"def preprocess(dataset):\n",
    "    return [\n",
    "        f\"Question: {clean_text(t['question'])}\\n\"\n",
    "        f\"Answer: {clean_text(t['answer'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\"\"\"\n",
    "def preprocess(dataset):\n",
    "    return [\n",
    "        f\"pregunta: {clean_text(t['pregunta'])}\\n\"\n",
    "        f\"respuesta: {clean_text(t['respuesta'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\n",
    "\n",
    "train_set, valid_set = map(preprocess, (train_set, valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [Phi-3 mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). At 3.8 billion parameters, Phi-3 mini is a high-quality model that is also fast to fine-tune on most Apple silicon machines. Also, it has a [permissive MIT License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE).\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3ff309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm.utils import load, generate\n",
    "from mlx_lm.tuner.trainer import train, evaluate, TrainingArgs\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters and make the training arguments. See the [training argument class](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/trainer.py#L31-L63) for a more detailed list of training parameters. \n",
    "\n",
    "Recall the LoRA update is $W^\\top \\mathbf{x} + c \\cdot \\mathbf{a} \\mathbf{b}^\\top \\mathbf{x}$ where $\\mathbf{a}$ has shape `(D, rank)`.\n",
    "\n",
    "With that in mind, the LoRA parameters to attend to are:\n",
    "- `lora_layers`: The number of Transformer blocks from the top of the model to adapt.\n",
    "- `rank`: The rank of the low-rank adapters. A larger rank implies more adapter parameters per linear layer.\n",
    "- `scale`: This is the constant $c$ that scales the low-rank update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0851dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save the adapter config and weights\n",
    "adapter_path = Path(\"../trained_models/adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"lora_layers\": lora_layers,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": lora_r,\n",
    "    \"scale\": lora_layers_scale,\n",
    "    \"dropout\": lora_dropout,\n",
    "    \"epochs\": epochs\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)    \n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=output_dir,\n",
    "    iters=steps,\n",
    "    steps_per_eval=50,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_length=sequence_len,\n",
    "    grad_checkpoint=grad_checkpoint_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the Phi-3 mini model. Note this may take a few minutes to download from HuggingFace if you haven't downloaded it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb0b16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../original_models/Llama-3.2-1B-Instruct-bf16\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the model, freeze it's parameters so we don't train them. Then convert linear layers to LoRA layers using the MLX LM utility `linear_to_lora_layers`. The adapters in the `LoRA` layers are not frozen, so they will be included in the model's `trainable_parameters`. Check-out the [LoRA layer implementation](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/lora.py#L72-L104) to see how it all works.\n",
    "\n",
    "By default, MLX LM only adapts the query, key, and value projection matrices for Phi-3. You can specify the layers to adapt by setting `lora_parameters[\"keys\"]` to a list of layer names. In this case it defaults to `[\"attn.qkv_proj\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50e1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 425984\n"
     ]
    }
   ],
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"lora_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34ee27",
   "metadata": {},
   "source": [
    "### Evaluate Functions\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each example in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37e55f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > 0.8\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    # Adjust generation parameters\n",
    "    generation_params = {\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    for example in tqdm(dataset):\n",
    "        # Get prompt and true answer\n",
    "        prompt = example[\"prompt\"]\n",
    "        true_answer = example[\"response\"]\n",
    "        \n",
    "        # Generate prediction\n",
    "        response = generate(model, tokenizer, prompt, **generation_params)\n",
    "        \n",
    "        # Store prediction and true label\n",
    "        all_preds.append(response)\n",
    "        all_labels.append(true_answer)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(true_answer, response)\n",
    "        for key, score in scores.items():\n",
    "            rouge_scores[key].append(score.fmeasure)\n",
    "\n",
    "        # Calculate loss/perplexity\n",
    "        tokens = tokenizer.encode(prompt + true_answer)\n",
    "        tokens = mx.array(tokens)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(tokens[None])[0]\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        targets = tokens[1:]\n",
    "        logits = logits[:-1]\n",
    "        loss = mx.mean(nn.losses.cross_entropy(logits, targets))\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = sum(1 for x,y in zip(all_preds, all_labels) if similar(x.strip(), y.strip())) / len(all_preds)\n",
    "    \n",
    "    # Convert predictions and labels to match format for F1\n",
    "    pred_labels = [1 if similar(p.strip(), l.strip()) else 0 for p,l in zip(all_preds, all_labels)]\n",
    "    true_labels = [1] * len(all_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='binary')\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge_scores = {key: np.mean(scores) for key, scores in rouge_scores.items()}\n",
    "\n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    perplexity = float(mx.exp(mx.array(avg_loss)))\n",
    "\n",
    "    return accuracy, f1, perplexity, avg_rouge_scores\n",
    "\n",
    "# Load test dataset\n",
    "test_set_path = \"./datasets/Parph_Data/FAQs_200_testing.jsonl\" \n",
    "with open(test_set_path, 'r') as file:\n",
    "    test_set = [json.loads(line) for line in file]\n",
    "\n",
    "# Define number of test samples\n",
    "num_test = len(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer, but you can specify any [optimizer](https://ml-explore.github.io/mlx/build/html/python/optimizers/common_optimizers.html) with any [scheduler](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html). We also added a custom class to capture the training and validation loss to plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "984516d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder ../trained_models/adapters_40_2_4e4/ created\n",
      "Starting training..., iters: 20480\n",
      "Iter 1: Val loss 3.567, Val took 5.919s\n",
      "Iter 10: Train loss 3.240, Learning Rate 4.000e-04, It/sec 2.177, Tokens/sec 323.509, Trained Tokens 1486, Peak mem 19.940 GB\n",
      "Iter 20: Train loss 3.516, Learning Rate 4.000e-04, It/sec 2.156, Tokens/sec 371.258, Trained Tokens 3208, Peak mem 19.940 GB\n",
      "Iter 30: Train loss 3.566, Learning Rate 4.000e-04, It/sec 2.150, Tokens/sec 371.713, Trained Tokens 4937, Peak mem 19.940 GB\n",
      "Iter 40: Train loss 3.218, Learning Rate 4.000e-04, It/sec 2.230, Tokens/sec 354.734, Trained Tokens 6528, Peak mem 19.940 GB\n",
      "Iter 50: Val loss 3.359, Val took 3.511s\n",
      "Iter 50: Train loss 3.260, Learning Rate 4.000e-04, It/sec 42.085, Tokens/sec 6392.708, Trained Tokens 8047, Peak mem 19.940 GB\n",
      "Iter 60: Train loss 3.155, Learning Rate 4.000e-04, It/sec 2.071, Tokens/sec 334.501, Trained Tokens 9662, Peak mem 19.940 GB\n",
      "Iter 70: Train loss 2.957, Learning Rate 4.000e-04, It/sec 2.659, Tokens/sec 374.175, Trained Tokens 11069, Peak mem 19.940 GB\n",
      "Iter 80: Train loss 3.146, Learning Rate 4.000e-04, It/sec 2.168, Tokens/sec 373.686, Trained Tokens 12793, Peak mem 19.940 GB\n",
      "Iter 90: Train loss 3.085, Learning Rate 4.000e-04, It/sec 2.736, Tokens/sec 375.711, Trained Tokens 14166, Peak mem 19.940 GB\n",
      "Iter 100: Val loss 3.071, Val took 3.137s\n",
      "Iter 100: Train loss 3.073, Learning Rate 4.000e-04, It/sec 12.130, Tokens/sec 1982.071, Trained Tokens 15800, Peak mem 19.940 GB\n",
      "Iter 100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 2.995, Learning Rate 4.000e-04, It/sec 2.495, Tokens/sec 354.028, Trained Tokens 17219, Peak mem 19.940 GB\n",
      "Iter 120: Train loss 2.996, Learning Rate 4.000e-04, It/sec 2.092, Tokens/sec 361.029, Trained Tokens 18945, Peak mem 19.940 GB\n",
      "Iter 130: Train loss 2.769, Learning Rate 4.000e-04, It/sec 1.408, Tokens/sec 367.084, Trained Tokens 21553, Peak mem 19.940 GB\n",
      "Iter 140: Train loss 2.627, Learning Rate 4.000e-04, It/sec 1.808, Tokens/sec 397.859, Trained Tokens 23753, Peak mem 19.940 GB\n",
      "Iter 150: Val loss 2.852, Val took 3.517s\n",
      "Iter 150: Train loss 2.991, Learning Rate 4.000e-04, It/sec 25.713, Tokens/sec 6201.916, Trained Tokens 26165, Peak mem 19.940 GB\n",
      "Iter 160: Train loss 2.981, Learning Rate 4.000e-04, It/sec 2.503, Tokens/sec 355.629, Trained Tokens 27586, Peak mem 19.940 GB\n",
      "Iter 170: Train loss 2.810, Learning Rate 4.000e-04, It/sec 2.115, Tokens/sec 399.322, Trained Tokens 29474, Peak mem 19.940 GB\n",
      "Iter 180: Train loss 2.823, Learning Rate 4.000e-04, It/sec 3.107, Tokens/sec 362.270, Trained Tokens 30640, Peak mem 19.940 GB\n",
      "Iter 190: Train loss 2.752, Learning Rate 4.000e-04, It/sec 2.609, Tokens/sec 363.177, Trained Tokens 32032, Peak mem 19.940 GB\n",
      "Iter 200: Val loss 3.016, Val took 3.612s\n",
      "Iter 200: Train loss 2.711, Learning Rate 4.000e-04, It/sec 16.507, Tokens/sec 2868.884, Trained Tokens 33770, Peak mem 19.940 GB\n",
      "Iter 200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 2.728, Learning Rate 4.000e-04, It/sec 2.646, Tokens/sec 354.081, Trained Tokens 35108, Peak mem 19.940 GB\n",
      "Iter 220: Train loss 2.813, Learning Rate 4.000e-04, It/sec 2.708, Tokens/sec 363.926, Trained Tokens 36452, Peak mem 19.940 GB\n",
      "Iter 230: Train loss 2.557, Learning Rate 4.000e-04, It/sec 3.134, Tokens/sec 367.263, Trained Tokens 37624, Peak mem 19.940 GB\n",
      "Iter 240: Train loss 2.727, Learning Rate 4.000e-04, It/sec 2.304, Tokens/sec 385.270, Trained Tokens 39296, Peak mem 19.940 GB\n",
      "Iter 250: Val loss 2.822, Val took 3.122s\n",
      "Iter 250: Train loss 2.776, Learning Rate 4.000e-04, It/sec 12.154, Tokens/sec 1922.740, Trained Tokens 40878, Peak mem 19.940 GB\n",
      "Iter 260: Train loss 2.772, Learning Rate 4.000e-04, It/sec 2.853, Tokens/sec 367.523, Trained Tokens 42166, Peak mem 19.940 GB\n",
      "Iter 270: Train loss 2.653, Learning Rate 4.000e-04, It/sec 2.137, Tokens/sec 398.173, Trained Tokens 44029, Peak mem 19.940 GB\n",
      "Iter 280: Train loss 2.745, Learning Rate 4.000e-04, It/sec 2.471, Tokens/sec 407.196, Trained Tokens 45677, Peak mem 19.940 GB\n",
      "Iter 290: Train loss 2.765, Learning Rate 4.000e-04, It/sec 1.950, Tokens/sec 371.178, Trained Tokens 47580, Peak mem 19.940 GB\n",
      "Iter 300: Val loss 2.878, Val took 4.362s\n",
      "Iter 300: Train loss 2.565, Learning Rate 4.000e-04, It/sec 33.691, Tokens/sec 3985.702, Trained Tokens 48763, Peak mem 19.940 GB\n",
      "Iter 300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 2.729, Learning Rate 4.000e-04, It/sec 2.180, Tokens/sec 323.146, Trained Tokens 50245, Peak mem 19.940 GB\n",
      "Iter 320: Train loss 2.652, Learning Rate 4.000e-04, It/sec 2.766, Tokens/sec 378.093, Trained Tokens 51612, Peak mem 19.940 GB\n",
      "Iter 330: Train loss 2.560, Learning Rate 4.000e-04, It/sec 2.752, Tokens/sec 387.276, Trained Tokens 53019, Peak mem 19.940 GB\n",
      "Iter 340: Train loss 2.693, Learning Rate 4.000e-04, It/sec 1.194, Tokens/sec 384.093, Trained Tokens 56236, Peak mem 19.940 GB\n",
      "Iter 350: Val loss 2.661, Val took 3.230s\n",
      "Iter 350: Train loss 2.591, Learning Rate 4.000e-04, It/sec 16.368, Tokens/sec 2602.568, Trained Tokens 57826, Peak mem 19.940 GB\n",
      "Iter 360: Train loss 2.711, Learning Rate 4.000e-04, It/sec 2.719, Tokens/sec 361.687, Trained Tokens 59156, Peak mem 19.940 GB\n",
      "Iter 370: Train loss 2.490, Learning Rate 4.000e-04, It/sec 1.534, Tokens/sec 386.613, Trained Tokens 61677, Peak mem 19.940 GB\n",
      "Iter 380: Train loss 2.800, Learning Rate 4.000e-04, It/sec 1.833, Tokens/sec 350.661, Trained Tokens 63590, Peak mem 19.940 GB\n",
      "Iter 390: Train loss 2.724, Learning Rate 4.000e-04, It/sec 2.115, Tokens/sec 368.021, Trained Tokens 65330, Peak mem 19.940 GB\n",
      "Iter 400: Val loss 2.715, Val took 3.118s\n",
      "Iter 400: Train loss 2.434, Learning Rate 4.000e-04, It/sec 39.797, Tokens/sec 5233.290, Trained Tokens 66645, Peak mem 19.940 GB\n",
      "Iter 400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 2.448, Learning Rate 4.000e-04, It/sec 2.726, Tokens/sec 346.226, Trained Tokens 67915, Peak mem 19.940 GB\n",
      "Iter 420: Train loss 2.405, Learning Rate 4.000e-04, It/sec 2.487, Tokens/sec 377.022, Trained Tokens 69431, Peak mem 19.940 GB\n",
      "Iter 430: Train loss 2.288, Learning Rate 4.000e-04, It/sec 2.384, Tokens/sec 379.595, Trained Tokens 71023, Peak mem 19.940 GB\n",
      "Iter 440: Train loss 2.235, Learning Rate 4.000e-04, It/sec 2.183, Tokens/sec 364.569, Trained Tokens 72693, Peak mem 19.940 GB\n",
      "Iter 450: Val loss 2.732, Val took 4.242s\n",
      "Iter 450: Train loss 2.394, Learning Rate 4.000e-04, It/sec 9.908, Tokens/sec 2207.400, Trained Tokens 74921, Peak mem 19.940 GB\n",
      "Iter 460: Train loss 2.260, Learning Rate 4.000e-04, It/sec 2.019, Tokens/sec 311.281, Trained Tokens 76463, Peak mem 19.940 GB\n",
      "Iter 470: Train loss 2.393, Learning Rate 4.000e-04, It/sec 1.928, Tokens/sec 388.981, Trained Tokens 78481, Peak mem 19.940 GB\n",
      "Iter 480: Train loss 2.415, Learning Rate 4.000e-04, It/sec 2.822, Tokens/sec 355.866, Trained Tokens 79742, Peak mem 19.940 GB\n",
      "Iter 490: Train loss 2.463, Learning Rate 4.000e-04, It/sec 2.374, Tokens/sec 365.123, Trained Tokens 81280, Peak mem 19.940 GB\n",
      "Iter 500: Val loss 2.663, Val took 3.514s\n",
      "Iter 500: Train loss 2.311, Learning Rate 4.000e-04, It/sec 41.464, Tokens/sec 5411.039, Trained Tokens 82585, Peak mem 19.940 GB\n",
      "Iter 500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 2.313, Learning Rate 4.000e-04, It/sec 2.641, Tokens/sec 345.940, Trained Tokens 83895, Peak mem 19.940 GB\n",
      "Iter 520: Train loss 2.210, Learning Rate 4.000e-04, It/sec 2.746, Tokens/sec 361.098, Trained Tokens 85210, Peak mem 19.940 GB\n",
      "Iter 530: Train loss 2.439, Learning Rate 4.000e-04, It/sec 2.578, Tokens/sec 389.490, Trained Tokens 86721, Peak mem 19.940 GB\n",
      "Iter 540: Train loss 2.698, Learning Rate 4.000e-04, It/sec 1.529, Tokens/sec 374.646, Trained Tokens 89172, Peak mem 19.940 GB\n",
      "Iter 550: Val loss 2.555, Val took 3.031s\n",
      "Iter 550: Train loss 2.236, Learning Rate 4.000e-04, It/sec 27.037, Tokens/sec 3933.835, Trained Tokens 90627, Peak mem 19.940 GB\n",
      "Iter 560: Train loss 2.046, Learning Rate 4.000e-04, It/sec 2.265, Tokens/sec 328.595, Trained Tokens 92078, Peak mem 19.940 GB\n",
      "Iter 570: Train loss 2.496, Learning Rate 4.000e-04, It/sec 1.662, Tokens/sec 399.826, Trained Tokens 94483, Peak mem 19.940 GB\n",
      "Iter 580: Train loss 2.484, Learning Rate 4.000e-04, It/sec 2.118, Tokens/sec 343.341, Trained Tokens 96104, Peak mem 19.940 GB\n",
      "Iter 590: Train loss 2.291, Learning Rate 4.000e-04, It/sec 2.303, Tokens/sec 383.402, Trained Tokens 97769, Peak mem 19.940 GB\n",
      "Iter 600: Val loss 2.454, Val took 3.220s\n",
      "Iter 600: Train loss 2.446, Learning Rate 4.000e-04, It/sec 8.419, Tokens/sec 1473.288, Trained Tokens 99519, Peak mem 19.940 GB\n",
      "Iter 600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 2.218, Learning Rate 4.000e-04, It/sec 2.500, Tokens/sec 399.044, Trained Tokens 101115, Peak mem 19.940 GB\n",
      "Iter 620: Train loss 2.569, Learning Rate 4.000e-04, It/sec 2.258, Tokens/sec 367.302, Trained Tokens 102742, Peak mem 19.940 GB\n",
      "Iter 630: Train loss 2.410, Learning Rate 4.000e-04, It/sec 2.350, Tokens/sec 385.640, Trained Tokens 104383, Peak mem 19.940 GB\n",
      "Iter 640: Train loss 2.394, Learning Rate 4.000e-04, It/sec 2.327, Tokens/sec 379.575, Trained Tokens 106014, Peak mem 19.940 GB\n",
      "Iter 650: Val loss 2.563, Val took 2.771s\n",
      "Iter 650: Train loss 2.353, Learning Rate 4.000e-04, It/sec 18.179, Tokens/sec 2595.940, Trained Tokens 107442, Peak mem 19.940 GB\n",
      "Iter 660: Train loss 2.197, Learning Rate 4.000e-04, It/sec 2.909, Tokens/sec 346.115, Trained Tokens 108632, Peak mem 19.940 GB\n",
      "Iter 670: Train loss 2.289, Learning Rate 4.000e-04, It/sec 2.666, Tokens/sec 386.356, Trained Tokens 110081, Peak mem 19.940 GB\n",
      "Iter 680: Train loss 2.237, Learning Rate 4.000e-04, It/sec 2.309, Tokens/sec 366.849, Trained Tokens 111670, Peak mem 19.940 GB\n",
      "Iter 690: Train loss 2.430, Learning Rate 4.000e-04, It/sec 2.457, Tokens/sec 373.705, Trained Tokens 113191, Peak mem 19.940 GB\n",
      "Iter 700: Val loss 2.740, Val took 4.258s\n",
      "Iter 700: Train loss 2.302, Learning Rate 4.000e-04, It/sec 31.535, Tokens/sec 5067.735, Trained Tokens 114798, Peak mem 19.940 GB\n",
      "Iter 700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 2.411, Learning Rate 4.000e-04, It/sec 1.791, Tokens/sec 386.645, Trained Tokens 116957, Peak mem 19.940 GB\n",
      "Iter 720: Train loss 2.256, Learning Rate 4.000e-04, It/sec 2.610, Tokens/sec 381.579, Trained Tokens 118419, Peak mem 19.940 GB\n",
      "Iter 730: Train loss 2.147, Learning Rate 4.000e-04, It/sec 4.018, Tokens/sec 362.059, Trained Tokens 119320, Peak mem 19.940 GB\n",
      "Iter 740: Train loss 2.383, Learning Rate 4.000e-04, It/sec 2.444, Tokens/sec 388.831, Trained Tokens 120911, Peak mem 19.940 GB\n",
      "Iter 750: Val loss 2.672, Val took 3.884s\n",
      "Iter 750: Train loss 2.281, Learning Rate 4.000e-04, It/sec 23.763, Tokens/sec 3856.684, Trained Tokens 122534, Peak mem 19.940 GB\n",
      "Iter 760: Train loss 2.299, Learning Rate 4.000e-04, It/sec 1.633, Tokens/sec 382.602, Trained Tokens 124877, Peak mem 19.940 GB\n",
      "Iter 770: Train loss 2.446, Learning Rate 4.000e-04, It/sec 2.217, Tokens/sec 358.733, Trained Tokens 126495, Peak mem 19.940 GB\n",
      "Iter 780: Train loss 2.224, Learning Rate 4.000e-04, It/sec 2.783, Tokens/sec 365.655, Trained Tokens 127809, Peak mem 19.940 GB\n",
      "Iter 790: Train loss 2.374, Learning Rate 4.000e-04, It/sec 1.359, Tokens/sec 388.813, Trained Tokens 130670, Peak mem 19.940 GB\n",
      "Iter 800: Val loss 2.514, Val took 2.816s\n",
      "Iter 800: Train loss 2.506, Learning Rate 4.000e-04, It/sec 6.950, Tokens/sec 1257.882, Trained Tokens 132480, Peak mem 19.940 GB\n",
      "Iter 800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 2.256, Learning Rate 4.000e-04, It/sec 2.013, Tokens/sec 389.630, Trained Tokens 134416, Peak mem 19.940 GB\n",
      "Iter 820: Train loss 2.157, Learning Rate 4.000e-04, It/sec 2.744, Tokens/sec 388.061, Trained Tokens 135830, Peak mem 19.940 GB\n",
      "Iter 830: Train loss 2.077, Learning Rate 4.000e-04, It/sec 2.559, Tokens/sec 371.572, Trained Tokens 137282, Peak mem 19.940 GB\n",
      "Iter 840: Train loss 2.191, Learning Rate 4.000e-04, It/sec 1.376, Tokens/sec 409.611, Trained Tokens 140259, Peak mem 19.940 GB\n",
      "Iter 850: Val loss 2.382, Val took 3.388s\n",
      "Iter 850: Train loss 2.101, Learning Rate 4.000e-04, It/sec 35.588, Tokens/sec 4284.741, Trained Tokens 141463, Peak mem 19.940 GB\n",
      "Iter 860: Train loss 1.964, Learning Rate 4.000e-04, It/sec 1.901, Tokens/sec 378.957, Trained Tokens 143456, Peak mem 19.940 GB\n",
      "Iter 870: Train loss 2.018, Learning Rate 4.000e-04, It/sec 2.004, Tokens/sec 380.995, Trained Tokens 145357, Peak mem 19.940 GB\n",
      "Iter 880: Train loss 2.084, Learning Rate 4.000e-04, It/sec 2.614, Tokens/sec 376.350, Trained Tokens 146797, Peak mem 19.940 GB\n",
      "Iter 890: Train loss 1.982, Learning Rate 4.000e-04, It/sec 2.565, Tokens/sec 389.416, Trained Tokens 148315, Peak mem 19.940 GB\n",
      "Iter 900: Val loss 2.436, Val took 2.751s\n",
      "Iter 900: Train loss 2.033, Learning Rate 4.000e-04, It/sec 24.891, Tokens/sec 4186.592, Trained Tokens 149997, Peak mem 19.940 GB\n",
      "Iter 900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 1.907, Learning Rate 4.000e-04, It/sec 2.689, Tokens/sec 372.431, Trained Tokens 151382, Peak mem 19.940 GB\n",
      "Iter 920: Train loss 2.101, Learning Rate 4.000e-04, It/sec 1.483, Tokens/sec 377.769, Trained Tokens 153930, Peak mem 19.940 GB\n",
      "Iter 930: Train loss 1.921, Learning Rate 4.000e-04, It/sec 2.615, Tokens/sec 337.561, Trained Tokens 155221, Peak mem 19.940 GB\n",
      "Iter 940: Train loss 2.098, Learning Rate 4.000e-04, It/sec 1.805, Tokens/sec 376.171, Trained Tokens 157305, Peak mem 19.940 GB\n",
      "Iter 950: Val loss 2.610, Val took 3.452s\n",
      "Iter 950: Train loss 2.231, Learning Rate 4.000e-04, It/sec 39.985, Tokens/sec 6273.624, Trained Tokens 158874, Peak mem 19.940 GB\n",
      "Iter 960: Train loss 2.238, Learning Rate 4.000e-04, It/sec 1.370, Tokens/sec 350.657, Trained Tokens 161434, Peak mem 19.940 GB\n",
      "Iter 970: Train loss 2.152, Learning Rate 4.000e-04, It/sec 2.257, Tokens/sec 407.478, Trained Tokens 163239, Peak mem 19.940 GB\n",
      "Iter 980: Train loss 2.124, Learning Rate 4.000e-04, It/sec 2.179, Tokens/sec 383.641, Trained Tokens 165000, Peak mem 19.940 GB\n",
      "Iter 990: Train loss 1.758, Learning Rate 4.000e-04, It/sec 2.690, Tokens/sec 377.947, Trained Tokens 166405, Peak mem 19.940 GB\n",
      "Iter 1000: Val loss 2.618, Val took 3.732s\n",
      "Iter 1000: Train loss 2.156, Learning Rate 4.000e-04, It/sec 20.875, Tokens/sec 2924.606, Trained Tokens 167806, Peak mem 19.940 GB\n",
      "Iter 1000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001000_adapters.safetensors.\n",
      "Iter 1010: Train loss 2.085, Learning Rate 4.000e-04, It/sec 2.030, Tokens/sec 294.346, Trained Tokens 169256, Peak mem 19.940 GB\n",
      "Iter 1020: Train loss 2.029, Learning Rate 4.000e-04, It/sec 3.116, Tokens/sec 370.489, Trained Tokens 170445, Peak mem 19.940 GB\n",
      "Iter 1030: Train loss 2.072, Learning Rate 4.000e-04, It/sec 2.662, Tokens/sec 396.356, Trained Tokens 171934, Peak mem 19.940 GB\n",
      "Iter 1040: Train loss 2.211, Learning Rate 4.000e-04, It/sec 2.039, Tokens/sec 377.817, Trained Tokens 173787, Peak mem 19.940 GB\n",
      "Iter 1050: Val loss 2.567, Val took 3.592s\n",
      "Iter 1050: Train loss 2.192, Learning Rate 4.000e-04, It/sec 25.580, Tokens/sec 3952.083, Trained Tokens 175332, Peak mem 19.940 GB\n",
      "Iter 1060: Train loss 2.131, Learning Rate 4.000e-04, It/sec 1.911, Tokens/sec 395.925, Trained Tokens 177404, Peak mem 19.940 GB\n",
      "Iter 1070: Train loss 2.096, Learning Rate 4.000e-04, It/sec 2.319, Tokens/sec 390.727, Trained Tokens 179089, Peak mem 19.940 GB\n",
      "Iter 1080: Train loss 1.954, Learning Rate 4.000e-04, It/sec 1.982, Tokens/sec 310.199, Trained Tokens 180654, Peak mem 19.940 GB\n",
      "Iter 1090: Train loss 2.300, Learning Rate 4.000e-04, It/sec 2.098, Tokens/sec 286.145, Trained Tokens 182018, Peak mem 19.940 GB\n",
      "Iter 1100: Val loss 2.368, Val took 4.612s\n",
      "Iter 1100: Train loss 2.261, Learning Rate 4.000e-04, It/sec 8.315, Tokens/sec 1650.589, Trained Tokens 184003, Peak mem 19.940 GB\n",
      "Iter 1100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001100_adapters.safetensors.\n",
      "Iter 1110: Train loss 2.115, Learning Rate 4.000e-04, It/sec 2.540, Tokens/sec 374.416, Trained Tokens 185477, Peak mem 19.940 GB\n",
      "Iter 1120: Train loss 1.969, Learning Rate 4.000e-04, It/sec 3.161, Tokens/sec 371.384, Trained Tokens 186652, Peak mem 19.940 GB\n",
      "Iter 1130: Train loss 2.509, Learning Rate 4.000e-04, It/sec 1.914, Tokens/sec 407.780, Trained Tokens 188783, Peak mem 19.940 GB\n",
      "Iter 1140: Train loss 1.984, Learning Rate 4.000e-04, It/sec 2.630, Tokens/sec 387.687, Trained Tokens 190257, Peak mem 19.940 GB\n",
      "Iter 1150: Val loss 2.526, Val took 3.454s\n",
      "Iter 1150: Train loss 2.248, Learning Rate 4.000e-04, It/sec 26.031, Tokens/sec 4086.926, Trained Tokens 191827, Peak mem 19.940 GB\n",
      "Iter 1160: Train loss 1.988, Learning Rate 4.000e-04, It/sec 2.709, Tokens/sec 392.734, Trained Tokens 193277, Peak mem 19.940 GB\n",
      "Iter 1170: Train loss 2.078, Learning Rate 4.000e-04, It/sec 2.864, Tokens/sec 382.622, Trained Tokens 194613, Peak mem 19.940 GB\n",
      "Iter 1180: Train loss 2.004, Learning Rate 4.000e-04, It/sec 2.879, Tokens/sec 366.505, Trained Tokens 195886, Peak mem 19.940 GB\n",
      "Iter 1190: Train loss 2.207, Learning Rate 4.000e-04, It/sec 2.421, Tokens/sec 389.099, Trained Tokens 197493, Peak mem 19.940 GB\n",
      "Iter 1200: Val loss 2.508, Val took 3.367s\n",
      "Iter 1200: Train loss 2.168, Learning Rate 4.000e-04, It/sec 27.672, Tokens/sec 5061.288, Trained Tokens 199322, Peak mem 19.940 GB\n",
      "Iter 1200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001200_adapters.safetensors.\n",
      "Iter 1210: Train loss 2.116, Learning Rate 4.000e-04, It/sec 2.300, Tokens/sec 345.198, Trained Tokens 200823, Peak mem 19.940 GB\n",
      "Iter 1220: Train loss 1.884, Learning Rate 4.000e-04, It/sec 2.462, Tokens/sec 375.447, Trained Tokens 202348, Peak mem 19.940 GB\n",
      "Iter 1230: Train loss 2.205, Learning Rate 4.000e-04, It/sec 2.822, Tokens/sec 394.288, Trained Tokens 203745, Peak mem 19.940 GB\n",
      "Iter 1240: Train loss 1.929, Learning Rate 4.000e-04, It/sec 2.578, Tokens/sec 404.244, Trained Tokens 205313, Peak mem 19.940 GB\n",
      "Iter 1250: Val loss 2.339, Val took 3.208s\n",
      "Iter 1250: Train loss 1.779, Learning Rate 4.000e-04, It/sec 15.836, Tokens/sec 2517.956, Trained Tokens 206903, Peak mem 19.940 GB\n",
      "Iter 1260: Train loss 1.920, Learning Rate 4.000e-04, It/sec 1.853, Tokens/sec 341.933, Trained Tokens 208748, Peak mem 19.940 GB\n",
      "Iter 1270: Train loss 1.952, Learning Rate 4.000e-04, It/sec 2.249, Tokens/sec 390.823, Trained Tokens 210486, Peak mem 19.940 GB\n",
      "Iter 1280: Train loss 1.792, Learning Rate 4.000e-04, It/sec 2.519, Tokens/sec 419.643, Trained Tokens 212152, Peak mem 19.940 GB\n",
      "Iter 1290: Train loss 1.629, Learning Rate 4.000e-04, It/sec 3.212, Tokens/sec 373.874, Trained Tokens 213316, Peak mem 19.940 GB\n",
      "Iter 1300: Val loss 2.556, Val took 3.411s\n",
      "Iter 1300: Train loss 2.132, Learning Rate 4.000e-04, It/sec 24.914, Tokens/sec 6240.967, Trained Tokens 215821, Peak mem 19.940 GB\n",
      "Iter 1300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001300_adapters.safetensors.\n",
      "Iter 1310: Train loss 2.098, Learning Rate 4.000e-04, It/sec 2.288, Tokens/sec 400.658, Trained Tokens 217572, Peak mem 19.940 GB\n",
      "Iter 1320: Train loss 1.968, Learning Rate 4.000e-04, It/sec 1.949, Tokens/sec 394.149, Trained Tokens 219594, Peak mem 19.940 GB\n",
      "Iter 1330: Train loss 1.643, Learning Rate 4.000e-04, It/sec 3.098, Tokens/sec 371.090, Trained Tokens 220792, Peak mem 19.940 GB\n",
      "Iter 1340: Train loss 2.030, Learning Rate 4.000e-04, It/sec 2.039, Tokens/sec 384.785, Trained Tokens 222679, Peak mem 19.940 GB\n",
      "Iter 1350: Val loss 2.275, Val took 3.188s\n",
      "Iter 1350: Train loss 1.826, Learning Rate 4.000e-04, It/sec 27.686, Tokens/sec 3449.669, Trained Tokens 223925, Peak mem 19.940 GB\n",
      "Iter 1360: Train loss 1.821, Learning Rate 4.000e-04, It/sec 2.654, Tokens/sec 386.677, Trained Tokens 225382, Peak mem 19.940 GB\n",
      "Iter 1370: Train loss 1.720, Learning Rate 4.000e-04, It/sec 3.248, Tokens/sec 349.785, Trained Tokens 226459, Peak mem 19.940 GB\n",
      "Iter 1380: Train loss 1.922, Learning Rate 4.000e-04, It/sec 2.414, Tokens/sec 365.207, Trained Tokens 227972, Peak mem 19.940 GB\n",
      "Iter 1390: Train loss 1.954, Learning Rate 4.000e-04, It/sec 2.147, Tokens/sec 396.845, Trained Tokens 229820, Peak mem 19.940 GB\n",
      "Iter 1400: Val loss 2.381, Val took 2.675s\n",
      "Iter 1400: Train loss 2.133, Learning Rate 4.000e-04, It/sec 14.987, Tokens/sec 4241.365, Trained Tokens 232650, Peak mem 19.940 GB\n",
      "Iter 1400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001400_adapters.safetensors.\n",
      "Iter 1410: Train loss 1.880, Learning Rate 4.000e-04, It/sec 2.158, Tokens/sec 349.145, Trained Tokens 234268, Peak mem 19.940 GB\n",
      "Iter 1420: Train loss 1.977, Learning Rate 4.000e-04, It/sec 2.036, Tokens/sec 393.527, Trained Tokens 236201, Peak mem 19.940 GB\n",
      "Iter 1430: Train loss 1.779, Learning Rate 4.000e-04, It/sec 2.131, Tokens/sec 359.037, Trained Tokens 237886, Peak mem 19.940 GB\n",
      "Iter 1440: Train loss 1.999, Learning Rate 4.000e-04, It/sec 1.723, Tokens/sec 391.214, Trained Tokens 240156, Peak mem 19.940 GB\n",
      "Iter 1450: Val loss 2.616, Val took 4.236s\n",
      "Iter 1450: Train loss 1.742, Learning Rate 4.000e-04, It/sec 43.877, Tokens/sec 5304.696, Trained Tokens 241365, Peak mem 19.940 GB\n",
      "Iter 1460: Train loss 1.986, Learning Rate 4.000e-04, It/sec 2.167, Tokens/sec 388.955, Trained Tokens 243160, Peak mem 19.940 GB\n",
      "Iter 1470: Train loss 1.902, Learning Rate 4.000e-04, It/sec 2.113, Tokens/sec 392.166, Trained Tokens 245016, Peak mem 19.940 GB\n",
      "Iter 1480: Train loss 1.651, Learning Rate 4.000e-04, It/sec 2.942, Tokens/sec 363.972, Trained Tokens 246253, Peak mem 19.940 GB\n",
      "Iter 1490: Train loss 1.752, Learning Rate 4.000e-04, It/sec 2.382, Tokens/sec 373.299, Trained Tokens 247820, Peak mem 19.940 GB\n",
      "Iter 1500: Val loss 2.431, Val took 3.869s\n",
      "Iter 1500: Train loss 1.759, Learning Rate 4.000e-04, It/sec 32.329, Tokens/sec 3963.573, Trained Tokens 249046, Peak mem 19.940 GB\n",
      "Iter 1500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001500_adapters.safetensors.\n",
      "Iter 1510: Train loss 1.792, Learning Rate 4.000e-04, It/sec 2.520, Tokens/sec 345.936, Trained Tokens 250419, Peak mem 19.940 GB\n",
      "Iter 1520: Train loss 2.176, Learning Rate 4.000e-04, It/sec 2.484, Tokens/sec 368.561, Trained Tokens 251903, Peak mem 19.940 GB\n",
      "Iter 1530: Train loss 2.040, Learning Rate 4.000e-04, It/sec 2.401, Tokens/sec 376.707, Trained Tokens 253472, Peak mem 19.940 GB\n",
      "Iter 1540: Train loss 1.790, Learning Rate 4.000e-04, It/sec 3.002, Tokens/sec 371.595, Trained Tokens 254710, Peak mem 19.940 GB\n",
      "Iter 1550: Val loss 2.201, Val took 3.016s\n",
      "Iter 1550: Train loss 1.806, Learning Rate 4.000e-04, It/sec 45.373, Tokens/sec 7681.644, Trained Tokens 256403, Peak mem 19.940 GB\n",
      "Iter 1560: Train loss 1.926, Learning Rate 4.000e-04, It/sec 2.677, Tokens/sec 346.456, Trained Tokens 257697, Peak mem 19.940 GB\n",
      "Iter 1570: Train loss 1.903, Learning Rate 4.000e-04, It/sec 2.243, Tokens/sec 384.172, Trained Tokens 259410, Peak mem 19.940 GB\n",
      "Iter 1580: Train loss 1.958, Learning Rate 4.000e-04, It/sec 1.401, Tokens/sec 402.604, Trained Tokens 262284, Peak mem 19.940 GB\n",
      "Iter 1590: Train loss 2.087, Learning Rate 4.000e-04, It/sec 1.980, Tokens/sec 359.712, Trained Tokens 264101, Peak mem 19.940 GB\n",
      "Iter 1600: Val loss 2.215, Val took 2.717s\n",
      "Iter 1600: Train loss 1.970, Learning Rate 4.000e-04, It/sec 5.459, Tokens/sec 898.480, Trained Tokens 265747, Peak mem 19.940 GB\n",
      "Iter 1600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001600_adapters.safetensors.\n",
      "Iter 1610: Train loss 2.092, Learning Rate 4.000e-04, It/sec 2.850, Tokens/sec 370.190, Trained Tokens 267046, Peak mem 19.940 GB\n",
      "Iter 1620: Train loss 2.125, Learning Rate 4.000e-04, It/sec 2.252, Tokens/sec 384.199, Trained Tokens 268752, Peak mem 19.940 GB\n",
      "Iter 1630: Train loss 1.876, Learning Rate 4.000e-04, It/sec 2.380, Tokens/sec 408.867, Trained Tokens 270470, Peak mem 19.940 GB\n",
      "Iter 1640: Train loss 1.788, Learning Rate 4.000e-04, It/sec 3.073, Tokens/sec 365.713, Trained Tokens 271660, Peak mem 19.940 GB\n",
      "Iter 1650: Val loss 2.202, Val took 2.726s\n",
      "Iter 1650: Train loss 1.765, Learning Rate 4.000e-04, It/sec 27.504, Tokens/sec 4986.478, Trained Tokens 273473, Peak mem 19.940 GB\n",
      "Iter 1660: Train loss 1.479, Learning Rate 4.000e-04, It/sec 3.154, Tokens/sec 389.246, Trained Tokens 274707, Peak mem 19.940 GB\n",
      "Iter 1670: Train loss 1.855, Learning Rate 4.000e-04, It/sec 1.840, Tokens/sec 394.768, Trained Tokens 276853, Peak mem 19.940 GB\n",
      "Iter 1680: Train loss 1.698, Learning Rate 4.000e-04, It/sec 1.837, Tokens/sec 381.072, Trained Tokens 278927, Peak mem 19.940 GB\n",
      "Iter 1690: Train loss 1.667, Learning Rate 4.000e-04, It/sec 2.574, Tokens/sec 386.430, Trained Tokens 280428, Peak mem 19.940 GB\n",
      "Iter 1700: Val loss 2.516, Val took 3.461s\n",
      "Iter 1700: Train loss 1.441, Learning Rate 4.000e-04, It/sec 25.279, Tokens/sec 3066.340, Trained Tokens 281641, Peak mem 19.940 GB\n",
      "Iter 1700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001700_adapters.safetensors.\n",
      "Iter 1710: Train loss 1.682, Learning Rate 4.000e-04, It/sec 2.030, Tokens/sec 373.980, Trained Tokens 283483, Peak mem 19.940 GB\n",
      "Iter 1720: Train loss 1.861, Learning Rate 4.000e-04, It/sec 2.288, Tokens/sec 383.713, Trained Tokens 285160, Peak mem 19.940 GB\n",
      "Iter 1730: Train loss 1.708, Learning Rate 4.000e-04, It/sec 2.963, Tokens/sec 375.748, Trained Tokens 286428, Peak mem 19.940 GB\n",
      "Iter 1740: Train loss 1.608, Learning Rate 4.000e-04, It/sec 2.627, Tokens/sec 384.072, Trained Tokens 287890, Peak mem 19.940 GB\n",
      "Iter 1750: Val loss 2.431, Val took 3.211s\n",
      "Iter 1750: Train loss 1.749, Learning Rate 4.000e-04, It/sec 40.092, Tokens/sec 7505.247, Trained Tokens 289762, Peak mem 19.940 GB\n",
      "Iter 1760: Train loss 1.883, Learning Rate 4.000e-04, It/sec 1.459, Tokens/sec 370.990, Trained Tokens 292305, Peak mem 19.940 GB\n",
      "Iter 1770: Train loss 1.715, Learning Rate 4.000e-04, It/sec 2.722, Tokens/sec 382.718, Trained Tokens 293711, Peak mem 19.940 GB\n",
      "Iter 1780: Train loss 1.591, Learning Rate 4.000e-04, It/sec 2.241, Tokens/sec 347.105, Trained Tokens 295260, Peak mem 19.940 GB\n",
      "Iter 1790: Train loss 1.603, Learning Rate 4.000e-04, It/sec 3.332, Tokens/sec 373.824, Trained Tokens 296382, Peak mem 19.940 GB\n",
      "Iter 1800: Val loss 2.380, Val took 3.461s\n",
      "Iter 1800: Train loss 1.734, Learning Rate 4.000e-04, It/sec 42.756, Tokens/sec 6943.575, Trained Tokens 298006, Peak mem 19.940 GB\n",
      "Iter 1800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001800_adapters.safetensors.\n",
      "Iter 1810: Train loss 1.621, Learning Rate 4.000e-04, It/sec 2.980, Tokens/sec 378.508, Trained Tokens 299276, Peak mem 19.940 GB\n",
      "Iter 1820: Train loss 1.677, Learning Rate 4.000e-04, It/sec 2.387, Tokens/sec 380.451, Trained Tokens 300870, Peak mem 19.940 GB\n",
      "Iter 1830: Train loss 1.720, Learning Rate 4.000e-04, It/sec 1.841, Tokens/sec 382.448, Trained Tokens 302947, Peak mem 19.940 GB\n",
      "Iter 1840: Train loss 1.765, Learning Rate 4.000e-04, It/sec 2.074, Tokens/sec 390.715, Trained Tokens 304831, Peak mem 19.940 GB\n",
      "Iter 1850: Val loss 2.320, Val took 3.793s\n",
      "Iter 1850: Train loss 1.802, Learning Rate 4.000e-04, It/sec 57.361, Tokens/sec 14357.508, Trained Tokens 307334, Peak mem 19.940 GB\n",
      "Iter 1860: Train loss 1.997, Learning Rate 4.000e-04, It/sec 1.247, Tokens/sec 359.741, Trained Tokens 310220, Peak mem 19.940 GB\n",
      "Iter 1870: Train loss 1.683, Learning Rate 4.000e-04, It/sec 2.488, Tokens/sec 397.164, Trained Tokens 311816, Peak mem 19.940 GB\n",
      "Iter 1880: Train loss 1.649, Learning Rate 4.000e-04, It/sec 2.365, Tokens/sec 392.070, Trained Tokens 313474, Peak mem 19.940 GB\n",
      "Iter 1890: Train loss 1.841, Learning Rate 4.000e-04, It/sec 2.631, Tokens/sec 395.387, Trained Tokens 314977, Peak mem 19.940 GB\n",
      "Iter 1900: Val loss 2.256, Val took 2.496s\n",
      "Iter 1900: Train loss 1.473, Learning Rate 4.000e-04, It/sec 44.487, Tokens/sec 4075.039, Trained Tokens 315893, Peak mem 19.940 GB\n",
      "Iter 1900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0001900_adapters.safetensors.\n",
      "Iter 1910: Train loss 1.980, Learning Rate 4.000e-04, It/sec 2.225, Tokens/sec 393.780, Trained Tokens 317663, Peak mem 19.940 GB\n",
      "Iter 1920: Train loss 1.928, Learning Rate 4.000e-04, It/sec 2.559, Tokens/sec 390.784, Trained Tokens 319190, Peak mem 19.940 GB\n",
      "Iter 1930: Train loss 1.692, Learning Rate 4.000e-04, It/sec 2.566, Tokens/sec 403.824, Trained Tokens 320764, Peak mem 19.940 GB\n",
      "Iter 1940: Train loss 1.721, Learning Rate 4.000e-04, It/sec 2.856, Tokens/sec 387.333, Trained Tokens 322120, Peak mem 19.940 GB\n",
      "Iter 1950: Val loss 2.098, Val took 2.955s\n",
      "Iter 1950: Train loss 1.943, Learning Rate 4.000e-04, It/sec 18.270, Tokens/sec 3347.133, Trained Tokens 323952, Peak mem 19.940 GB\n",
      "Iter 1960: Train loss 1.743, Learning Rate 4.000e-04, It/sec 2.236, Tokens/sec 325.982, Trained Tokens 325410, Peak mem 19.940 GB\n",
      "Iter 1970: Train loss 1.841, Learning Rate 4.000e-04, It/sec 2.762, Tokens/sec 356.624, Trained Tokens 326701, Peak mem 19.940 GB\n",
      "Iter 1980: Train loss 2.028, Learning Rate 4.000e-04, It/sec 1.747, Tokens/sec 392.423, Trained Tokens 328947, Peak mem 19.940 GB\n",
      "Iter 1990: Train loss 1.884, Learning Rate 4.000e-04, It/sec 2.739, Tokens/sec 374.359, Trained Tokens 330314, Peak mem 19.940 GB\n",
      "Iter 2000: Val loss 2.396, Val took 3.901s\n",
      "Iter 2000: Train loss 1.849, Learning Rate 4.000e-04, It/sec 41.660, Tokens/sec 6040.721, Trained Tokens 331764, Peak mem 19.940 GB\n",
      "Iter 2000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002000_adapters.safetensors.\n",
      "Iter 2010: Train loss 1.795, Learning Rate 4.000e-04, It/sec 2.657, Tokens/sec 382.008, Trained Tokens 333202, Peak mem 19.940 GB\n",
      "Iter 2020: Train loss 1.693, Learning Rate 4.000e-04, It/sec 2.307, Tokens/sec 396.331, Trained Tokens 334920, Peak mem 19.940 GB\n",
      "Iter 2030: Train loss 1.882, Learning Rate 4.000e-04, It/sec 1.894, Tokens/sec 319.457, Trained Tokens 336607, Peak mem 19.940 GB\n",
      "Iter 2040: Train loss 1.761, Learning Rate 4.000e-04, It/sec 2.380, Tokens/sec 394.925, Trained Tokens 338266, Peak mem 19.940 GB\n",
      "Iter 2050: Val loss 2.433, Val took 4.057s\n",
      "Iter 2050: Train loss 1.696, Learning Rate 4.000e-04, It/sec 45.551, Tokens/sec 5962.689, Trained Tokens 339575, Peak mem 19.940 GB\n",
      "Iter 2060: Train loss 1.543, Learning Rate 4.000e-04, It/sec 1.411, Tokens/sec 326.390, Trained Tokens 341888, Peak mem 19.940 GB\n",
      "Iter 2070: Train loss 1.520, Learning Rate 4.000e-04, It/sec 2.369, Tokens/sec 351.540, Trained Tokens 343372, Peak mem 19.940 GB\n",
      "Iter 2080: Train loss 1.414, Learning Rate 4.000e-04, It/sec 2.429, Tokens/sec 382.333, Trained Tokens 344946, Peak mem 19.940 GB\n",
      "Iter 2090: Train loss 1.674, Learning Rate 4.000e-04, It/sec 1.847, Tokens/sec 397.380, Trained Tokens 347098, Peak mem 19.940 GB\n",
      "Iter 2100: Val loss 2.385, Val took 3.906s\n",
      "Iter 2100: Train loss 1.539, Learning Rate 4.000e-04, It/sec 19.554, Tokens/sec 3238.062, Trained Tokens 348754, Peak mem 19.940 GB\n",
      "Iter 2100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002100_adapters.safetensors.\n",
      "Iter 2110: Train loss 1.741, Learning Rate 4.000e-04, It/sec 1.988, Tokens/sec 386.810, Trained Tokens 350700, Peak mem 19.940 GB\n",
      "Iter 2120: Train loss 1.427, Learning Rate 4.000e-04, It/sec 2.635, Tokens/sec 383.178, Trained Tokens 352154, Peak mem 19.940 GB\n",
      "Iter 2130: Train loss 1.701, Learning Rate 4.000e-04, It/sec 2.171, Tokens/sec 364.439, Trained Tokens 353833, Peak mem 19.940 GB\n",
      "Iter 2140: Train loss 1.554, Learning Rate 4.000e-04, It/sec 2.504, Tokens/sec 351.262, Trained Tokens 355236, Peak mem 19.940 GB\n",
      "Iter 2150: Val loss 2.392, Val took 3.791s\n",
      "Iter 2150: Train loss 1.584, Learning Rate 4.000e-04, It/sec 17.920, Tokens/sec 3333.142, Trained Tokens 357096, Peak mem 19.940 GB\n",
      "Iter 2160: Train loss 1.630, Learning Rate 4.000e-04, It/sec 2.612, Tokens/sec 377.649, Trained Tokens 358542, Peak mem 19.940 GB\n",
      "Iter 2170: Train loss 1.575, Learning Rate 4.000e-04, It/sec 2.896, Tokens/sec 366.905, Trained Tokens 359809, Peak mem 19.940 GB\n",
      "Iter 2180: Train loss 1.800, Learning Rate 4.000e-04, It/sec 2.398, Tokens/sec 393.288, Trained Tokens 361449, Peak mem 19.940 GB\n",
      "Iter 2190: Train loss 1.882, Learning Rate 4.000e-04, It/sec 2.125, Tokens/sec 376.519, Trained Tokens 363221, Peak mem 19.940 GB\n",
      "Iter 2200: Val loss 2.439, Val took 3.258s\n",
      "Iter 2200: Train loss 1.687, Learning Rate 4.000e-04, It/sec 59.343, Tokens/sec 10396.819, Trained Tokens 364973, Peak mem 19.940 GB\n",
      "Iter 2200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002200_adapters.safetensors.\n",
      "Iter 2210: Train loss 1.669, Learning Rate 4.000e-04, It/sec 1.728, Tokens/sec 290.717, Trained Tokens 366655, Peak mem 19.940 GB\n",
      "Iter 2220: Train loss 1.694, Learning Rate 4.000e-04, It/sec 2.091, Tokens/sec 391.138, Trained Tokens 368526, Peak mem 19.940 GB\n",
      "Iter 2230: Train loss 1.602, Learning Rate 4.000e-04, It/sec 2.418, Tokens/sec 370.221, Trained Tokens 370057, Peak mem 19.940 GB\n",
      "Iter 2240: Train loss 1.530, Learning Rate 4.000e-04, It/sec 1.721, Tokens/sec 400.476, Trained Tokens 372384, Peak mem 19.940 GB\n",
      "Iter 2250: Val loss 2.437, Val took 3.659s\n",
      "Iter 2250: Train loss 1.639, Learning Rate 4.000e-04, It/sec 25.051, Tokens/sec 3394.412, Trained Tokens 373739, Peak mem 19.940 GB\n",
      "Iter 2260: Train loss 1.700, Learning Rate 4.000e-04, It/sec 2.167, Tokens/sec 318.584, Trained Tokens 375209, Peak mem 19.940 GB\n",
      "Iter 2270: Train loss 1.396, Learning Rate 4.000e-04, It/sec 3.349, Tokens/sec 385.474, Trained Tokens 376360, Peak mem 19.940 GB\n",
      "Iter 2280: Train loss 1.622, Learning Rate 4.000e-04, It/sec 2.219, Tokens/sec 394.689, Trained Tokens 378139, Peak mem 19.940 GB\n",
      "Iter 2290: Train loss 1.827, Learning Rate 4.000e-04, It/sec 2.168, Tokens/sec 413.621, Trained Tokens 380047, Peak mem 19.940 GB\n",
      "Iter 2300: Val loss 2.280, Val took 3.257s\n",
      "Iter 2300: Train loss 1.660, Learning Rate 4.000e-04, It/sec 34.717, Tokens/sec 4704.157, Trained Tokens 381402, Peak mem 19.940 GB\n",
      "Iter 2300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002300_adapters.safetensors.\n",
      "Iter 2310: Train loss 1.758, Learning Rate 4.000e-04, It/sec 1.643, Tokens/sec 262.754, Trained Tokens 383001, Peak mem 19.940 GB\n",
      "Iter 2320: Train loss 1.429, Learning Rate 4.000e-04, It/sec 2.834, Tokens/sec 316.889, Trained Tokens 384119, Peak mem 19.940 GB\n",
      "Iter 2330: Train loss 1.531, Learning Rate 4.000e-04, It/sec 2.272, Tokens/sec 372.460, Trained Tokens 385758, Peak mem 19.940 GB\n",
      "Iter 2340: Train loss 1.526, Learning Rate 4.000e-04, It/sec 2.786, Tokens/sec 378.606, Trained Tokens 387117, Peak mem 19.940 GB\n",
      "Iter 2350: Val loss 2.410, Val took 2.900s\n",
      "Iter 2350: Train loss 1.885, Learning Rate 4.000e-04, It/sec 27.395, Tokens/sec 6152.827, Trained Tokens 389363, Peak mem 19.940 GB\n",
      "Iter 2360: Train loss 1.811, Learning Rate 4.000e-04, It/sec 2.445, Tokens/sec 361.062, Trained Tokens 390840, Peak mem 19.940 GB\n",
      "Iter 2370: Train loss 1.779, Learning Rate 4.000e-04, It/sec 1.715, Tokens/sec 374.959, Trained Tokens 393026, Peak mem 19.940 GB\n",
      "Iter 2380: Train loss 1.730, Learning Rate 4.000e-04, It/sec 1.989, Tokens/sec 389.721, Trained Tokens 394985, Peak mem 19.940 GB\n",
      "Iter 2390: Train loss 1.623, Learning Rate 4.000e-04, It/sec 2.034, Tokens/sec 388.405, Trained Tokens 396895, Peak mem 19.940 GB\n",
      "Iter 2400: Val loss 2.332, Val took 3.175s\n",
      "Iter 2400: Train loss 1.602, Learning Rate 4.000e-04, It/sec 14.607, Tokens/sec 2315.285, Trained Tokens 398480, Peak mem 19.940 GB\n",
      "Iter 2400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002400_adapters.safetensors.\n",
      "Iter 2410: Train loss 1.493, Learning Rate 4.000e-04, It/sec 2.409, Tokens/sec 299.962, Trained Tokens 399725, Peak mem 19.940 GB\n",
      "Iter 2420: Train loss 1.850, Learning Rate 4.000e-04, It/sec 2.082, Tokens/sec 363.286, Trained Tokens 401470, Peak mem 19.940 GB\n",
      "Iter 2430: Train loss 1.525, Learning Rate 4.000e-04, It/sec 3.159, Tokens/sec 376.196, Trained Tokens 402661, Peak mem 19.940 GB\n",
      "Iter 2440: Train loss 1.496, Learning Rate 4.000e-04, It/sec 2.813, Tokens/sec 363.125, Trained Tokens 403952, Peak mem 19.940 GB\n",
      "Iter 2450: Val loss 2.291, Val took 3.193s\n",
      "Iter 2450: Train loss 1.650, Learning Rate 4.000e-04, It/sec 35.320, Tokens/sec 6357.671, Trained Tokens 405752, Peak mem 19.940 GB\n",
      "Iter 2460: Train loss 1.623, Learning Rate 4.000e-04, It/sec 1.916, Tokens/sec 333.020, Trained Tokens 407490, Peak mem 19.940 GB\n",
      "Iter 2470: Train loss 1.447, Learning Rate 4.000e-04, It/sec 2.556, Tokens/sec 381.670, Trained Tokens 408983, Peak mem 19.940 GB\n",
      "Iter 2480: Train loss 1.594, Learning Rate 4.000e-04, It/sec 2.053, Tokens/sec 391.554, Trained Tokens 410890, Peak mem 19.940 GB\n",
      "Iter 2490: Train loss 1.284, Learning Rate 4.000e-04, It/sec 2.390, Tokens/sec 382.455, Trained Tokens 412490, Peak mem 19.940 GB\n",
      "Iter 2500: Val loss 2.445, Val took 3.460s\n",
      "Iter 2500: Train loss 1.360, Learning Rate 4.000e-04, It/sec 38.327, Tokens/sec 6021.194, Trained Tokens 414061, Peak mem 19.940 GB\n",
      "Iter 2500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002500_adapters.safetensors.\n",
      "Iter 2510: Train loss 1.587, Learning Rate 4.000e-04, It/sec 1.597, Tokens/sec 316.695, Trained Tokens 416044, Peak mem 19.940 GB\n",
      "Iter 2520: Train loss 1.470, Learning Rate 4.000e-04, It/sec 2.218, Tokens/sec 394.821, Trained Tokens 417824, Peak mem 19.940 GB\n",
      "Iter 2530: Train loss 1.430, Learning Rate 4.000e-04, It/sec 2.772, Tokens/sec 385.519, Trained Tokens 419215, Peak mem 19.940 GB\n",
      "Iter 2540: Train loss 1.625, Learning Rate 4.000e-04, It/sec 1.732, Tokens/sec 406.805, Trained Tokens 421564, Peak mem 19.940 GB\n",
      "Iter 2550: Val loss 2.329, Val took 3.345s\n",
      "Iter 2550: Train loss 1.381, Learning Rate 4.000e-04, It/sec 57.602, Tokens/sec 6278.650, Trained Tokens 422654, Peak mem 19.940 GB\n",
      "Iter 2560: Train loss 1.466, Learning Rate 4.000e-04, It/sec 1.984, Tokens/sec 369.287, Trained Tokens 424515, Peak mem 19.940 GB\n",
      "Iter 2570: Train loss 1.394, Learning Rate 4.000e-04, It/sec 2.241, Tokens/sec 378.890, Trained Tokens 426206, Peak mem 19.940 GB\n",
      "Iter 2580: Train loss 1.849, Learning Rate 4.000e-04, It/sec 1.571, Tokens/sec 417.177, Trained Tokens 428861, Peak mem 19.940 GB\n",
      "Iter 2590: Train loss 1.598, Learning Rate 4.000e-04, It/sec 2.526, Tokens/sec 340.474, Trained Tokens 430209, Peak mem 19.940 GB\n",
      "Iter 2600: Val loss 2.432, Val took 4.115s\n",
      "Iter 2600: Train loss 1.427, Learning Rate 4.000e-04, It/sec 13.767, Tokens/sec 2166.910, Trained Tokens 431783, Peak mem 19.940 GB\n",
      "Iter 2600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002600_adapters.safetensors.\n",
      "Iter 2610: Train loss 1.377, Learning Rate 4.000e-04, It/sec 2.538, Tokens/sec 376.569, Trained Tokens 433267, Peak mem 19.940 GB\n",
      "Iter 2620: Train loss 1.563, Learning Rate 4.000e-04, It/sec 2.392, Tokens/sec 376.916, Trained Tokens 434843, Peak mem 19.940 GB\n",
      "Iter 2630: Train loss 1.677, Learning Rate 4.000e-04, It/sec 2.188, Tokens/sec 376.305, Trained Tokens 436563, Peak mem 19.940 GB\n",
      "Iter 2640: Train loss 1.552, Learning Rate 4.000e-04, It/sec 2.731, Tokens/sec 366.185, Trained Tokens 437904, Peak mem 19.940 GB\n",
      "Iter 2650: Val loss 2.423, Val took 3.520s\n",
      "Iter 2650: Train loss 1.541, Learning Rate 4.000e-04, It/sec 33.292, Tokens/sec 4887.264, Trained Tokens 439372, Peak mem 19.940 GB\n",
      "Iter 2660: Train loss 1.653, Learning Rate 4.000e-04, It/sec 1.484, Tokens/sec 273.741, Trained Tokens 441216, Peak mem 19.940 GB\n",
      "Iter 2670: Train loss 1.337, Learning Rate 4.000e-04, It/sec 2.329, Tokens/sec 331.393, Trained Tokens 442639, Peak mem 19.940 GB\n",
      "Iter 2680: Train loss 1.466, Learning Rate 4.000e-04, It/sec 2.662, Tokens/sec 348.504, Trained Tokens 443948, Peak mem 19.940 GB\n",
      "Iter 2690: Train loss 1.559, Learning Rate 4.000e-04, It/sec 1.816, Tokens/sec 407.394, Trained Tokens 446191, Peak mem 19.940 GB\n",
      "Iter 2700: Val loss 2.382, Val took 3.395s\n",
      "Iter 2700: Train loss 1.398, Learning Rate 4.000e-04, It/sec 39.478, Tokens/sec 5787.417, Trained Tokens 447657, Peak mem 19.940 GB\n",
      "Iter 2700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002700_adapters.safetensors.\n",
      "Iter 2710: Train loss 1.359, Learning Rate 4.000e-04, It/sec 2.997, Tokens/sec 371.326, Trained Tokens 448896, Peak mem 19.940 GB\n",
      "Iter 2720: Train loss 1.382, Learning Rate 4.000e-04, It/sec 3.205, Tokens/sec 356.985, Trained Tokens 450010, Peak mem 19.940 GB\n",
      "Iter 2730: Train loss 1.596, Learning Rate 4.000e-04, It/sec 1.810, Tokens/sec 385.756, Trained Tokens 452141, Peak mem 19.940 GB\n",
      "Iter 2740: Train loss 1.509, Learning Rate 4.000e-04, It/sec 1.817, Tokens/sec 371.058, Trained Tokens 454183, Peak mem 19.940 GB\n",
      "Iter 2750: Val loss 2.183, Val took 2.905s\n",
      "Iter 2750: Train loss 1.525, Learning Rate 4.000e-04, It/sec 44.514, Tokens/sec 5978.242, Trained Tokens 455526, Peak mem 19.940 GB\n",
      "Iter 2760: Train loss 1.536, Learning Rate 4.000e-04, It/sec 2.901, Tokens/sec 366.700, Trained Tokens 456790, Peak mem 19.940 GB\n",
      "Iter 2770: Train loss 1.543, Learning Rate 4.000e-04, It/sec 2.765, Tokens/sec 367.956, Trained Tokens 458121, Peak mem 19.940 GB\n",
      "Iter 2780: Train loss 1.580, Learning Rate 4.000e-04, It/sec 1.897, Tokens/sec 389.465, Trained Tokens 460174, Peak mem 19.940 GB\n",
      "Iter 2790: Train loss 1.507, Learning Rate 4.000e-04, It/sec 1.507, Tokens/sec 382.036, Trained Tokens 462709, Peak mem 19.940 GB\n",
      "Iter 2800: Val loss 2.268, Val took 3.161s\n",
      "Iter 2800: Train loss 1.518, Learning Rate 4.000e-04, It/sec 18.696, Tokens/sec 3290.458, Trained Tokens 464469, Peak mem 19.940 GB\n",
      "Iter 2800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002800_adapters.safetensors.\n",
      "Iter 2810: Train loss 1.370, Learning Rate 4.000e-04, It/sec 2.256, Tokens/sec 305.936, Trained Tokens 465825, Peak mem 19.940 GB\n",
      "Iter 2820: Train loss 1.640, Learning Rate 4.000e-04, It/sec 2.201, Tokens/sec 362.245, Trained Tokens 467471, Peak mem 19.940 GB\n",
      "Iter 2830: Train loss 1.558, Learning Rate 4.000e-04, It/sec 2.053, Tokens/sec 388.386, Trained Tokens 469363, Peak mem 19.940 GB\n",
      "Iter 2840: Train loss 1.627, Learning Rate 4.000e-04, It/sec 2.442, Tokens/sec 386.781, Trained Tokens 470947, Peak mem 19.940 GB\n",
      "Iter 2850: Val loss 2.187, Val took 3.160s\n",
      "Iter 2850: Train loss 1.691, Learning Rate 4.000e-04, It/sec 27.178, Tokens/sec 5310.659, Trained Tokens 472901, Peak mem 19.940 GB\n",
      "Iter 2860: Train loss 1.534, Learning Rate 4.000e-04, It/sec 2.366, Tokens/sec 347.977, Trained Tokens 474372, Peak mem 19.940 GB\n",
      "Iter 2870: Train loss 1.383, Learning Rate 4.000e-04, It/sec 3.366, Tokens/sec 347.665, Trained Tokens 475405, Peak mem 19.940 GB\n",
      "Iter 2880: Train loss 1.259, Learning Rate 4.000e-04, It/sec 2.955, Tokens/sec 367.619, Trained Tokens 476649, Peak mem 19.940 GB\n",
      "Iter 2890: Train loss 1.135, Learning Rate 4.000e-04, It/sec 3.113, Tokens/sec 377.901, Trained Tokens 477863, Peak mem 19.940 GB\n",
      "Iter 2900: Val loss 2.562, Val took 3.500s\n",
      "Iter 2900: Train loss 1.289, Learning Rate 4.000e-04, It/sec 27.637, Tokens/sec 4894.553, Trained Tokens 479634, Peak mem 19.940 GB\n",
      "Iter 2900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0002900_adapters.safetensors.\n",
      "Iter 2910: Train loss 1.430, Learning Rate 4.000e-04, It/sec 2.188, Tokens/sec 393.789, Trained Tokens 481434, Peak mem 19.940 GB\n",
      "Iter 2920: Train loss 1.253, Learning Rate 4.000e-04, It/sec 2.924, Tokens/sec 343.816, Trained Tokens 482610, Peak mem 19.940 GB\n",
      "Iter 2930: Train loss 1.365, Learning Rate 4.000e-04, It/sec 2.560, Tokens/sec 379.623, Trained Tokens 484093, Peak mem 19.940 GB\n",
      "Iter 2940: Train loss 1.539, Learning Rate 4.000e-04, It/sec 2.029, Tokens/sec 400.526, Trained Tokens 486067, Peak mem 19.940 GB\n",
      "Iter 2950: Val loss 2.282, Val took 3.430s\n",
      "Iter 2950: Train loss 1.355, Learning Rate 4.000e-04, It/sec 35.342, Tokens/sec 8365.337, Trained Tokens 488434, Peak mem 19.940 GB\n",
      "Iter 2960: Train loss 1.408, Learning Rate 4.000e-04, It/sec 1.874, Tokens/sec 317.995, Trained Tokens 490131, Peak mem 19.940 GB\n",
      "Iter 2970: Train loss 1.293, Learning Rate 4.000e-04, It/sec 2.083, Tokens/sec 404.425, Trained Tokens 492073, Peak mem 19.940 GB\n",
      "Iter 2980: Train loss 1.317, Learning Rate 4.000e-04, It/sec 2.439, Tokens/sec 374.822, Trained Tokens 493610, Peak mem 19.940 GB\n",
      "Iter 2990: Train loss 1.245, Learning Rate 4.000e-04, It/sec 2.319, Tokens/sec 367.623, Trained Tokens 495195, Peak mem 19.940 GB\n",
      "Iter 3000: Val loss 2.263, Val took 3.532s\n",
      "Iter 3000: Train loss 1.297, Learning Rate 4.000e-04, It/sec 25.687, Tokens/sec 2992.520, Trained Tokens 496360, Peak mem 19.940 GB\n",
      "Iter 3000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003000_adapters.safetensors.\n",
      "Iter 3010: Train loss 1.419, Learning Rate 4.000e-04, It/sec 1.531, Tokens/sec 349.462, Trained Tokens 498642, Peak mem 19.940 GB\n",
      "Iter 3020: Train loss 1.218, Learning Rate 4.000e-04, It/sec 2.918, Tokens/sec 377.650, Trained Tokens 499936, Peak mem 19.940 GB\n",
      "Iter 3030: Train loss 1.451, Learning Rate 4.000e-04, It/sec 2.247, Tokens/sec 356.578, Trained Tokens 501523, Peak mem 19.940 GB\n",
      "Iter 3040: Train loss 1.456, Learning Rate 4.000e-04, It/sec 1.416, Tokens/sec 409.927, Trained Tokens 504418, Peak mem 19.940 GB\n",
      "Iter 3050: Val loss 2.376, Val took 3.134s\n",
      "Iter 3050: Train loss 1.581, Learning Rate 4.000e-04, It/sec 6.413, Tokens/sec 1364.769, Trained Tokens 506546, Peak mem 19.940 GB\n",
      "Iter 3060: Train loss 1.477, Learning Rate 4.000e-04, It/sec 2.220, Tokens/sec 402.026, Trained Tokens 508357, Peak mem 19.940 GB\n",
      "Iter 3070: Train loss 1.424, Learning Rate 4.000e-04, It/sec 2.063, Tokens/sec 381.989, Trained Tokens 510209, Peak mem 19.940 GB\n",
      "Iter 3080: Train loss 1.490, Learning Rate 4.000e-04, It/sec 2.492, Tokens/sec 377.722, Trained Tokens 511725, Peak mem 19.940 GB\n",
      "Iter 3090: Train loss 1.478, Learning Rate 4.000e-04, It/sec 2.358, Tokens/sec 373.930, Trained Tokens 513311, Peak mem 19.940 GB\n",
      "Iter 3100: Val loss 2.233, Val took 2.977s\n",
      "Iter 3100: Train loss 1.340, Learning Rate 4.000e-04, It/sec 45.543, Tokens/sec 5765.764, Trained Tokens 514577, Peak mem 19.940 GB\n",
      "Iter 3100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003100_adapters.safetensors.\n",
      "Iter 3110: Train loss 1.429, Learning Rate 4.000e-04, It/sec 2.672, Tokens/sec 383.905, Trained Tokens 516014, Peak mem 19.940 GB\n",
      "Iter 3120: Train loss 1.370, Learning Rate 4.000e-04, It/sec 2.454, Tokens/sec 385.238, Trained Tokens 517584, Peak mem 19.940 GB\n",
      "Iter 3130: Train loss 1.569, Learning Rate 4.000e-04, It/sec 2.242, Tokens/sec 366.393, Trained Tokens 519218, Peak mem 19.940 GB\n",
      "Iter 3140: Train loss 1.464, Learning Rate 4.000e-04, It/sec 2.867, Tokens/sec 367.573, Trained Tokens 520500, Peak mem 19.940 GB\n",
      "Iter 3150: Val loss 2.174, Val took 2.963s\n",
      "Iter 3150: Train loss 1.484, Learning Rate 4.000e-04, It/sec 32.685, Tokens/sec 5223.115, Trained Tokens 522098, Peak mem 19.940 GB\n",
      "Iter 3160: Train loss 1.635, Learning Rate 4.000e-04, It/sec 1.771, Tokens/sec 345.205, Trained Tokens 524047, Peak mem 19.940 GB\n",
      "Iter 3170: Train loss 1.505, Learning Rate 4.000e-04, It/sec 2.614, Tokens/sec 386.342, Trained Tokens 525525, Peak mem 19.940 GB\n",
      "Iter 3180: Train loss 1.466, Learning Rate 4.000e-04, It/sec 2.582, Tokens/sec 369.175, Trained Tokens 526955, Peak mem 19.940 GB\n",
      "Iter 3190: Train loss 1.281, Learning Rate 4.000e-04, It/sec 3.058, Tokens/sec 370.042, Trained Tokens 528165, Peak mem 19.940 GB\n",
      "Iter 3200: Val loss 2.571, Val took 3.797s\n",
      "Iter 3200: Train loss 1.627, Learning Rate 4.000e-04, It/sec 9.284, Tokens/sec 1575.437, Trained Tokens 529862, Peak mem 19.940 GB\n",
      "Iter 3200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003200_adapters.safetensors.\n",
      "Iter 3210: Train loss 1.451, Learning Rate 4.000e-04, It/sec 2.051, Tokens/sec 303.372, Trained Tokens 531341, Peak mem 19.940 GB\n",
      "Iter 3220: Train loss 1.495, Learning Rate 4.000e-04, It/sec 2.149, Tokens/sec 409.244, Trained Tokens 533245, Peak mem 19.940 GB\n",
      "Iter 3230: Train loss 1.379, Learning Rate 4.000e-04, It/sec 2.380, Tokens/sec 362.701, Trained Tokens 534769, Peak mem 19.940 GB\n",
      "Iter 3240: Train loss 1.602, Learning Rate 4.000e-04, It/sec 2.478, Tokens/sec 375.855, Trained Tokens 536286, Peak mem 19.940 GB\n",
      "Iter 3250: Val loss 2.402, Val took 3.192s\n",
      "Iter 3250: Train loss 1.341, Learning Rate 4.000e-04, It/sec 14.125, Tokens/sec 1761.439, Trained Tokens 537533, Peak mem 19.940 GB\n",
      "Iter 3260: Train loss 1.302, Learning Rate 4.000e-04, It/sec 2.523, Tokens/sec 328.752, Trained Tokens 538836, Peak mem 19.940 GB\n",
      "Iter 3270: Train loss 1.900, Learning Rate 4.000e-04, It/sec 1.664, Tokens/sec 402.906, Trained Tokens 541258, Peak mem 19.940 GB\n",
      "Iter 3280: Train loss 1.484, Learning Rate 4.000e-04, It/sec 1.848, Tokens/sec 381.040, Trained Tokens 543320, Peak mem 19.940 GB\n",
      "Iter 3290: Train loss 1.228, Learning Rate 4.000e-04, It/sec 1.944, Tokens/sec 327.630, Trained Tokens 545005, Peak mem 19.940 GB\n",
      "Iter 3300: Val loss 2.316, Val took 3.084s\n",
      "Iter 3300: Train loss 1.345, Learning Rate 4.000e-04, It/sec 25.864, Tokens/sec 4976.189, Trained Tokens 546929, Peak mem 19.940 GB\n",
      "Iter 3300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003300_adapters.safetensors.\n",
      "Iter 3310: Train loss 1.352, Learning Rate 4.000e-04, It/sec 2.082, Tokens/sec 350.575, Trained Tokens 548613, Peak mem 19.940 GB\n",
      "Iter 3320: Train loss 1.333, Learning Rate 4.000e-04, It/sec 2.622, Tokens/sec 395.885, Trained Tokens 550123, Peak mem 19.940 GB\n",
      "Iter 3330: Train loss 1.106, Learning Rate 4.000e-04, It/sec 2.657, Tokens/sec 357.891, Trained Tokens 551470, Peak mem 19.940 GB\n",
      "Iter 3340: Train loss 1.426, Learning Rate 4.000e-04, It/sec 1.167, Tokens/sec 380.332, Trained Tokens 554728, Peak mem 19.940 GB\n",
      "Iter 3350: Val loss 2.510, Val took 3.675s\n",
      "Iter 3350: Train loss 1.203, Learning Rate 4.000e-04, It/sec 5.188, Tokens/sec 787.995, Trained Tokens 556247, Peak mem 19.940 GB\n",
      "Iter 3360: Train loss 1.080, Learning Rate 4.000e-04, It/sec 2.742, Tokens/sec 384.922, Trained Tokens 557651, Peak mem 19.940 GB\n",
      "Iter 3370: Train loss 1.226, Learning Rate 4.000e-04, It/sec 2.341, Tokens/sec 362.331, Trained Tokens 559199, Peak mem 19.940 GB\n",
      "Iter 3380: Train loss 1.290, Learning Rate 4.000e-04, It/sec 2.356, Tokens/sec 377.837, Trained Tokens 560803, Peak mem 19.940 GB\n",
      "Iter 3390: Train loss 1.322, Learning Rate 4.000e-04, It/sec 2.680, Tokens/sec 388.123, Trained Tokens 562251, Peak mem 19.940 GB\n",
      "Iter 3400: Val loss 2.299, Val took 3.241s\n",
      "Iter 3400: Train loss 1.439, Learning Rate 4.000e-04, It/sec 19.589, Tokens/sec 2946.248, Trained Tokens 563755, Peak mem 19.940 GB\n",
      "Iter 3400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003400_adapters.safetensors.\n",
      "Iter 3410: Train loss 1.456, Learning Rate 4.000e-04, It/sec 2.345, Tokens/sec 374.033, Trained Tokens 565350, Peak mem 19.940 GB\n",
      "Iter 3420: Train loss 1.543, Learning Rate 4.000e-04, It/sec 1.959, Tokens/sec 393.609, Trained Tokens 567359, Peak mem 19.940 GB\n",
      "Iter 3430: Train loss 1.301, Learning Rate 4.000e-04, It/sec 2.622, Tokens/sec 382.071, Trained Tokens 568816, Peak mem 19.940 GB\n",
      "Iter 3440: Train loss 1.468, Learning Rate 4.000e-04, It/sec 2.325, Tokens/sec 391.925, Trained Tokens 570502, Peak mem 19.940 GB\n",
      "Iter 3450: Val loss 2.160, Val took 3.176s\n",
      "Iter 3450: Train loss 1.422, Learning Rate 4.000e-04, It/sec 4.188, Tokens/sec 936.763, Trained Tokens 572739, Peak mem 19.940 GB\n",
      "Iter 3460: Train loss 1.133, Learning Rate 4.000e-04, It/sec 2.377, Tokens/sec 387.235, Trained Tokens 574368, Peak mem 19.940 GB\n",
      "Iter 3470: Train loss 1.407, Learning Rate 4.000e-04, It/sec 2.288, Tokens/sec 374.319, Trained Tokens 576004, Peak mem 19.940 GB\n",
      "Iter 3480: Train loss 1.362, Learning Rate 4.000e-04, It/sec 2.159, Tokens/sec 361.192, Trained Tokens 577677, Peak mem 19.940 GB\n",
      "Iter 3490: Train loss 1.227, Learning Rate 4.000e-04, It/sec 2.310, Tokens/sec 335.591, Trained Tokens 579130, Peak mem 19.940 GB\n",
      "Iter 3500: Val loss 2.391, Val took 3.232s\n",
      "Iter 3500: Train loss 1.302, Learning Rate 4.000e-04, It/sec 22.548, Tokens/sec 3580.552, Trained Tokens 580718, Peak mem 19.940 GB\n",
      "Iter 3500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003500_adapters.safetensors.\n",
      "Iter 3510: Train loss 1.324, Learning Rate 4.000e-04, It/sec 2.370, Tokens/sec 339.088, Trained Tokens 582149, Peak mem 19.940 GB\n",
      "Iter 3520: Train loss 1.447, Learning Rate 4.000e-04, It/sec 2.467, Tokens/sec 384.092, Trained Tokens 583706, Peak mem 19.940 GB\n",
      "Iter 3530: Train loss 1.032, Learning Rate 4.000e-04, It/sec 4.012, Tokens/sec 344.660, Trained Tokens 584565, Peak mem 19.940 GB\n",
      "Iter 3540: Train loss 1.435, Learning Rate 4.000e-04, It/sec 1.360, Tokens/sec 383.373, Trained Tokens 587383, Peak mem 19.940 GB\n",
      "Iter 3550: Val loss 2.169, Val took 3.637s\n",
      "Iter 3550: Train loss 1.418, Learning Rate 4.000e-04, It/sec 6.627, Tokens/sec 1274.963, Trained Tokens 589307, Peak mem 19.940 GB\n",
      "Iter 3560: Train loss 1.346, Learning Rate 4.000e-04, It/sec 2.749, Tokens/sec 364.509, Trained Tokens 590633, Peak mem 19.940 GB\n",
      "Iter 3570: Train loss 1.172, Learning Rate 4.000e-04, It/sec 2.936, Tokens/sec 383.126, Trained Tokens 591938, Peak mem 19.940 GB\n",
      "Iter 3580: Train loss 1.440, Learning Rate 4.000e-04, It/sec 2.324, Tokens/sec 381.389, Trained Tokens 593579, Peak mem 19.940 GB\n",
      "Iter 3590: Train loss 1.631, Learning Rate 4.000e-04, It/sec 2.167, Tokens/sec 402.360, Trained Tokens 595436, Peak mem 19.940 GB\n",
      "Iter 3600: Val loss 2.357, Val took 3.352s\n",
      "Iter 3600: Train loss 1.250, Learning Rate 4.000e-04, It/sec 26.389, Tokens/sec 3615.352, Trained Tokens 596806, Peak mem 19.940 GB\n",
      "Iter 3600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003600_adapters.safetensors.\n",
      "Iter 3610: Train loss 1.335, Learning Rate 4.000e-04, It/sec 2.306, Tokens/sec 357.853, Trained Tokens 598358, Peak mem 19.940 GB\n",
      "Iter 3620: Train loss 1.200, Learning Rate 4.000e-04, It/sec 2.670, Tokens/sec 350.580, Trained Tokens 599671, Peak mem 19.940 GB\n",
      "Iter 3630: Train loss 1.503, Learning Rate 4.000e-04, It/sec 1.561, Tokens/sec 402.071, Trained Tokens 602247, Peak mem 19.940 GB\n",
      "Iter 3640: Train loss 1.362, Learning Rate 4.000e-04, It/sec 2.134, Tokens/sec 394.617, Trained Tokens 604096, Peak mem 19.940 GB\n",
      "Iter 3650: Val loss 2.271, Val took 3.429s\n",
      "Iter 3650: Train loss 1.366, Learning Rate 4.000e-04, It/sec 33.152, Tokens/sec 4220.281, Trained Tokens 605369, Peak mem 19.940 GB\n",
      "Iter 3660: Train loss 1.302, Learning Rate 4.000e-04, It/sec 2.684, Tokens/sec 365.521, Trained Tokens 606731, Peak mem 19.940 GB\n",
      "Iter 3670: Train loss 1.410, Learning Rate 4.000e-04, It/sec 2.320, Tokens/sec 367.646, Trained Tokens 608316, Peak mem 19.940 GB\n",
      "Iter 3680: Train loss 1.432, Learning Rate 4.000e-04, It/sec 2.362, Tokens/sec 374.606, Trained Tokens 609902, Peak mem 19.940 GB\n",
      "Iter 3690: Train loss 1.311, Learning Rate 4.000e-04, It/sec 2.848, Tokens/sec 379.704, Trained Tokens 611235, Peak mem 19.940 GB\n",
      "Iter 3700: Val loss 2.093, Val took 2.847s\n",
      "Iter 3700: Train loss 1.311, Learning Rate 4.000e-04, It/sec 19.874, Tokens/sec 4334.436, Trained Tokens 613416, Peak mem 19.940 GB\n",
      "Iter 3700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003700_adapters.safetensors.\n",
      "Iter 3710: Train loss 1.312, Learning Rate 4.000e-04, It/sec 1.982, Tokens/sec 352.271, Trained Tokens 615193, Peak mem 19.940 GB\n",
      "Iter 3720: Train loss 1.012, Learning Rate 4.000e-04, It/sec 2.883, Tokens/sec 375.384, Trained Tokens 616495, Peak mem 19.940 GB\n",
      "Iter 3730: Train loss 1.060, Learning Rate 4.000e-04, It/sec 2.634, Tokens/sec 370.644, Trained Tokens 617902, Peak mem 19.940 GB\n",
      "Iter 3740: Train loss 1.330, Learning Rate 4.000e-04, It/sec 1.572, Tokens/sec 398.278, Trained Tokens 620436, Peak mem 19.940 GB\n",
      "Iter 3750: Val loss 2.216, Val took 2.828s\n",
      "Iter 3750: Train loss 1.274, Learning Rate 4.000e-04, It/sec 21.997, Tokens/sec 4933.851, Trained Tokens 622679, Peak mem 19.940 GB\n",
      "Iter 3760: Train loss 1.225, Learning Rate 4.000e-04, It/sec 1.930, Tokens/sec 338.905, Trained Tokens 624435, Peak mem 19.940 GB\n",
      "Iter 3770: Train loss 1.264, Learning Rate 4.000e-04, It/sec 2.395, Tokens/sec 348.725, Trained Tokens 625891, Peak mem 19.940 GB\n",
      "Iter 3780: Train loss 1.183, Learning Rate 4.000e-04, It/sec 2.452, Tokens/sec 372.482, Trained Tokens 627410, Peak mem 19.940 GB\n",
      "Iter 3790: Train loss 1.228, Learning Rate 4.000e-04, It/sec 1.383, Tokens/sec 359.563, Trained Tokens 630010, Peak mem 19.940 GB\n",
      "Iter 3800: Val loss 2.063, Val took 2.966s\n",
      "Iter 3800: Train loss 1.348, Learning Rate 4.000e-04, It/sec 30.760, Tokens/sec 6053.594, Trained Tokens 631978, Peak mem 19.940 GB\n",
      "Iter 3800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003800_adapters.safetensors.\n",
      "Iter 3810: Train loss 1.380, Learning Rate 4.000e-04, It/sec 2.297, Tokens/sec 374.652, Trained Tokens 633609, Peak mem 19.940 GB\n",
      "Iter 3820: Train loss 1.102, Learning Rate 4.000e-04, It/sec 3.401, Tokens/sec 358.799, Trained Tokens 634664, Peak mem 19.940 GB\n",
      "Iter 3830: Train loss 1.278, Learning Rate 4.000e-04, It/sec 2.700, Tokens/sec 379.686, Trained Tokens 636070, Peak mem 19.940 GB\n",
      "Iter 3840: Train loss 1.308, Learning Rate 4.000e-04, It/sec 2.232, Tokens/sec 368.508, Trained Tokens 637721, Peak mem 19.940 GB\n",
      "Iter 3850: Val loss 2.319, Val took 3.151s\n",
      "Iter 3850: Train loss 1.097, Learning Rate 4.000e-04, It/sec 35.221, Tokens/sec 4127.948, Trained Tokens 638893, Peak mem 19.940 GB\n",
      "Iter 3860: Train loss 1.258, Learning Rate 4.000e-04, It/sec 2.250, Tokens/sec 365.464, Trained Tokens 640517, Peak mem 19.940 GB\n",
      "Iter 3870: Train loss 1.319, Learning Rate 4.000e-04, It/sec 2.547, Tokens/sec 375.164, Trained Tokens 641990, Peak mem 19.940 GB\n",
      "Iter 3880: Train loss 1.150, Learning Rate 4.000e-04, It/sec 3.211, Tokens/sec 359.978, Trained Tokens 643111, Peak mem 19.940 GB\n",
      "Iter 3890: Train loss 1.274, Learning Rate 4.000e-04, It/sec 2.975, Tokens/sec 359.321, Trained Tokens 644319, Peak mem 19.940 GB\n",
      "Iter 3900: Val loss 2.192, Val took 3.747s\n",
      "Iter 3900: Train loss 1.202, Learning Rate 4.000e-04, It/sec 22.646, Tokens/sec 3569.066, Trained Tokens 645895, Peak mem 19.940 GB\n",
      "Iter 3900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0003900_adapters.safetensors.\n",
      "Iter 3910: Train loss 1.339, Learning Rate 4.000e-04, It/sec 2.001, Tokens/sec 373.392, Trained Tokens 647761, Peak mem 19.940 GB\n",
      "Iter 3920: Train loss 1.313, Learning Rate 4.000e-04, It/sec 2.226, Tokens/sec 381.396, Trained Tokens 649474, Peak mem 19.940 GB\n",
      "Iter 3930: Train loss 1.282, Learning Rate 4.000e-04, It/sec 2.304, Tokens/sec 364.713, Trained Tokens 651057, Peak mem 19.940 GB\n",
      "Iter 3940: Train loss 1.334, Learning Rate 4.000e-04, It/sec 1.734, Tokens/sec 373.900, Trained Tokens 653213, Peak mem 19.940 GB\n",
      "Iter 3950: Val loss 2.102, Val took 3.108s\n",
      "Iter 3950: Train loss 1.134, Learning Rate 4.000e-04, It/sec 18.841, Tokens/sec 2569.904, Trained Tokens 654577, Peak mem 19.940 GB\n",
      "Iter 3960: Train loss 1.284, Learning Rate 4.000e-04, It/sec 2.485, Tokens/sec 371.308, Trained Tokens 656071, Peak mem 19.940 GB\n",
      "Iter 3970: Train loss 1.350, Learning Rate 4.000e-04, It/sec 1.820, Tokens/sec 376.344, Trained Tokens 658139, Peak mem 19.940 GB\n",
      "Iter 3980: Train loss 1.357, Learning Rate 4.000e-04, It/sec 2.042, Tokens/sec 399.033, Trained Tokens 660093, Peak mem 19.940 GB\n",
      "Iter 3990: Train loss 1.407, Learning Rate 4.000e-04, It/sec 2.426, Tokens/sec 377.029, Trained Tokens 661647, Peak mem 19.940 GB\n",
      "Iter 4000: Val loss 2.367, Val took 4.232s\n",
      "Iter 4000: Train loss 1.348, Learning Rate 4.000e-04, It/sec 25.613, Tokens/sec 3662.692, Trained Tokens 663077, Peak mem 19.940 GB\n",
      "Iter 4000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004000_adapters.safetensors.\n",
      "Iter 4010: Train loss 1.185, Learning Rate 4.000e-04, It/sec 2.633, Tokens/sec 358.344, Trained Tokens 664438, Peak mem 19.940 GB\n",
      "Iter 4020: Train loss 1.276, Learning Rate 4.000e-04, It/sec 2.384, Tokens/sec 362.888, Trained Tokens 665960, Peak mem 19.940 GB\n",
      "Iter 4030: Train loss 1.377, Learning Rate 4.000e-04, It/sec 1.646, Tokens/sec 393.597, Trained Tokens 668351, Peak mem 19.940 GB\n",
      "Iter 4040: Train loss 1.211, Learning Rate 4.000e-04, It/sec 2.545, Tokens/sec 348.151, Trained Tokens 669719, Peak mem 19.940 GB\n",
      "Iter 4050: Val loss 2.115, Val took 3.764s\n",
      "Iter 4050: Train loss 1.368, Learning Rate 4.000e-04, It/sec 32.564, Tokens/sec 4116.075, Trained Tokens 670983, Peak mem 19.940 GB\n",
      "Iter 4060: Train loss 1.435, Learning Rate 4.000e-04, It/sec 2.316, Tokens/sec 380.810, Trained Tokens 672627, Peak mem 19.940 GB\n",
      "Iter 4070: Train loss 1.338, Learning Rate 4.000e-04, It/sec 2.445, Tokens/sec 390.189, Trained Tokens 674223, Peak mem 19.940 GB\n",
      "Iter 4080: Train loss 1.186, Learning Rate 4.000e-04, It/sec 2.428, Tokens/sec 369.365, Trained Tokens 675744, Peak mem 19.940 GB\n",
      "Iter 4090: Train loss 1.432, Learning Rate 4.000e-04, It/sec 2.781, Tokens/sec 372.092, Trained Tokens 677082, Peak mem 19.940 GB\n",
      "Iter 4100: Val loss 2.503, Val took 3.275s\n",
      "Iter 4100: Train loss 1.361, Learning Rate 4.000e-04, It/sec 34.223, Tokens/sec 7077.298, Trained Tokens 679150, Peak mem 19.940 GB\n",
      "Iter 4100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004100_adapters.safetensors.\n",
      "Iter 4110: Train loss 0.969, Learning Rate 4.000e-04, It/sec 2.429, Tokens/sec 340.270, Trained Tokens 680551, Peak mem 19.940 GB\n",
      "Iter 4120: Train loss 1.098, Learning Rate 4.000e-04, It/sec 2.349, Tokens/sec 371.111, Trained Tokens 682131, Peak mem 19.940 GB\n",
      "Iter 4130: Train loss 1.114, Learning Rate 4.000e-04, It/sec 2.184, Tokens/sec 376.661, Trained Tokens 683856, Peak mem 19.940 GB\n",
      "Iter 4140: Train loss 1.199, Learning Rate 4.000e-04, It/sec 1.753, Tokens/sec 399.039, Trained Tokens 686132, Peak mem 19.940 GB\n",
      "Iter 4150: Val loss 2.405, Val took 2.763s\n",
      "Iter 4150: Train loss 1.260, Learning Rate 4.000e-04, It/sec 17.586, Tokens/sec 3737.096, Trained Tokens 688257, Peak mem 19.940 GB\n",
      "Iter 4160: Train loss 1.073, Learning Rate 4.000e-04, It/sec 2.221, Tokens/sec 335.520, Trained Tokens 689768, Peak mem 19.940 GB\n",
      "Iter 4170: Train loss 1.226, Learning Rate 4.000e-04, It/sec 2.090, Tokens/sec 375.574, Trained Tokens 691565, Peak mem 19.940 GB\n",
      "Iter 4180: Train loss 1.220, Learning Rate 4.000e-04, It/sec 2.284, Tokens/sec 379.525, Trained Tokens 693227, Peak mem 19.940 GB\n",
      "Iter 4190: Train loss 1.120, Learning Rate 4.000e-04, It/sec 2.377, Tokens/sec 379.160, Trained Tokens 694822, Peak mem 19.940 GB\n",
      "Iter 4200: Val loss 2.449, Val took 3.389s\n",
      "Iter 4200: Train loss 1.070, Learning Rate 4.000e-04, It/sec 16.503, Tokens/sec 2496.966, Trained Tokens 696335, Peak mem 19.940 GB\n",
      "Iter 4200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004200_adapters.safetensors.\n",
      "Iter 4210: Train loss 1.025, Learning Rate 4.000e-04, It/sec 2.774, Tokens/sec 343.133, Trained Tokens 697572, Peak mem 19.940 GB\n",
      "Iter 4220: Train loss 1.023, Learning Rate 4.000e-04, It/sec 2.931, Tokens/sec 350.789, Trained Tokens 698769, Peak mem 19.940 GB\n",
      "Iter 4230: Train loss 1.057, Learning Rate 4.000e-04, It/sec 3.110, Tokens/sec 362.939, Trained Tokens 699936, Peak mem 19.940 GB\n",
      "Iter 4240: Train loss 1.330, Learning Rate 4.000e-04, It/sec 2.161, Tokens/sec 404.890, Trained Tokens 701810, Peak mem 19.940 GB\n",
      "Iter 4250: Val loss 2.424, Val took 3.366s\n",
      "Iter 4250: Train loss 1.081, Learning Rate 4.000e-04, It/sec 45.239, Tokens/sec 5089.395, Trained Tokens 702935, Peak mem 19.940 GB\n",
      "Iter 4260: Train loss 1.106, Learning Rate 4.000e-04, It/sec 2.446, Tokens/sec 358.635, Trained Tokens 704401, Peak mem 19.940 GB\n",
      "Iter 4270: Train loss 1.200, Learning Rate 4.000e-04, It/sec 2.248, Tokens/sec 402.926, Trained Tokens 706193, Peak mem 19.940 GB\n",
      "Iter 4280: Train loss 1.301, Learning Rate 4.000e-04, It/sec 2.462, Tokens/sec 361.481, Trained Tokens 707661, Peak mem 19.940 GB\n",
      "Iter 4290: Train loss 1.274, Learning Rate 4.000e-04, It/sec 2.282, Tokens/sec 390.018, Trained Tokens 709370, Peak mem 19.940 GB\n",
      "Iter 4300: Val loss 2.384, Val took 2.935s\n",
      "Iter 4300: Train loss 1.213, Learning Rate 4.000e-04, It/sec 23.160, Tokens/sec 4453.605, Trained Tokens 711293, Peak mem 19.940 GB\n",
      "Iter 4300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004300_adapters.safetensors.\n",
      "Iter 4310: Train loss 1.313, Learning Rate 4.000e-04, It/sec 1.930, Tokens/sec 391.016, Trained Tokens 713319, Peak mem 19.940 GB\n",
      "Iter 4320: Train loss 1.276, Learning Rate 4.000e-04, It/sec 2.716, Tokens/sec 390.573, Trained Tokens 714757, Peak mem 19.940 GB\n",
      "Iter 4330: Train loss 1.240, Learning Rate 4.000e-04, It/sec 2.789, Tokens/sec 397.949, Trained Tokens 716184, Peak mem 19.940 GB\n",
      "Iter 4340: Train loss 1.009, Learning Rate 4.000e-04, It/sec 3.663, Tokens/sec 375.448, Trained Tokens 717209, Peak mem 19.940 GB\n",
      "Iter 4350: Val loss 2.400, Val took 3.517s\n",
      "Iter 4350: Train loss 1.093, Learning Rate 4.000e-04, It/sec 24.809, Tokens/sec 3329.374, Trained Tokens 718551, Peak mem 19.940 GB\n",
      "Iter 4360: Train loss 1.389, Learning Rate 4.000e-04, It/sec 1.719, Tokens/sec 386.563, Trained Tokens 720800, Peak mem 19.940 GB\n",
      "Iter 4370: Train loss 1.362, Learning Rate 4.000e-04, It/sec 1.726, Tokens/sec 395.673, Trained Tokens 723093, Peak mem 19.940 GB\n",
      "Iter 4380: Train loss 1.156, Learning Rate 4.000e-04, It/sec 2.604, Tokens/sec 362.202, Trained Tokens 724484, Peak mem 19.940 GB\n",
      "Iter 4390: Train loss 1.177, Learning Rate 4.000e-04, It/sec 1.990, Tokens/sec 381.561, Trained Tokens 726401, Peak mem 19.940 GB\n",
      "Iter 4400: Val loss 2.313, Val took 3.746s\n",
      "Iter 4400: Train loss 1.390, Learning Rate 4.000e-04, It/sec 7.715, Tokens/sec 1885.507, Trained Tokens 728845, Peak mem 19.940 GB\n",
      "Iter 4400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004400_adapters.safetensors.\n",
      "Iter 4410: Train loss 1.237, Learning Rate 4.000e-04, It/sec 2.372, Tokens/sec 386.697, Trained Tokens 730475, Peak mem 19.940 GB\n",
      "Iter 4420: Train loss 1.363, Learning Rate 4.000e-04, It/sec 2.569, Tokens/sec 397.468, Trained Tokens 732022, Peak mem 19.940 GB\n",
      "Iter 4430: Train loss 1.044, Learning Rate 4.000e-04, It/sec 3.721, Tokens/sec 359.475, Trained Tokens 732988, Peak mem 19.940 GB\n",
      "Iter 4440: Train loss 1.459, Learning Rate 4.000e-04, It/sec 1.451, Tokens/sec 404.220, Trained Tokens 735773, Peak mem 19.940 GB\n",
      "Iter 4450: Val loss 2.302, Val took 3.622s\n",
      "Iter 4450: Train loss 1.173, Learning Rate 4.000e-04, It/sec 15.159, Tokens/sec 2170.725, Trained Tokens 737205, Peak mem 19.940 GB\n",
      "Iter 4460: Train loss 1.276, Learning Rate 4.000e-04, It/sec 1.940, Tokens/sec 329.248, Trained Tokens 738902, Peak mem 19.940 GB\n",
      "Iter 4470: Train loss 1.403, Learning Rate 4.000e-04, It/sec 1.937, Tokens/sec 385.832, Trained Tokens 740894, Peak mem 19.940 GB\n",
      "Iter 4480: Train loss 1.177, Learning Rate 4.000e-04, It/sec 3.092, Tokens/sec 371.660, Trained Tokens 742096, Peak mem 19.940 GB\n",
      "Iter 4490: Train loss 1.344, Learning Rate 4.000e-04, It/sec 2.364, Tokens/sec 364.580, Trained Tokens 743638, Peak mem 19.940 GB\n",
      "Iter 4500: Val loss 2.203, Val took 3.185s\n",
      "Iter 4500: Train loss 1.398, Learning Rate 4.000e-04, It/sec 26.116, Tokens/sec 4481.524, Trained Tokens 745354, Peak mem 19.940 GB\n",
      "Iter 4500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004500_adapters.safetensors.\n",
      "Iter 4510: Train loss 1.431, Learning Rate 4.000e-04, It/sec 2.148, Tokens/sec 367.582, Trained Tokens 747065, Peak mem 19.940 GB\n",
      "Iter 4520: Train loss 1.104, Learning Rate 4.000e-04, It/sec 2.188, Tokens/sec 390.510, Trained Tokens 748850, Peak mem 19.940 GB\n",
      "Iter 4530: Train loss 0.998, Learning Rate 4.000e-04, It/sec 2.741, Tokens/sec 388.372, Trained Tokens 750267, Peak mem 19.940 GB\n",
      "Iter 4540: Train loss 1.145, Learning Rate 4.000e-04, It/sec 1.288, Tokens/sec 363.775, Trained Tokens 753091, Peak mem 19.940 GB\n",
      "Iter 4550: Val loss 2.552, Val took 5.142s\n",
      "Iter 4550: Train loss 0.958, Learning Rate 4.000e-04, It/sec 15.363, Tokens/sec 2370.453, Trained Tokens 754634, Peak mem 19.940 GB\n",
      "Iter 4560: Train loss 1.021, Learning Rate 4.000e-04, It/sec 2.615, Tokens/sec 324.791, Trained Tokens 755876, Peak mem 19.940 GB\n",
      "Iter 4570: Train loss 1.150, Learning Rate 4.000e-04, It/sec 2.002, Tokens/sec 353.902, Trained Tokens 757644, Peak mem 19.940 GB\n",
      "Iter 4580: Train loss 1.142, Learning Rate 4.000e-04, It/sec 2.177, Tokens/sec 391.634, Trained Tokens 759443, Peak mem 19.940 GB\n",
      "Iter 4590: Train loss 1.349, Learning Rate 4.000e-04, It/sec 1.532, Tokens/sec 360.130, Trained Tokens 761793, Peak mem 19.940 GB\n",
      "Iter 4600: Val loss 2.251, Val took 3.610s\n",
      "Iter 4600: Train loss 1.224, Learning Rate 4.000e-04, It/sec 6.983, Tokens/sec 1403.586, Trained Tokens 763803, Peak mem 19.940 GB\n",
      "Iter 4600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004600_adapters.safetensors.\n",
      "Iter 4610: Train loss 0.939, Learning Rate 4.000e-04, It/sec 3.851, Tokens/sec 344.289, Trained Tokens 764697, Peak mem 19.940 GB\n",
      "Iter 4620: Train loss 0.952, Learning Rate 4.000e-04, It/sec 3.024, Tokens/sec 372.211, Trained Tokens 765928, Peak mem 19.940 GB\n",
      "Iter 4630: Train loss 1.243, Learning Rate 4.000e-04, It/sec 1.979, Tokens/sec 415.873, Trained Tokens 768029, Peak mem 19.940 GB\n",
      "Iter 4640: Train loss 0.895, Learning Rate 4.000e-04, It/sec 3.770, Tokens/sec 367.162, Trained Tokens 769003, Peak mem 19.940 GB\n",
      "Iter 4650: Val loss 2.531, Val took 3.635s\n",
      "Iter 4650: Train loss 1.016, Learning Rate 4.000e-04, It/sec 45.621, Tokens/sec 5620.524, Trained Tokens 770235, Peak mem 19.940 GB\n",
      "Iter 4660: Train loss 1.115, Learning Rate 4.000e-04, It/sec 2.596, Tokens/sec 360.025, Trained Tokens 771622, Peak mem 19.940 GB\n",
      "Iter 4670: Train loss 1.359, Learning Rate 4.000e-04, It/sec 1.515, Tokens/sec 388.667, Trained Tokens 774188, Peak mem 19.940 GB\n",
      "Iter 4680: Train loss 1.009, Learning Rate 4.000e-04, It/sec 2.419, Tokens/sec 344.030, Trained Tokens 775610, Peak mem 19.940 GB\n",
      "Iter 4690: Train loss 1.137, Learning Rate 4.000e-04, It/sec 2.441, Tokens/sec 403.177, Trained Tokens 777262, Peak mem 19.940 GB\n",
      "Iter 4700: Val loss 2.053, Val took 3.023s\n",
      "Iter 4700: Train loss 1.020, Learning Rate 4.000e-04, It/sec 27.482, Tokens/sec 3646.899, Trained Tokens 778589, Peak mem 19.940 GB\n",
      "Iter 4700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004700_adapters.safetensors.\n",
      "Iter 4710: Train loss 1.201, Learning Rate 4.000e-04, It/sec 1.950, Tokens/sec 355.333, Trained Tokens 780411, Peak mem 19.940 GB\n",
      "Iter 4720: Train loss 1.192, Learning Rate 4.000e-04, It/sec 2.029, Tokens/sec 380.224, Trained Tokens 782285, Peak mem 19.940 GB\n",
      "Iter 4730: Train loss 1.270, Learning Rate 4.000e-04, It/sec 1.921, Tokens/sec 391.823, Trained Tokens 784325, Peak mem 19.940 GB\n",
      "Iter 4740: Train loss 1.076, Learning Rate 4.000e-04, It/sec 2.943, Tokens/sec 378.755, Trained Tokens 785612, Peak mem 19.940 GB\n",
      "Iter 4750: Val loss 2.228, Val took 3.218s\n",
      "Iter 4750: Train loss 1.022, Learning Rate 4.000e-04, It/sec 26.375, Tokens/sec 3608.154, Trained Tokens 786980, Peak mem 19.940 GB\n",
      "Iter 4760: Train loss 1.243, Learning Rate 4.000e-04, It/sec 2.083, Tokens/sec 374.075, Trained Tokens 788776, Peak mem 19.940 GB\n",
      "Iter 4770: Train loss 1.157, Learning Rate 4.000e-04, It/sec 2.101, Tokens/sec 385.590, Trained Tokens 790611, Peak mem 19.940 GB\n",
      "Iter 4780: Train loss 1.117, Learning Rate 4.000e-04, It/sec 2.688, Tokens/sec 372.566, Trained Tokens 791997, Peak mem 19.940 GB\n",
      "Iter 4790: Train loss 1.045, Learning Rate 4.000e-04, It/sec 2.628, Tokens/sec 343.252, Trained Tokens 793303, Peak mem 19.940 GB\n",
      "Iter 4800: Val loss 2.348, Val took 3.309s\n",
      "Iter 4800: Train loss 1.181, Learning Rate 4.000e-04, It/sec 42.796, Tokens/sec 6162.637, Trained Tokens 794743, Peak mem 19.940 GB\n",
      "Iter 4800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004800_adapters.safetensors.\n",
      "Iter 4810: Train loss 1.416, Learning Rate 4.000e-04, It/sec 1.527, Tokens/sec 387.900, Trained Tokens 797283, Peak mem 19.940 GB\n",
      "Iter 4820: Train loss 1.216, Learning Rate 4.000e-04, It/sec 2.763, Tokens/sec 379.939, Trained Tokens 798658, Peak mem 19.940 GB\n",
      "Iter 4830: Train loss 1.211, Learning Rate 4.000e-04, It/sec 2.382, Tokens/sec 398.549, Trained Tokens 800331, Peak mem 19.940 GB\n",
      "Iter 4840: Train loss 1.192, Learning Rate 4.000e-04, It/sec 2.295, Tokens/sec 401.088, Trained Tokens 802079, Peak mem 19.940 GB\n",
      "Iter 4850: Val loss 2.461, Val took 2.798s\n",
      "Iter 4850: Train loss 1.066, Learning Rate 4.000e-04, It/sec 44.441, Tokens/sec 7079.516, Trained Tokens 803672, Peak mem 19.940 GB\n",
      "Iter 4860: Train loss 1.212, Learning Rate 4.000e-04, It/sec 2.607, Tokens/sec 363.649, Trained Tokens 805067, Peak mem 19.940 GB\n",
      "Iter 4870: Train loss 1.170, Learning Rate 4.000e-04, It/sec 1.976, Tokens/sec 375.221, Trained Tokens 806966, Peak mem 19.940 GB\n",
      "Iter 4880: Train loss 1.225, Learning Rate 4.000e-04, It/sec 3.087, Tokens/sec 363.944, Trained Tokens 808145, Peak mem 19.940 GB\n",
      "Iter 4890: Train loss 1.237, Learning Rate 4.000e-04, It/sec 2.273, Tokens/sec 385.540, Trained Tokens 809841, Peak mem 19.940 GB\n",
      "Iter 4900: Val loss 2.116, Val took 2.854s\n",
      "Iter 4900: Train loss 1.497, Learning Rate 4.000e-04, It/sec 58.494, Tokens/sec 11102.091, Trained Tokens 811739, Peak mem 19.940 GB\n",
      "Iter 4900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0004900_adapters.safetensors.\n",
      "Iter 4910: Train loss 1.251, Learning Rate 4.000e-04, It/sec 2.010, Tokens/sec 351.751, Trained Tokens 813489, Peak mem 19.940 GB\n",
      "Iter 4920: Train loss 1.125, Learning Rate 4.000e-04, It/sec 2.546, Tokens/sec 379.602, Trained Tokens 814980, Peak mem 19.940 GB\n",
      "Iter 4930: Train loss 0.894, Learning Rate 4.000e-04, It/sec 2.950, Tokens/sec 376.173, Trained Tokens 816255, Peak mem 19.940 GB\n",
      "Iter 4940: Train loss 1.044, Learning Rate 4.000e-04, It/sec 2.347, Tokens/sec 383.951, Trained Tokens 817891, Peak mem 19.940 GB\n",
      "Iter 4950: Val loss 2.424, Val took 3.285s\n",
      "Iter 4950: Train loss 1.017, Learning Rate 4.000e-04, It/sec 44.586, Tokens/sec 6705.763, Trained Tokens 819395, Peak mem 19.940 GB\n",
      "Iter 4960: Train loss 0.990, Learning Rate 4.000e-04, It/sec 3.006, Tokens/sec 375.134, Trained Tokens 820643, Peak mem 19.940 GB\n",
      "Iter 4970: Train loss 1.337, Learning Rate 4.000e-04, It/sec 1.338, Tokens/sec 379.948, Trained Tokens 823482, Peak mem 19.940 GB\n",
      "Iter 4980: Train loss 1.038, Learning Rate 4.000e-04, It/sec 1.969, Tokens/sec 394.137, Trained Tokens 825484, Peak mem 19.940 GB\n",
      "Iter 4990: Train loss 0.936, Learning Rate 4.000e-04, It/sec 2.466, Tokens/sec 382.761, Trained Tokens 827036, Peak mem 19.940 GB\n",
      "Iter 5000: Val loss 2.311, Val took 3.752s\n",
      "Iter 5000: Train loss 1.017, Learning Rate 4.000e-04, It/sec 44.559, Tokens/sec 7436.979, Trained Tokens 828705, Peak mem 19.940 GB\n",
      "Iter 5000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005000_adapters.safetensors.\n",
      "Iter 5010: Train loss 0.921, Learning Rate 4.000e-04, It/sec 2.304, Tokens/sec 386.866, Trained Tokens 830384, Peak mem 19.940 GB\n",
      "Iter 5020: Train loss 0.918, Learning Rate 4.000e-04, It/sec 2.530, Tokens/sec 381.290, Trained Tokens 831891, Peak mem 19.940 GB\n",
      "Iter 5030: Train loss 1.214, Learning Rate 4.000e-04, It/sec 2.021, Tokens/sec 410.557, Trained Tokens 833922, Peak mem 19.940 GB\n",
      "Iter 5040: Train loss 1.004, Learning Rate 4.000e-04, It/sec 1.902, Tokens/sec 381.665, Trained Tokens 835929, Peak mem 19.940 GB\n",
      "Iter 5050: Val loss 2.317, Val took 3.672s\n",
      "Iter 5050: Train loss 0.955, Learning Rate 4.000e-04, It/sec 24.050, Tokens/sec 3244.375, Trained Tokens 837278, Peak mem 19.940 GB\n",
      "Iter 5060: Train loss 1.132, Learning Rate 4.000e-04, It/sec 1.970, Tokens/sec 357.465, Trained Tokens 839093, Peak mem 19.940 GB\n",
      "Iter 5070: Train loss 1.193, Learning Rate 4.000e-04, It/sec 2.087, Tokens/sec 395.692, Trained Tokens 840989, Peak mem 19.940 GB\n",
      "Iter 5080: Train loss 1.127, Learning Rate 4.000e-04, It/sec 2.168, Tokens/sec 367.034, Trained Tokens 842682, Peak mem 19.940 GB\n",
      "Iter 5090: Train loss 1.141, Learning Rate 4.000e-04, It/sec 2.448, Tokens/sec 395.767, Trained Tokens 844299, Peak mem 19.940 GB\n",
      "Iter 5100: Val loss 2.293, Val took 3.465s\n",
      "Iter 5100: Train loss 0.921, Learning Rate 4.000e-04, It/sec 45.545, Tokens/sec 6103.092, Trained Tokens 845639, Peak mem 19.940 GB\n",
      "Iter 5100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005100_adapters.safetensors.\n",
      "Iter 5110: Train loss 0.975, Learning Rate 4.000e-04, It/sec 2.543, Tokens/sec 312.495, Trained Tokens 846868, Peak mem 19.940 GB\n",
      "Iter 5120: Train loss 1.256, Learning Rate 4.000e-04, It/sec 2.051, Tokens/sec 391.190, Trained Tokens 848775, Peak mem 19.940 GB\n",
      "Iter 5130: Train loss 1.147, Learning Rate 4.000e-04, It/sec 2.801, Tokens/sec 380.921, Trained Tokens 850135, Peak mem 19.940 GB\n",
      "Iter 5140: Train loss 1.109, Learning Rate 4.000e-04, It/sec 2.152, Tokens/sec 384.333, Trained Tokens 851921, Peak mem 19.940 GB\n",
      "Iter 5150: Val loss 2.577, Val took 3.170s\n",
      "Iter 5150: Train loss 1.023, Learning Rate 4.000e-04, It/sec 23.374, Tokens/sec 3716.482, Trained Tokens 853511, Peak mem 19.940 GB\n",
      "Iter 5160: Train loss 1.035, Learning Rate 4.000e-04, It/sec 2.988, Tokens/sec 321.528, Trained Tokens 854587, Peak mem 19.940 GB\n",
      "Iter 5170: Train loss 1.168, Learning Rate 4.000e-04, It/sec 2.555, Tokens/sec 370.543, Trained Tokens 856037, Peak mem 19.940 GB\n",
      "Iter 5180: Train loss 1.034, Learning Rate 4.000e-04, It/sec 3.219, Tokens/sec 370.862, Trained Tokens 857189, Peak mem 19.940 GB\n",
      "Iter 5190: Train loss 1.349, Learning Rate 4.000e-04, It/sec 1.226, Tokens/sec 390.721, Trained Tokens 860375, Peak mem 19.940 GB\n",
      "Iter 5200: Val loss 2.514, Val took 4.296s\n",
      "Iter 5200: Train loss 1.036, Learning Rate 4.000e-04, It/sec 31.960, Tokens/sec 4378.494, Trained Tokens 861745, Peak mem 19.940 GB\n",
      "Iter 5200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005200_adapters.safetensors.\n",
      "Iter 5210: Train loss 1.364, Learning Rate 4.000e-04, It/sec 1.976, Tokens/sec 387.841, Trained Tokens 863708, Peak mem 19.940 GB\n",
      "Iter 5220: Train loss 1.196, Learning Rate 4.000e-04, It/sec 2.040, Tokens/sec 387.076, Trained Tokens 865605, Peak mem 19.940 GB\n",
      "Iter 5230: Train loss 0.955, Learning Rate 4.000e-04, It/sec 3.199, Tokens/sec 398.542, Trained Tokens 866851, Peak mem 19.940 GB\n",
      "Iter 5240: Train loss 1.294, Learning Rate 4.000e-04, It/sec 2.181, Tokens/sec 383.417, Trained Tokens 868609, Peak mem 19.940 GB\n",
      "Iter 5250: Val loss 2.468, Val took 2.557s\n",
      "Iter 5250: Train loss 1.113, Learning Rate 4.000e-04, It/sec 43.032, Tokens/sec 7500.457, Trained Tokens 870352, Peak mem 19.940 GB\n",
      "Iter 5260: Train loss 1.167, Learning Rate 4.000e-04, It/sec 2.170, Tokens/sec 337.443, Trained Tokens 871907, Peak mem 19.940 GB\n",
      "Iter 5270: Train loss 1.017, Learning Rate 4.000e-04, It/sec 2.981, Tokens/sec 344.341, Trained Tokens 873062, Peak mem 19.940 GB\n",
      "Iter 5280: Train loss 1.020, Learning Rate 4.000e-04, It/sec 2.747, Tokens/sec 381.505, Trained Tokens 874451, Peak mem 19.940 GB\n",
      "Iter 5290: Train loss 1.192, Learning Rate 4.000e-04, It/sec 2.484, Tokens/sec 355.254, Trained Tokens 875881, Peak mem 19.940 GB\n",
      "Iter 5300: Val loss 2.266, Val took 3.136s\n",
      "Iter 5300: Train loss 1.164, Learning Rate 4.000e-04, It/sec 30.970, Tokens/sec 4948.948, Trained Tokens 877479, Peak mem 19.940 GB\n",
      "Iter 5300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005300_adapters.safetensors.\n",
      "Iter 5310: Train loss 1.442, Learning Rate 4.000e-04, It/sec 1.520, Tokens/sec 375.145, Trained Tokens 879947, Peak mem 19.940 GB\n",
      "Iter 5320: Train loss 1.198, Learning Rate 4.000e-04, It/sec 2.780, Tokens/sec 379.495, Trained Tokens 881312, Peak mem 19.940 GB\n",
      "Iter 5330: Train loss 1.164, Learning Rate 4.000e-04, It/sec 2.339, Tokens/sec 370.289, Trained Tokens 882895, Peak mem 19.940 GB\n",
      "Iter 5340: Train loss 1.103, Learning Rate 4.000e-04, It/sec 2.113, Tokens/sec 387.526, Trained Tokens 884729, Peak mem 19.940 GB\n",
      "Iter 5350: Val loss 2.472, Val took 3.550s\n",
      "Iter 5350: Train loss 0.885, Learning Rate 4.000e-04, It/sec 26.802, Tokens/sec 3401.227, Trained Tokens 885998, Peak mem 19.940 GB\n",
      "Iter 5360: Train loss 1.040, Learning Rate 4.000e-04, It/sec 1.888, Tokens/sec 317.182, Trained Tokens 887678, Peak mem 19.940 GB\n",
      "Iter 5370: Train loss 1.127, Learning Rate 4.000e-04, It/sec 1.957, Tokens/sec 411.854, Trained Tokens 889782, Peak mem 19.940 GB\n",
      "Iter 5380: Train loss 0.963, Learning Rate 4.000e-04, It/sec 2.531, Tokens/sec 374.610, Trained Tokens 891262, Peak mem 19.940 GB\n",
      "Iter 5390: Train loss 0.930, Learning Rate 4.000e-04, It/sec 2.414, Tokens/sec 391.369, Trained Tokens 892883, Peak mem 19.940 GB\n",
      "Iter 5400: Val loss 2.340, Val took 3.800s\n",
      "Iter 5400: Train loss 0.949, Learning Rate 4.000e-04, It/sec 9.856, Tokens/sec 1928.731, Trained Tokens 894840, Peak mem 19.940 GB\n",
      "Iter 5400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005400_adapters.safetensors.\n",
      "Iter 5410: Train loss 1.017, Learning Rate 4.000e-04, It/sec 2.113, Tokens/sec 339.963, Trained Tokens 896449, Peak mem 19.940 GB\n",
      "Iter 5420: Train loss 0.997, Learning Rate 4.000e-04, It/sec 2.146, Tokens/sec 384.198, Trained Tokens 898239, Peak mem 19.940 GB\n",
      "Iter 5430: Train loss 0.900, Learning Rate 4.000e-04, It/sec 3.938, Tokens/sec 370.212, Trained Tokens 899179, Peak mem 19.940 GB\n",
      "Iter 5440: Train loss 1.062, Learning Rate 4.000e-04, It/sec 2.001, Tokens/sec 382.729, Trained Tokens 901092, Peak mem 19.940 GB\n",
      "Iter 5450: Val loss 2.297, Val took 3.093s\n",
      "Iter 5450: Train loss 1.030, Learning Rate 4.000e-04, It/sec 15.852, Tokens/sec 2480.786, Trained Tokens 902657, Peak mem 19.940 GB\n",
      "Iter 5460: Train loss 1.136, Learning Rate 4.000e-04, It/sec 2.091, Tokens/sec 330.603, Trained Tokens 904238, Peak mem 19.940 GB\n",
      "Iter 5470: Train loss 1.000, Learning Rate 4.000e-04, It/sec 2.166, Tokens/sec 362.766, Trained Tokens 905913, Peak mem 19.940 GB\n",
      "Iter 5480: Train loss 0.862, Learning Rate 4.000e-04, It/sec 2.995, Tokens/sec 371.082, Trained Tokens 907152, Peak mem 19.940 GB\n",
      "Iter 5490: Train loss 0.931, Learning Rate 4.000e-04, It/sec 2.506, Tokens/sec 379.123, Trained Tokens 908665, Peak mem 19.940 GB\n",
      "Iter 5500: Val loss 2.092, Val took 3.046s\n",
      "Iter 5500: Train loss 1.051, Learning Rate 4.000e-04, It/sec 24.778, Tokens/sec 3538.313, Trained Tokens 910093, Peak mem 19.940 GB\n",
      "Iter 5500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005500_adapters.safetensors.\n",
      "Iter 5510: Train loss 1.073, Learning Rate 4.000e-04, It/sec 2.319, Tokens/sec 373.111, Trained Tokens 911702, Peak mem 19.940 GB\n",
      "Iter 5520: Train loss 0.924, Learning Rate 4.000e-04, It/sec 2.698, Tokens/sec 390.727, Trained Tokens 913150, Peak mem 19.940 GB\n",
      "Iter 5530: Train loss 1.059, Learning Rate 4.000e-04, It/sec 2.467, Tokens/sec 380.679, Trained Tokens 914693, Peak mem 19.940 GB\n",
      "Iter 5540: Train loss 1.208, Learning Rate 4.000e-04, It/sec 1.842, Tokens/sec 412.458, Trained Tokens 916932, Peak mem 19.940 GB\n",
      "Iter 5550: Val loss 2.282, Val took 2.906s\n",
      "Iter 5550: Train loss 1.134, Learning Rate 4.000e-04, It/sec 9.980, Tokens/sec 1645.759, Trained Tokens 918581, Peak mem 19.940 GB\n",
      "Iter 5560: Train loss 0.853, Learning Rate 4.000e-04, It/sec 2.802, Tokens/sec 328.057, Trained Tokens 919752, Peak mem 19.940 GB\n",
      "Iter 5570: Train loss 1.025, Learning Rate 4.000e-04, It/sec 2.561, Tokens/sec 391.359, Trained Tokens 921280, Peak mem 19.940 GB\n",
      "Iter 5580: Train loss 1.022, Learning Rate 4.000e-04, It/sec 2.816, Tokens/sec 369.513, Trained Tokens 922592, Peak mem 19.940 GB\n",
      "Iter 5590: Train loss 1.101, Learning Rate 4.000e-04, It/sec 2.663, Tokens/sec 404.768, Trained Tokens 924112, Peak mem 19.940 GB\n",
      "Iter 5600: Val loss 2.579, Val took 5.597s\n",
      "Iter 5600: Train loss 1.162, Learning Rate 4.000e-04, It/sec 40.364, Tokens/sec 7632.905, Trained Tokens 926003, Peak mem 19.940 GB\n",
      "Iter 5600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005600_adapters.safetensors.\n",
      "Iter 5610: Train loss 0.962, Learning Rate 4.000e-04, It/sec 2.772, Tokens/sec 346.516, Trained Tokens 927253, Peak mem 19.940 GB\n",
      "Iter 5620: Train loss 1.260, Learning Rate 4.000e-04, It/sec 2.092, Tokens/sec 406.859, Trained Tokens 929198, Peak mem 19.940 GB\n",
      "Iter 5630: Train loss 0.975, Learning Rate 4.000e-04, It/sec 3.161, Tokens/sec 381.182, Trained Tokens 930404, Peak mem 19.940 GB\n",
      "Iter 5640: Train loss 1.153, Learning Rate 4.000e-04, It/sec 2.164, Tokens/sec 385.870, Trained Tokens 932187, Peak mem 19.940 GB\n",
      "Iter 5650: Val loss 2.432, Val took 3.678s\n",
      "Iter 5650: Train loss 1.382, Learning Rate 4.000e-04, It/sec 43.416, Tokens/sec 8535.491, Trained Tokens 934153, Peak mem 19.940 GB\n",
      "Iter 5660: Train loss 0.969, Learning Rate 4.000e-04, It/sec 3.045, Tokens/sec 376.944, Trained Tokens 935391, Peak mem 19.940 GB\n",
      "Iter 5670: Train loss 0.981, Learning Rate 4.000e-04, It/sec 2.868, Tokens/sec 362.523, Trained Tokens 936655, Peak mem 19.940 GB\n",
      "Iter 5680: Train loss 1.136, Learning Rate 4.000e-04, It/sec 1.595, Tokens/sec 404.032, Trained Tokens 939188, Peak mem 19.940 GB\n",
      "Iter 5690: Train loss 1.184, Learning Rate 4.000e-04, It/sec 2.425, Tokens/sec 329.775, Trained Tokens 940548, Peak mem 19.940 GB\n",
      "Iter 5700: Val loss 2.331, Val took 2.888s\n",
      "Iter 5700: Train loss 1.208, Learning Rate 4.000e-04, It/sec 44.307, Tokens/sec 7235.388, Trained Tokens 942181, Peak mem 19.940 GB\n",
      "Iter 5700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005700_adapters.safetensors.\n",
      "Iter 5710: Train loss 1.088, Learning Rate 4.000e-04, It/sec 2.354, Tokens/sec 337.625, Trained Tokens 943615, Peak mem 19.940 GB\n",
      "Iter 5720: Train loss 1.256, Learning Rate 4.000e-04, It/sec 1.600, Tokens/sec 409.604, Trained Tokens 946175, Peak mem 19.940 GB\n",
      "Iter 5730: Train loss 1.267, Learning Rate 4.000e-04, It/sec 2.004, Tokens/sec 368.795, Trained Tokens 948015, Peak mem 19.940 GB\n",
      "Iter 5740: Train loss 1.346, Learning Rate 4.000e-04, It/sec 1.377, Tokens/sec 384.950, Trained Tokens 950810, Peak mem 19.940 GB\n",
      "Iter 5750: Val loss 2.641, Val took 3.232s\n",
      "Iter 5750: Train loss 0.763, Learning Rate 4.000e-04, It/sec 11.923, Tokens/sec 1219.701, Trained Tokens 951833, Peak mem 19.940 GB\n",
      "Iter 5760: Train loss 0.854, Learning Rate 4.000e-04, It/sec 2.840, Tokens/sec 362.684, Trained Tokens 953110, Peak mem 19.940 GB\n",
      "Iter 5770: Train loss 0.910, Learning Rate 4.000e-04, It/sec 3.051, Tokens/sec 355.778, Trained Tokens 954276, Peak mem 19.940 GB\n",
      "Iter 5780: Train loss 0.922, Learning Rate 4.000e-04, It/sec 2.644, Tokens/sec 383.105, Trained Tokens 955725, Peak mem 19.940 GB\n",
      "Iter 5790: Train loss 1.066, Learning Rate 4.000e-04, It/sec 1.574, Tokens/sec 382.505, Trained Tokens 958155, Peak mem 19.940 GB\n",
      "Iter 5800: Val loss 2.360, Val took 3.528s\n",
      "Iter 5800: Train loss 0.797, Learning Rate 4.000e-04, It/sec 28.821, Tokens/sec 3905.304, Trained Tokens 959510, Peak mem 19.940 GB\n",
      "Iter 5800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005800_adapters.safetensors.\n",
      "Iter 5810: Train loss 0.984, Learning Rate 4.000e-04, It/sec 2.105, Tokens/sec 295.170, Trained Tokens 960912, Peak mem 19.940 GB\n",
      "Iter 5820: Train loss 0.954, Learning Rate 4.000e-04, It/sec 2.273, Tokens/sec 385.344, Trained Tokens 962607, Peak mem 19.940 GB\n",
      "Iter 5830: Train loss 1.054, Learning Rate 4.000e-04, It/sec 1.830, Tokens/sec 405.815, Trained Tokens 964825, Peak mem 19.940 GB\n",
      "Iter 5840: Train loss 0.823, Learning Rate 4.000e-04, It/sec 3.122, Tokens/sec 367.497, Trained Tokens 966002, Peak mem 19.940 GB\n",
      "Iter 5850: Val loss 2.414, Val took 3.300s\n",
      "Iter 5850: Train loss 0.969, Learning Rate 4.000e-04, It/sec 15.027, Tokens/sec 2632.798, Trained Tokens 967754, Peak mem 19.940 GB\n",
      "Iter 5860: Train loss 0.989, Learning Rate 4.000e-04, It/sec 2.200, Tokens/sec 379.660, Trained Tokens 969480, Peak mem 19.940 GB\n",
      "Iter 5870: Train loss 0.909, Learning Rate 4.000e-04, It/sec 2.740, Tokens/sec 341.097, Trained Tokens 970725, Peak mem 19.940 GB\n",
      "Iter 5880: Train loss 0.888, Learning Rate 4.000e-04, It/sec 2.975, Tokens/sec 348.414, Trained Tokens 971896, Peak mem 19.940 GB\n",
      "Iter 5890: Train loss 1.069, Learning Rate 4.000e-04, It/sec 2.037, Tokens/sec 360.771, Trained Tokens 973667, Peak mem 19.940 GB\n",
      "Iter 5900: Val loss 2.628, Val took 4.244s\n",
      "Iter 5900: Train loss 1.010, Learning Rate 4.000e-04, It/sec 32.323, Tokens/sec 4796.796, Trained Tokens 975151, Peak mem 19.940 GB\n",
      "Iter 5900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0005900_adapters.safetensors.\n",
      "Iter 5910: Train loss 0.938, Learning Rate 4.000e-04, It/sec 2.396, Tokens/sec 368.263, Trained Tokens 976688, Peak mem 19.940 GB\n",
      "Iter 5920: Train loss 1.113, Learning Rate 4.000e-04, It/sec 2.094, Tokens/sec 401.613, Trained Tokens 978606, Peak mem 19.940 GB\n",
      "Iter 5930: Train loss 1.049, Learning Rate 4.000e-04, It/sec 2.691, Tokens/sec 378.935, Trained Tokens 980014, Peak mem 19.940 GB\n",
      "Iter 5940: Train loss 0.968, Learning Rate 4.000e-04, It/sec 2.035, Tokens/sec 366.353, Trained Tokens 981814, Peak mem 19.940 GB\n",
      "Iter 5950: Val loss 2.370, Val took 2.967s\n",
      "Iter 5950: Train loss 1.143, Learning Rate 4.000e-04, It/sec 44.124, Tokens/sec 7315.830, Trained Tokens 983472, Peak mem 19.940 GB\n",
      "Iter 5960: Train loss 1.010, Learning Rate 4.000e-04, It/sec 2.637, Tokens/sec 370.530, Trained Tokens 984877, Peak mem 19.940 GB\n",
      "Iter 5970: Train loss 1.135, Learning Rate 4.000e-04, It/sec 2.402, Tokens/sec 372.065, Trained Tokens 986426, Peak mem 19.940 GB\n",
      "Iter 5980: Train loss 1.328, Learning Rate 4.000e-04, It/sec 1.563, Tokens/sec 411.816, Trained Tokens 989060, Peak mem 19.940 GB\n",
      "Iter 5990: Train loss 1.014, Learning Rate 4.000e-04, It/sec 2.426, Tokens/sec 350.532, Trained Tokens 990505, Peak mem 19.940 GB\n",
      "Iter 6000: Val loss 2.458, Val took 3.128s\n",
      "Iter 6000: Train loss 1.021, Learning Rate 4.000e-04, It/sec 24.537, Tokens/sec 3894.056, Trained Tokens 992092, Peak mem 19.940 GB\n",
      "Iter 6000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006000_adapters.safetensors.\n",
      "Iter 6010: Train loss 0.969, Learning Rate 4.000e-04, It/sec 2.702, Tokens/sec 387.535, Trained Tokens 993526, Peak mem 19.940 GB\n",
      "Iter 6020: Train loss 0.931, Learning Rate 4.000e-04, It/sec 2.382, Tokens/sec 351.415, Trained Tokens 995001, Peak mem 19.940 GB\n",
      "Iter 6030: Train loss 1.050, Learning Rate 4.000e-04, It/sec 2.114, Tokens/sec 421.594, Trained Tokens 996995, Peak mem 19.940 GB\n",
      "Iter 6040: Train loss 1.218, Learning Rate 4.000e-04, It/sec 1.842, Tokens/sec 411.378, Trained Tokens 999228, Peak mem 19.940 GB\n",
      "Iter 6050: Val loss 2.119, Val took 2.960s\n",
      "Iter 6050: Train loss 0.968, Learning Rate 4.000e-04, It/sec 13.106, Tokens/sec 1889.846, Trained Tokens 1000670, Peak mem 19.940 GB\n",
      "Iter 6060: Train loss 1.000, Learning Rate 4.000e-04, It/sec 2.674, Tokens/sec 356.928, Trained Tokens 1002005, Peak mem 19.940 GB\n",
      "Iter 6070: Train loss 1.035, Learning Rate 4.000e-04, It/sec 2.202, Tokens/sec 394.677, Trained Tokens 1003797, Peak mem 19.940 GB\n",
      "Iter 6080: Train loss 1.308, Learning Rate 4.000e-04, It/sec 1.601, Tokens/sec 393.616, Trained Tokens 1006255, Peak mem 19.940 GB\n",
      "Iter 6090: Train loss 1.007, Learning Rate 4.000e-04, It/sec 2.459, Tokens/sec 331.275, Trained Tokens 1007602, Peak mem 19.940 GB\n",
      "Iter 6100: Val loss 2.233, Val took 2.841s\n",
      "Iter 6100: Train loss 0.968, Learning Rate 4.000e-04, It/sec 44.406, Tokens/sec 6176.819, Trained Tokens 1008993, Peak mem 19.940 GB\n",
      "Iter 6100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006100_adapters.safetensors.\n",
      "Iter 6110: Train loss 1.317, Learning Rate 4.000e-04, It/sec 1.189, Tokens/sec 365.737, Trained Tokens 1012070, Peak mem 19.940 GB\n",
      "Iter 6120: Train loss 1.040, Learning Rate 4.000e-04, It/sec 2.286, Tokens/sec 345.704, Trained Tokens 1013582, Peak mem 19.940 GB\n",
      "Iter 6130: Train loss 1.051, Learning Rate 4.000e-04, It/sec 2.535, Tokens/sec 376.424, Trained Tokens 1015067, Peak mem 19.940 GB\n",
      "Iter 6140: Train loss 1.118, Learning Rate 4.000e-04, It/sec 2.079, Tokens/sec 400.876, Trained Tokens 1016995, Peak mem 19.940 GB\n",
      "Iter 6150: Val loss 2.180, Val took 3.131s\n",
      "Iter 6150: Train loss 1.130, Learning Rate 4.000e-04, It/sec 35.474, Tokens/sec 6137.032, Trained Tokens 1018725, Peak mem 19.940 GB\n",
      "Iter 6160: Train loss 0.825, Learning Rate 4.000e-04, It/sec 3.158, Tokens/sec 340.750, Trained Tokens 1019804, Peak mem 19.940 GB\n",
      "Iter 6170: Train loss 1.030, Learning Rate 4.000e-04, It/sec 1.908, Tokens/sec 390.279, Trained Tokens 1021850, Peak mem 19.940 GB\n",
      "Iter 6180: Train loss 0.920, Learning Rate 4.000e-04, It/sec 2.536, Tokens/sec 386.026, Trained Tokens 1023372, Peak mem 19.940 GB\n",
      "Iter 6190: Train loss 0.919, Learning Rate 4.000e-04, It/sec 2.152, Tokens/sec 341.135, Trained Tokens 1024957, Peak mem 19.940 GB\n",
      "Iter 6200: Val loss 2.107, Val took 2.823s\n",
      "Iter 6200: Train loss 0.949, Learning Rate 4.000e-04, It/sec 34.859, Tokens/sec 5940.027, Trained Tokens 1026661, Peak mem 19.940 GB\n",
      "Iter 6200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006200_adapters.safetensors.\n",
      "Iter 6210: Train loss 0.911, Learning Rate 4.000e-04, It/sec 2.575, Tokens/sec 363.879, Trained Tokens 1028074, Peak mem 19.940 GB\n",
      "Iter 6220: Train loss 1.022, Learning Rate 4.000e-04, It/sec 2.067, Tokens/sec 366.958, Trained Tokens 1029849, Peak mem 19.940 GB\n",
      "Iter 6230: Train loss 0.797, Learning Rate 4.000e-04, It/sec 3.249, Tokens/sec 352.238, Trained Tokens 1030933, Peak mem 19.940 GB\n",
      "Iter 6240: Train loss 1.188, Learning Rate 4.000e-04, It/sec 1.546, Tokens/sec 364.746, Trained Tokens 1033292, Peak mem 19.940 GB\n",
      "Iter 6250: Val loss 2.073, Val took 3.194s\n",
      "Iter 6250: Train loss 0.843, Learning Rate 4.000e-04, It/sec 27.719, Tokens/sec 3667.290, Trained Tokens 1034615, Peak mem 19.940 GB\n",
      "Iter 6260: Train loss 0.927, Learning Rate 4.000e-04, It/sec 1.678, Tokens/sec 327.626, Trained Tokens 1036567, Peak mem 19.940 GB\n",
      "Iter 6270: Train loss 0.951, Learning Rate 4.000e-04, It/sec 2.449, Tokens/sec 369.583, Trained Tokens 1038076, Peak mem 19.940 GB\n",
      "Iter 6280: Train loss 0.816, Learning Rate 4.000e-04, It/sec 2.821, Tokens/sec 363.293, Trained Tokens 1039364, Peak mem 19.940 GB\n",
      "Iter 6290: Train loss 1.068, Learning Rate 4.000e-04, It/sec 2.135, Tokens/sec 375.550, Trained Tokens 1041123, Peak mem 19.940 GB\n",
      "Iter 6300: Val loss 2.207, Val took 2.667s\n",
      "Iter 6300: Train loss 0.945, Learning Rate 4.000e-04, It/sec 43.623, Tokens/sec 6818.303, Trained Tokens 1042686, Peak mem 19.940 GB\n",
      "Iter 6300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006300_adapters.safetensors.\n",
      "Iter 6310: Train loss 1.235, Learning Rate 4.000e-04, It/sec 1.090, Tokens/sec 364.586, Trained Tokens 1046032, Peak mem 19.940 GB\n",
      "Iter 6320: Train loss 0.877, Learning Rate 4.000e-04, It/sec 2.613, Tokens/sec 368.890, Trained Tokens 1047444, Peak mem 19.940 GB\n",
      "Iter 6330: Train loss 1.100, Learning Rate 4.000e-04, It/sec 1.918, Tokens/sec 385.862, Trained Tokens 1049456, Peak mem 19.940 GB\n",
      "Iter 6340: Train loss 1.101, Learning Rate 4.000e-04, It/sec 1.960, Tokens/sec 407.037, Trained Tokens 1051533, Peak mem 19.940 GB\n",
      "Iter 6350: Val loss 2.525, Val took 3.355s\n",
      "Iter 6350: Train loss 0.920, Learning Rate 4.000e-04, It/sec 45.198, Tokens/sec 7462.223, Trained Tokens 1053184, Peak mem 19.940 GB\n",
      "Iter 6360: Train loss 0.943, Learning Rate 4.000e-04, It/sec 2.714, Tokens/sec 378.097, Trained Tokens 1054577, Peak mem 19.940 GB\n",
      "Iter 6370: Train loss 1.055, Learning Rate 4.000e-04, It/sec 2.246, Tokens/sec 395.910, Trained Tokens 1056340, Peak mem 19.940 GB\n",
      "Iter 6380: Train loss 0.994, Learning Rate 4.000e-04, It/sec 1.957, Tokens/sec 367.604, Trained Tokens 1058218, Peak mem 19.940 GB\n",
      "Iter 6390: Train loss 0.939, Learning Rate 4.000e-04, It/sec 2.520, Tokens/sec 362.668, Trained Tokens 1059657, Peak mem 19.940 GB\n",
      "Iter 6400: Val loss 2.165, Val took 2.827s\n",
      "Iter 6400: Train loss 0.923, Learning Rate 4.000e-04, It/sec 20.547, Tokens/sec 2350.630, Trained Tokens 1060801, Peak mem 19.940 GB\n",
      "Iter 6400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006400_adapters.safetensors.\n",
      "Iter 6410: Train loss 1.107, Learning Rate 4.000e-04, It/sec 2.191, Tokens/sec 354.320, Trained Tokens 1062418, Peak mem 19.940 GB\n",
      "Iter 6420: Train loss 1.176, Learning Rate 4.000e-04, It/sec 1.659, Tokens/sec 398.987, Trained Tokens 1064823, Peak mem 19.940 GB\n",
      "Iter 6430: Train loss 0.966, Learning Rate 4.000e-04, It/sec 2.028, Tokens/sec 374.312, Trained Tokens 1066669, Peak mem 19.940 GB\n",
      "Iter 6440: Train loss 0.992, Learning Rate 4.000e-04, It/sec 2.161, Tokens/sec 386.947, Trained Tokens 1068460, Peak mem 19.940 GB\n",
      "Iter 6450: Val loss 2.356, Val took 3.083s\n",
      "Iter 6450: Train loss 0.861, Learning Rate 4.000e-04, It/sec 32.564, Tokens/sec 3539.748, Trained Tokens 1069547, Peak mem 19.940 GB\n",
      "Iter 6460: Train loss 1.183, Learning Rate 4.000e-04, It/sec 1.626, Tokens/sec 383.190, Trained Tokens 1071904, Peak mem 19.940 GB\n",
      "Iter 6470: Train loss 1.142, Learning Rate 4.000e-04, It/sec 2.357, Tokens/sec 407.758, Trained Tokens 1073634, Peak mem 19.940 GB\n",
      "Iter 6480: Train loss 0.989, Learning Rate 4.000e-04, It/sec 2.935, Tokens/sec 366.247, Trained Tokens 1074882, Peak mem 19.940 GB\n",
      "Iter 6490: Train loss 0.994, Learning Rate 4.000e-04, It/sec 2.623, Tokens/sec 375.641, Trained Tokens 1076314, Peak mem 19.940 GB\n",
      "Iter 6500: Val loss 2.138, Val took 3.254s\n",
      "Iter 6500: Train loss 1.060, Learning Rate 4.000e-04, It/sec 9.965, Tokens/sec 1765.787, Trained Tokens 1078086, Peak mem 19.940 GB\n",
      "Iter 6500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006500_adapters.safetensors.\n",
      "Iter 6510: Train loss 1.056, Learning Rate 4.000e-04, It/sec 2.248, Tokens/sec 380.895, Trained Tokens 1079780, Peak mem 19.940 GB\n",
      "Iter 6520: Train loss 0.926, Learning Rate 4.000e-04, It/sec 3.046, Tokens/sec 379.846, Trained Tokens 1081027, Peak mem 19.940 GB\n",
      "Iter 6530: Train loss 1.068, Learning Rate 4.000e-04, It/sec 2.656, Tokens/sec 398.911, Trained Tokens 1082529, Peak mem 19.940 GB\n",
      "Iter 6540: Train loss 1.123, Learning Rate 4.000e-04, It/sec 2.318, Tokens/sec 378.693, Trained Tokens 1084163, Peak mem 19.940 GB\n",
      "Iter 6550: Val loss 2.405, Val took 3.112s\n",
      "Iter 6550: Train loss 0.946, Learning Rate 4.000e-04, It/sec 34.955, Tokens/sec 4135.140, Trained Tokens 1085346, Peak mem 19.940 GB\n",
      "Iter 6560: Train loss 1.054, Learning Rate 4.000e-04, It/sec 2.679, Tokens/sec 346.605, Trained Tokens 1086640, Peak mem 19.940 GB\n",
      "Iter 6570: Train loss 0.871, Learning Rate 4.000e-04, It/sec 2.997, Tokens/sec 371.918, Trained Tokens 1087881, Peak mem 19.940 GB\n",
      "Iter 6580: Train loss 1.136, Learning Rate 4.000e-04, It/sec 1.605, Tokens/sec 406.864, Trained Tokens 1090416, Peak mem 19.940 GB\n",
      "Iter 6590: Train loss 0.896, Learning Rate 4.000e-04, It/sec 2.338, Tokens/sec 346.279, Trained Tokens 1091897, Peak mem 19.940 GB\n",
      "Iter 6600: Val loss 2.299, Val took 3.155s\n",
      "Iter 6600: Train loss 1.032, Learning Rate 4.000e-04, It/sec 27.192, Tokens/sec 5712.999, Trained Tokens 1093998, Peak mem 19.940 GB\n",
      "Iter 6600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006600_adapters.safetensors.\n",
      "Iter 6610: Train loss 0.966, Learning Rate 4.000e-04, It/sec 1.807, Tokens/sec 366.418, Trained Tokens 1096026, Peak mem 19.940 GB\n",
      "Iter 6620: Train loss 0.866, Learning Rate 4.000e-04, It/sec 2.340, Tokens/sec 378.599, Trained Tokens 1097644, Peak mem 19.940 GB\n",
      "Iter 6630: Train loss 0.986, Learning Rate 4.000e-04, It/sec 1.681, Tokens/sec 401.523, Trained Tokens 1100033, Peak mem 19.940 GB\n",
      "Iter 6640: Train loss 0.882, Learning Rate 4.000e-04, It/sec 2.302, Tokens/sec 385.377, Trained Tokens 1101707, Peak mem 19.940 GB\n",
      "Iter 6650: Val loss 2.651, Val took 2.803s\n",
      "Iter 6650: Train loss 0.792, Learning Rate 4.000e-04, It/sec 25.211, Tokens/sec 3605.201, Trained Tokens 1103137, Peak mem 19.940 GB\n",
      "Iter 6660: Train loss 0.972, Learning Rate 4.000e-04, It/sec 2.602, Tokens/sec 373.181, Trained Tokens 1104571, Peak mem 19.940 GB\n",
      "Iter 6670: Train loss 0.918, Learning Rate 4.000e-04, It/sec 2.577, Tokens/sec 397.955, Trained Tokens 1106115, Peak mem 19.940 GB\n",
      "Iter 6680: Train loss 0.999, Learning Rate 4.000e-04, It/sec 2.059, Tokens/sec 402.765, Trained Tokens 1108071, Peak mem 19.940 GB\n",
      "Iter 6690: Train loss 0.879, Learning Rate 4.000e-04, It/sec 2.879, Tokens/sec 405.048, Trained Tokens 1109478, Peak mem 19.940 GB\n",
      "Iter 6700: Val loss 2.542, Val took 2.837s\n",
      "Iter 6700: Train loss 0.909, Learning Rate 4.000e-04, It/sec 27.870, Tokens/sec 3871.157, Trained Tokens 1110867, Peak mem 19.940 GB\n",
      "Iter 6700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006700_adapters.safetensors.\n",
      "Iter 6710: Train loss 1.066, Learning Rate 4.000e-04, It/sec 1.896, Tokens/sec 376.297, Trained Tokens 1112852, Peak mem 19.940 GB\n",
      "Iter 6720: Train loss 1.014, Learning Rate 4.000e-04, It/sec 2.036, Tokens/sec 357.182, Trained Tokens 1114606, Peak mem 19.940 GB\n",
      "Iter 6730: Train loss 0.978, Learning Rate 4.000e-04, It/sec 2.267, Tokens/sec 381.586, Trained Tokens 1116289, Peak mem 19.940 GB\n",
      "Iter 6740: Train loss 1.023, Learning Rate 4.000e-04, It/sec 2.700, Tokens/sec 405.026, Trained Tokens 1117789, Peak mem 19.940 GB\n",
      "Iter 6750: Val loss 2.515, Val took 3.575s\n",
      "Iter 6750: Train loss 1.042, Learning Rate 4.000e-04, It/sec 26.092, Tokens/sec 4735.719, Trained Tokens 1119604, Peak mem 19.940 GB\n",
      "Iter 6760: Train loss 1.000, Learning Rate 4.000e-04, It/sec 1.741, Tokens/sec 326.364, Trained Tokens 1121479, Peak mem 19.940 GB\n",
      "Iter 6770: Train loss 0.891, Learning Rate 4.000e-04, It/sec 2.899, Tokens/sec 394.892, Trained Tokens 1122841, Peak mem 19.940 GB\n",
      "Iter 6780: Train loss 0.934, Learning Rate 4.000e-04, It/sec 3.183, Tokens/sec 374.669, Trained Tokens 1124018, Peak mem 19.940 GB\n",
      "Iter 6790: Train loss 1.085, Learning Rate 4.000e-04, It/sec 2.472, Tokens/sec 397.977, Trained Tokens 1125628, Peak mem 19.940 GB\n",
      "Iter 6800: Val loss 2.407, Val took 2.997s\n",
      "Iter 6800: Train loss 0.954, Learning Rate 4.000e-04, It/sec 15.068, Tokens/sec 2091.479, Trained Tokens 1127016, Peak mem 19.940 GB\n",
      "Iter 6800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006800_adapters.safetensors.\n",
      "Iter 6810: Train loss 1.077, Learning Rate 4.000e-04, It/sec 1.448, Tokens/sec 382.954, Trained Tokens 1129660, Peak mem 19.940 GB\n",
      "Iter 6820: Train loss 1.033, Learning Rate 4.000e-04, It/sec 2.072, Tokens/sec 374.741, Trained Tokens 1131469, Peak mem 19.940 GB\n",
      "Iter 6830: Train loss 1.037, Learning Rate 4.000e-04, It/sec 2.392, Tokens/sec 375.854, Trained Tokens 1133040, Peak mem 19.940 GB\n",
      "Iter 6840: Train loss 0.967, Learning Rate 4.000e-04, It/sec 2.145, Tokens/sec 387.215, Trained Tokens 1134845, Peak mem 19.940 GB\n",
      "Iter 6850: Val loss 2.249, Val took 3.070s\n",
      "Iter 6850: Train loss 0.991, Learning Rate 4.000e-04, It/sec 24.597, Tokens/sec 3783.034, Trained Tokens 1136383, Peak mem 19.940 GB\n",
      "Iter 6860: Train loss 0.966, Learning Rate 4.000e-04, It/sec 2.303, Tokens/sec 348.927, Trained Tokens 1137898, Peak mem 19.940 GB\n",
      "Iter 6870: Train loss 0.936, Learning Rate 4.000e-04, It/sec 2.459, Tokens/sec 399.811, Trained Tokens 1139524, Peak mem 19.940 GB\n",
      "Iter 6880: Train loss 0.856, Learning Rate 4.000e-04, It/sec 3.330, Tokens/sec 392.927, Trained Tokens 1140704, Peak mem 19.940 GB\n",
      "Iter 6890: Train loss 0.926, Learning Rate 4.000e-04, It/sec 2.758, Tokens/sec 391.681, Trained Tokens 1142124, Peak mem 19.940 GB\n",
      "Iter 6900: Val loss 2.246, Val took 2.918s\n",
      "Iter 6900: Train loss 1.030, Learning Rate 4.000e-04, It/sec 18.331, Tokens/sec 2927.441, Trained Tokens 1143721, Peak mem 19.940 GB\n",
      "Iter 6900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0006900_adapters.safetensors.\n",
      "Iter 6910: Train loss 1.068, Learning Rate 4.000e-04, It/sec 1.710, Tokens/sec 354.982, Trained Tokens 1145797, Peak mem 19.940 GB\n",
      "Iter 6920: Train loss 1.028, Learning Rate 4.000e-04, It/sec 2.225, Tokens/sec 395.629, Trained Tokens 1147575, Peak mem 19.940 GB\n",
      "Iter 6930: Train loss 1.036, Learning Rate 4.000e-04, It/sec 2.675, Tokens/sec 384.167, Trained Tokens 1149011, Peak mem 19.940 GB\n",
      "Iter 6940: Train loss 1.084, Learning Rate 4.000e-04, It/sec 2.359, Tokens/sec 360.901, Trained Tokens 1150541, Peak mem 19.940 GB\n",
      "Iter 6950: Val loss 2.142, Val took 3.151s\n",
      "Iter 6950: Train loss 1.112, Learning Rate 4.000e-04, It/sec 61.524, Tokens/sec 9080.942, Trained Tokens 1152017, Peak mem 19.940 GB\n",
      "Iter 6960: Train loss 0.972, Learning Rate 4.000e-04, It/sec 2.295, Tokens/sec 350.894, Trained Tokens 1153546, Peak mem 19.940 GB\n",
      "Iter 6970: Train loss 0.984, Learning Rate 4.000e-04, It/sec 3.734, Tokens/sec 376.782, Trained Tokens 1154555, Peak mem 19.940 GB\n",
      "Iter 6980: Train loss 0.907, Learning Rate 4.000e-04, It/sec 2.296, Tokens/sec 387.777, Trained Tokens 1156244, Peak mem 19.940 GB\n",
      "Iter 6990: Train loss 0.869, Learning Rate 4.000e-04, It/sec 2.516, Tokens/sec 372.942, Trained Tokens 1157726, Peak mem 19.940 GB\n",
      "Iter 7000: Val loss 2.622, Val took 3.019s\n",
      "Iter 7000: Train loss 0.938, Learning Rate 4.000e-04, It/sec 43.775, Tokens/sec 6211.741, Trained Tokens 1159145, Peak mem 19.940 GB\n",
      "Iter 7000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007000_adapters.safetensors.\n",
      "Iter 7010: Train loss 0.902, Learning Rate 4.000e-04, It/sec 2.576, Tokens/sec 375.785, Trained Tokens 1160604, Peak mem 19.940 GB\n",
      "Iter 7020: Train loss 0.873, Learning Rate 4.000e-04, It/sec 2.093, Tokens/sec 399.332, Trained Tokens 1162512, Peak mem 19.940 GB\n",
      "Iter 7030: Train loss 0.941, Learning Rate 4.000e-04, It/sec 2.410, Tokens/sec 372.811, Trained Tokens 1164059, Peak mem 19.940 GB\n",
      "Iter 7040: Train loss 0.848, Learning Rate 4.000e-04, It/sec 2.840, Tokens/sec 380.255, Trained Tokens 1165398, Peak mem 19.940 GB\n",
      "Iter 7050: Val loss 2.476, Val took 2.971s\n",
      "Iter 7050: Train loss 0.755, Learning Rate 4.000e-04, It/sec 44.484, Tokens/sec 6641.433, Trained Tokens 1166891, Peak mem 19.940 GB\n",
      "Iter 7060: Train loss 1.139, Learning Rate 4.000e-04, It/sec 1.471, Tokens/sec 371.973, Trained Tokens 1169419, Peak mem 19.940 GB\n",
      "Iter 7070: Train loss 0.881, Learning Rate 4.000e-04, It/sec 2.359, Tokens/sec 376.765, Trained Tokens 1171016, Peak mem 19.940 GB\n",
      "Iter 7080: Train loss 0.896, Learning Rate 4.000e-04, It/sec 2.670, Tokens/sec 400.530, Trained Tokens 1172516, Peak mem 19.940 GB\n",
      "Iter 7090: Train loss 0.923, Learning Rate 4.000e-04, It/sec 3.020, Tokens/sec 387.715, Trained Tokens 1173800, Peak mem 19.940 GB\n",
      "Iter 7100: Val loss 2.417, Val took 3.000s\n",
      "Iter 7100: Train loss 1.172, Learning Rate 4.000e-04, It/sec 7.092, Tokens/sec 1802.809, Trained Tokens 1176342, Peak mem 19.940 GB\n",
      "Iter 7100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007100_adapters.safetensors.\n",
      "Iter 7110: Train loss 1.018, Learning Rate 4.000e-04, It/sec 1.972, Tokens/sec 321.781, Trained Tokens 1177974, Peak mem 19.940 GB\n",
      "Iter 7120: Train loss 1.033, Learning Rate 4.000e-04, It/sec 2.198, Tokens/sec 393.746, Trained Tokens 1179765, Peak mem 19.940 GB\n",
      "Iter 7130: Train loss 0.819, Learning Rate 4.000e-04, It/sec 3.024, Tokens/sec 376.470, Trained Tokens 1181010, Peak mem 19.940 GB\n",
      "Iter 7140: Train loss 0.846, Learning Rate 4.000e-04, It/sec 3.076, Tokens/sec 368.453, Trained Tokens 1182208, Peak mem 19.940 GB\n",
      "Iter 7150: Val loss 2.077, Val took 3.169s\n",
      "Iter 7150: Train loss 0.824, Learning Rate 4.000e-04, It/sec 26.955, Tokens/sec 3326.262, Trained Tokens 1183442, Peak mem 19.940 GB\n",
      "Iter 7160: Train loss 0.999, Learning Rate 4.000e-04, It/sec 2.329, Tokens/sec 382.921, Trained Tokens 1185086, Peak mem 19.940 GB\n",
      "Iter 7170: Train loss 0.933, Learning Rate 4.000e-04, It/sec 2.517, Tokens/sec 373.527, Trained Tokens 1186570, Peak mem 19.940 GB\n",
      "Iter 7180: Train loss 0.865, Learning Rate 4.000e-04, It/sec 2.703, Tokens/sec 384.411, Trained Tokens 1187992, Peak mem 19.940 GB\n",
      "Iter 7190: Train loss 1.048, Learning Rate 4.000e-04, It/sec 2.171, Tokens/sec 369.515, Trained Tokens 1189694, Peak mem 19.940 GB\n",
      "Iter 7200: Val loss 2.147, Val took 2.820s\n",
      "Iter 7200: Train loss 0.888, Learning Rate 4.000e-04, It/sec 25.263, Tokens/sec 3536.778, Trained Tokens 1191094, Peak mem 19.940 GB\n",
      "Iter 7200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007200_adapters.safetensors.\n",
      "Iter 7210: Train loss 1.017, Learning Rate 4.000e-04, It/sec 2.404, Tokens/sec 357.184, Trained Tokens 1192580, Peak mem 19.940 GB\n",
      "Iter 7220: Train loss 1.041, Learning Rate 4.000e-04, It/sec 2.129, Tokens/sec 391.252, Trained Tokens 1194418, Peak mem 19.940 GB\n",
      "Iter 7230: Train loss 0.969, Learning Rate 4.000e-04, It/sec 2.450, Tokens/sec 391.751, Trained Tokens 1196017, Peak mem 19.940 GB\n",
      "Iter 7240: Train loss 1.025, Learning Rate 4.000e-04, It/sec 2.481, Tokens/sec 401.349, Trained Tokens 1197635, Peak mem 19.940 GB\n",
      "Iter 7250: Val loss 2.351, Val took 3.115s\n",
      "Iter 7250: Train loss 0.885, Learning Rate 4.000e-04, It/sec 45.639, Tokens/sec 6886.856, Trained Tokens 1199144, Peak mem 19.940 GB\n",
      "Iter 7260: Train loss 0.858, Learning Rate 4.000e-04, It/sec 1.921, Tokens/sec 291.073, Trained Tokens 1200659, Peak mem 19.940 GB\n",
      "Iter 7270: Train loss 0.945, Learning Rate 4.000e-04, It/sec 2.664, Tokens/sec 393.956, Trained Tokens 1202138, Peak mem 19.940 GB\n",
      "Iter 7280: Train loss 1.061, Learning Rate 4.000e-04, It/sec 2.315, Tokens/sec 383.784, Trained Tokens 1203796, Peak mem 19.940 GB\n",
      "Iter 7290: Train loss 0.937, Learning Rate 4.000e-04, It/sec 2.782, Tokens/sec 340.547, Trained Tokens 1205020, Peak mem 19.940 GB\n",
      "Iter 7300: Val loss 2.345, Val took 3.124s\n",
      "Iter 7300: Train loss 1.072, Learning Rate 4.000e-04, It/sec 43.350, Tokens/sec 9272.515, Trained Tokens 1207159, Peak mem 19.940 GB\n",
      "Iter 7300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007300_adapters.safetensors.\n",
      "Iter 7310: Train loss 0.986, Learning Rate 4.000e-04, It/sec 1.946, Tokens/sec 317.446, Trained Tokens 1208790, Peak mem 19.940 GB\n",
      "Iter 7320: Train loss 0.986, Learning Rate 4.000e-04, It/sec 2.485, Tokens/sec 355.593, Trained Tokens 1210221, Peak mem 19.940 GB\n",
      "Iter 7330: Train loss 1.401, Learning Rate 4.000e-04, It/sec 0.843, Tokens/sec 393.369, Trained Tokens 1214887, Peak mem 19.940 GB\n",
      "Iter 7340: Train loss 0.990, Learning Rate 4.000e-04, It/sec 1.993, Tokens/sec 321.888, Trained Tokens 1216502, Peak mem 19.940 GB\n",
      "Iter 7350: Val loss 2.379, Val took 3.714s\n",
      "Iter 7350: Train loss 0.865, Learning Rate 4.000e-04, It/sec 44.153, Tokens/sec 5739.950, Trained Tokens 1217802, Peak mem 19.940 GB\n",
      "Iter 7360: Train loss 0.956, Learning Rate 4.000e-04, It/sec 3.036, Tokens/sec 371.923, Trained Tokens 1219027, Peak mem 19.940 GB\n",
      "Iter 7370: Train loss 0.939, Learning Rate 4.000e-04, It/sec 2.684, Tokens/sec 349.138, Trained Tokens 1220328, Peak mem 19.940 GB\n",
      "Iter 7380: Train loss 1.221, Learning Rate 4.000e-04, It/sec 1.826, Tokens/sec 391.121, Trained Tokens 1222470, Peak mem 19.940 GB\n",
      "Iter 7390: Train loss 0.934, Learning Rate 4.000e-04, It/sec 2.111, Tokens/sec 361.266, Trained Tokens 1224181, Peak mem 19.940 GB\n",
      "Iter 7400: Val loss 2.650, Val took 4.187s\n",
      "Iter 7400: Train loss 0.782, Learning Rate 4.000e-04, It/sec 36.039, Tokens/sec 4919.367, Trained Tokens 1225546, Peak mem 19.940 GB\n",
      "Iter 7400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007400_adapters.safetensors.\n",
      "Iter 7410: Train loss 0.744, Learning Rate 4.000e-04, It/sec 2.787, Tokens/sec 361.992, Trained Tokens 1226845, Peak mem 19.940 GB\n",
      "Iter 7420: Train loss 0.999, Learning Rate 4.000e-04, It/sec 1.443, Tokens/sec 389.689, Trained Tokens 1229546, Peak mem 19.940 GB\n",
      "Iter 7430: Train loss 0.890, Learning Rate 4.000e-04, It/sec 1.967, Tokens/sec 359.785, Trained Tokens 1231375, Peak mem 19.940 GB\n",
      "Iter 7440: Train loss 0.830, Learning Rate 4.000e-04, It/sec 1.840, Tokens/sec 381.612, Trained Tokens 1233449, Peak mem 19.940 GB\n",
      "Iter 7450: Val loss 2.619, Val took 4.968s\n",
      "Iter 7450: Train loss 0.702, Learning Rate 4.000e-04, It/sec 26.859, Tokens/sec 3486.276, Trained Tokens 1234747, Peak mem 19.940 GB\n",
      "Iter 7460: Train loss 0.811, Learning Rate 4.000e-04, It/sec 2.526, Tokens/sec 349.386, Trained Tokens 1236130, Peak mem 19.940 GB\n",
      "Iter 7470: Train loss 0.967, Learning Rate 4.000e-04, It/sec 1.917, Tokens/sec 389.527, Trained Tokens 1238162, Peak mem 19.940 GB\n",
      "Iter 7480: Train loss 0.731, Learning Rate 4.000e-04, It/sec 2.972, Tokens/sec 341.528, Trained Tokens 1239311, Peak mem 19.940 GB\n",
      "Iter 7490: Train loss 0.849, Learning Rate 4.000e-04, It/sec 2.894, Tokens/sec 374.792, Trained Tokens 1240606, Peak mem 19.940 GB\n",
      "Iter 7500: Val loss 2.352, Val took 3.496s\n",
      "Iter 7500: Train loss 0.847, Learning Rate 4.000e-04, It/sec 23.603, Tokens/sec 3592.324, Trained Tokens 1242128, Peak mem 19.940 GB\n",
      "Iter 7500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007500_adapters.safetensors.\n",
      "Iter 7510: Train loss 0.831, Learning Rate 4.000e-04, It/sec 2.258, Tokens/sec 354.095, Trained Tokens 1243696, Peak mem 19.940 GB\n",
      "Iter 7520: Train loss 0.855, Learning Rate 4.000e-04, It/sec 1.960, Tokens/sec 365.283, Trained Tokens 1245560, Peak mem 19.940 GB\n",
      "Iter 7530: Train loss 0.867, Learning Rate 4.000e-04, It/sec 2.233, Tokens/sec 388.529, Trained Tokens 1247300, Peak mem 19.940 GB\n",
      "Iter 7540: Train loss 0.902, Learning Rate 4.000e-04, It/sec 2.596, Tokens/sec 355.874, Trained Tokens 1248671, Peak mem 19.940 GB\n",
      "Iter 7550: Val loss 2.315, Val took 3.576s\n",
      "Iter 7550: Train loss 0.906, Learning Rate 4.000e-04, It/sec 26.381, Tokens/sec 4925.309, Trained Tokens 1250538, Peak mem 19.940 GB\n",
      "Iter 7560: Train loss 0.912, Learning Rate 4.000e-04, It/sec 1.933, Tokens/sec 390.488, Trained Tokens 1252558, Peak mem 19.940 GB\n",
      "Iter 7570: Train loss 0.881, Learning Rate 4.000e-04, It/sec 2.581, Tokens/sec 390.223, Trained Tokens 1254070, Peak mem 19.940 GB\n",
      "Iter 7580: Train loss 0.839, Learning Rate 4.000e-04, It/sec 3.646, Tokens/sec 382.079, Trained Tokens 1255118, Peak mem 19.940 GB\n",
      "Iter 7590: Train loss 1.015, Learning Rate 4.000e-04, It/sec 1.204, Tokens/sec 375.326, Trained Tokens 1258236, Peak mem 19.940 GB\n",
      "Iter 7600: Val loss 2.394, Val took 3.423s\n",
      "Iter 7600: Train loss 0.941, Learning Rate 4.000e-04, It/sec 37.726, Tokens/sec 6356.910, Trained Tokens 1259921, Peak mem 19.940 GB\n",
      "Iter 7600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007600_adapters.safetensors.\n",
      "Iter 7610: Train loss 1.127, Learning Rate 4.000e-04, It/sec 1.634, Tokens/sec 393.650, Trained Tokens 1262330, Peak mem 19.940 GB\n",
      "Iter 7620: Train loss 0.812, Learning Rate 4.000e-04, It/sec 3.139, Tokens/sec 391.686, Trained Tokens 1263578, Peak mem 19.940 GB\n",
      "Iter 7630: Train loss 1.028, Learning Rate 4.000e-04, It/sec 1.979, Tokens/sec 384.084, Trained Tokens 1265519, Peak mem 19.940 GB\n",
      "Iter 7640: Train loss 0.881, Learning Rate 4.000e-04, It/sec 3.372, Tokens/sec 361.529, Trained Tokens 1266591, Peak mem 19.940 GB\n",
      "Iter 7650: Val loss 2.621, Val took 3.419s\n",
      "Iter 7650: Train loss 0.974, Learning Rate 4.000e-04, It/sec 15.703, Tokens/sec 2557.959, Trained Tokens 1268220, Peak mem 19.940 GB\n",
      "Iter 7660: Train loss 0.932, Learning Rate 4.000e-04, It/sec 2.165, Tokens/sec 317.781, Trained Tokens 1269688, Peak mem 19.940 GB\n",
      "Iter 7670: Train loss 1.066, Learning Rate 4.000e-04, It/sec 2.197, Tokens/sec 393.738, Trained Tokens 1271480, Peak mem 19.940 GB\n",
      "Iter 7680: Train loss 0.911, Learning Rate 4.000e-04, It/sec 2.935, Tokens/sec 366.025, Trained Tokens 1272727, Peak mem 19.940 GB\n",
      "Iter 7690: Train loss 0.955, Learning Rate 4.000e-04, It/sec 2.835, Tokens/sec 377.101, Trained Tokens 1274057, Peak mem 19.940 GB\n",
      "Iter 7700: Val loss 2.156, Val took 2.597s\n",
      "Iter 7700: Train loss 0.895, Learning Rate 4.000e-04, It/sec 35.438, Tokens/sec 3972.638, Trained Tokens 1275178, Peak mem 19.940 GB\n",
      "Iter 7700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007700_adapters.safetensors.\n",
      "Iter 7710: Train loss 1.087, Learning Rate 4.000e-04, It/sec 1.748, Tokens/sec 340.607, Trained Tokens 1277127, Peak mem 19.940 GB\n",
      "Iter 7720: Train loss 0.904, Learning Rate 4.000e-04, It/sec 2.787, Tokens/sec 369.497, Trained Tokens 1278453, Peak mem 19.940 GB\n",
      "Iter 7730: Train loss 0.965, Learning Rate 4.000e-04, It/sec 2.506, Tokens/sec 390.228, Trained Tokens 1280010, Peak mem 19.940 GB\n",
      "Iter 7740: Train loss 1.196, Learning Rate 4.000e-04, It/sec 1.744, Tokens/sec 389.810, Trained Tokens 1282245, Peak mem 19.940 GB\n",
      "Iter 7750: Val loss 2.300, Val took 3.408s\n",
      "Iter 7750: Train loss 1.084, Learning Rate 4.000e-04, It/sec 14.487, Tokens/sec 2287.456, Trained Tokens 1283824, Peak mem 19.940 GB\n",
      "Iter 7760: Train loss 1.093, Learning Rate 4.000e-04, It/sec 2.594, Tokens/sec 375.841, Trained Tokens 1285273, Peak mem 19.940 GB\n",
      "Iter 7770: Train loss 1.003, Learning Rate 4.000e-04, It/sec 2.414, Tokens/sec 369.174, Trained Tokens 1286802, Peak mem 19.940 GB\n",
      "Iter 7780: Train loss 1.115, Learning Rate 4.000e-04, It/sec 1.683, Tokens/sec 364.149, Trained Tokens 1288966, Peak mem 19.940 GB\n",
      "Iter 7790: Train loss 0.938, Learning Rate 4.000e-04, It/sec 2.709, Tokens/sec 384.378, Trained Tokens 1290385, Peak mem 19.940 GB\n",
      "Iter 7800: Val loss 2.271, Val took 3.679s\n",
      "Iter 7800: Train loss 0.756, Learning Rate 4.000e-04, It/sec 56.120, Tokens/sec 7217.066, Trained Tokens 1291671, Peak mem 19.940 GB\n",
      "Iter 7800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007800_adapters.safetensors.\n",
      "Iter 7810: Train loss 0.743, Learning Rate 4.000e-04, It/sec 2.997, Tokens/sec 349.199, Trained Tokens 1292836, Peak mem 19.940 GB\n",
      "Iter 7820: Train loss 0.886, Learning Rate 4.000e-04, It/sec 2.350, Tokens/sec 397.142, Trained Tokens 1294526, Peak mem 19.940 GB\n",
      "Iter 7830: Train loss 0.832, Learning Rate 4.000e-04, It/sec 2.624, Tokens/sec 389.904, Trained Tokens 1296012, Peak mem 19.940 GB\n",
      "Iter 7840: Train loss 0.844, Learning Rate 4.000e-04, It/sec 2.042, Tokens/sec 383.490, Trained Tokens 1297890, Peak mem 19.940 GB\n",
      "Iter 7850: Val loss 2.747, Val took 4.964s\n",
      "Iter 7850: Train loss 0.874, Learning Rate 4.000e-04, It/sec 43.371, Tokens/sec 5672.983, Trained Tokens 1299198, Peak mem 19.940 GB\n",
      "Iter 7860: Train loss 0.722, Learning Rate 4.000e-04, It/sec 2.893, Tokens/sec 389.137, Trained Tokens 1300543, Peak mem 19.940 GB\n",
      "Iter 7870: Train loss 0.978, Learning Rate 4.000e-04, It/sec 2.055, Tokens/sec 394.468, Trained Tokens 1302463, Peak mem 19.940 GB\n",
      "Iter 7880: Train loss 0.925, Learning Rate 4.000e-04, It/sec 2.589, Tokens/sec 369.971, Trained Tokens 1303892, Peak mem 19.940 GB\n",
      "Iter 7890: Train loss 0.831, Learning Rate 4.000e-04, It/sec 2.813, Tokens/sec 383.991, Trained Tokens 1305257, Peak mem 19.940 GB\n",
      "Iter 7900: Val loss 2.506, Val took 3.448s\n",
      "Iter 7900: Train loss 0.848, Learning Rate 4.000e-04, It/sec 3.999, Tokens/sec 785.402, Trained Tokens 1307221, Peak mem 19.940 GB\n",
      "Iter 7900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0007900_adapters.safetensors.\n",
      "Iter 7910: Train loss 0.739, Learning Rate 4.000e-04, It/sec 2.908, Tokens/sec 367.900, Trained Tokens 1308486, Peak mem 19.940 GB\n",
      "Iter 7920: Train loss 0.808, Learning Rate 4.000e-04, It/sec 2.877, Tokens/sec 371.670, Trained Tokens 1309778, Peak mem 19.940 GB\n",
      "Iter 7930: Train loss 0.815, Learning Rate 4.000e-04, It/sec 2.795, Tokens/sec 375.979, Trained Tokens 1311123, Peak mem 19.940 GB\n",
      "Iter 7940: Train loss 1.011, Learning Rate 4.000e-04, It/sec 1.811, Tokens/sec 368.733, Trained Tokens 1313159, Peak mem 19.940 GB\n",
      "Iter 7950: Val loss 2.333, Val took 4.287s\n",
      "Iter 7950: Train loss 0.750, Learning Rate 4.000e-04, It/sec 43.473, Tokens/sec 6112.325, Trained Tokens 1314565, Peak mem 19.940 GB\n",
      "Iter 7960: Train loss 0.965, Learning Rate 4.000e-04, It/sec 2.102, Tokens/sec 366.631, Trained Tokens 1316309, Peak mem 19.940 GB\n",
      "Iter 7970: Train loss 0.834, Learning Rate 4.000e-04, It/sec 2.281, Tokens/sec 390.355, Trained Tokens 1318020, Peak mem 19.940 GB\n",
      "Iter 7980: Train loss 0.918, Learning Rate 4.000e-04, It/sec 2.598, Tokens/sec 361.126, Trained Tokens 1319410, Peak mem 19.940 GB\n",
      "Iter 7990: Train loss 0.909, Learning Rate 4.000e-04, It/sec 2.338, Tokens/sec 390.623, Trained Tokens 1321081, Peak mem 19.940 GB\n",
      "Iter 8000: Val loss 2.102, Val took 3.928s\n",
      "Iter 8000: Train loss 0.849, Learning Rate 4.000e-04, It/sec 18.959, Tokens/sec 3103.602, Trained Tokens 1322718, Peak mem 19.940 GB\n",
      "Iter 8000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008000_adapters.safetensors.\n",
      "Iter 8010: Train loss 0.937, Learning Rate 4.000e-04, It/sec 2.439, Tokens/sec 354.886, Trained Tokens 1324173, Peak mem 19.940 GB\n",
      "Iter 8020: Train loss 1.048, Learning Rate 4.000e-04, It/sec 1.882, Tokens/sec 363.536, Trained Tokens 1326105, Peak mem 19.940 GB\n",
      "Iter 8030: Train loss 0.923, Learning Rate 4.000e-04, It/sec 2.330, Tokens/sec 372.098, Trained Tokens 1327702, Peak mem 19.940 GB\n",
      "Iter 8040: Train loss 0.890, Learning Rate 4.000e-04, It/sec 2.263, Tokens/sec 341.722, Trained Tokens 1329212, Peak mem 19.940 GB\n",
      "Iter 8050: Val loss 2.515, Val took 3.104s\n",
      "Iter 8050: Train loss 0.847, Learning Rate 4.000e-04, It/sec 15.354, Tokens/sec 2280.021, Trained Tokens 1330697, Peak mem 19.940 GB\n",
      "Iter 8060: Train loss 0.836, Learning Rate 4.000e-04, It/sec 3.629, Tokens/sec 352.328, Trained Tokens 1331668, Peak mem 19.940 GB\n",
      "Iter 8070: Train loss 0.893, Learning Rate 4.000e-04, It/sec 2.175, Tokens/sec 387.358, Trained Tokens 1333449, Peak mem 19.940 GB\n",
      "Iter 8080: Train loss 1.198, Learning Rate 4.000e-04, It/sec 1.822, Tokens/sec 384.794, Trained Tokens 1335561, Peak mem 19.940 GB\n",
      "Iter 8090: Train loss 0.872, Learning Rate 4.000e-04, It/sec 2.658, Tokens/sec 359.850, Trained Tokens 1336915, Peak mem 19.940 GB\n",
      "Iter 8100: Val loss 2.622, Val took 3.396s\n",
      "Iter 8100: Train loss 0.948, Learning Rate 4.000e-04, It/sec 22.517, Tokens/sec 4426.830, Trained Tokens 1338881, Peak mem 19.940 GB\n",
      "Iter 8100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008100_adapters.safetensors.\n",
      "Iter 8110: Train loss 0.935, Learning Rate 4.000e-04, It/sec 2.447, Tokens/sec 323.954, Trained Tokens 1340205, Peak mem 19.940 GB\n",
      "Iter 8120: Train loss 0.887, Learning Rate 4.000e-04, It/sec 2.988, Tokens/sec 374.339, Trained Tokens 1341458, Peak mem 19.940 GB\n",
      "Iter 8130: Train loss 1.071, Learning Rate 4.000e-04, It/sec 2.174, Tokens/sec 375.218, Trained Tokens 1343184, Peak mem 19.940 GB\n",
      "Iter 8140: Train loss 1.133, Learning Rate 4.000e-04, It/sec 1.759, Tokens/sec 390.062, Trained Tokens 1345402, Peak mem 19.940 GB\n",
      "Iter 8150: Val loss 2.426, Val took 3.325s\n",
      "Iter 8150: Train loss 1.170, Learning Rate 4.000e-04, It/sec 58.077, Tokens/sec 14809.749, Trained Tokens 1347952, Peak mem 19.940 GB\n",
      "Iter 8160: Train loss 1.044, Learning Rate 4.000e-04, It/sec 1.993, Tokens/sec 351.585, Trained Tokens 1349716, Peak mem 19.940 GB\n",
      "Iter 8170: Train loss 1.009, Learning Rate 4.000e-04, It/sec 1.259, Tokens/sec 410.350, Trained Tokens 1352975, Peak mem 19.940 GB\n",
      "Iter 8180: Train loss 0.843, Learning Rate 4.000e-04, It/sec 2.598, Tokens/sec 375.919, Trained Tokens 1354422, Peak mem 19.940 GB\n",
      "Iter 8190: Train loss 1.082, Learning Rate 4.000e-04, It/sec 1.961, Tokens/sec 400.388, Trained Tokens 1356464, Peak mem 19.940 GB\n",
      "Iter 8200: Val loss 2.618, Val took 3.677s\n",
      "Iter 8200: Train loss 1.063, Learning Rate 4.000e-04, It/sec 23.840, Tokens/sec 4377.054, Trained Tokens 1358300, Peak mem 19.940 GB\n",
      "Iter 8200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008200_adapters.safetensors.\n",
      "Iter 8210: Train loss 0.806, Learning Rate 4.000e-04, It/sec 1.985, Tokens/sec 331.049, Trained Tokens 1359968, Peak mem 19.940 GB\n",
      "Iter 8220: Train loss 0.817, Learning Rate 4.000e-04, It/sec 2.000, Tokens/sec 404.118, Trained Tokens 1361989, Peak mem 19.940 GB\n",
      "Iter 8230: Train loss 0.777, Learning Rate 4.000e-04, It/sec 2.662, Tokens/sec 398.021, Trained Tokens 1363484, Peak mem 19.940 GB\n",
      "Iter 8240: Train loss 0.848, Learning Rate 4.000e-04, It/sec 2.492, Tokens/sec 363.145, Trained Tokens 1364941, Peak mem 19.940 GB\n",
      "Iter 8250: Val loss 2.675, Val took 3.547s\n",
      "Iter 8250: Train loss 0.902, Learning Rate 4.000e-04, It/sec 2.185, Tokens/sec 546.098, Trained Tokens 1367440, Peak mem 19.940 GB\n",
      "Iter 8260: Train loss 0.799, Learning Rate 4.000e-04, It/sec 2.238, Tokens/sec 311.685, Trained Tokens 1368833, Peak mem 19.940 GB\n",
      "Iter 8270: Train loss 0.806, Learning Rate 4.000e-04, It/sec 2.695, Tokens/sec 378.166, Trained Tokens 1370236, Peak mem 19.940 GB\n",
      "Iter 8280: Train loss 0.836, Learning Rate 4.000e-04, It/sec 2.713, Tokens/sec 368.391, Trained Tokens 1371594, Peak mem 19.940 GB\n",
      "Iter 8290: Train loss 0.815, Learning Rate 4.000e-04, It/sec 2.915, Tokens/sec 367.580, Trained Tokens 1372855, Peak mem 19.940 GB\n",
      "Iter 8300: Val loss 2.315, Val took 2.610s\n",
      "Iter 8300: Train loss 1.070, Learning Rate 4.000e-04, It/sec 44.343, Tokens/sec 11977.013, Trained Tokens 1375556, Peak mem 19.940 GB\n",
      "Iter 8300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008300_adapters.safetensors.\n",
      "Iter 8310: Train loss 0.797, Learning Rate 4.000e-04, It/sec 2.799, Tokens/sec 387.166, Trained Tokens 1376939, Peak mem 19.940 GB\n",
      "Iter 8320: Train loss 0.872, Learning Rate 4.000e-04, It/sec 2.204, Tokens/sec 340.488, Trained Tokens 1378484, Peak mem 19.940 GB\n",
      "Iter 8330: Train loss 0.789, Learning Rate 4.000e-04, It/sec 2.567, Tokens/sec 383.222, Trained Tokens 1379977, Peak mem 19.940 GB\n",
      "Iter 8340: Train loss 1.087, Learning Rate 4.000e-04, It/sec 1.767, Tokens/sec 386.061, Trained Tokens 1382162, Peak mem 19.940 GB\n",
      "Iter 8350: Val loss 2.317, Val took 3.476s\n",
      "Iter 8350: Train loss 1.067, Learning Rate 4.000e-04, It/sec 4.818, Tokens/sec 1071.584, Trained Tokens 1384386, Peak mem 19.940 GB\n",
      "Iter 8360: Train loss 0.775, Learning Rate 4.000e-04, It/sec 3.120, Tokens/sec 361.601, Trained Tokens 1385545, Peak mem 19.940 GB\n",
      "Iter 8370: Train loss 0.801, Learning Rate 4.000e-04, It/sec 2.483, Tokens/sec 365.801, Trained Tokens 1387018, Peak mem 19.940 GB\n",
      "Iter 8380: Train loss 0.770, Learning Rate 4.000e-04, It/sec 2.837, Tokens/sec 393.826, Trained Tokens 1388406, Peak mem 19.940 GB\n",
      "Iter 8390: Train loss 0.894, Learning Rate 4.000e-04, It/sec 2.687, Tokens/sec 369.980, Trained Tokens 1389783, Peak mem 19.940 GB\n",
      "Iter 8400: Val loss 2.436, Val took 2.940s\n",
      "Iter 8400: Train loss 0.958, Learning Rate 4.000e-04, It/sec 43.632, Tokens/sec 7234.151, Trained Tokens 1391441, Peak mem 19.940 GB\n",
      "Iter 8400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008400_adapters.safetensors.\n",
      "Iter 8410: Train loss 0.950, Learning Rate 4.000e-04, It/sec 1.789, Tokens/sec 342.498, Trained Tokens 1393356, Peak mem 19.940 GB\n",
      "Iter 8420: Train loss 0.866, Learning Rate 4.000e-04, It/sec 2.063, Tokens/sec 375.616, Trained Tokens 1395177, Peak mem 19.940 GB\n",
      "Iter 8430: Train loss 0.824, Learning Rate 4.000e-04, It/sec 2.736, Tokens/sec 377.012, Trained Tokens 1396555, Peak mem 19.940 GB\n",
      "Iter 8440: Train loss 0.869, Learning Rate 4.000e-04, It/sec 2.399, Tokens/sec 371.595, Trained Tokens 1398104, Peak mem 19.940 GB\n",
      "Iter 8450: Val loss 2.535, Val took 2.625s\n",
      "Iter 8450: Train loss 0.858, Learning Rate 4.000e-04, It/sec 24.892, Tokens/sec 3305.642, Trained Tokens 1399432, Peak mem 19.940 GB\n",
      "Iter 8460: Train loss 0.817, Learning Rate 4.000e-04, It/sec 2.965, Tokens/sec 409.186, Trained Tokens 1400812, Peak mem 19.940 GB\n",
      "Iter 8470: Train loss 0.924, Learning Rate 4.000e-04, It/sec 2.427, Tokens/sec 367.034, Trained Tokens 1402324, Peak mem 19.940 GB\n",
      "Iter 8480: Train loss 0.939, Learning Rate 4.000e-04, It/sec 2.611, Tokens/sec 380.923, Trained Tokens 1403783, Peak mem 19.940 GB\n",
      "Iter 8490: Train loss 1.169, Learning Rate 4.000e-04, It/sec 1.531, Tokens/sec 400.054, Trained Tokens 1406396, Peak mem 19.940 GB\n",
      "Iter 8500: Val loss 2.157, Val took 3.018s\n",
      "Iter 8500: Train loss 1.012, Learning Rate 4.000e-04, It/sec 33.636, Tokens/sec 6158.741, Trained Tokens 1408227, Peak mem 19.940 GB\n",
      "Iter 8500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008500_adapters.safetensors.\n",
      "Iter 8510: Train loss 0.854, Learning Rate 4.000e-04, It/sec 2.583, Tokens/sec 285.667, Trained Tokens 1409333, Peak mem 19.940 GB\n",
      "Iter 8520: Train loss 0.882, Learning Rate 4.000e-04, It/sec 2.279, Tokens/sec 391.347, Trained Tokens 1411050, Peak mem 19.940 GB\n",
      "Iter 8530: Train loss 0.886, Learning Rate 4.000e-04, It/sec 2.901, Tokens/sec 386.353, Trained Tokens 1412382, Peak mem 19.940 GB\n",
      "Iter 8540: Train loss 1.006, Learning Rate 4.000e-04, It/sec 1.595, Tokens/sec 401.411, Trained Tokens 1414899, Peak mem 19.940 GB\n",
      "Iter 8550: Val loss 1.989, Val took 2.991s\n",
      "Iter 8550: Train loss 1.009, Learning Rate 4.000e-04, It/sec 21.206, Tokens/sec 3320.878, Trained Tokens 1416465, Peak mem 19.940 GB\n",
      "Iter 8560: Train loss 1.195, Learning Rate 4.000e-04, It/sec 1.458, Tokens/sec 395.496, Trained Tokens 1419178, Peak mem 19.940 GB\n",
      "Iter 8570: Train loss 0.874, Learning Rate 4.000e-04, It/sec 2.019, Tokens/sec 371.822, Trained Tokens 1421020, Peak mem 19.940 GB\n",
      "Iter 8580: Train loss 0.874, Learning Rate 4.000e-04, It/sec 2.743, Tokens/sec 379.096, Trained Tokens 1422402, Peak mem 19.940 GB\n",
      "Iter 8590: Train loss 0.828, Learning Rate 4.000e-04, It/sec 3.424, Tokens/sec 343.393, Trained Tokens 1423405, Peak mem 19.940 GB\n",
      "Iter 8600: Val loss 2.411, Val took 3.536s\n",
      "Iter 8600: Train loss 0.845, Learning Rate 4.000e-04, It/sec 35.594, Tokens/sec 3844.201, Trained Tokens 1424485, Peak mem 19.940 GB\n",
      "Iter 8600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008600_adapters.safetensors.\n",
      "Iter 8610: Train loss 0.981, Learning Rate 4.000e-04, It/sec 2.054, Tokens/sec 355.335, Trained Tokens 1426215, Peak mem 19.940 GB\n",
      "Iter 8620: Train loss 0.836, Learning Rate 4.000e-04, It/sec 2.510, Tokens/sec 382.088, Trained Tokens 1427737, Peak mem 19.940 GB\n",
      "Iter 8630: Train loss 0.790, Learning Rate 4.000e-04, It/sec 2.433, Tokens/sec 390.329, Trained Tokens 1429341, Peak mem 19.940 GB\n",
      "Iter 8640: Train loss 0.733, Learning Rate 4.000e-04, It/sec 2.415, Tokens/sec 382.248, Trained Tokens 1430924, Peak mem 19.940 GB\n",
      "Iter 8650: Val loss 1.965, Val took 2.902s\n",
      "Iter 8650: Train loss 0.751, Learning Rate 4.000e-04, It/sec 24.662, Tokens/sec 3684.446, Trained Tokens 1432418, Peak mem 19.940 GB\n",
      "Iter 8660: Train loss 0.737, Learning Rate 4.000e-04, It/sec 2.983, Tokens/sec 365.683, Trained Tokens 1433644, Peak mem 19.940 GB\n",
      "Iter 8670: Train loss 0.760, Learning Rate 4.000e-04, It/sec 2.420, Tokens/sec 351.559, Trained Tokens 1435097, Peak mem 19.940 GB\n",
      "Iter 8680: Train loss 0.760, Learning Rate 4.000e-04, It/sec 2.731, Tokens/sec 383.452, Trained Tokens 1436501, Peak mem 19.940 GB\n",
      "Iter 8690: Train loss 0.946, Learning Rate 4.000e-04, It/sec 2.052, Tokens/sec 388.859, Trained Tokens 1438396, Peak mem 19.940 GB\n",
      "Iter 8700: Val loss 2.440, Val took 2.815s\n",
      "Iter 8700: Train loss 0.960, Learning Rate 4.000e-04, It/sec 24.218, Tokens/sec 4274.544, Trained Tokens 1440161, Peak mem 19.940 GB\n",
      "Iter 8700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008700_adapters.safetensors.\n",
      "Iter 8710: Train loss 0.758, Learning Rate 4.000e-04, It/sec 2.289, Tokens/sec 371.767, Trained Tokens 1441785, Peak mem 19.940 GB\n",
      "Iter 8720: Train loss 0.950, Learning Rate 4.000e-04, It/sec 1.818, Tokens/sec 400.463, Trained Tokens 1443988, Peak mem 19.940 GB\n",
      "Iter 8730: Train loss 0.880, Learning Rate 4.000e-04, It/sec 2.224, Tokens/sec 400.798, Trained Tokens 1445790, Peak mem 19.940 GB\n",
      "Iter 8740: Train loss 0.764, Learning Rate 4.000e-04, It/sec 2.243, Tokens/sec 385.373, Trained Tokens 1447508, Peak mem 19.940 GB\n",
      "Iter 8750: Val loss 2.288, Val took 3.215s\n",
      "Iter 8750: Train loss 0.830, Learning Rate 4.000e-04, It/sec 25.081, Tokens/sec 4078.177, Trained Tokens 1449134, Peak mem 19.940 GB\n",
      "Iter 8760: Train loss 0.857, Learning Rate 4.000e-04, It/sec 1.869, Tokens/sec 352.708, Trained Tokens 1451021, Peak mem 19.940 GB\n",
      "Iter 8770: Train loss 0.734, Learning Rate 4.000e-04, It/sec 2.751, Tokens/sec 371.628, Trained Tokens 1452372, Peak mem 19.940 GB\n",
      "Iter 8780: Train loss 0.805, Learning Rate 4.000e-04, It/sec 2.580, Tokens/sec 395.990, Trained Tokens 1453907, Peak mem 19.940 GB\n",
      "Iter 8790: Train loss 0.734, Learning Rate 4.000e-04, It/sec 3.227, Tokens/sec 373.037, Trained Tokens 1455063, Peak mem 19.940 GB\n",
      "Iter 8800: Val loss 2.522, Val took 3.480s\n",
      "Iter 8800: Train loss 0.867, Learning Rate 4.000e-04, It/sec 44.605, Tokens/sec 7404.432, Trained Tokens 1456723, Peak mem 19.940 GB\n",
      "Iter 8800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008800_adapters.safetensors.\n",
      "Iter 8810: Train loss 0.879, Learning Rate 4.000e-04, It/sec 1.947, Tokens/sec 358.634, Trained Tokens 1458565, Peak mem 19.940 GB\n",
      "Iter 8820: Train loss 0.766, Learning Rate 4.000e-04, It/sec 3.120, Tokens/sec 379.353, Trained Tokens 1459781, Peak mem 19.940 GB\n",
      "Iter 8830: Train loss 0.827, Learning Rate 4.000e-04, It/sec 2.433, Tokens/sec 399.790, Trained Tokens 1461424, Peak mem 19.940 GB\n",
      "Iter 8840: Train loss 0.920, Learning Rate 4.000e-04, It/sec 2.582, Tokens/sec 381.399, Trained Tokens 1462901, Peak mem 19.940 GB\n",
      "Iter 8850: Val loss 2.416, Val took 2.938s\n",
      "Iter 8850: Train loss 1.018, Learning Rate 4.000e-04, It/sec 3.789, Tokens/sec 826.829, Trained Tokens 1465083, Peak mem 19.940 GB\n",
      "Iter 8860: Train loss 0.990, Learning Rate 4.000e-04, It/sec 1.912, Tokens/sec 386.033, Trained Tokens 1467102, Peak mem 19.940 GB\n",
      "Iter 8870: Train loss 0.870, Learning Rate 4.000e-04, It/sec 2.286, Tokens/sec 369.000, Trained Tokens 1468716, Peak mem 19.940 GB\n",
      "Iter 8880: Train loss 0.807, Learning Rate 4.000e-04, It/sec 2.739, Tokens/sec 394.213, Trained Tokens 1470155, Peak mem 19.940 GB\n",
      "Iter 8890: Train loss 0.973, Learning Rate 4.000e-04, It/sec 2.118, Tokens/sec 382.050, Trained Tokens 1471959, Peak mem 19.940 GB\n",
      "Iter 8900: Val loss 2.495, Val took 3.295s\n",
      "Iter 8900: Train loss 1.024, Learning Rate 4.000e-04, It/sec 3.021, Tokens/sec 577.049, Trained Tokens 1473869, Peak mem 19.940 GB\n",
      "Iter 8900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0008900_adapters.safetensors.\n",
      "Iter 8910: Train loss 0.847, Learning Rate 4.000e-04, It/sec 2.946, Tokens/sec 361.491, Trained Tokens 1475096, Peak mem 19.940 GB\n",
      "Iter 8920: Train loss 0.953, Learning Rate 4.000e-04, It/sec 2.381, Tokens/sec 347.833, Trained Tokens 1476557, Peak mem 19.940 GB\n",
      "Iter 8930: Train loss 1.134, Learning Rate 4.000e-04, It/sec 1.495, Tokens/sec 409.571, Trained Tokens 1479297, Peak mem 19.940 GB\n",
      "Iter 8940: Train loss 0.860, Learning Rate 4.000e-04, It/sec 3.116, Tokens/sec 366.147, Trained Tokens 1480472, Peak mem 19.940 GB\n",
      "Iter 8950: Val loss 1.982, Val took 3.070s\n",
      "Iter 8950: Train loss 0.864, Learning Rate 4.000e-04, It/sec 35.249, Tokens/sec 3835.120, Trained Tokens 1481560, Peak mem 19.940 GB\n",
      "Iter 8960: Train loss 0.977, Learning Rate 4.000e-04, It/sec 1.999, Tokens/sec 341.279, Trained Tokens 1483267, Peak mem 19.940 GB\n",
      "Iter 8970: Train loss 0.868, Learning Rate 4.000e-04, It/sec 2.587, Tokens/sec 383.892, Trained Tokens 1484751, Peak mem 19.940 GB\n",
      "Iter 8980: Train loss 1.015, Learning Rate 4.000e-04, It/sec 2.018, Tokens/sec 366.432, Trained Tokens 1486567, Peak mem 19.940 GB\n",
      "Iter 8990: Train loss 0.959, Learning Rate 4.000e-04, It/sec 2.164, Tokens/sec 350.947, Trained Tokens 1488189, Peak mem 19.940 GB\n",
      "Iter 9000: Val loss 2.194, Val took 3.590s\n",
      "Iter 9000: Train loss 1.095, Learning Rate 4.000e-04, It/sec 21.439, Tokens/sec 5145.312, Trained Tokens 1490589, Peak mem 19.940 GB\n",
      "Iter 9000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009000_adapters.safetensors.\n",
      "Iter 9010: Train loss 0.901, Learning Rate 4.000e-04, It/sec 2.615, Tokens/sec 366.651, Trained Tokens 1491991, Peak mem 19.940 GB\n",
      "Iter 9020: Train loss 1.061, Learning Rate 4.000e-04, It/sec 1.844, Tokens/sec 394.444, Trained Tokens 1494130, Peak mem 19.940 GB\n",
      "Iter 9030: Train loss 0.705, Learning Rate 4.000e-04, It/sec 2.665, Tokens/sec 378.231, Trained Tokens 1495549, Peak mem 19.940 GB\n",
      "Iter 9040: Train loss 1.054, Learning Rate 4.000e-04, It/sec 1.508, Tokens/sec 402.777, Trained Tokens 1498220, Peak mem 19.940 GB\n",
      "Iter 9050: Val loss 2.587, Val took 3.498s\n",
      "Iter 9050: Train loss 0.863, Learning Rate 4.000e-04, It/sec 34.102, Tokens/sec 5892.794, Trained Tokens 1499948, Peak mem 19.940 GB\n",
      "Iter 9060: Train loss 0.820, Learning Rate 4.000e-04, It/sec 2.457, Tokens/sec 338.784, Trained Tokens 1501327, Peak mem 19.940 GB\n",
      "Iter 9070: Train loss 0.702, Learning Rate 4.000e-04, It/sec 2.945, Tokens/sec 365.524, Trained Tokens 1502568, Peak mem 19.940 GB\n",
      "Iter 9080: Train loss 0.735, Learning Rate 4.000e-04, It/sec 2.964, Tokens/sec 398.021, Trained Tokens 1503911, Peak mem 19.940 GB\n",
      "Iter 9090: Train loss 0.716, Learning Rate 4.000e-04, It/sec 3.014, Tokens/sec 367.951, Trained Tokens 1505132, Peak mem 19.940 GB\n",
      "Iter 9100: Val loss 2.668, Val took 3.358s\n",
      "Iter 9100: Train loss 0.831, Learning Rate 4.000e-04, It/sec 9.914, Tokens/sec 1500.914, Trained Tokens 1506646, Peak mem 19.940 GB\n",
      "Iter 9100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009100_adapters.safetensors.\n",
      "Iter 9110: Train loss 0.792, Learning Rate 4.000e-04, It/sec 2.508, Tokens/sec 379.248, Trained Tokens 1508158, Peak mem 19.940 GB\n",
      "Iter 9120: Train loss 0.924, Learning Rate 4.000e-04, It/sec 2.225, Tokens/sec 403.620, Trained Tokens 1509972, Peak mem 19.940 GB\n",
      "Iter 9130: Train loss 0.831, Learning Rate 4.000e-04, It/sec 2.421, Tokens/sec 400.475, Trained Tokens 1511626, Peak mem 19.940 GB\n",
      "Iter 9140: Train loss 0.849, Learning Rate 4.000e-04, It/sec 2.217, Tokens/sec 382.378, Trained Tokens 1513351, Peak mem 19.940 GB\n",
      "Iter 9150: Val loss 2.381, Val took 3.214s\n",
      "Iter 9150: Train loss 0.825, Learning Rate 4.000e-04, It/sec 35.729, Tokens/sec 4869.920, Trained Tokens 1514714, Peak mem 19.940 GB\n",
      "Iter 9160: Train loss 0.872, Learning Rate 4.000e-04, It/sec 2.544, Tokens/sec 389.813, Trained Tokens 1516246, Peak mem 19.940 GB\n",
      "Iter 9170: Train loss 0.809, Learning Rate 4.000e-04, It/sec 2.158, Tokens/sec 385.928, Trained Tokens 1518034, Peak mem 19.940 GB\n",
      "Iter 9180: Train loss 0.795, Learning Rate 4.000e-04, It/sec 3.079, Tokens/sec 372.526, Trained Tokens 1519244, Peak mem 19.940 GB\n",
      "Iter 9190: Train loss 0.853, Learning Rate 4.000e-04, It/sec 2.510, Tokens/sec 397.532, Trained Tokens 1520828, Peak mem 19.940 GB\n",
      "Iter 9200: Val loss 2.512, Val took 3.552s\n",
      "Iter 9200: Train loss 0.821, Learning Rate 4.000e-04, It/sec 35.337, Tokens/sec 5957.823, Trained Tokens 1522514, Peak mem 19.940 GB\n",
      "Iter 9200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009200_adapters.safetensors.\n",
      "Iter 9210: Train loss 0.845, Learning Rate 4.000e-04, It/sec 2.281, Tokens/sec 372.737, Trained Tokens 1524148, Peak mem 19.940 GB\n",
      "Iter 9220: Train loss 0.912, Learning Rate 4.000e-04, It/sec 2.635, Tokens/sec 386.865, Trained Tokens 1525616, Peak mem 19.940 GB\n",
      "Iter 9230: Train loss 0.896, Learning Rate 4.000e-04, It/sec 2.599, Tokens/sec 383.904, Trained Tokens 1527093, Peak mem 19.940 GB\n",
      "Iter 9240: Train loss 1.007, Learning Rate 4.000e-04, It/sec 1.966, Tokens/sec 384.737, Trained Tokens 1529050, Peak mem 19.940 GB\n",
      "Iter 9250: Val loss 2.401, Val took 3.350s\n",
      "Iter 9250: Train loss 0.966, Learning Rate 4.000e-04, It/sec 16.431, Tokens/sec 2720.933, Trained Tokens 1530706, Peak mem 19.940 GB\n",
      "Iter 9260: Train loss 0.730, Learning Rate 4.000e-04, It/sec 3.098, Tokens/sec 383.816, Trained Tokens 1531945, Peak mem 19.940 GB\n",
      "Iter 9270: Train loss 0.945, Learning Rate 4.000e-04, It/sec 1.795, Tokens/sec 402.535, Trained Tokens 1534188, Peak mem 19.940 GB\n",
      "Iter 9280: Train loss 1.120, Learning Rate 4.000e-04, It/sec 1.117, Tokens/sec 378.294, Trained Tokens 1537575, Peak mem 19.940 GB\n",
      "Iter 9290: Train loss 0.779, Learning Rate 4.000e-04, It/sec 3.525, Tokens/sec 346.814, Trained Tokens 1538559, Peak mem 19.940 GB\n",
      "Iter 9300: Val loss 2.301, Val took 2.744s\n",
      "Iter 9300: Train loss 0.767, Learning Rate 4.000e-04, It/sec 9.323, Tokens/sec 1799.270, Trained Tokens 1540489, Peak mem 19.940 GB\n",
      "Iter 9300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009300_adapters.safetensors.\n",
      "Iter 9310: Train loss 0.996, Learning Rate 4.000e-04, It/sec 1.647, Tokens/sec 390.429, Trained Tokens 1542860, Peak mem 19.940 GB\n",
      "Iter 9320: Train loss 0.995, Learning Rate 4.000e-04, It/sec 2.543, Tokens/sec 362.868, Trained Tokens 1544287, Peak mem 19.940 GB\n",
      "Iter 9330: Train loss 0.862, Learning Rate 4.000e-04, It/sec 2.447, Tokens/sec 406.258, Trained Tokens 1545947, Peak mem 19.940 GB\n",
      "Iter 9340: Train loss 0.809, Learning Rate 4.000e-04, It/sec 3.141, Tokens/sec 362.111, Trained Tokens 1547100, Peak mem 19.940 GB\n",
      "Iter 9350: Val loss 2.203, Val took 2.990s\n",
      "Iter 9350: Train loss 0.966, Learning Rate 4.000e-04, It/sec 15.350, Tokens/sec 2205.747, Trained Tokens 1548537, Peak mem 19.940 GB\n",
      "Iter 9360: Train loss 0.870, Learning Rate 4.000e-04, It/sec 2.624, Tokens/sec 384.701, Trained Tokens 1550003, Peak mem 19.940 GB\n",
      "Iter 9370: Train loss 0.806, Learning Rate 4.000e-04, It/sec 2.325, Tokens/sec 365.015, Trained Tokens 1551573, Peak mem 19.940 GB\n",
      "Iter 9380: Train loss 0.848, Learning Rate 4.000e-04, It/sec 1.936, Tokens/sec 358.528, Trained Tokens 1553425, Peak mem 19.940 GB\n",
      "Iter 9390: Train loss 0.925, Learning Rate 4.000e-04, It/sec 1.864, Tokens/sec 375.562, Trained Tokens 1555440, Peak mem 19.940 GB\n",
      "Iter 9400: Val loss 2.497, Val took 3.268s\n",
      "Iter 9400: Train loss 0.968, Learning Rate 4.000e-04, It/sec 4.080, Tokens/sec 788.318, Trained Tokens 1557372, Peak mem 19.940 GB\n",
      "Iter 9400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009400_adapters.safetensors.\n",
      "Iter 9410: Train loss 0.964, Learning Rate 4.000e-04, It/sec 2.469, Tokens/sec 361.651, Trained Tokens 1558837, Peak mem 19.940 GB\n",
      "Iter 9420: Train loss 0.932, Learning Rate 4.000e-04, It/sec 2.043, Tokens/sec 385.861, Trained Tokens 1560726, Peak mem 19.940 GB\n",
      "Iter 9430: Train loss 0.870, Learning Rate 4.000e-04, It/sec 2.788, Tokens/sec 367.718, Trained Tokens 1562045, Peak mem 19.940 GB\n",
      "Iter 9440: Train loss 0.771, Learning Rate 4.000e-04, It/sec 2.863, Tokens/sec 389.056, Trained Tokens 1563404, Peak mem 19.940 GB\n",
      "Iter 9450: Val loss 2.651, Val took 3.171s\n",
      "Iter 9450: Train loss 0.984, Learning Rate 4.000e-04, It/sec 35.036, Tokens/sec 7827.118, Trained Tokens 1565638, Peak mem 19.940 GB\n",
      "Iter 9460: Train loss 0.793, Learning Rate 4.000e-04, It/sec 1.844, Tokens/sec 387.651, Trained Tokens 1567740, Peak mem 19.940 GB\n",
      "Iter 9470: Train loss 0.834, Learning Rate 4.000e-04, It/sec 2.179, Tokens/sec 390.995, Trained Tokens 1569534, Peak mem 19.940 GB\n",
      "Iter 9480: Train loss 0.748, Learning Rate 4.000e-04, It/sec 2.482, Tokens/sec 395.080, Trained Tokens 1571126, Peak mem 19.940 GB\n",
      "Iter 9490: Train loss 0.632, Learning Rate 4.000e-04, It/sec 2.934, Tokens/sec 362.389, Trained Tokens 1572361, Peak mem 19.940 GB\n",
      "Iter 9500: Val loss 2.433, Val took 3.198s\n",
      "Iter 9500: Train loss 0.774, Learning Rate 4.000e-04, It/sec 35.461, Tokens/sec 5854.581, Trained Tokens 1574012, Peak mem 19.940 GB\n",
      "Iter 9500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009500_adapters.safetensors.\n",
      "Iter 9510: Train loss 0.710, Learning Rate 4.000e-04, It/sec 2.794, Tokens/sec 353.690, Trained Tokens 1575278, Peak mem 19.940 GB\n",
      "Iter 9520: Train loss 0.804, Learning Rate 4.000e-04, It/sec 2.442, Tokens/sec 374.820, Trained Tokens 1576813, Peak mem 19.940 GB\n",
      "Iter 9530: Train loss 0.749, Learning Rate 4.000e-04, It/sec 3.044, Tokens/sec 367.425, Trained Tokens 1578020, Peak mem 19.940 GB\n",
      "Iter 9540: Train loss 0.771, Learning Rate 4.000e-04, It/sec 2.620, Tokens/sec 378.377, Trained Tokens 1579464, Peak mem 19.940 GB\n",
      "Iter 9550: Val loss 2.729, Val took 3.072s\n",
      "Iter 9550: Train loss 0.773, Learning Rate 4.000e-04, It/sec 15.497, Tokens/sec 2722.845, Trained Tokens 1581221, Peak mem 19.940 GB\n",
      "Iter 9560: Train loss 0.799, Learning Rate 4.000e-04, It/sec 1.886, Tokens/sec 320.661, Trained Tokens 1582921, Peak mem 19.940 GB\n",
      "Iter 9570: Train loss 0.772, Learning Rate 4.000e-04, It/sec 2.983, Tokens/sec 387.439, Trained Tokens 1584220, Peak mem 19.940 GB\n",
      "Iter 9580: Train loss 0.804, Learning Rate 4.000e-04, It/sec 2.791, Tokens/sec 376.572, Trained Tokens 1585569, Peak mem 19.940 GB\n",
      "Iter 9590: Train loss 0.861, Learning Rate 4.000e-04, It/sec 2.175, Tokens/sec 386.762, Trained Tokens 1587347, Peak mem 19.940 GB\n",
      "Iter 9600: Val loss 2.134, Val took 3.166s\n",
      "Iter 9600: Train loss 0.798, Learning Rate 4.000e-04, It/sec 61.496, Tokens/sec 7730.017, Trained Tokens 1588604, Peak mem 19.940 GB\n",
      "Iter 9600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009600_adapters.safetensors.\n",
      "Iter 9610: Train loss 0.926, Learning Rate 4.000e-04, It/sec 1.286, Tokens/sec 394.519, Trained Tokens 1591671, Peak mem 19.940 GB\n",
      "Iter 9620: Train loss 0.804, Learning Rate 4.000e-04, It/sec 2.105, Tokens/sec 391.490, Trained Tokens 1593531, Peak mem 19.940 GB\n",
      "Iter 9630: Train loss 0.801, Learning Rate 4.000e-04, It/sec 2.478, Tokens/sec 363.993, Trained Tokens 1595000, Peak mem 19.940 GB\n",
      "Iter 9640: Train loss 0.769, Learning Rate 4.000e-04, It/sec 3.187, Tokens/sec 358.490, Trained Tokens 1596125, Peak mem 19.940 GB\n",
      "Iter 9650: Val loss 2.254, Val took 2.655s\n",
      "Iter 9650: Train loss 0.883, Learning Rate 4.000e-04, It/sec 15.815, Tokens/sec 3069.768, Trained Tokens 1598066, Peak mem 19.940 GB\n",
      "Iter 9660: Train loss 0.789, Learning Rate 4.000e-04, It/sec 2.345, Tokens/sec 349.675, Trained Tokens 1599557, Peak mem 19.940 GB\n",
      "Iter 9670: Train loss 0.798, Learning Rate 4.000e-04, It/sec 2.516, Tokens/sec 385.666, Trained Tokens 1601090, Peak mem 19.940 GB\n",
      "Iter 9680: Train loss 0.837, Learning Rate 4.000e-04, It/sec 2.314, Tokens/sec 379.452, Trained Tokens 1602730, Peak mem 19.940 GB\n",
      "Iter 9690: Train loss 0.840, Learning Rate 4.000e-04, It/sec 2.308, Tokens/sec 381.808, Trained Tokens 1604384, Peak mem 19.940 GB\n",
      "Iter 9700: Val loss 2.260, Val took 3.408s\n",
      "Iter 9700: Train loss 1.089, Learning Rate 4.000e-04, It/sec 34.622, Tokens/sec 9271.897, Trained Tokens 1607062, Peak mem 19.940 GB\n",
      "Iter 9700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009700_adapters.safetensors.\n",
      "Iter 9710: Train loss 0.963, Learning Rate 4.000e-04, It/sec 1.834, Tokens/sec 358.152, Trained Tokens 1609015, Peak mem 19.940 GB\n",
      "Iter 9720: Train loss 0.942, Learning Rate 4.000e-04, It/sec 2.557, Tokens/sec 388.471, Trained Tokens 1610534, Peak mem 19.940 GB\n",
      "Iter 9730: Train loss 0.768, Learning Rate 4.000e-04, It/sec 2.756, Tokens/sec 345.656, Trained Tokens 1611788, Peak mem 19.940 GB\n",
      "Iter 9740: Train loss 0.759, Learning Rate 4.000e-04, It/sec 2.797, Tokens/sec 394.877, Trained Tokens 1613200, Peak mem 19.940 GB\n",
      "Iter 9750: Val loss 2.309, Val took 3.215s\n",
      "Iter 9750: Train loss 0.785, Learning Rate 4.000e-04, It/sec 33.194, Tokens/sec 4534.253, Trained Tokens 1614566, Peak mem 19.940 GB\n",
      "Iter 9760: Train loss 1.131, Learning Rate 4.000e-04, It/sec 1.437, Tokens/sec 390.893, Trained Tokens 1617286, Peak mem 19.940 GB\n",
      "Iter 9770: Train loss 0.985, Learning Rate 4.000e-04, It/sec 1.844, Tokens/sec 405.634, Trained Tokens 1619486, Peak mem 19.940 GB\n",
      "Iter 9780: Train loss 0.792, Learning Rate 4.000e-04, It/sec 2.143, Tokens/sec 349.093, Trained Tokens 1621115, Peak mem 19.940 GB\n",
      "Iter 9790: Train loss 0.860, Learning Rate 4.000e-04, It/sec 2.347, Tokens/sec 375.074, Trained Tokens 1622713, Peak mem 19.940 GB\n",
      "Iter 9800: Val loss 2.314, Val took 4.219s\n",
      "Iter 9800: Train loss 0.795, Learning Rate 4.000e-04, It/sec 32.538, Tokens/sec 4477.203, Trained Tokens 1624089, Peak mem 19.940 GB\n",
      "Iter 9800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009800_adapters.safetensors.\n",
      "Iter 9810: Train loss 0.876, Learning Rate 4.000e-04, It/sec 2.388, Tokens/sec 370.888, Trained Tokens 1625642, Peak mem 19.940 GB\n",
      "Iter 9820: Train loss 0.828, Learning Rate 4.000e-04, It/sec 2.770, Tokens/sec 387.002, Trained Tokens 1627039, Peak mem 19.940 GB\n",
      "Iter 9830: Train loss 0.914, Learning Rate 4.000e-04, It/sec 2.574, Tokens/sec 395.175, Trained Tokens 1628574, Peak mem 19.940 GB\n",
      "Iter 9840: Train loss 0.834, Learning Rate 4.000e-04, It/sec 2.726, Tokens/sec 377.756, Trained Tokens 1629960, Peak mem 19.940 GB\n",
      "Iter 9850: Val loss 2.634, Val took 3.771s\n",
      "Iter 9850: Train loss 0.820, Learning Rate 4.000e-04, It/sec 15.140, Tokens/sec 2934.124, Trained Tokens 1631898, Peak mem 19.940 GB\n",
      "Iter 9860: Train loss 0.628, Learning Rate 4.000e-04, It/sec 2.759, Tokens/sec 359.817, Trained Tokens 1633202, Peak mem 19.940 GB\n",
      "Iter 9870: Train loss 0.667, Learning Rate 4.000e-04, It/sec 2.938, Tokens/sec 335.554, Trained Tokens 1634344, Peak mem 19.940 GB\n",
      "Iter 9880: Train loss 0.759, Learning Rate 4.000e-04, It/sec 2.605, Tokens/sec 378.830, Trained Tokens 1635798, Peak mem 19.940 GB\n",
      "Iter 9890: Train loss 0.802, Learning Rate 4.000e-04, It/sec 2.428, Tokens/sec 388.494, Trained Tokens 1637398, Peak mem 19.940 GB\n",
      "Iter 9900: Val loss 2.427, Val took 3.091s\n",
      "Iter 9900: Train loss 0.729, Learning Rate 4.000e-04, It/sec 18.432, Tokens/sec 2766.655, Trained Tokens 1638899, Peak mem 19.940 GB\n",
      "Iter 9900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0009900_adapters.safetensors.\n",
      "Iter 9910: Train loss 0.925, Learning Rate 4.000e-04, It/sec 1.751, Tokens/sec 347.868, Trained Tokens 1640886, Peak mem 19.940 GB\n",
      "Iter 9920: Train loss 0.938, Learning Rate 4.000e-04, It/sec 1.804, Tokens/sec 375.121, Trained Tokens 1642965, Peak mem 19.940 GB\n",
      "Iter 9930: Train loss 0.719, Learning Rate 4.000e-04, It/sec 2.768, Tokens/sec 371.744, Trained Tokens 1644308, Peak mem 19.940 GB\n",
      "Iter 9940: Train loss 0.730, Learning Rate 4.000e-04, It/sec 2.859, Tokens/sec 387.909, Trained Tokens 1645665, Peak mem 19.940 GB\n",
      "Iter 9950: Val loss 2.255, Val took 2.657s\n",
      "Iter 9950: Train loss 0.826, Learning Rate 4.000e-04, It/sec 26.196, Tokens/sec 4673.370, Trained Tokens 1647449, Peak mem 19.940 GB\n",
      "Iter 9960: Train loss 0.713, Learning Rate 4.000e-04, It/sec 2.986, Tokens/sec 361.043, Trained Tokens 1648658, Peak mem 19.940 GB\n",
      "Iter 9970: Train loss 0.807, Learning Rate 4.000e-04, It/sec 2.383, Tokens/sec 373.443, Trained Tokens 1650225, Peak mem 19.940 GB\n",
      "Iter 9980: Train loss 0.843, Learning Rate 4.000e-04, It/sec 2.296, Tokens/sec 362.144, Trained Tokens 1651802, Peak mem 19.940 GB\n",
      "Iter 9990: Train loss 0.801, Learning Rate 4.000e-04, It/sec 2.784, Tokens/sec 356.875, Trained Tokens 1653084, Peak mem 19.940 GB\n",
      "Iter 10000: Val loss 2.366, Val took 3.658s\n",
      "Iter 10000: Train loss 0.732, Learning Rate 4.000e-04, It/sec 17.850, Tokens/sec 2922.088, Trained Tokens 1654721, Peak mem 19.940 GB\n",
      "Iter 10000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010000_adapters.safetensors.\n",
      "Iter 10010: Train loss 0.753, Learning Rate 4.000e-04, It/sec 2.142, Tokens/sec 379.529, Trained Tokens 1656493, Peak mem 19.940 GB\n",
      "Iter 10020: Train loss 0.799, Learning Rate 4.000e-04, It/sec 2.408, Tokens/sec 388.696, Trained Tokens 1658107, Peak mem 19.940 GB\n",
      "Iter 10030: Train loss 0.781, Learning Rate 4.000e-04, It/sec 2.630, Tokens/sec 372.619, Trained Tokens 1659524, Peak mem 19.940 GB\n",
      "Iter 10040: Train loss 0.828, Learning Rate 4.000e-04, It/sec 2.737, Tokens/sec 386.783, Trained Tokens 1660937, Peak mem 19.940 GB\n",
      "Iter 10050: Val loss 1.980, Val took 2.860s\n",
      "Iter 10050: Train loss 0.814, Learning Rate 4.000e-04, It/sec 34.589, Tokens/sec 6775.897, Trained Tokens 1662896, Peak mem 19.940 GB\n",
      "Iter 10060: Train loss 0.886, Learning Rate 4.000e-04, It/sec 2.039, Tokens/sec 329.932, Trained Tokens 1664514, Peak mem 19.940 GB\n",
      "Iter 10070: Train loss 0.777, Learning Rate 4.000e-04, It/sec 2.867, Tokens/sec 377.544, Trained Tokens 1665831, Peak mem 19.940 GB\n",
      "Iter 10080: Train loss 0.880, Learning Rate 4.000e-04, It/sec 2.496, Tokens/sec 389.807, Trained Tokens 1667393, Peak mem 19.940 GB\n",
      "Iter 10090: Train loss 0.874, Learning Rate 4.000e-04, It/sec 1.320, Tokens/sec 367.427, Trained Tokens 1670177, Peak mem 19.940 GB\n",
      "Iter 10100: Val loss 2.340, Val took 3.829s\n",
      "Iter 10100: Train loss 0.727, Learning Rate 4.000e-04, It/sec 23.360, Tokens/sec 2541.588, Trained Tokens 1671265, Peak mem 19.940 GB\n",
      "Iter 10100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010100_adapters.safetensors.\n",
      "Iter 10110: Train loss 0.896, Learning Rate 4.000e-04, It/sec 2.303, Tokens/sec 384.829, Trained Tokens 1672936, Peak mem 19.940 GB\n",
      "Iter 10120: Train loss 1.160, Learning Rate 4.000e-04, It/sec 1.663, Tokens/sec 419.827, Trained Tokens 1675460, Peak mem 19.940 GB\n",
      "Iter 10130: Train loss 0.984, Learning Rate 4.000e-04, It/sec 2.226, Tokens/sec 386.404, Trained Tokens 1677196, Peak mem 19.940 GB\n",
      "Iter 10140: Train loss 0.887, Learning Rate 4.000e-04, It/sec 2.473, Tokens/sec 339.602, Trained Tokens 1678569, Peak mem 19.940 GB\n",
      "Iter 10150: Val loss 2.341, Val took 3.937s\n",
      "Iter 10150: Train loss 0.793, Learning Rate 4.000e-04, It/sec 43.308, Tokens/sec 6067.502, Trained Tokens 1679970, Peak mem 19.940 GB\n",
      "Iter 10160: Train loss 0.861, Learning Rate 4.000e-04, It/sec 1.676, Tokens/sec 265.498, Trained Tokens 1681554, Peak mem 19.940 GB\n",
      "Iter 10170: Train loss 0.926, Learning Rate 4.000e-04, It/sec 2.458, Tokens/sec 338.743, Trained Tokens 1682932, Peak mem 19.940 GB\n",
      "Iter 10180: Train loss 0.985, Learning Rate 4.000e-04, It/sec 2.085, Tokens/sec 379.197, Trained Tokens 1684751, Peak mem 19.940 GB\n",
      "Iter 10190: Train loss 0.947, Learning Rate 4.000e-04, It/sec 1.811, Tokens/sec 389.955, Trained Tokens 1686904, Peak mem 19.940 GB\n",
      "Iter 10200: Val loss 2.605, Val took 4.184s\n",
      "Iter 10200: Train loss 0.801, Learning Rate 4.000e-04, It/sec 27.130, Tokens/sec 4159.049, Trained Tokens 1688437, Peak mem 19.940 GB\n",
      "Iter 10200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010200_adapters.safetensors.\n",
      "Iter 10210: Train loss 0.846, Learning Rate 4.000e-04, It/sec 2.134, Tokens/sec 355.365, Trained Tokens 1690102, Peak mem 19.940 GB\n",
      "Iter 10220: Train loss 1.075, Learning Rate 4.000e-04, It/sec 1.758, Tokens/sec 395.495, Trained Tokens 1692352, Peak mem 19.940 GB\n",
      "Iter 10230: Train loss 0.788, Learning Rate 4.000e-04, It/sec 3.339, Tokens/sec 358.931, Trained Tokens 1693427, Peak mem 19.940 GB\n",
      "Iter 10240: Train loss 1.026, Learning Rate 4.000e-04, It/sec 2.293, Tokens/sec 403.031, Trained Tokens 1695185, Peak mem 19.940 GB\n",
      "Iter 10250: Val loss 2.461, Val took 3.323s\n",
      "Iter 10250: Train loss 1.047, Learning Rate 4.000e-04, It/sec 24.039, Tokens/sec 6466.480, Trained Tokens 1697875, Peak mem 19.940 GB\n",
      "Iter 10260: Train loss 0.698, Learning Rate 4.000e-04, It/sec 2.385, Tokens/sec 299.097, Trained Tokens 1699129, Peak mem 19.940 GB\n",
      "Iter 10270: Train loss 0.879, Learning Rate 4.000e-04, It/sec 2.075, Tokens/sec 397.558, Trained Tokens 1701045, Peak mem 19.940 GB\n",
      "Iter 10280: Train loss 0.687, Learning Rate 4.000e-04, It/sec 2.792, Tokens/sec 367.704, Trained Tokens 1702362, Peak mem 19.940 GB\n",
      "Iter 10290: Train loss 0.693, Learning Rate 4.000e-04, It/sec 2.594, Tokens/sec 338.515, Trained Tokens 1703667, Peak mem 19.940 GB\n",
      "Iter 10300: Val loss 2.486, Val took 4.024s\n",
      "Iter 10300: Train loss 0.723, Learning Rate 4.000e-04, It/sec 35.127, Tokens/sec 3825.315, Trained Tokens 1704756, Peak mem 19.940 GB\n",
      "Iter 10300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010300_adapters.safetensors.\n",
      "Iter 10310: Train loss 0.790, Learning Rate 4.000e-04, It/sec 2.488, Tokens/sec 345.823, Trained Tokens 1706146, Peak mem 19.940 GB\n",
      "Iter 10320: Train loss 0.691, Learning Rate 4.000e-04, It/sec 2.814, Tokens/sec 381.611, Trained Tokens 1707502, Peak mem 19.940 GB\n",
      "Iter 10330: Train loss 0.654, Learning Rate 4.000e-04, It/sec 3.581, Tokens/sec 351.695, Trained Tokens 1708484, Peak mem 19.940 GB\n",
      "Iter 10340: Train loss 0.708, Learning Rate 4.000e-04, It/sec 2.585, Tokens/sec 374.245, Trained Tokens 1709932, Peak mem 19.940 GB\n",
      "Iter 10350: Val loss 2.238, Val took 2.796s\n",
      "Iter 10350: Train loss 0.887, Learning Rate 4.000e-04, It/sec 35.525, Tokens/sec 6817.266, Trained Tokens 1711851, Peak mem 19.940 GB\n",
      "Iter 10360: Train loss 0.879, Learning Rate 4.000e-04, It/sec 1.981, Tokens/sec 316.331, Trained Tokens 1713448, Peak mem 19.940 GB\n",
      "Iter 10370: Train loss 0.808, Learning Rate 4.000e-04, It/sec 2.551, Tokens/sec 376.789, Trained Tokens 1714925, Peak mem 19.940 GB\n",
      "Iter 10380: Train loss 0.753, Learning Rate 4.000e-04, It/sec 2.492, Tokens/sec 366.808, Trained Tokens 1716397, Peak mem 19.940 GB\n",
      "Iter 10390: Train loss 0.786, Learning Rate 4.000e-04, It/sec 2.281, Tokens/sec 377.919, Trained Tokens 1718054, Peak mem 19.940 GB\n",
      "Iter 10400: Val loss 2.412, Val took 3.465s\n",
      "Iter 10400: Train loss 0.822, Learning Rate 4.000e-04, It/sec 31.868, Tokens/sec 6086.876, Trained Tokens 1719964, Peak mem 19.940 GB\n",
      "Iter 10400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010400_adapters.safetensors.\n",
      "Iter 10410: Train loss 0.764, Learning Rate 4.000e-04, It/sec 2.787, Tokens/sec 366.251, Trained Tokens 1721278, Peak mem 19.940 GB\n",
      "Iter 10420: Train loss 0.754, Learning Rate 4.000e-04, It/sec 1.693, Tokens/sec 375.940, Trained Tokens 1723498, Peak mem 19.940 GB\n",
      "Iter 10430: Train loss 0.752, Learning Rate 4.000e-04, It/sec 2.622, Tokens/sec 368.145, Trained Tokens 1724902, Peak mem 19.940 GB\n",
      "Iter 10440: Train loss 0.892, Learning Rate 4.000e-04, It/sec 1.871, Tokens/sec 369.749, Trained Tokens 1726878, Peak mem 19.940 GB\n",
      "Iter 10450: Val loss 2.641, Val took 3.846s\n",
      "Iter 10450: Train loss 0.765, Learning Rate 4.000e-04, It/sec 42.190, Tokens/sec 6741.987, Trained Tokens 1728476, Peak mem 19.940 GB\n",
      "Iter 10460: Train loss 0.901, Learning Rate 4.000e-04, It/sec 2.152, Tokens/sec 370.977, Trained Tokens 1730200, Peak mem 19.940 GB\n",
      "Iter 10470: Train loss 0.824, Learning Rate 4.000e-04, It/sec 1.576, Tokens/sec 400.841, Trained Tokens 1732744, Peak mem 19.940 GB\n",
      "Iter 10480: Train loss 0.783, Learning Rate 4.000e-04, It/sec 2.374, Tokens/sec 340.675, Trained Tokens 1734179, Peak mem 19.940 GB\n",
      "Iter 10490: Train loss 0.746, Learning Rate 4.000e-04, It/sec 2.625, Tokens/sec 379.291, Trained Tokens 1735624, Peak mem 19.940 GB\n",
      "Iter 10500: Val loss 2.714, Val took 3.680s\n",
      "Iter 10500: Train loss 1.005, Learning Rate 4.000e-04, It/sec 19.742, Tokens/sec 5576.985, Trained Tokens 1738449, Peak mem 19.940 GB\n",
      "Iter 10500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010500_adapters.safetensors.\n",
      "Iter 10510: Train loss 0.752, Learning Rate 4.000e-04, It/sec 2.031, Tokens/sec 341.219, Trained Tokens 1740129, Peak mem 19.940 GB\n",
      "Iter 10520: Train loss 0.854, Learning Rate 4.000e-04, It/sec 2.372, Tokens/sec 385.747, Trained Tokens 1741755, Peak mem 19.940 GB\n",
      "Iter 10530: Train loss 0.887, Learning Rate 4.000e-04, It/sec 2.493, Tokens/sec 403.117, Trained Tokens 1743372, Peak mem 19.940 GB\n",
      "Iter 10540: Train loss 0.811, Learning Rate 4.000e-04, It/sec 2.016, Tokens/sec 397.555, Trained Tokens 1745344, Peak mem 19.940 GB\n",
      "Iter 10550: Val loss 2.309, Val took 3.141s\n",
      "Iter 10550: Train loss 0.974, Learning Rate 4.000e-04, It/sec 44.699, Tokens/sec 9619.269, Trained Tokens 1747496, Peak mem 19.940 GB\n",
      "Iter 10560: Train loss 0.777, Learning Rate 4.000e-04, It/sec 2.860, Tokens/sec 372.137, Trained Tokens 1748797, Peak mem 19.940 GB\n",
      "Iter 10570: Train loss 0.768, Learning Rate 4.000e-04, It/sec 2.666, Tokens/sec 367.065, Trained Tokens 1750174, Peak mem 19.940 GB\n",
      "Iter 10580: Train loss 0.808, Learning Rate 4.000e-04, It/sec 2.297, Tokens/sec 356.334, Trained Tokens 1751725, Peak mem 19.940 GB\n",
      "Iter 10590: Train loss 0.868, Learning Rate 4.000e-04, It/sec 1.914, Tokens/sec 401.661, Trained Tokens 1753824, Peak mem 19.940 GB\n",
      "Iter 10600: Val loss 2.314, Val took 3.161s\n",
      "Iter 10600: Train loss 0.948, Learning Rate 4.000e-04, It/sec 10.449, Tokens/sec 1659.239, Trained Tokens 1755412, Peak mem 19.940 GB\n",
      "Iter 10600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010600_adapters.safetensors.\n",
      "Iter 10610: Train loss 0.983, Learning Rate 4.000e-04, It/sec 1.835, Tokens/sec 406.864, Trained Tokens 1757629, Peak mem 19.940 GB\n",
      "Iter 10620: Train loss 0.900, Learning Rate 4.000e-04, It/sec 2.631, Tokens/sec 381.429, Trained Tokens 1759079, Peak mem 19.940 GB\n",
      "Iter 10630: Train loss 0.927, Learning Rate 4.000e-04, It/sec 1.984, Tokens/sec 391.584, Trained Tokens 1761053, Peak mem 19.940 GB\n",
      "Iter 10640: Train loss 0.919, Learning Rate 4.000e-04, It/sec 2.552, Tokens/sec 385.682, Trained Tokens 1762564, Peak mem 19.940 GB\n",
      "Iter 10650: Val loss 2.245, Val took 3.046s\n",
      "Iter 10650: Train loss 0.852, Learning Rate 4.000e-04, It/sec 5.578, Tokens/sec 910.872, Trained Tokens 1764197, Peak mem 19.940 GB\n",
      "Iter 10660: Train loss 0.896, Learning Rate 4.000e-04, It/sec 2.345, Tokens/sec 373.570, Trained Tokens 1765790, Peak mem 19.940 GB\n",
      "Iter 10670: Train loss 0.821, Learning Rate 4.000e-04, It/sec 2.056, Tokens/sec 399.949, Trained Tokens 1767735, Peak mem 19.940 GB\n",
      "Iter 10680: Train loss 0.734, Learning Rate 4.000e-04, It/sec 2.584, Tokens/sec 361.807, Trained Tokens 1769135, Peak mem 19.940 GB\n",
      "Iter 10690: Train loss 0.737, Learning Rate 4.000e-04, It/sec 2.595, Tokens/sec 377.260, Trained Tokens 1770589, Peak mem 19.940 GB\n",
      "Iter 10700: Val loss 2.548, Val took 3.235s\n",
      "Iter 10700: Train loss 0.762, Learning Rate 4.000e-04, It/sec 61.789, Tokens/sec 7884.219, Trained Tokens 1771865, Peak mem 19.940 GB\n",
      "Iter 10700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010700_adapters.safetensors.\n",
      "Iter 10710: Train loss 0.668, Learning Rate 4.000e-04, It/sec 3.596, Tokens/sec 379.343, Trained Tokens 1772920, Peak mem 19.940 GB\n",
      "Iter 10720: Train loss 0.888, Learning Rate 4.000e-04, It/sec 2.057, Tokens/sec 338.113, Trained Tokens 1774564, Peak mem 19.940 GB\n",
      "Iter 10730: Train loss 0.738, Learning Rate 4.000e-04, It/sec 2.084, Tokens/sec 332.015, Trained Tokens 1776157, Peak mem 19.940 GB\n",
      "Iter 10740: Train loss 0.730, Learning Rate 4.000e-04, It/sec 2.305, Tokens/sec 375.637, Trained Tokens 1777787, Peak mem 19.940 GB\n",
      "Iter 10750: Val loss 2.432, Val took 3.366s\n",
      "Iter 10750: Train loss 0.735, Learning Rate 4.000e-04, It/sec 44.232, Tokens/sec 5621.934, Trained Tokens 1779058, Peak mem 19.940 GB\n",
      "Iter 10760: Train loss 0.777, Learning Rate 4.000e-04, It/sec 1.752, Tokens/sec 370.945, Trained Tokens 1781175, Peak mem 19.940 GB\n",
      "Iter 10770: Train loss 0.903, Learning Rate 4.000e-04, It/sec 2.269, Tokens/sec 382.696, Trained Tokens 1782862, Peak mem 19.940 GB\n",
      "Iter 10780: Train loss 0.800, Learning Rate 4.000e-04, It/sec 1.685, Tokens/sec 395.487, Trained Tokens 1785209, Peak mem 19.940 GB\n",
      "Iter 10790: Train loss 0.769, Learning Rate 4.000e-04, It/sec 1.900, Tokens/sec 359.094, Trained Tokens 1787099, Peak mem 19.940 GB\n",
      "Iter 10800: Val loss 2.327, Val took 3.375s\n",
      "Iter 10800: Train loss 0.821, Learning Rate 4.000e-04, It/sec 22.971, Tokens/sec 3854.553, Trained Tokens 1788777, Peak mem 19.940 GB\n",
      "Iter 10800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010800_adapters.safetensors.\n",
      "Iter 10810: Train loss 0.742, Learning Rate 4.000e-04, It/sec 2.970, Tokens/sec 370.595, Trained Tokens 1790025, Peak mem 19.940 GB\n",
      "Iter 10820: Train loss 0.784, Learning Rate 4.000e-04, It/sec 2.479, Tokens/sec 378.554, Trained Tokens 1791552, Peak mem 19.940 GB\n",
      "Iter 10830: Train loss 0.784, Learning Rate 4.000e-04, It/sec 2.137, Tokens/sec 351.064, Trained Tokens 1793195, Peak mem 19.940 GB\n",
      "Iter 10840: Train loss 0.794, Learning Rate 4.000e-04, It/sec 2.618, Tokens/sec 374.642, Trained Tokens 1794626, Peak mem 19.940 GB\n",
      "Iter 10850: Val loss 2.476, Val took 3.312s\n",
      "Iter 10850: Train loss 0.809, Learning Rate 4.000e-04, It/sec 23.118, Tokens/sec 3100.090, Trained Tokens 1795967, Peak mem 19.940 GB\n",
      "Iter 10860: Train loss 0.765, Learning Rate 4.000e-04, It/sec 2.289, Tokens/sec 354.351, Trained Tokens 1797515, Peak mem 19.940 GB\n",
      "Iter 10870: Train loss 0.780, Learning Rate 4.000e-04, It/sec 2.905, Tokens/sec 356.388, Trained Tokens 1798742, Peak mem 19.940 GB\n",
      "Iter 10880: Train loss 0.864, Learning Rate 4.000e-04, It/sec 2.875, Tokens/sec 356.825, Trained Tokens 1799983, Peak mem 19.940 GB\n",
      "Iter 10890: Train loss 0.855, Learning Rate 4.000e-04, It/sec 2.309, Tokens/sec 377.721, Trained Tokens 1801619, Peak mem 19.940 GB\n",
      "Iter 10900: Val loss 2.475, Val took 4.152s\n",
      "Iter 10900: Train loss 0.842, Learning Rate 4.000e-04, It/sec 31.788, Tokens/sec 5013.015, Trained Tokens 1803196, Peak mem 19.940 GB\n",
      "Iter 10900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0010900_adapters.safetensors.\n",
      "Iter 10910: Train loss 1.006, Learning Rate 4.000e-04, It/sec 1.914, Tokens/sec 396.002, Trained Tokens 1805265, Peak mem 19.940 GB\n",
      "Iter 10920: Train loss 0.797, Learning Rate 4.000e-04, It/sec 2.976, Tokens/sec 383.868, Trained Tokens 1806555, Peak mem 19.940 GB\n",
      "Iter 10930: Train loss 0.850, Learning Rate 4.000e-04, It/sec 1.759, Tokens/sec 347.196, Trained Tokens 1808529, Peak mem 19.940 GB\n",
      "Iter 10940: Train loss 1.114, Learning Rate 4.000e-04, It/sec 1.569, Tokens/sec 379.623, Trained Tokens 1810949, Peak mem 19.940 GB\n",
      "Iter 10950: Val loss 2.397, Val took 4.148s\n",
      "Iter 10950: Train loss 0.753, Learning Rate 4.000e-04, It/sec 56.729, Tokens/sec 7800.214, Trained Tokens 1812324, Peak mem 19.940 GB\n",
      "Iter 10960: Train loss 0.795, Learning Rate 4.000e-04, It/sec 2.418, Tokens/sec 369.493, Trained Tokens 1813852, Peak mem 19.940 GB\n",
      "Iter 10970: Train loss 0.885, Learning Rate 4.000e-04, It/sec 2.408, Tokens/sec 387.884, Trained Tokens 1815463, Peak mem 19.940 GB\n",
      "Iter 10980: Train loss 0.705, Learning Rate 4.000e-04, It/sec 2.447, Tokens/sec 397.635, Trained Tokens 1817088, Peak mem 19.940 GB\n",
      "Iter 10990: Train loss 0.845, Learning Rate 4.000e-04, It/sec 2.170, Tokens/sec 395.961, Trained Tokens 1818913, Peak mem 19.940 GB\n",
      "Iter 11000: Val loss 2.390, Val took 3.542s\n",
      "Iter 11000: Train loss 0.843, Learning Rate 4.000e-04, It/sec 17.980, Tokens/sec 2562.161, Trained Tokens 1820338, Peak mem 19.940 GB\n",
      "Iter 11000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011000_adapters.safetensors.\n",
      "Iter 11010: Train loss 0.718, Learning Rate 4.000e-04, It/sec 2.727, Tokens/sec 359.933, Trained Tokens 1821658, Peak mem 19.940 GB\n",
      "Iter 11020: Train loss 0.997, Learning Rate 4.000e-04, It/sec 1.488, Tokens/sec 367.108, Trained Tokens 1824125, Peak mem 19.940 GB\n",
      "Iter 11030: Train loss 0.896, Learning Rate 4.000e-04, It/sec 2.367, Tokens/sec 363.100, Trained Tokens 1825659, Peak mem 19.940 GB\n",
      "Iter 11040: Train loss 0.892, Learning Rate 4.000e-04, It/sec 2.269, Tokens/sec 363.106, Trained Tokens 1827259, Peak mem 19.940 GB\n",
      "Iter 11050: Val loss 2.536, Val took 2.986s\n",
      "Iter 11050: Train loss 1.045, Learning Rate 4.000e-04, It/sec 26.493, Tokens/sec 5423.107, Trained Tokens 1829306, Peak mem 19.940 GB\n",
      "Iter 11060: Train loss 0.966, Learning Rate 4.000e-04, It/sec 1.399, Tokens/sec 395.014, Trained Tokens 1832130, Peak mem 19.940 GB\n",
      "Iter 11070: Train loss 0.856, Learning Rate 4.000e-04, It/sec 2.458, Tokens/sec 387.114, Trained Tokens 1833705, Peak mem 19.940 GB\n",
      "Iter 11080: Train loss 0.867, Learning Rate 4.000e-04, It/sec 1.462, Tokens/sec 385.402, Trained Tokens 1836341, Peak mem 19.940 GB\n",
      "Iter 11090: Train loss 0.665, Learning Rate 4.000e-04, It/sec 2.491, Tokens/sec 351.190, Trained Tokens 1837751, Peak mem 19.940 GB\n",
      "Iter 11100: Val loss 2.633, Val took 3.346s\n",
      "Iter 11100: Train loss 0.623, Learning Rate 4.000e-04, It/sec 43.327, Tokens/sec 5017.324, Trained Tokens 1838909, Peak mem 19.940 GB\n",
      "Iter 11100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011100_adapters.safetensors.\n",
      "Iter 11110: Train loss 0.725, Learning Rate 4.000e-04, It/sec 2.164, Tokens/sec 329.997, Trained Tokens 1840434, Peak mem 19.940 GB\n",
      "Iter 11120: Train loss 0.712, Learning Rate 4.000e-04, It/sec 3.208, Tokens/sec 347.407, Trained Tokens 1841517, Peak mem 19.940 GB\n",
      "Iter 11130: Train loss 0.770, Learning Rate 4.000e-04, It/sec 2.557, Tokens/sec 373.047, Trained Tokens 1842976, Peak mem 19.940 GB\n",
      "Iter 11140: Train loss 0.677, Learning Rate 4.000e-04, It/sec 2.389, Tokens/sec 356.731, Trained Tokens 1844469, Peak mem 19.940 GB\n",
      "Iter 11150: Val loss 2.461, Val took 3.838s\n",
      "Iter 11150: Train loss 0.662, Learning Rate 4.000e-04, It/sec 23.461, Tokens/sec 2881.020, Trained Tokens 1845697, Peak mem 19.940 GB\n",
      "Iter 11160: Train loss 0.682, Learning Rate 4.000e-04, It/sec 2.319, Tokens/sec 379.802, Trained Tokens 1847335, Peak mem 19.940 GB\n",
      "Iter 11170: Train loss 0.698, Learning Rate 4.000e-04, It/sec 2.743, Tokens/sec 363.504, Trained Tokens 1848660, Peak mem 19.940 GB\n",
      "Iter 11180: Train loss 0.718, Learning Rate 4.000e-04, It/sec 2.261, Tokens/sec 405.695, Trained Tokens 1850454, Peak mem 19.940 GB\n",
      "Iter 11190: Train loss 0.755, Learning Rate 4.000e-04, It/sec 2.396, Tokens/sec 379.066, Trained Tokens 1852036, Peak mem 19.940 GB\n",
      "Iter 11200: Val loss 2.331, Val took 3.129s\n",
      "Iter 11200: Train loss 0.742, Learning Rate 4.000e-04, It/sec 27.499, Tokens/sec 3968.108, Trained Tokens 1853479, Peak mem 19.940 GB\n",
      "Iter 11200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011200_adapters.safetensors.\n",
      "Iter 11210: Train loss 0.762, Learning Rate 4.000e-04, It/sec 2.096, Tokens/sec 360.590, Trained Tokens 1855199, Peak mem 19.940 GB\n",
      "Iter 11220: Train loss 0.744, Learning Rate 4.000e-04, It/sec 2.472, Tokens/sec 325.348, Trained Tokens 1856515, Peak mem 19.940 GB\n",
      "Iter 11230: Train loss 0.737, Learning Rate 4.000e-04, It/sec 2.037, Tokens/sec 362.424, Trained Tokens 1858294, Peak mem 19.940 GB\n",
      "Iter 11240: Train loss 0.795, Learning Rate 4.000e-04, It/sec 1.871, Tokens/sec 308.114, Trained Tokens 1859941, Peak mem 19.940 GB\n",
      "Iter 11250: Val loss 2.521, Val took 3.202s\n",
      "Iter 11250: Train loss 0.771, Learning Rate 4.000e-04, It/sec 13.996, Tokens/sec 2849.665, Trained Tokens 1861977, Peak mem 19.940 GB\n",
      "Iter 11260: Train loss 0.713, Learning Rate 4.000e-04, It/sec 2.281, Tokens/sec 328.012, Trained Tokens 1863415, Peak mem 19.940 GB\n",
      "Iter 11270: Train loss 0.740, Learning Rate 4.000e-04, It/sec 2.070, Tokens/sec 334.102, Trained Tokens 1865029, Peak mem 19.940 GB\n",
      "Iter 11280: Train loss 0.701, Learning Rate 4.000e-04, It/sec 2.085, Tokens/sec 309.145, Trained Tokens 1866512, Peak mem 19.940 GB\n",
      "Iter 11290: Train loss 0.988, Learning Rate 4.000e-04, It/sec 1.429, Tokens/sec 326.593, Trained Tokens 1868797, Peak mem 19.940 GB\n",
      "Iter 11300: Val loss 2.566, Val took 5.849s\n",
      "Iter 11300: Train loss 0.897, Learning Rate 4.000e-04, It/sec 16.428, Tokens/sec 4821.525, Trained Tokens 1871732, Peak mem 19.940 GB\n",
      "Iter 11300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011300_adapters.safetensors.\n",
      "Iter 11310: Train loss 0.888, Learning Rate 4.000e-04, It/sec 1.969, Tokens/sec 302.886, Trained Tokens 1873270, Peak mem 19.940 GB\n",
      "Iter 11320: Train loss 0.753, Learning Rate 4.000e-04, It/sec 1.851, Tokens/sec 302.945, Trained Tokens 1874907, Peak mem 19.940 GB\n",
      "Iter 11330: Train loss 0.787, Learning Rate 4.000e-04, It/sec 1.970, Tokens/sec 323.912, Trained Tokens 1876551, Peak mem 19.940 GB\n",
      "Iter 11340: Train loss 0.891, Learning Rate 4.000e-04, It/sec 1.508, Tokens/sec 336.896, Trained Tokens 1878785, Peak mem 19.940 GB\n",
      "Iter 11350: Val loss 2.688, Val took 3.799s\n",
      "Iter 11350: Train loss 0.734, Learning Rate 4.000e-04, It/sec 13.499, Tokens/sec 1652.300, Trained Tokens 1880009, Peak mem 19.940 GB\n",
      "Iter 11360: Train loss 0.827, Learning Rate 4.000e-04, It/sec 1.501, Tokens/sec 247.003, Trained Tokens 1881655, Peak mem 19.940 GB\n",
      "Iter 11370: Train loss 0.901, Learning Rate 4.000e-04, It/sec 1.462, Tokens/sec 250.334, Trained Tokens 1883367, Peak mem 19.940 GB\n",
      "Iter 11380: Train loss 0.769, Learning Rate 4.000e-04, It/sec 2.453, Tokens/sec 313.215, Trained Tokens 1884644, Peak mem 19.940 GB\n",
      "Iter 11390: Train loss 0.837, Learning Rate 4.000e-04, It/sec 2.092, Tokens/sec 315.035, Trained Tokens 1886150, Peak mem 19.940 GB\n",
      "Iter 11400: Val loss 2.529, Val took 4.722s\n",
      "Iter 11400: Train loss 0.836, Learning Rate 4.000e-04, It/sec 34.210, Tokens/sec 5730.175, Trained Tokens 1887825, Peak mem 19.940 GB\n",
      "Iter 11400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011400_adapters.safetensors.\n",
      "Iter 11410: Train loss 0.840, Learning Rate 4.000e-04, It/sec 1.417, Tokens/sec 239.605, Trained Tokens 1889516, Peak mem 19.940 GB\n",
      "Iter 11420: Train loss 0.790, Learning Rate 4.000e-04, It/sec 2.160, Tokens/sec 315.092, Trained Tokens 1890975, Peak mem 19.940 GB\n",
      "Iter 11430: Train loss 0.792, Learning Rate 4.000e-04, It/sec 1.964, Tokens/sec 290.623, Trained Tokens 1892455, Peak mem 19.940 GB\n",
      "Iter 11440: Train loss 0.975, Learning Rate 4.000e-04, It/sec 1.388, Tokens/sec 297.513, Trained Tokens 1894599, Peak mem 19.940 GB\n",
      "Iter 11450: Val loss 2.415, Val took 3.920s\n",
      "Iter 11450: Train loss 0.887, Learning Rate 4.000e-04, It/sec 38.041, Tokens/sec 5645.347, Trained Tokens 1896083, Peak mem 19.940 GB\n",
      "Iter 11460: Train loss 0.864, Learning Rate 4.000e-04, It/sec 2.180, Tokens/sec 310.258, Trained Tokens 1897506, Peak mem 19.940 GB\n",
      "Iter 11470: Train loss 0.929, Learning Rate 4.000e-04, It/sec 1.900, Tokens/sec 338.403, Trained Tokens 1899287, Peak mem 19.940 GB\n",
      "Iter 11480: Train loss 1.000, Learning Rate 4.000e-04, It/sec 1.406, Tokens/sec 328.025, Trained Tokens 1901620, Peak mem 19.940 GB\n",
      "Iter 11490: Train loss 0.654, Learning Rate 4.000e-04, It/sec 2.870, Tokens/sec 335.213, Trained Tokens 1902788, Peak mem 19.940 GB\n",
      "Iter 11500: Val loss 2.570, Val took 6.093s\n",
      "Iter 11500: Train loss 0.805, Learning Rate 4.000e-04, It/sec 16.411, Tokens/sec 3099.950, Trained Tokens 1904677, Peak mem 19.940 GB\n",
      "Iter 11500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011500_adapters.safetensors.\n",
      "Iter 11510: Train loss 0.769, Learning Rate 4.000e-04, It/sec 1.164, Tokens/sec 340.192, Trained Tokens 1907599, Peak mem 19.940 GB\n",
      "Iter 11520: Train loss 0.778, Learning Rate 4.000e-04, It/sec 1.328, Tokens/sec 306.264, Trained Tokens 1909906, Peak mem 19.940 GB\n",
      "Iter 11530: Train loss 0.615, Learning Rate 4.000e-04, It/sec 2.202, Tokens/sec 293.126, Trained Tokens 1911237, Peak mem 19.940 GB\n",
      "Iter 11540: Train loss 1.028, Learning Rate 4.000e-04, It/sec 1.088, Tokens/sec 304.750, Trained Tokens 1914039, Peak mem 19.940 GB\n",
      "Iter 11550: Val loss 2.446, Val took 3.167s\n",
      "Iter 11550: Train loss 0.657, Learning Rate 4.000e-04, It/sec 30.725, Tokens/sec 3975.812, Trained Tokens 1915333, Peak mem 19.940 GB\n",
      "Iter 11560: Train loss 1.002, Learning Rate 4.000e-04, It/sec 1.158, Tokens/sec 332.028, Trained Tokens 1918200, Peak mem 19.940 GB\n",
      "Iter 11570: Train loss 0.713, Learning Rate 4.000e-04, It/sec 1.960, Tokens/sec 322.417, Trained Tokens 1919845, Peak mem 19.940 GB\n",
      "Iter 11580: Train loss 0.723, Learning Rate 4.000e-04, It/sec 1.877, Tokens/sec 273.032, Trained Tokens 1921300, Peak mem 19.940 GB\n",
      "Iter 11590: Train loss 0.791, Learning Rate 4.000e-04, It/sec 1.487, Tokens/sec 290.073, Trained Tokens 1923251, Peak mem 19.940 GB\n",
      "Iter 11600: Val loss 2.384, Val took 3.328s\n",
      "Iter 11600: Train loss 0.670, Learning Rate 4.000e-04, It/sec 33.457, Tokens/sec 4908.099, Trained Tokens 1924718, Peak mem 19.940 GB\n",
      "Iter 11600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011600_adapters.safetensors.\n",
      "Iter 11610: Train loss 0.781, Learning Rate 4.000e-04, It/sec 2.093, Tokens/sec 308.315, Trained Tokens 1926191, Peak mem 19.940 GB\n",
      "Iter 11620: Train loss 0.703, Learning Rate 4.000e-04, It/sec 1.843, Tokens/sec 281.498, Trained Tokens 1927718, Peak mem 19.940 GB\n",
      "Iter 11630: Train loss 0.708, Learning Rate 4.000e-04, It/sec 2.305, Tokens/sec 341.994, Trained Tokens 1929202, Peak mem 19.940 GB\n",
      "Iter 11640: Train loss 0.737, Learning Rate 4.000e-04, It/sec 2.166, Tokens/sec 301.484, Trained Tokens 1930594, Peak mem 19.940 GB\n",
      "Iter 11650: Val loss 2.310, Val took 3.182s\n",
      "Iter 11650: Train loss 0.673, Learning Rate 4.000e-04, It/sec 25.636, Tokens/sec 2694.326, Trained Tokens 1931645, Peak mem 19.940 GB\n",
      "Iter 11660: Train loss 0.735, Learning Rate 4.000e-04, It/sec 2.400, Tokens/sec 340.341, Trained Tokens 1933063, Peak mem 19.940 GB\n",
      "Iter 11670: Train loss 0.743, Learning Rate 4.000e-04, It/sec 1.616, Tokens/sec 294.725, Trained Tokens 1934887, Peak mem 19.940 GB\n",
      "Iter 11680: Train loss 0.732, Learning Rate 4.000e-04, It/sec 2.394, Tokens/sec 361.513, Trained Tokens 1936397, Peak mem 19.940 GB\n",
      "Iter 11690: Train loss 0.688, Learning Rate 4.000e-04, It/sec 3.026, Tokens/sec 370.627, Trained Tokens 1937622, Peak mem 19.940 GB\n",
      "Iter 11700: Val loss 2.346, Val took 4.550s\n",
      "Iter 11700: Train loss 0.833, Learning Rate 4.000e-04, It/sec 12.715, Tokens/sec 2119.648, Trained Tokens 1939289, Peak mem 19.940 GB\n",
      "Iter 11700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011700_adapters.safetensors.\n",
      "Iter 11710: Train loss 0.734, Learning Rate 4.000e-04, It/sec 1.190, Tokens/sec 187.913, Trained Tokens 1940868, Peak mem 19.940 GB\n",
      "Iter 11720: Train loss 0.890, Learning Rate 4.000e-04, It/sec 1.809, Tokens/sec 298.190, Trained Tokens 1942516, Peak mem 19.940 GB\n",
      "Iter 11730: Train loss 0.730, Learning Rate 4.000e-04, It/sec 1.499, Tokens/sec 219.206, Trained Tokens 1943978, Peak mem 19.940 GB\n",
      "Iter 11740: Train loss 0.759, Learning Rate 4.000e-04, It/sec 1.707, Tokens/sec 203.314, Trained Tokens 1945169, Peak mem 19.940 GB\n",
      "Iter 11750: Val loss 2.725, Val took 3.717s\n",
      "Iter 11750: Train loss 0.765, Learning Rate 4.000e-04, It/sec 27.271, Tokens/sec 3599.741, Trained Tokens 1946489, Peak mem 19.940 GB\n",
      "Iter 11760: Train loss 0.812, Learning Rate 4.000e-04, It/sec 2.039, Tokens/sec 328.488, Trained Tokens 1948100, Peak mem 19.940 GB\n",
      "Iter 11770: Train loss 0.879, Learning Rate 4.000e-04, It/sec 1.610, Tokens/sec 290.513, Trained Tokens 1949904, Peak mem 19.940 GB\n",
      "Iter 11780: Train loss 0.880, Learning Rate 4.000e-04, It/sec 2.215, Tokens/sec 330.898, Trained Tokens 1951398, Peak mem 19.940 GB\n",
      "Iter 11790: Train loss 0.802, Learning Rate 4.000e-04, It/sec 3.061, Tokens/sec 325.401, Trained Tokens 1952461, Peak mem 19.940 GB\n",
      "Iter 11800: Val loss 2.794, Val took 3.347s\n",
      "Iter 11800: Train loss 0.935, Learning Rate 4.000e-04, It/sec 35.484, Tokens/sec 8285.630, Trained Tokens 1954796, Peak mem 19.940 GB\n",
      "Iter 11800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011800_adapters.safetensors.\n",
      "Iter 11810: Train loss 0.867, Learning Rate 4.000e-04, It/sec 2.206, Tokens/sec 350.564, Trained Tokens 1956385, Peak mem 19.940 GB\n",
      "Iter 11820: Train loss 0.975, Learning Rate 4.000e-04, It/sec 1.106, Tokens/sec 305.925, Trained Tokens 1959151, Peak mem 19.940 GB\n",
      "Iter 11830: Train loss 0.736, Learning Rate 4.000e-04, It/sec 2.524, Tokens/sec 370.984, Trained Tokens 1960621, Peak mem 19.940 GB\n",
      "Iter 11840: Train loss 0.866, Learning Rate 4.000e-04, It/sec 1.818, Tokens/sec 319.435, Trained Tokens 1962378, Peak mem 19.940 GB\n",
      "Iter 11850: Val loss 2.279, Val took 3.062s\n",
      "Iter 11850: Train loss 0.888, Learning Rate 4.000e-04, It/sec 18.611, Tokens/sec 2547.804, Trained Tokens 1963747, Peak mem 19.940 GB\n",
      "Iter 11860: Train loss 0.801, Learning Rate 4.000e-04, It/sec 2.963, Tokens/sec 327.359, Trained Tokens 1964852, Peak mem 19.940 GB\n",
      "Iter 11870: Train loss 1.011, Learning Rate 4.000e-04, It/sec 1.668, Tokens/sec 312.236, Trained Tokens 1966724, Peak mem 19.940 GB\n",
      "Iter 11880: Train loss 0.880, Learning Rate 4.000e-04, It/sec 2.380, Tokens/sec 317.081, Trained Tokens 1968056, Peak mem 19.940 GB\n",
      "Iter 11890: Train loss 0.861, Learning Rate 4.000e-04, It/sec 1.975, Tokens/sec 292.121, Trained Tokens 1969535, Peak mem 19.940 GB\n",
      "Iter 11900: Val loss 2.633, Val took 4.603s\n",
      "Iter 11900: Train loss 0.794, Learning Rate 4.000e-04, It/sec 37.140, Tokens/sec 4683.380, Trained Tokens 1970796, Peak mem 19.940 GB\n",
      "Iter 11900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0011900_adapters.safetensors.\n",
      "Iter 11910: Train loss 0.734, Learning Rate 4.000e-04, It/sec 1.770, Tokens/sec 346.381, Trained Tokens 1972753, Peak mem 19.940 GB\n",
      "Iter 11920: Train loss 0.663, Learning Rate 4.000e-04, It/sec 2.367, Tokens/sec 319.987, Trained Tokens 1974105, Peak mem 19.940 GB\n",
      "Iter 11930: Train loss 0.672, Learning Rate 4.000e-04, It/sec 2.181, Tokens/sec 308.005, Trained Tokens 1975517, Peak mem 19.940 GB\n",
      "Iter 11940: Train loss 0.737, Learning Rate 4.000e-04, It/sec 2.850, Tokens/sec 341.994, Trained Tokens 1976717, Peak mem 19.940 GB\n",
      "Iter 11950: Val loss 2.407, Val took 4.327s\n",
      "Iter 11950: Train loss 0.899, Learning Rate 4.000e-04, It/sec 9.612, Tokens/sec 2574.977, Trained Tokens 1979396, Peak mem 19.940 GB\n",
      "Iter 11960: Train loss 0.707, Learning Rate 4.000e-04, It/sec 3.011, Tokens/sec 323.958, Trained Tokens 1980472, Peak mem 19.940 GB\n",
      "Iter 11970: Train loss 0.733, Learning Rate 4.000e-04, It/sec 2.510, Tokens/sec 335.523, Trained Tokens 1981809, Peak mem 19.940 GB\n",
      "Iter 11980: Train loss 0.830, Learning Rate 4.000e-04, It/sec 1.695, Tokens/sec 328.434, Trained Tokens 1983747, Peak mem 19.940 GB\n",
      "Iter 11990: Train loss 0.907, Learning Rate 4.000e-04, It/sec 1.293, Tokens/sec 303.831, Trained Tokens 1986097, Peak mem 19.940 GB\n",
      "Iter 12000: Val loss 2.172, Val took 4.404s\n",
      "Iter 12000: Train loss 0.782, Learning Rate 4.000e-04, It/sec 29.733, Tokens/sec 4596.699, Trained Tokens 1987643, Peak mem 19.940 GB\n",
      "Iter 12000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012000_adapters.safetensors.\n",
      "Iter 12010: Train loss 0.680, Learning Rate 4.000e-04, It/sec 2.100, Tokens/sec 313.783, Trained Tokens 1989137, Peak mem 19.940 GB\n",
      "Iter 12020: Train loss 0.846, Learning Rate 4.000e-04, It/sec 1.477, Tokens/sec 330.470, Trained Tokens 1991374, Peak mem 19.940 GB\n",
      "Iter 12030: Train loss 0.801, Learning Rate 4.000e-04, It/sec 2.449, Tokens/sec 332.116, Trained Tokens 1992730, Peak mem 19.940 GB\n",
      "Iter 12040: Train loss 0.738, Learning Rate 4.000e-04, It/sec 2.576, Tokens/sec 339.832, Trained Tokens 1994049, Peak mem 19.940 GB\n",
      "Iter 12050: Val loss 2.173, Val took 3.335s\n",
      "Iter 12050: Train loss 0.777, Learning Rate 4.000e-04, It/sec 24.054, Tokens/sec 3598.491, Trained Tokens 1995545, Peak mem 19.940 GB\n",
      "Iter 12060: Train loss 0.852, Learning Rate 4.000e-04, It/sec 1.621, Tokens/sec 302.169, Trained Tokens 1997409, Peak mem 19.940 GB\n",
      "Iter 12070: Train loss 0.834, Learning Rate 4.000e-04, It/sec 1.959, Tokens/sec 336.515, Trained Tokens 1999127, Peak mem 19.940 GB\n",
      "Iter 12080: Train loss 0.832, Learning Rate 4.000e-04, It/sec 1.640, Tokens/sec 299.549, Trained Tokens 2000953, Peak mem 19.940 GB\n",
      "Iter 12090: Train loss 0.785, Learning Rate 4.000e-04, It/sec 2.008, Tokens/sec 334.488, Trained Tokens 2002619, Peak mem 19.940 GB\n",
      "Iter 12100: Val loss 2.340, Val took 3.422s\n",
      "Iter 12100: Train loss 0.752, Learning Rate 4.000e-04, It/sec 24.506, Tokens/sec 3073.047, Trained Tokens 2003873, Peak mem 19.940 GB\n",
      "Iter 12100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012100_adapters.safetensors.\n",
      "Iter 12110: Train loss 0.862, Learning Rate 4.000e-04, It/sec 1.839, Tokens/sec 331.192, Trained Tokens 2005674, Peak mem 19.940 GB\n",
      "Iter 12120: Train loss 0.753, Learning Rate 4.000e-04, It/sec 2.395, Tokens/sec 347.332, Trained Tokens 2007124, Peak mem 19.940 GB\n",
      "Iter 12130: Train loss 0.763, Learning Rate 4.000e-04, It/sec 1.864, Tokens/sec 321.230, Trained Tokens 2008847, Peak mem 19.940 GB\n",
      "Iter 12140: Train loss 0.920, Learning Rate 4.000e-04, It/sec 1.605, Tokens/sec 336.118, Trained Tokens 2010941, Peak mem 19.940 GB\n",
      "Iter 12150: Val loss 2.751, Val took 3.429s\n",
      "Iter 12150: Train loss 0.936, Learning Rate 4.000e-04, It/sec 18.784, Tokens/sec 4072.287, Trained Tokens 2013109, Peak mem 19.940 GB\n",
      "Iter 12160: Train loss 0.802, Learning Rate 4.000e-04, It/sec 2.366, Tokens/sec 339.516, Trained Tokens 2014544, Peak mem 19.940 GB\n",
      "Iter 12170: Train loss 0.876, Learning Rate 4.000e-04, It/sec 1.777, Tokens/sec 362.628, Trained Tokens 2016585, Peak mem 19.940 GB\n",
      "Iter 12180: Train loss 0.694, Learning Rate 4.000e-04, It/sec 2.041, Tokens/sec 332.409, Trained Tokens 2018214, Peak mem 19.940 GB\n",
      "Iter 12190: Train loss 0.852, Learning Rate 4.000e-04, It/sec 1.631, Tokens/sec 300.999, Trained Tokens 2020059, Peak mem 19.940 GB\n",
      "Iter 12200: Val loss 2.231, Val took 3.198s\n",
      "Iter 12200: Train loss 0.948, Learning Rate 4.000e-04, It/sec 9.394, Tokens/sec 2161.609, Trained Tokens 2022360, Peak mem 19.940 GB\n",
      "Iter 12200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012200_adapters.safetensors.\n",
      "Iter 12210: Train loss 0.815, Learning Rate 4.000e-04, It/sec 2.327, Tokens/sec 296.183, Trained Tokens 2023633, Peak mem 19.940 GB\n",
      "Iter 12220: Train loss 0.786, Learning Rate 4.000e-04, It/sec 2.146, Tokens/sec 346.861, Trained Tokens 2025249, Peak mem 19.940 GB\n",
      "Iter 12230: Train loss 0.761, Learning Rate 4.000e-04, It/sec 2.091, Tokens/sec 311.952, Trained Tokens 2026741, Peak mem 19.940 GB\n",
      "Iter 12240: Train loss 0.780, Learning Rate 4.000e-04, It/sec 2.902, Tokens/sec 358.671, Trained Tokens 2027977, Peak mem 19.940 GB\n",
      "Iter 12250: Val loss 2.339, Val took 2.839s\n",
      "Iter 12250: Train loss 0.778, Learning Rate 4.000e-04, It/sec 33.493, Tokens/sec 5332.052, Trained Tokens 2029569, Peak mem 19.940 GB\n",
      "Iter 12260: Train loss 1.011, Learning Rate 4.000e-04, It/sec 1.465, Tokens/sec 344.768, Trained Tokens 2031922, Peak mem 19.940 GB\n",
      "Iter 12270: Train loss 0.874, Learning Rate 4.000e-04, It/sec 2.240, Tokens/sec 356.093, Trained Tokens 2033512, Peak mem 19.940 GB\n",
      "Iter 12280: Train loss 0.764, Learning Rate 4.000e-04, It/sec 2.508, Tokens/sec 320.787, Trained Tokens 2034791, Peak mem 19.940 GB\n",
      "Iter 12290: Train loss 0.766, Learning Rate 4.000e-04, It/sec 2.054, Tokens/sec 318.411, Trained Tokens 2036341, Peak mem 19.940 GB\n",
      "Iter 12300: Val loss 2.538, Val took 2.886s\n",
      "Iter 12300: Train loss 0.742, Learning Rate 4.000e-04, It/sec 42.853, Tokens/sec 4752.347, Trained Tokens 2037450, Peak mem 19.940 GB\n",
      "Iter 12300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012300_adapters.safetensors.\n",
      "Iter 12310: Train loss 0.768, Learning Rate 4.000e-04, It/sec 1.736, Tokens/sec 315.169, Trained Tokens 2039265, Peak mem 19.940 GB\n",
      "Iter 12320: Train loss 0.706, Learning Rate 4.000e-04, It/sec 2.645, Tokens/sec 332.798, Trained Tokens 2040523, Peak mem 19.940 GB\n",
      "Iter 12330: Train loss 0.754, Learning Rate 4.000e-04, It/sec 2.256, Tokens/sec 353.953, Trained Tokens 2042092, Peak mem 19.940 GB\n",
      "Iter 12340: Train loss 0.747, Learning Rate 4.000e-04, It/sec 2.036, Tokens/sec 311.887, Trained Tokens 2043624, Peak mem 19.940 GB\n",
      "Iter 12350: Val loss 2.354, Val took 3.021s\n",
      "Iter 12350: Train loss 0.874, Learning Rate 4.000e-04, It/sec 12.343, Tokens/sec 2382.182, Trained Tokens 2045554, Peak mem 19.940 GB\n",
      "Iter 12360: Train loss 0.702, Learning Rate 4.000e-04, It/sec 2.069, Tokens/sec 324.054, Trained Tokens 2047120, Peak mem 19.940 GB\n",
      "Iter 12370: Train loss 0.796, Learning Rate 4.000e-04, It/sec 1.662, Tokens/sec 287.423, Trained Tokens 2048849, Peak mem 19.940 GB\n",
      "Iter 12380: Train loss 0.709, Learning Rate 4.000e-04, It/sec 2.819, Tokens/sec 325.581, Trained Tokens 2050004, Peak mem 19.940 GB\n",
      "Iter 12390: Train loss 0.757, Learning Rate 4.000e-04, It/sec 1.809, Tokens/sec 303.343, Trained Tokens 2051681, Peak mem 19.940 GB\n",
      "Iter 12400: Val loss 2.528, Val took 3.411s\n",
      "Iter 12400: Train loss 0.692, Learning Rate 4.000e-04, It/sec 12.396, Tokens/sec 1834.569, Trained Tokens 2053161, Peak mem 19.940 GB\n",
      "Iter 12400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012400_adapters.safetensors.\n",
      "Iter 12410: Train loss 0.705, Learning Rate 4.000e-04, It/sec 2.190, Tokens/sec 344.116, Trained Tokens 2054732, Peak mem 19.940 GB\n",
      "Iter 12420: Train loss 0.916, Learning Rate 4.000e-04, It/sec 1.510, Tokens/sec 329.543, Trained Tokens 2056915, Peak mem 19.940 GB\n",
      "Iter 12430: Train loss 0.698, Learning Rate 4.000e-04, It/sec 2.439, Tokens/sec 361.037, Trained Tokens 2058395, Peak mem 19.940 GB\n",
      "Iter 12440: Train loss 0.781, Learning Rate 4.000e-04, It/sec 1.781, Tokens/sec 335.482, Trained Tokens 2060279, Peak mem 19.940 GB\n",
      "Iter 12450: Val loss 2.400, Val took 3.566s\n",
      "Iter 12450: Train loss 0.692, Learning Rate 4.000e-04, It/sec 13.960, Tokens/sec 1735.176, Trained Tokens 2061522, Peak mem 19.940 GB\n",
      "Iter 12460: Train loss 0.844, Learning Rate 4.000e-04, It/sec 1.535, Tokens/sec 319.810, Trained Tokens 2063606, Peak mem 19.940 GB\n",
      "Iter 12470: Train loss 0.881, Learning Rate 4.000e-04, It/sec 1.596, Tokens/sec 335.775, Trained Tokens 2065710, Peak mem 19.940 GB\n",
      "Iter 12480: Train loss 0.795, Learning Rate 4.000e-04, It/sec 2.556, Tokens/sec 348.927, Trained Tokens 2067075, Peak mem 19.940 GB\n",
      "Iter 12490: Train loss 0.686, Learning Rate 4.000e-04, It/sec 2.492, Tokens/sec 357.824, Trained Tokens 2068511, Peak mem 19.940 GB\n",
      "Iter 12500: Val loss 2.518, Val took 3.907s\n",
      "Iter 12500: Train loss 0.750, Learning Rate 4.000e-04, It/sec 29.060, Tokens/sec 4510.185, Trained Tokens 2070063, Peak mem 19.940 GB\n",
      "Iter 12500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012500_adapters.safetensors.\n",
      "Iter 12510: Train loss 0.932, Learning Rate 4.000e-04, It/sec 1.563, Tokens/sec 334.706, Trained Tokens 2072204, Peak mem 19.940 GB\n",
      "Iter 12520: Train loss 0.838, Learning Rate 4.000e-04, It/sec 2.047, Tokens/sec 352.471, Trained Tokens 2073926, Peak mem 19.940 GB\n",
      "Iter 12530: Train loss 0.750, Learning Rate 4.000e-04, It/sec 2.598, Tokens/sec 366.887, Trained Tokens 2075338, Peak mem 19.940 GB\n",
      "Iter 12540: Train loss 0.769, Learning Rate 4.000e-04, It/sec 1.914, Tokens/sec 317.533, Trained Tokens 2076997, Peak mem 19.940 GB\n",
      "Iter 12550: Val loss 2.265, Val took 3.569s\n",
      "Iter 12550: Train loss 0.749, Learning Rate 4.000e-04, It/sec 21.771, Tokens/sec 3846.935, Trained Tokens 2078764, Peak mem 19.940 GB\n",
      "Iter 12560: Train loss 0.722, Learning Rate 4.000e-04, It/sec 2.298, Tokens/sec 331.360, Trained Tokens 2080206, Peak mem 19.940 GB\n",
      "Iter 12570: Train loss 0.769, Learning Rate 4.000e-04, It/sec 2.235, Tokens/sec 311.551, Trained Tokens 2081600, Peak mem 19.940 GB\n",
      "Iter 12580: Train loss 0.996, Learning Rate 4.000e-04, It/sec 1.563, Tokens/sec 342.368, Trained Tokens 2083791, Peak mem 19.940 GB\n",
      "Iter 12590: Train loss 0.820, Learning Rate 4.000e-04, It/sec 2.006, Tokens/sec 330.435, Trained Tokens 2085438, Peak mem 19.940 GB\n",
      "Iter 12600: Val loss 2.328, Val took 3.747s\n",
      "Iter 12600: Train loss 0.789, Learning Rate 4.000e-04, It/sec 16.596, Tokens/sec 2202.326, Trained Tokens 2086765, Peak mem 19.940 GB\n",
      "Iter 12600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012600_adapters.safetensors.\n",
      "Iter 12610: Train loss 0.757, Learning Rate 4.000e-04, It/sec 1.984, Tokens/sec 287.289, Trained Tokens 2088213, Peak mem 19.940 GB\n",
      "Iter 12620: Train loss 0.873, Learning Rate 4.000e-04, It/sec 1.681, Tokens/sec 303.326, Trained Tokens 2090017, Peak mem 19.940 GB\n",
      "Iter 12630: Train loss 0.831, Learning Rate 4.000e-04, It/sec 2.032, Tokens/sec 307.294, Trained Tokens 2091529, Peak mem 19.940 GB\n",
      "Iter 12640: Train loss 0.731, Learning Rate 4.000e-04, It/sec 2.449, Tokens/sec 321.035, Trained Tokens 2092840, Peak mem 19.940 GB\n",
      "Iter 12650: Val loss 2.526, Val took 3.721s\n",
      "Iter 12650: Train loss 0.785, Learning Rate 4.000e-04, It/sec 7.903, Tokens/sec 995.035, Trained Tokens 2094099, Peak mem 19.940 GB\n",
      "Iter 12660: Train loss 1.015, Learning Rate 4.000e-04, It/sec 0.990, Tokens/sec 322.308, Trained Tokens 2097356, Peak mem 19.940 GB\n",
      "Iter 12670: Train loss 0.690, Learning Rate 4.000e-04, It/sec 2.666, Tokens/sec 315.112, Trained Tokens 2098538, Peak mem 19.940 GB\n",
      "Iter 12680: Train loss 0.932, Learning Rate 4.000e-04, It/sec 1.275, Tokens/sec 303.116, Trained Tokens 2100916, Peak mem 19.940 GB\n",
      "Iter 12690: Train loss 0.786, Learning Rate 4.000e-04, It/sec 2.655, Tokens/sec 318.810, Trained Tokens 2102117, Peak mem 19.940 GB\n",
      "Iter 12700: Val loss 2.780, Val took 3.893s\n",
      "Iter 12700: Train loss 0.931, Learning Rate 4.000e-04, It/sec 35.791, Tokens/sec 6392.198, Trained Tokens 2103903, Peak mem 19.940 GB\n",
      "Iter 12700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012700_adapters.safetensors.\n",
      "Iter 12710: Train loss 0.833, Learning Rate 4.000e-04, It/sec 2.056, Tokens/sec 300.571, Trained Tokens 2105365, Peak mem 19.940 GB\n",
      "Iter 12720: Train loss 0.712, Learning Rate 4.000e-04, It/sec 1.610, Tokens/sec 308.572, Trained Tokens 2107282, Peak mem 19.940 GB\n",
      "Iter 12730: Train loss 0.631, Learning Rate 4.000e-04, It/sec 2.179, Tokens/sec 375.618, Trained Tokens 2109006, Peak mem 19.940 GB\n",
      "Iter 12740: Train loss 0.569, Learning Rate 4.000e-04, It/sec 2.808, Tokens/sec 377.375, Trained Tokens 2110350, Peak mem 19.940 GB\n",
      "Iter 12750: Val loss 2.311, Val took 3.782s\n",
      "Iter 12750: Train loss 0.762, Learning Rate 4.000e-04, It/sec 43.589, Tokens/sec 7833.008, Trained Tokens 2112147, Peak mem 19.940 GB\n",
      "Iter 12760: Train loss 0.716, Learning Rate 4.000e-04, It/sec 2.062, Tokens/sec 315.250, Trained Tokens 2113676, Peak mem 19.940 GB\n",
      "Iter 12770: Train loss 0.769, Learning Rate 4.000e-04, It/sec 2.074, Tokens/sec 356.669, Trained Tokens 2115396, Peak mem 19.940 GB\n",
      "Iter 12780: Train loss 0.686, Learning Rate 4.000e-04, It/sec 1.687, Tokens/sec 291.776, Trained Tokens 2117126, Peak mem 19.940 GB\n",
      "Iter 12790: Train loss 0.684, Learning Rate 4.000e-04, It/sec 1.995, Tokens/sec 336.020, Trained Tokens 2118810, Peak mem 19.940 GB\n",
      "Iter 12800: Val loss 3.054, Val took 4.325s\n",
      "Iter 12800: Train loss 0.683, Learning Rate 4.000e-04, It/sec 24.332, Tokens/sec 3851.678, Trained Tokens 2120393, Peak mem 19.940 GB\n",
      "Iter 12800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012800_adapters.safetensors.\n",
      "Iter 12810: Train loss 0.838, Learning Rate 4.000e-04, It/sec 1.472, Tokens/sec 337.716, Trained Tokens 2122687, Peak mem 19.940 GB\n",
      "Iter 12820: Train loss 0.694, Learning Rate 4.000e-04, It/sec 1.951, Tokens/sec 348.463, Trained Tokens 2124473, Peak mem 19.940 GB\n",
      "Iter 12830: Train loss 0.653, Learning Rate 4.000e-04, It/sec 1.917, Tokens/sec 318.633, Trained Tokens 2126135, Peak mem 19.940 GB\n",
      "Iter 12840: Train loss 0.765, Learning Rate 4.000e-04, It/sec 2.953, Tokens/sec 322.212, Trained Tokens 2127226, Peak mem 19.940 GB\n",
      "Iter 12850: Val loss 2.275, Val took 3.763s\n",
      "Iter 12850: Train loss 0.843, Learning Rate 4.000e-04, It/sec 22.718, Tokens/sec 5781.837, Trained Tokens 2129771, Peak mem 19.940 GB\n",
      "Iter 12860: Train loss 0.774, Learning Rate 4.000e-04, It/sec 1.592, Tokens/sec 316.777, Trained Tokens 2131761, Peak mem 19.940 GB\n",
      "Iter 12870: Train loss 0.619, Learning Rate 4.000e-04, It/sec 2.266, Tokens/sec 317.224, Trained Tokens 2133161, Peak mem 19.940 GB\n",
      "Iter 12880: Train loss 0.683, Learning Rate 4.000e-04, It/sec 1.762, Tokens/sec 308.270, Trained Tokens 2134911, Peak mem 19.940 GB\n",
      "Iter 12890: Train loss 0.683, Learning Rate 4.000e-04, It/sec 2.358, Tokens/sec 317.870, Trained Tokens 2136259, Peak mem 19.940 GB\n",
      "Iter 12900: Val loss 2.231, Val took 2.570s\n",
      "Iter 12900: Train loss 0.692, Learning Rate 4.000e-04, It/sec 22.731, Tokens/sec 3041.429, Trained Tokens 2137597, Peak mem 19.940 GB\n",
      "Iter 12900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0012900_adapters.safetensors.\n",
      "Iter 12910: Train loss 0.849, Learning Rate 4.000e-04, It/sec 1.492, Tokens/sec 369.001, Trained Tokens 2140071, Peak mem 19.940 GB\n",
      "Iter 12920: Train loss 0.755, Learning Rate 4.000e-04, It/sec 2.262, Tokens/sec 301.042, Trained Tokens 2141402, Peak mem 19.940 GB\n",
      "Iter 12930: Train loss 0.719, Learning Rate 4.000e-04, It/sec 1.953, Tokens/sec 317.944, Trained Tokens 2143030, Peak mem 19.940 GB\n",
      "Iter 12940: Train loss 0.740, Learning Rate 4.000e-04, It/sec 1.956, Tokens/sec 341.269, Trained Tokens 2144775, Peak mem 19.940 GB\n",
      "Iter 12950: Val loss 2.567, Val took 3.800s\n",
      "Iter 12950: Train loss 0.798, Learning Rate 4.000e-04, It/sec 9.316, Tokens/sec 1190.648, Trained Tokens 2146053, Peak mem 19.940 GB\n",
      "Iter 12960: Train loss 0.812, Learning Rate 4.000e-04, It/sec 1.980, Tokens/sec 318.783, Trained Tokens 2147663, Peak mem 19.940 GB\n",
      "Iter 12970: Train loss 0.880, Learning Rate 4.000e-04, It/sec 1.465, Tokens/sec 318.900, Trained Tokens 2149840, Peak mem 19.940 GB\n",
      "Iter 12980: Train loss 0.796, Learning Rate 4.000e-04, It/sec 1.921, Tokens/sec 336.886, Trained Tokens 2151594, Peak mem 19.940 GB\n",
      "Iter 12990: Train loss 0.769, Learning Rate 4.000e-04, It/sec 2.492, Tokens/sec 315.688, Trained Tokens 2152861, Peak mem 19.940 GB\n",
      "Iter 13000: Val loss 2.298, Val took 3.664s\n",
      "Iter 13000: Train loss 0.802, Learning Rate 4.000e-04, It/sec 39.053, Tokens/sec 5416.664, Trained Tokens 2154248, Peak mem 19.940 GB\n",
      "Iter 13000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013000_adapters.safetensors.\n",
      "Iter 13010: Train loss 0.864, Learning Rate 4.000e-04, It/sec 1.348, Tokens/sec 302.464, Trained Tokens 2156492, Peak mem 19.940 GB\n",
      "Iter 13020: Train loss 0.897, Learning Rate 4.000e-04, It/sec 1.644, Tokens/sec 341.441, Trained Tokens 2158569, Peak mem 19.940 GB\n",
      "Iter 13030: Train loss 0.753, Learning Rate 4.000e-04, It/sec 2.802, Tokens/sec 317.794, Trained Tokens 2159703, Peak mem 19.940 GB\n",
      "Iter 13040: Train loss 0.877, Learning Rate 4.000e-04, It/sec 1.875, Tokens/sec 326.673, Trained Tokens 2161445, Peak mem 19.940 GB\n",
      "Iter 13050: Val loss 2.285, Val took 3.531s\n",
      "Iter 13050: Train loss 0.822, Learning Rate 4.000e-04, It/sec 19.401, Tokens/sec 3375.779, Trained Tokens 2163185, Peak mem 19.940 GB\n",
      "Iter 13060: Train loss 0.880, Learning Rate 4.000e-04, It/sec 2.543, Tokens/sec 361.861, Trained Tokens 2164608, Peak mem 19.940 GB\n",
      "Iter 13070: Train loss 0.707, Learning Rate 4.000e-04, It/sec 2.868, Tokens/sec 350.800, Trained Tokens 2165831, Peak mem 19.940 GB\n",
      "Iter 13080: Train loss 0.843, Learning Rate 4.000e-04, It/sec 2.503, Tokens/sec 359.250, Trained Tokens 2167266, Peak mem 19.940 GB\n",
      "Iter 13090: Train loss 0.773, Learning Rate 4.000e-04, It/sec 2.981, Tokens/sec 333.007, Trained Tokens 2168383, Peak mem 19.940 GB\n",
      "Iter 13100: Val loss 2.472, Val took 3.079s\n",
      "Iter 13100: Train loss 0.776, Learning Rate 4.000e-04, It/sec 24.296, Tokens/sec 3367.379, Trained Tokens 2169769, Peak mem 19.940 GB\n",
      "Iter 13100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013100_adapters.safetensors.\n",
      "Iter 13110: Train loss 0.814, Learning Rate 4.000e-04, It/sec 1.758, Tokens/sec 282.588, Trained Tokens 2171376, Peak mem 19.940 GB\n",
      "Iter 13120: Train loss 0.977, Learning Rate 4.000e-04, It/sec 1.839, Tokens/sec 350.229, Trained Tokens 2173280, Peak mem 19.940 GB\n",
      "Iter 13130: Train loss 0.681, Learning Rate 4.000e-04, It/sec 2.323, Tokens/sec 325.747, Trained Tokens 2174682, Peak mem 19.940 GB\n",
      "Iter 13140: Train loss 0.772, Learning Rate 4.000e-04, It/sec 1.491, Tokens/sec 301.694, Trained Tokens 2176706, Peak mem 19.940 GB\n",
      "Iter 13150: Val loss 2.447, Val took 3.494s\n",
      "Iter 13150: Train loss 0.617, Learning Rate 4.000e-04, It/sec 59.407, Tokens/sec 6582.278, Trained Tokens 2177814, Peak mem 19.940 GB\n",
      "Iter 13160: Train loss 0.675, Learning Rate 4.000e-04, It/sec 1.718, Tokens/sec 280.613, Trained Tokens 2179447, Peak mem 19.940 GB\n",
      "Iter 13170: Train loss 0.653, Learning Rate 4.000e-04, It/sec 2.762, Tokens/sec 326.190, Trained Tokens 2180628, Peak mem 19.940 GB\n",
      "Iter 13180: Train loss 0.660, Learning Rate 4.000e-04, It/sec 3.068, Tokens/sec 328.848, Trained Tokens 2181700, Peak mem 19.940 GB\n",
      "Iter 13190: Train loss 0.810, Learning Rate 4.000e-04, It/sec 1.599, Tokens/sec 316.032, Trained Tokens 2183677, Peak mem 19.940 GB\n",
      "Iter 13200: Val loss 2.497, Val took 4.194s\n",
      "Iter 13200: Train loss 0.647, Learning Rate 4.000e-04, It/sec 18.082, Tokens/sec 2531.424, Trained Tokens 2185077, Peak mem 19.940 GB\n",
      "Iter 13200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013200_adapters.safetensors.\n",
      "Iter 13210: Train loss 0.768, Learning Rate 4.000e-04, It/sec 2.152, Tokens/sec 306.674, Trained Tokens 2186502, Peak mem 19.940 GB\n",
      "Iter 13220: Train loss 0.891, Learning Rate 4.000e-04, It/sec 1.431, Tokens/sec 318.515, Trained Tokens 2188728, Peak mem 19.940 GB\n",
      "Iter 13230: Train loss 0.799, Learning Rate 4.000e-04, It/sec 1.749, Tokens/sec 296.013, Trained Tokens 2190420, Peak mem 19.940 GB\n",
      "Iter 13240: Train loss 0.928, Learning Rate 4.000e-04, It/sec 1.400, Tokens/sec 351.910, Trained Tokens 2192933, Peak mem 19.940 GB\n",
      "Iter 13250: Val loss 2.273, Val took 3.365s\n",
      "Iter 13250: Train loss 0.707, Learning Rate 4.000e-04, It/sec 18.642, Tokens/sec 2809.356, Trained Tokens 2194440, Peak mem 19.940 GB\n",
      "Iter 13260: Train loss 0.735, Learning Rate 4.000e-04, It/sec 2.171, Tokens/sec 343.076, Trained Tokens 2196020, Peak mem 19.940 GB\n",
      "Iter 13270: Train loss 0.749, Learning Rate 4.000e-04, It/sec 1.883, Tokens/sec 325.088, Trained Tokens 2197746, Peak mem 19.940 GB\n",
      "Iter 13280: Train loss 0.936, Learning Rate 4.000e-04, It/sec 0.835, Tokens/sec 295.580, Trained Tokens 2201287, Peak mem 19.940 GB\n",
      "Iter 13290: Train loss 0.728, Learning Rate 4.000e-04, It/sec 2.162, Tokens/sec 315.039, Trained Tokens 2202744, Peak mem 19.940 GB\n",
      "Iter 13300: Val loss 2.260, Val took 4.258s\n",
      "Iter 13300: Train loss 0.614, Learning Rate 4.000e-04, It/sec 16.413, Tokens/sec 2440.541, Trained Tokens 2204231, Peak mem 19.940 GB\n",
      "Iter 13300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013300_adapters.safetensors.\n",
      "Iter 13310: Train loss 0.743, Learning Rate 4.000e-04, It/sec 2.381, Tokens/sec 300.667, Trained Tokens 2205494, Peak mem 19.940 GB\n",
      "Iter 13320: Train loss 0.710, Learning Rate 4.000e-04, It/sec 2.248, Tokens/sec 353.207, Trained Tokens 2207065, Peak mem 19.940 GB\n",
      "Iter 13330: Train loss 0.704, Learning Rate 4.000e-04, It/sec 2.597, Tokens/sec 358.890, Trained Tokens 2208447, Peak mem 19.940 GB\n",
      "Iter 13340: Train loss 0.793, Learning Rate 4.000e-04, It/sec 1.806, Tokens/sec 364.779, Trained Tokens 2210467, Peak mem 19.940 GB\n",
      "Iter 13350: Val loss 2.397, Val took 3.788s\n",
      "Iter 13350: Train loss 0.706, Learning Rate 4.000e-04, It/sec 24.064, Tokens/sec 2967.145, Trained Tokens 2211700, Peak mem 19.940 GB\n",
      "Iter 13360: Train loss 0.776, Learning Rate 4.000e-04, It/sec 1.461, Tokens/sec 361.438, Trained Tokens 2214174, Peak mem 19.940 GB\n",
      "Iter 13370: Train loss 0.792, Learning Rate 4.000e-04, It/sec 2.373, Tokens/sec 367.805, Trained Tokens 2215724, Peak mem 19.940 GB\n",
      "Iter 13380: Train loss 0.922, Learning Rate 4.000e-04, It/sec 1.741, Tokens/sec 387.181, Trained Tokens 2217948, Peak mem 19.940 GB\n",
      "Iter 13390: Train loss 0.726, Learning Rate 4.000e-04, It/sec 2.291, Tokens/sec 322.052, Trained Tokens 2219354, Peak mem 19.940 GB\n",
      "Iter 13400: Val loss 2.326, Val took 3.412s\n",
      "Iter 13400: Train loss 0.850, Learning Rate 4.000e-04, It/sec 58.755, Tokens/sec 9347.858, Trained Tokens 2220945, Peak mem 19.940 GB\n",
      "Iter 13400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013400_adapters.safetensors.\n",
      "Iter 13410: Train loss 0.714, Learning Rate 4.000e-04, It/sec 2.562, Tokens/sec 357.385, Trained Tokens 2222340, Peak mem 19.940 GB\n",
      "Iter 13420: Train loss 0.799, Learning Rate 4.000e-04, It/sec 2.321, Tokens/sec 350.630, Trained Tokens 2223851, Peak mem 19.940 GB\n",
      "Iter 13430: Train loss 0.768, Learning Rate 4.000e-04, It/sec 2.793, Tokens/sec 368.673, Trained Tokens 2225171, Peak mem 19.940 GB\n",
      "Iter 13440: Train loss 0.826, Learning Rate 4.000e-04, It/sec 2.187, Tokens/sec 374.715, Trained Tokens 2226884, Peak mem 19.940 GB\n",
      "Iter 13450: Val loss 2.568, Val took 4.280s\n",
      "Iter 13450: Train loss 0.803, Learning Rate 4.000e-04, It/sec 19.274, Tokens/sec 3210.967, Trained Tokens 2228550, Peak mem 19.940 GB\n",
      "Iter 13460: Train loss 0.859, Learning Rate 4.000e-04, It/sec 2.115, Tokens/sec 375.409, Trained Tokens 2230325, Peak mem 19.940 GB\n",
      "Iter 13470: Train loss 0.746, Learning Rate 4.000e-04, It/sec 2.762, Tokens/sec 370.647, Trained Tokens 2231667, Peak mem 19.940 GB\n",
      "Iter 13480: Train loss 0.738, Learning Rate 4.000e-04, It/sec 2.894, Tokens/sec 384.080, Trained Tokens 2232994, Peak mem 19.940 GB\n",
      "Iter 13490: Train loss 0.797, Learning Rate 4.000e-04, It/sec 2.240, Tokens/sec 369.214, Trained Tokens 2234642, Peak mem 19.940 GB\n",
      "Iter 13500: Val loss 2.424, Val took 3.541s\n",
      "Iter 13500: Train loss 0.888, Learning Rate 4.000e-04, It/sec 43.260, Tokens/sec 7592.138, Trained Tokens 2236397, Peak mem 19.940 GB\n",
      "Iter 13500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013500_adapters.safetensors.\n",
      "Iter 13510: Train loss 0.781, Learning Rate 4.000e-04, It/sec 2.745, Tokens/sec 344.809, Trained Tokens 2237653, Peak mem 19.940 GB\n",
      "Iter 13520: Train loss 0.815, Learning Rate 4.000e-04, It/sec 2.839, Tokens/sec 359.443, Trained Tokens 2238919, Peak mem 19.940 GB\n",
      "Iter 13530: Train loss 1.004, Learning Rate 4.000e-04, It/sec 1.679, Tokens/sec 382.076, Trained Tokens 2241195, Peak mem 19.940 GB\n",
      "Iter 13540: Train loss 0.862, Learning Rate 4.000e-04, It/sec 1.450, Tokens/sec 372.128, Trained Tokens 2243761, Peak mem 19.940 GB\n",
      "Iter 13550: Val loss 2.340, Val took 3.193s\n",
      "Iter 13550: Train loss 0.733, Learning Rate 4.000e-04, It/sec 13.090, Tokens/sec 2002.764, Trained Tokens 2245291, Peak mem 19.940 GB\n",
      "Iter 13560: Train loss 0.763, Learning Rate 4.000e-04, It/sec 2.254, Tokens/sec 359.322, Trained Tokens 2246885, Peak mem 19.940 GB\n",
      "Iter 13570: Train loss 0.646, Learning Rate 4.000e-04, It/sec 2.486, Tokens/sec 373.640, Trained Tokens 2248388, Peak mem 19.940 GB\n",
      "Iter 13580: Train loss 0.758, Learning Rate 4.000e-04, It/sec 2.274, Tokens/sec 369.570, Trained Tokens 2250013, Peak mem 19.940 GB\n",
      "Iter 13590: Train loss 0.650, Learning Rate 4.000e-04, It/sec 2.201, Tokens/sec 353.920, Trained Tokens 2251621, Peak mem 19.940 GB\n",
      "Iter 13600: Val loss 2.623, Val took 3.763s\n",
      "Iter 13600: Train loss 0.608, Learning Rate 4.000e-04, It/sec 13.616, Tokens/sec 2193.534, Trained Tokens 2253232, Peak mem 19.940 GB\n",
      "Iter 13600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013600_adapters.safetensors.\n",
      "Iter 13610: Train loss 0.723, Learning Rate 4.000e-04, It/sec 2.399, Tokens/sec 358.913, Trained Tokens 2254728, Peak mem 19.940 GB\n",
      "Iter 13620: Train loss 0.741, Learning Rate 4.000e-04, It/sec 2.219, Tokens/sec 383.471, Trained Tokens 2256456, Peak mem 19.940 GB\n",
      "Iter 13630: Train loss 0.635, Learning Rate 4.000e-04, It/sec 2.378, Tokens/sec 356.975, Trained Tokens 2257957, Peak mem 19.940 GB\n",
      "Iter 13640: Train loss 0.713, Learning Rate 4.000e-04, It/sec 2.302, Tokens/sec 383.498, Trained Tokens 2259623, Peak mem 19.940 GB\n",
      "Iter 13650: Val loss 2.266, Val took 2.549s\n",
      "Iter 13650: Train loss 0.658, Learning Rate 4.000e-04, It/sec 27.229, Tokens/sec 3349.154, Trained Tokens 2260853, Peak mem 19.940 GB\n",
      "Iter 13660: Train loss 0.794, Learning Rate 4.000e-04, It/sec 2.126, Tokens/sec 378.852, Trained Tokens 2262635, Peak mem 19.940 GB\n",
      "Iter 13670: Train loss 0.691, Learning Rate 4.000e-04, It/sec 2.209, Tokens/sec 386.643, Trained Tokens 2264385, Peak mem 19.940 GB\n",
      "Iter 13680: Train loss 0.724, Learning Rate 4.000e-04, It/sec 2.969, Tokens/sec 377.642, Trained Tokens 2265657, Peak mem 19.940 GB\n",
      "Iter 13690: Train loss 0.764, Learning Rate 4.000e-04, It/sec 1.874, Tokens/sec 371.523, Trained Tokens 2267640, Peak mem 19.940 GB\n",
      "Iter 13700: Val loss 2.648, Val took 4.095s\n",
      "Iter 13700: Train loss 0.736, Learning Rate 4.000e-04, It/sec 14.368, Tokens/sec 2030.199, Trained Tokens 2269053, Peak mem 19.940 GB\n",
      "Iter 13700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013700_adapters.safetensors.\n",
      "Iter 13710: Train loss 0.752, Learning Rate 4.000e-04, It/sec 2.891, Tokens/sec 329.816, Trained Tokens 2270194, Peak mem 19.940 GB\n",
      "Iter 13720: Train loss 0.736, Learning Rate 4.000e-04, It/sec 2.210, Tokens/sec 370.416, Trained Tokens 2271870, Peak mem 19.940 GB\n",
      "Iter 13730: Train loss 0.823, Learning Rate 4.000e-04, It/sec 2.363, Tokens/sec 378.751, Trained Tokens 2273473, Peak mem 19.940 GB\n",
      "Iter 13740: Train loss 0.714, Learning Rate 4.000e-04, It/sec 2.677, Tokens/sec 359.795, Trained Tokens 2274817, Peak mem 19.940 GB\n",
      "Iter 13750: Val loss 2.573, Val took 3.823s\n",
      "Iter 13750: Train loss 0.784, Learning Rate 4.000e-04, It/sec 5.762, Tokens/sec 934.532, Trained Tokens 2276439, Peak mem 19.940 GB\n",
      "Iter 13760: Train loss 0.906, Learning Rate 4.000e-04, It/sec 1.848, Tokens/sec 389.621, Trained Tokens 2278547, Peak mem 19.940 GB\n",
      "Iter 13770: Train loss 0.904, Learning Rate 4.000e-04, It/sec 1.940, Tokens/sec 344.385, Trained Tokens 2280322, Peak mem 19.940 GB\n",
      "Iter 13780: Train loss 0.738, Learning Rate 4.000e-04, It/sec 2.400, Tokens/sec 355.003, Trained Tokens 2281801, Peak mem 19.940 GB\n",
      "Iter 13790: Train loss 0.683, Learning Rate 4.000e-04, It/sec 2.492, Tokens/sec 356.624, Trained Tokens 2283232, Peak mem 19.940 GB\n",
      "Iter 13800: Val loss 2.717, Val took 3.531s\n",
      "Iter 13800: Train loss 0.782, Learning Rate 4.000e-04, It/sec 44.323, Tokens/sec 6289.430, Trained Tokens 2284651, Peak mem 19.940 GB\n",
      "Iter 13800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013800_adapters.safetensors.\n",
      "Iter 13810: Train loss 0.934, Learning Rate 4.000e-04, It/sec 1.660, Tokens/sec 389.587, Trained Tokens 2286998, Peak mem 19.940 GB\n",
      "Iter 13820: Train loss 0.861, Learning Rate 4.000e-04, It/sec 1.417, Tokens/sec 410.684, Trained Tokens 2289896, Peak mem 19.940 GB\n",
      "Iter 13830: Train loss 0.718, Learning Rate 4.000e-04, It/sec 2.638, Tokens/sec 376.708, Trained Tokens 2291324, Peak mem 19.940 GB\n",
      "Iter 13840: Train loss 0.816, Learning Rate 4.000e-04, It/sec 1.898, Tokens/sec 385.074, Trained Tokens 2293353, Peak mem 19.940 GB\n",
      "Iter 13850: Val loss 2.918, Val took 3.133s\n",
      "Iter 13850: Train loss 0.751, Learning Rate 4.000e-04, It/sec 13.076, Tokens/sec 2519.822, Trained Tokens 2295280, Peak mem 19.940 GB\n",
      "Iter 13860: Train loss 0.792, Learning Rate 4.000e-04, It/sec 1.831, Tokens/sec 330.692, Trained Tokens 2297086, Peak mem 19.940 GB\n",
      "Iter 13870: Train loss 0.764, Learning Rate 4.000e-04, It/sec 2.222, Tokens/sec 378.905, Trained Tokens 2298791, Peak mem 19.940 GB\n",
      "Iter 13880: Train loss 0.683, Learning Rate 4.000e-04, It/sec 2.508, Tokens/sec 374.685, Trained Tokens 2300285, Peak mem 19.940 GB\n",
      "Iter 13890: Train loss 0.745, Learning Rate 4.000e-04, It/sec 3.135, Tokens/sec 362.690, Trained Tokens 2301442, Peak mem 19.940 GB\n",
      "Iter 13900: Val loss 2.763, Val took 3.661s\n",
      "Iter 13900: Train loss 0.890, Learning Rate 4.000e-04, It/sec 33.899, Tokens/sec 4403.472, Trained Tokens 2302741, Peak mem 19.940 GB\n",
      "Iter 13900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0013900_adapters.safetensors.\n",
      "Iter 13910: Train loss 0.798, Learning Rate 4.000e-04, It/sec 2.841, Tokens/sec 357.401, Trained Tokens 2303999, Peak mem 19.940 GB\n",
      "Iter 13920: Train loss 0.865, Learning Rate 4.000e-04, It/sec 2.495, Tokens/sec 354.339, Trained Tokens 2305419, Peak mem 19.940 GB\n",
      "Iter 13930: Train loss 1.000, Learning Rate 4.000e-04, It/sec 1.545, Tokens/sec 373.037, Trained Tokens 2307833, Peak mem 19.940 GB\n",
      "Iter 13940: Train loss 0.747, Learning Rate 4.000e-04, It/sec 2.473, Tokens/sec 315.864, Trained Tokens 2309110, Peak mem 19.940 GB\n",
      "Iter 13950: Val loss 2.433, Val took 3.064s\n",
      "Iter 13950: Train loss 0.939, Learning Rate 4.000e-04, It/sec 24.535, Tokens/sec 5777.926, Trained Tokens 2311465, Peak mem 19.940 GB\n",
      "Iter 13960: Train loss 0.626, Learning Rate 4.000e-04, It/sec 2.190, Tokens/sec 371.894, Trained Tokens 2313163, Peak mem 19.940 GB\n",
      "Iter 13970: Train loss 0.754, Learning Rate 4.000e-04, It/sec 2.493, Tokens/sec 363.211, Trained Tokens 2314620, Peak mem 19.940 GB\n",
      "Iter 13980: Train loss 0.653, Learning Rate 4.000e-04, It/sec 2.679, Tokens/sec 371.901, Trained Tokens 2316008, Peak mem 19.940 GB\n",
      "Iter 13990: Train loss 0.748, Learning Rate 4.000e-04, It/sec 2.120, Tokens/sec 375.443, Trained Tokens 2317779, Peak mem 19.940 GB\n",
      "Iter 14000: Val loss 2.509, Val took 4.036s\n",
      "Iter 14000: Train loss 0.704, Learning Rate 4.000e-04, It/sec 42.962, Tokens/sec 7097.241, Trained Tokens 2319431, Peak mem 19.940 GB\n",
      "Iter 14000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014000_adapters.safetensors.\n",
      "Iter 14010: Train loss 0.706, Learning Rate 4.000e-04, It/sec 2.395, Tokens/sec 331.994, Trained Tokens 2320817, Peak mem 19.940 GB\n",
      "Iter 14020: Train loss 0.767, Learning Rate 4.000e-04, It/sec 1.814, Tokens/sec 404.441, Trained Tokens 2323047, Peak mem 19.940 GB\n",
      "Iter 14030: Train loss 0.712, Learning Rate 4.000e-04, It/sec 3.217, Tokens/sec 362.528, Trained Tokens 2324174, Peak mem 19.940 GB\n",
      "Iter 14040: Train loss 0.650, Learning Rate 4.000e-04, It/sec 2.150, Tokens/sec 371.317, Trained Tokens 2325901, Peak mem 19.940 GB\n",
      "Iter 14050: Val loss 2.366, Val took 3.898s\n",
      "Iter 14050: Train loss 0.829, Learning Rate 4.000e-04, It/sec 20.065, Tokens/sec 5463.640, Trained Tokens 2328624, Peak mem 19.940 GB\n",
      "Iter 14060: Train loss 0.687, Learning Rate 4.000e-04, It/sec 2.279, Tokens/sec 341.809, Trained Tokens 2330124, Peak mem 19.940 GB\n",
      "Iter 14070: Train loss 0.701, Learning Rate 4.000e-04, It/sec 2.760, Tokens/sec 380.931, Trained Tokens 2331504, Peak mem 19.940 GB\n",
      "Iter 14080: Train loss 0.742, Learning Rate 4.000e-04, It/sec 2.677, Tokens/sec 378.552, Trained Tokens 2332918, Peak mem 19.940 GB\n",
      "Iter 14090: Train loss 0.681, Learning Rate 4.000e-04, It/sec 2.974, Tokens/sec 359.804, Trained Tokens 2334128, Peak mem 19.940 GB\n",
      "Iter 14100: Val loss 2.442, Val took 3.399s\n",
      "Iter 14100: Train loss 0.875, Learning Rate 4.000e-04, It/sec 35.107, Tokens/sec 6754.673, Trained Tokens 2336052, Peak mem 19.940 GB\n",
      "Iter 14100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014100_adapters.safetensors.\n",
      "Iter 14110: Train loss 0.706, Learning Rate 4.000e-04, It/sec 2.792, Tokens/sec 347.589, Trained Tokens 2337297, Peak mem 19.940 GB\n",
      "Iter 14120: Train loss 0.892, Learning Rate 4.000e-04, It/sec 1.878, Tokens/sec 386.726, Trained Tokens 2339356, Peak mem 19.940 GB\n",
      "Iter 14130: Train loss 0.743, Learning Rate 4.000e-04, It/sec 1.925, Tokens/sec 378.762, Trained Tokens 2341324, Peak mem 19.940 GB\n",
      "Iter 14140: Train loss 0.725, Learning Rate 4.000e-04, It/sec 2.509, Tokens/sec 390.936, Trained Tokens 2342882, Peak mem 19.940 GB\n",
      "Iter 14150: Val loss 2.434, Val took 3.516s\n",
      "Iter 14150: Train loss 0.704, Learning Rate 4.000e-04, It/sec 44.501, Tokens/sec 5629.358, Trained Tokens 2344147, Peak mem 19.940 GB\n",
      "Iter 14160: Train loss 0.837, Learning Rate 4.000e-04, It/sec 2.449, Tokens/sec 361.219, Trained Tokens 2345622, Peak mem 19.940 GB\n",
      "Iter 14170: Train loss 0.844, Learning Rate 4.000e-04, It/sec 1.442, Tokens/sec 289.794, Trained Tokens 2347632, Peak mem 19.940 GB\n",
      "Iter 14180: Train loss 0.803, Learning Rate 4.000e-04, It/sec 1.864, Tokens/sec 314.438, Trained Tokens 2349319, Peak mem 19.940 GB\n",
      "Iter 14190: Train loss 0.663, Learning Rate 4.000e-04, It/sec 1.980, Tokens/sec 337.219, Trained Tokens 2351022, Peak mem 19.940 GB\n",
      "Iter 14200: Val loss 2.359, Val took 3.729s\n",
      "Iter 14200: Train loss 0.706, Learning Rate 4.000e-04, It/sec 15.040, Tokens/sec 2287.512, Trained Tokens 2352543, Peak mem 19.940 GB\n",
      "Iter 14200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014200_adapters.safetensors.\n",
      "Iter 14210: Train loss 0.820, Learning Rate 4.000e-04, It/sec 1.989, Tokens/sec 331.915, Trained Tokens 2354212, Peak mem 19.940 GB\n",
      "Iter 14220: Train loss 0.783, Learning Rate 4.000e-04, It/sec 2.084, Tokens/sec 355.281, Trained Tokens 2355917, Peak mem 19.940 GB\n",
      "Iter 14230: Train loss 0.746, Learning Rate 4.000e-04, It/sec 2.530, Tokens/sec 342.011, Trained Tokens 2357269, Peak mem 19.940 GB\n",
      "Iter 14240: Train loss 0.720, Learning Rate 4.000e-04, It/sec 2.656, Tokens/sec 348.161, Trained Tokens 2358580, Peak mem 19.940 GB\n",
      "Iter 14250: Val loss 2.279, Val took 2.985s\n",
      "Iter 14250: Train loss 0.721, Learning Rate 4.000e-04, It/sec 31.291, Tokens/sec 4618.490, Trained Tokens 2360056, Peak mem 19.940 GB\n",
      "Iter 14260: Train loss 0.754, Learning Rate 4.000e-04, It/sec 2.324, Tokens/sec 342.497, Trained Tokens 2361530, Peak mem 19.940 GB\n",
      "Iter 14270: Train loss 0.936, Learning Rate 4.000e-04, It/sec 1.450, Tokens/sec 345.582, Trained Tokens 2363914, Peak mem 19.940 GB\n",
      "Iter 14280: Train loss 0.730, Learning Rate 4.000e-04, It/sec 2.998, Tokens/sec 326.140, Trained Tokens 2365002, Peak mem 19.940 GB\n",
      "Iter 14290: Train loss 0.743, Learning Rate 4.000e-04, It/sec 2.045, Tokens/sec 328.588, Trained Tokens 2366609, Peak mem 19.940 GB\n",
      "Iter 14300: Val loss 2.555, Val took 3.691s\n",
      "Iter 14300: Train loss 0.954, Learning Rate 4.000e-04, It/sec 38.243, Tokens/sec 8478.483, Trained Tokens 2368826, Peak mem 19.940 GB\n",
      "Iter 14300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014300_adapters.safetensors.\n",
      "Iter 14310: Train loss 0.879, Learning Rate 4.000e-04, It/sec 2.203, Tokens/sec 345.220, Trained Tokens 2370393, Peak mem 19.940 GB\n",
      "Iter 14320: Train loss 0.667, Learning Rate 4.000e-04, It/sec 2.287, Tokens/sec 345.540, Trained Tokens 2371904, Peak mem 19.940 GB\n",
      "Iter 14330: Train loss 0.866, Learning Rate 4.000e-04, It/sec 1.860, Tokens/sec 343.529, Trained Tokens 2373751, Peak mem 19.940 GB\n",
      "Iter 14340: Train loss 0.830, Learning Rate 4.000e-04, It/sec 1.992, Tokens/sec 296.748, Trained Tokens 2375241, Peak mem 19.940 GB\n",
      "Iter 14350: Val loss 2.286, Val took 3.642s\n",
      "Iter 14350: Train loss 0.821, Learning Rate 4.000e-04, It/sec 40.925, Tokens/sec 7301.070, Trained Tokens 2377025, Peak mem 19.940 GB\n",
      "Iter 14360: Train loss 0.712, Learning Rate 4.000e-04, It/sec 2.118, Tokens/sec 353.138, Trained Tokens 2378692, Peak mem 19.940 GB\n",
      "Iter 14370: Train loss 0.584, Learning Rate 4.000e-04, It/sec 2.531, Tokens/sec 331.585, Trained Tokens 2380002, Peak mem 19.940 GB\n",
      "Iter 14380: Train loss 0.720, Learning Rate 4.000e-04, It/sec 1.706, Tokens/sec 355.032, Trained Tokens 2382083, Peak mem 19.940 GB\n",
      "Iter 14390: Train loss 0.603, Learning Rate 4.000e-04, It/sec 2.513, Tokens/sec 340.071, Trained Tokens 2383436, Peak mem 19.940 GB\n",
      "Iter 14400: Val loss 2.346, Val took 4.040s\n",
      "Iter 14400: Train loss 0.711, Learning Rate 4.000e-04, It/sec 15.910, Tokens/sec 2429.425, Trained Tokens 2384963, Peak mem 19.940 GB\n",
      "Iter 14400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014400_adapters.safetensors.\n",
      "Iter 14410: Train loss 0.666, Learning Rate 4.000e-04, It/sec 1.911, Tokens/sec 341.418, Trained Tokens 2386750, Peak mem 19.940 GB\n",
      "Iter 14420: Train loss 0.628, Learning Rate 4.000e-04, It/sec 3.157, Tokens/sec 320.727, Trained Tokens 2387766, Peak mem 19.940 GB\n",
      "Iter 14430: Train loss 0.763, Learning Rate 4.000e-04, It/sec 2.103, Tokens/sec 350.771, Trained Tokens 2389434, Peak mem 19.940 GB\n",
      "Iter 14440: Train loss 0.802, Learning Rate 4.000e-04, It/sec 1.546, Tokens/sec 366.515, Trained Tokens 2391804, Peak mem 19.940 GB\n",
      "Iter 14450: Val loss 2.048, Val took 3.758s\n",
      "Iter 14450: Train loss 0.703, Learning Rate 4.000e-04, It/sec 50.563, Tokens/sec 8610.835, Trained Tokens 2393507, Peak mem 19.940 GB\n",
      "Iter 14460: Train loss 0.757, Learning Rate 4.000e-04, It/sec 2.447, Tokens/sec 314.953, Trained Tokens 2394794, Peak mem 19.940 GB\n",
      "Iter 14470: Train loss 0.675, Learning Rate 4.000e-04, It/sec 2.993, Tokens/sec 328.930, Trained Tokens 2395893, Peak mem 19.940 GB\n",
      "Iter 14480: Train loss 0.912, Learning Rate 4.000e-04, It/sec 1.367, Tokens/sec 350.976, Trained Tokens 2398460, Peak mem 19.940 GB\n",
      "Iter 14490: Train loss 0.759, Learning Rate 4.000e-04, It/sec 1.647, Tokens/sec 319.642, Trained Tokens 2400401, Peak mem 19.940 GB\n",
      "Iter 14500: Val loss 2.164, Val took 2.587s\n",
      "Iter 14500: Train loss 0.688, Learning Rate 4.000e-04, It/sec 30.982, Tokens/sec 5626.355, Trained Tokens 2402217, Peak mem 19.940 GB\n",
      "Iter 14500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014500_adapters.safetensors.\n",
      "Iter 14510: Train loss 0.663, Learning Rate 4.000e-04, It/sec 2.200, Tokens/sec 313.722, Trained Tokens 2403643, Peak mem 19.940 GB\n",
      "Iter 14520: Train loss 0.678, Learning Rate 4.000e-04, It/sec 2.252, Tokens/sec 339.377, Trained Tokens 2405150, Peak mem 19.940 GB\n",
      "Iter 14530: Train loss 0.777, Learning Rate 4.000e-04, It/sec 1.686, Tokens/sec 351.992, Trained Tokens 2407238, Peak mem 19.940 GB\n",
      "Iter 14540: Train loss 0.646, Learning Rate 4.000e-04, It/sec 2.693, Tokens/sec 324.463, Trained Tokens 2408443, Peak mem 19.940 GB\n",
      "Iter 14550: Val loss 2.507, Val took 3.788s\n",
      "Iter 14550: Train loss 0.776, Learning Rate 4.000e-04, It/sec 12.168, Tokens/sec 2247.391, Trained Tokens 2410290, Peak mem 19.940 GB\n",
      "Iter 14560: Train loss 0.673, Learning Rate 4.000e-04, It/sec 2.512, Tokens/sec 337.817, Trained Tokens 2411635, Peak mem 19.940 GB\n",
      "Iter 14570: Train loss 0.706, Learning Rate 4.000e-04, It/sec 2.558, Tokens/sec 326.942, Trained Tokens 2412913, Peak mem 19.940 GB\n",
      "Iter 14580: Train loss 0.714, Learning Rate 4.000e-04, It/sec 1.710, Tokens/sec 339.190, Trained Tokens 2414897, Peak mem 19.940 GB\n",
      "Iter 14590: Train loss 0.707, Learning Rate 4.000e-04, It/sec 2.078, Tokens/sec 353.818, Trained Tokens 2416600, Peak mem 19.940 GB\n",
      "Iter 14600: Val loss 2.329, Val took 4.147s\n",
      "Iter 14600: Train loss 0.857, Learning Rate 4.000e-04, It/sec 30.458, Tokens/sec 7437.826, Trained Tokens 2419042, Peak mem 19.940 GB\n",
      "Iter 14600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014600_adapters.safetensors.\n",
      "Iter 14610: Train loss 0.675, Learning Rate 4.000e-04, It/sec 2.127, Tokens/sec 316.499, Trained Tokens 2420530, Peak mem 19.940 GB\n",
      "Iter 14620: Train loss 0.760, Learning Rate 4.000e-04, It/sec 1.395, Tokens/sec 357.135, Trained Tokens 2423091, Peak mem 19.940 GB\n",
      "Iter 14630: Train loss 0.815, Learning Rate 4.000e-04, It/sec 1.640, Tokens/sec 337.130, Trained Tokens 2425147, Peak mem 19.940 GB\n",
      "Iter 14640: Train loss 0.770, Learning Rate 4.000e-04, It/sec 1.724, Tokens/sec 331.688, Trained Tokens 2427071, Peak mem 19.940 GB\n",
      "Iter 14650: Val loss 2.233, Val took 3.840s\n",
      "Iter 14650: Train loss 0.706, Learning Rate 4.000e-04, It/sec 16.014, Tokens/sec 2158.651, Trained Tokens 2428419, Peak mem 19.940 GB\n",
      "Iter 14660: Train loss 0.754, Learning Rate 4.000e-04, It/sec 1.437, Tokens/sec 306.570, Trained Tokens 2430553, Peak mem 19.940 GB\n",
      "Iter 14670: Train loss 0.775, Learning Rate 4.000e-04, It/sec 1.980, Tokens/sec 362.753, Trained Tokens 2432385, Peak mem 19.940 GB\n",
      "Iter 14680: Train loss 0.705, Learning Rate 4.000e-04, It/sec 2.895, Tokens/sec 327.468, Trained Tokens 2433516, Peak mem 19.940 GB\n",
      "Iter 14690: Train loss 0.740, Learning Rate 4.000e-04, It/sec 3.003, Tokens/sec 322.214, Trained Tokens 2434589, Peak mem 19.940 GB\n",
      "Iter 14700: Val loss 2.378, Val took 4.354s\n",
      "Iter 14700: Train loss 0.844, Learning Rate 4.000e-04, It/sec 10.746, Tokens/sec 1982.571, Trained Tokens 2436434, Peak mem 19.940 GB\n",
      "Iter 14700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014700_adapters.safetensors.\n",
      "Iter 14710: Train loss 0.788, Learning Rate 4.000e-04, It/sec 2.185, Tokens/sec 302.862, Trained Tokens 2437820, Peak mem 19.940 GB\n",
      "Iter 14720: Train loss 0.775, Learning Rate 4.000e-04, It/sec 2.674, Tokens/sec 304.300, Trained Tokens 2438958, Peak mem 19.940 GB\n",
      "Iter 14730: Train loss 0.773, Learning Rate 4.000e-04, It/sec 2.481, Tokens/sec 340.161, Trained Tokens 2440329, Peak mem 19.940 GB\n",
      "Iter 14740: Train loss 0.729, Learning Rate 4.000e-04, It/sec 1.996, Tokens/sec 355.603, Trained Tokens 2442111, Peak mem 19.940 GB\n",
      "Iter 14750: Val loss 2.697, Val took 4.004s\n",
      "Iter 14750: Train loss 0.728, Learning Rate 4.000e-04, It/sec 16.225, Tokens/sec 2169.298, Trained Tokens 2443448, Peak mem 19.940 GB\n",
      "Iter 14760: Train loss 0.765, Learning Rate 4.000e-04, It/sec 2.141, Tokens/sec 319.415, Trained Tokens 2444940, Peak mem 19.940 GB\n",
      "Iter 14770: Train loss 0.667, Learning Rate 4.000e-04, It/sec 2.770, Tokens/sec 330.965, Trained Tokens 2446135, Peak mem 19.940 GB\n",
      "Iter 14780: Train loss 0.697, Learning Rate 4.000e-04, It/sec 2.613, Tokens/sec 336.333, Trained Tokens 2447422, Peak mem 19.940 GB\n",
      "Iter 14790: Train loss 0.648, Learning Rate 4.000e-04, It/sec 2.159, Tokens/sec 375.718, Trained Tokens 2449162, Peak mem 19.940 GB\n",
      "Iter 14800: Val loss 2.535, Val took 2.943s\n",
      "Iter 14800: Train loss 0.746, Learning Rate 4.000e-04, It/sec 20.872, Tokens/sec 4128.465, Trained Tokens 2451140, Peak mem 19.940 GB\n",
      "Iter 14800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014800_adapters.safetensors.\n",
      "Iter 14810: Train loss 0.664, Learning Rate 4.000e-04, It/sec 2.822, Tokens/sec 347.416, Trained Tokens 2452371, Peak mem 19.940 GB\n",
      "Iter 14820: Train loss 0.612, Learning Rate 4.000e-04, It/sec 2.311, Tokens/sec 326.542, Trained Tokens 2453784, Peak mem 19.940 GB\n",
      "Iter 14830: Train loss 0.703, Learning Rate 4.000e-04, It/sec 1.876, Tokens/sec 330.260, Trained Tokens 2455544, Peak mem 19.940 GB\n",
      "Iter 14840: Train loss 0.665, Learning Rate 4.000e-04, It/sec 2.369, Tokens/sec 328.318, Trained Tokens 2456930, Peak mem 19.940 GB\n",
      "Iter 14850: Val loss 2.649, Val took 4.527s\n",
      "Iter 14850: Train loss 0.609, Learning Rate 4.000e-04, It/sec 21.081, Tokens/sec 2883.932, Trained Tokens 2458298, Peak mem 19.940 GB\n",
      "Iter 14860: Train loss 0.692, Learning Rate 4.000e-04, It/sec 1.664, Tokens/sec 278.225, Trained Tokens 2459970, Peak mem 19.940 GB\n",
      "Iter 14870: Train loss 0.699, Learning Rate 4.000e-04, It/sec 2.101, Tokens/sec 310.391, Trained Tokens 2461447, Peak mem 19.940 GB\n",
      "Iter 14880: Train loss 0.631, Learning Rate 4.000e-04, It/sec 2.176, Tokens/sec 338.747, Trained Tokens 2463004, Peak mem 19.940 GB\n",
      "Iter 14890: Train loss 0.919, Learning Rate 4.000e-04, It/sec 1.490, Tokens/sec 375.456, Trained Tokens 2465523, Peak mem 19.940 GB\n",
      "Iter 14900: Val loss 2.539, Val took 4.233s\n",
      "Iter 14900: Train loss 0.666, Learning Rate 4.000e-04, It/sec 14.016, Tokens/sec 1655.266, Trained Tokens 2466704, Peak mem 19.940 GB\n",
      "Iter 14900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0014900_adapters.safetensors.\n",
      "Iter 14910: Train loss 0.667, Learning Rate 4.000e-04, It/sec 2.509, Tokens/sec 368.333, Trained Tokens 2468172, Peak mem 19.940 GB\n",
      "Iter 14920: Train loss 0.769, Learning Rate 4.000e-04, It/sec 2.027, Tokens/sec 383.656, Trained Tokens 2470065, Peak mem 19.940 GB\n",
      "Iter 14930: Train loss 0.707, Learning Rate 4.000e-04, It/sec 2.881, Tokens/sec 378.591, Trained Tokens 2471379, Peak mem 19.940 GB\n",
      "Iter 14940: Train loss 0.715, Learning Rate 4.000e-04, It/sec 2.532, Tokens/sec 384.394, Trained Tokens 2472897, Peak mem 19.940 GB\n",
      "Iter 14950: Val loss 2.305, Val took 4.936s\n",
      "Iter 14950: Train loss 0.734, Learning Rate 4.000e-04, It/sec 13.305, Tokens/sec 2641.009, Trained Tokens 2474882, Peak mem 19.940 GB\n",
      "Iter 14960: Train loss 0.767, Learning Rate 4.000e-04, It/sec 2.155, Tokens/sec 373.507, Trained Tokens 2476615, Peak mem 19.940 GB\n",
      "Iter 14970: Train loss 0.704, Learning Rate 4.000e-04, It/sec 2.225, Tokens/sec 357.586, Trained Tokens 2478222, Peak mem 19.940 GB\n",
      "Iter 14980: Train loss 0.824, Learning Rate 4.000e-04, It/sec 1.759, Tokens/sec 365.006, Trained Tokens 2480297, Peak mem 19.940 GB\n",
      "Iter 14990: Train loss 0.688, Learning Rate 4.000e-04, It/sec 2.423, Tokens/sec 369.697, Trained Tokens 2481823, Peak mem 19.940 GB\n",
      "Iter 15000: Val loss 2.362, Val took 2.903s\n",
      "Iter 15000: Train loss 0.663, Learning Rate 4.000e-04, It/sec 12.647, Tokens/sec 1967.889, Trained Tokens 2483379, Peak mem 19.940 GB\n",
      "Iter 15000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015000_adapters.safetensors.\n",
      "Iter 15010: Train loss 0.706, Learning Rate 4.000e-04, It/sec 2.000, Tokens/sec 372.917, Trained Tokens 2485244, Peak mem 19.940 GB\n",
      "Iter 15020: Train loss 0.827, Learning Rate 4.000e-04, It/sec 1.993, Tokens/sec 383.569, Trained Tokens 2487169, Peak mem 19.940 GB\n",
      "Iter 15030: Train loss 0.670, Learning Rate 4.000e-04, It/sec 2.560, Tokens/sec 387.318, Trained Tokens 2488682, Peak mem 19.940 GB\n",
      "Iter 15040: Train loss 0.862, Learning Rate 4.000e-04, It/sec 1.363, Tokens/sec 404.381, Trained Tokens 2491648, Peak mem 19.940 GB\n",
      "Iter 15050: Val loss 2.070, Val took 3.074s\n",
      "Iter 15050: Train loss 0.793, Learning Rate 4.000e-04, It/sec 15.901, Tokens/sec 2878.161, Trained Tokens 2493458, Peak mem 19.940 GB\n",
      "Iter 15060: Train loss 0.642, Learning Rate 4.000e-04, It/sec 2.490, Tokens/sec 338.080, Trained Tokens 2494816, Peak mem 19.940 GB\n",
      "Iter 15070: Train loss 0.786, Learning Rate 4.000e-04, It/sec 2.067, Tokens/sec 373.252, Trained Tokens 2496622, Peak mem 19.940 GB\n",
      "Iter 15080: Train loss 0.881, Learning Rate 4.000e-04, It/sec 1.547, Tokens/sec 359.736, Trained Tokens 2498948, Peak mem 19.940 GB\n",
      "Iter 15090: Train loss 0.762, Learning Rate 4.000e-04, It/sec 2.740, Tokens/sec 377.585, Trained Tokens 2500326, Peak mem 19.940 GB\n",
      "Iter 15100: Val loss 2.498, Val took 2.986s\n",
      "Iter 15100: Train loss 0.876, Learning Rate 4.000e-04, It/sec 5.230, Tokens/sec 933.075, Trained Tokens 2502110, Peak mem 19.940 GB\n",
      "Iter 15100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015100_adapters.safetensors.\n",
      "Iter 15110: Train loss 0.873, Learning Rate 4.000e-04, It/sec 2.888, Tokens/sec 371.680, Trained Tokens 2503397, Peak mem 19.940 GB\n",
      "Iter 15120: Train loss 0.798, Learning Rate 4.000e-04, It/sec 2.658, Tokens/sec 354.580, Trained Tokens 2504731, Peak mem 19.940 GB\n",
      "Iter 15130: Train loss 0.889, Learning Rate 4.000e-04, It/sec 2.152, Tokens/sec 392.091, Trained Tokens 2506553, Peak mem 19.940 GB\n",
      "Iter 15140: Train loss 0.834, Learning Rate 4.000e-04, It/sec 2.630, Tokens/sec 372.208, Trained Tokens 2507968, Peak mem 19.940 GB\n",
      "Iter 15150: Val loss 2.228, Val took 2.831s\n",
      "Iter 15150: Train loss 0.769, Learning Rate 4.000e-04, It/sec 20.566, Tokens/sec 3298.813, Trained Tokens 2509572, Peak mem 19.940 GB\n",
      "Iter 15160: Train loss 0.961, Learning Rate 4.000e-04, It/sec 1.620, Tokens/sec 375.919, Trained Tokens 2511893, Peak mem 19.940 GB\n",
      "Iter 15170: Train loss 0.762, Learning Rate 4.000e-04, It/sec 3.802, Tokens/sec 365.727, Trained Tokens 2512855, Peak mem 19.940 GB\n",
      "Iter 15180: Train loss 0.609, Learning Rate 4.000e-04, It/sec 3.273, Tokens/sec 375.760, Trained Tokens 2514003, Peak mem 19.940 GB\n",
      "Iter 15190: Train loss 0.642, Learning Rate 4.000e-04, It/sec 3.096, Tokens/sec 401.555, Trained Tokens 2515300, Peak mem 19.940 GB\n",
      "Iter 15200: Val loss 2.795, Val took 3.969s\n",
      "Iter 15200: Train loss 0.638, Learning Rate 4.000e-04, It/sec 28.666, Tokens/sec 4993.546, Trained Tokens 2517042, Peak mem 19.940 GB\n",
      "Iter 15200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015200_adapters.safetensors.\n",
      "Iter 15210: Train loss 0.697, Learning Rate 4.000e-04, It/sec 2.564, Tokens/sec 363.780, Trained Tokens 2518461, Peak mem 19.940 GB\n",
      "Iter 15220: Train loss 0.635, Learning Rate 4.000e-04, It/sec 2.853, Tokens/sec 369.709, Trained Tokens 2519757, Peak mem 19.940 GB\n",
      "Iter 15230: Train loss 0.730, Learning Rate 4.000e-04, It/sec 1.760, Tokens/sec 396.075, Trained Tokens 2522008, Peak mem 19.940 GB\n",
      "Iter 15240: Train loss 0.645, Learning Rate 4.000e-04, It/sec 2.185, Tokens/sec 346.315, Trained Tokens 2523593, Peak mem 19.940 GB\n",
      "Iter 15250: Val loss 2.489, Val took 3.963s\n",
      "Iter 15250: Train loss 0.681, Learning Rate 4.000e-04, It/sec 12.310, Tokens/sec 2542.093, Trained Tokens 2525658, Peak mem 19.940 GB\n",
      "Iter 15260: Train loss 0.586, Learning Rate 4.000e-04, It/sec 2.808, Tokens/sec 350.766, Trained Tokens 2526907, Peak mem 19.940 GB\n",
      "Iter 15270: Train loss 0.580, Learning Rate 4.000e-04, It/sec 2.212, Tokens/sec 386.604, Trained Tokens 2528655, Peak mem 19.940 GB\n",
      "Iter 15280: Train loss 0.657, Learning Rate 4.000e-04, It/sec 1.995, Tokens/sec 352.268, Trained Tokens 2530421, Peak mem 19.940 GB\n",
      "Iter 15290: Train loss 0.684, Learning Rate 4.000e-04, It/sec 2.241, Tokens/sec 331.610, Trained Tokens 2531901, Peak mem 19.940 GB\n",
      "Iter 15300: Val loss 2.454, Val took 4.136s\n",
      "Iter 15300: Train loss 0.833, Learning Rate 4.000e-04, It/sec 14.140, Tokens/sec 3430.331, Trained Tokens 2534327, Peak mem 19.940 GB\n",
      "Iter 15300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015300_adapters.safetensors.\n",
      "Iter 15310: Train loss 0.620, Learning Rate 4.000e-04, It/sec 2.138, Tokens/sec 316.570, Trained Tokens 2535808, Peak mem 19.940 GB\n",
      "Iter 15320: Train loss 0.683, Learning Rate 4.000e-04, It/sec 2.405, Tokens/sec 342.054, Trained Tokens 2537230, Peak mem 19.940 GB\n",
      "Iter 15330: Train loss 0.760, Learning Rate 4.000e-04, It/sec 2.212, Tokens/sec 360.800, Trained Tokens 2538861, Peak mem 19.940 GB\n",
      "Iter 15340: Train loss 0.714, Learning Rate 4.000e-04, It/sec 3.075, Tokens/sec 334.529, Trained Tokens 2539949, Peak mem 19.940 GB\n",
      "Iter 15350: Val loss 2.620, Val took 3.471s\n",
      "Iter 15350: Train loss 0.701, Learning Rate 4.000e-04, It/sec 22.464, Tokens/sec 3239.263, Trained Tokens 2541391, Peak mem 19.940 GB\n",
      "Iter 15360: Train loss 0.679, Learning Rate 4.000e-04, It/sec 2.175, Tokens/sec 353.200, Trained Tokens 2543015, Peak mem 19.940 GB\n",
      "Iter 15370: Train loss 0.708, Learning Rate 4.000e-04, It/sec 2.635, Tokens/sec 358.869, Trained Tokens 2544377, Peak mem 19.940 GB\n",
      "Iter 15380: Train loss 0.750, Learning Rate 4.000e-04, It/sec 2.060, Tokens/sec 363.392, Trained Tokens 2546141, Peak mem 19.940 GB\n",
      "Iter 15390: Train loss 0.686, Learning Rate 4.000e-04, It/sec 2.881, Tokens/sec 357.514, Trained Tokens 2547382, Peak mem 19.940 GB\n",
      "Iter 15400: Val loss 2.368, Val took 3.452s\n",
      "Iter 15400: Train loss 0.811, Learning Rate 4.000e-04, It/sec 33.058, Tokens/sec 5352.056, Trained Tokens 2549001, Peak mem 19.940 GB\n",
      "Iter 15400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015400_adapters.safetensors.\n",
      "Iter 15410: Train loss 0.773, Learning Rate 4.000e-04, It/sec 2.330, Tokens/sec 370.234, Trained Tokens 2550590, Peak mem 19.940 GB\n",
      "Iter 15420: Train loss 0.697, Learning Rate 4.000e-04, It/sec 2.362, Tokens/sec 369.820, Trained Tokens 2552156, Peak mem 19.940 GB\n",
      "Iter 15430: Train loss 0.846, Learning Rate 4.000e-04, It/sec 1.941, Tokens/sec 367.277, Trained Tokens 2554048, Peak mem 19.940 GB\n",
      "Iter 15440: Train loss 0.762, Learning Rate 4.000e-04, It/sec 2.740, Tokens/sec 347.191, Trained Tokens 2555315, Peak mem 19.940 GB\n",
      "Iter 15450: Val loss 2.486, Val took 3.611s\n",
      "Iter 15450: Train loss 0.829, Learning Rate 4.000e-04, It/sec 39.749, Tokens/sec 7564.262, Trained Tokens 2557218, Peak mem 19.940 GB\n",
      "Iter 15460: Train loss 0.742, Learning Rate 4.000e-04, It/sec 2.141, Tokens/sec 369.301, Trained Tokens 2558943, Peak mem 19.940 GB\n",
      "Iter 15470: Train loss 0.777, Learning Rate 4.000e-04, It/sec 2.483, Tokens/sec 357.502, Trained Tokens 2560383, Peak mem 19.940 GB\n",
      "Iter 15480: Train loss 0.835, Learning Rate 4.000e-04, It/sec 2.030, Tokens/sec 355.590, Trained Tokens 2562135, Peak mem 19.940 GB\n",
      "Iter 15490: Train loss 0.776, Learning Rate 4.000e-04, It/sec 2.705, Tokens/sec 357.085, Trained Tokens 2563455, Peak mem 19.940 GB\n",
      "Iter 15500: Val loss 2.375, Val took 3.126s\n",
      "Iter 15500: Train loss 0.706, Learning Rate 4.000e-04, It/sec 9.234, Tokens/sec 1575.345, Trained Tokens 2565161, Peak mem 19.940 GB\n",
      "Iter 15500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015500_adapters.safetensors.\n",
      "Iter 15510: Train loss 0.888, Learning Rate 4.000e-04, It/sec 1.969, Tokens/sec 374.725, Trained Tokens 2567064, Peak mem 19.940 GB\n",
      "Iter 15520: Train loss 0.885, Learning Rate 4.000e-04, It/sec 2.051, Tokens/sec 385.958, Trained Tokens 2568946, Peak mem 19.940 GB\n",
      "Iter 15530: Train loss 0.789, Learning Rate 4.000e-04, It/sec 2.824, Tokens/sec 375.369, Trained Tokens 2570275, Peak mem 19.940 GB\n",
      "Iter 15540: Train loss 0.756, Learning Rate 4.000e-04, It/sec 2.697, Tokens/sec 375.635, Trained Tokens 2571668, Peak mem 19.940 GB\n",
      "Iter 15550: Val loss 2.545, Val took 4.489s\n",
      "Iter 15550: Train loss 0.998, Learning Rate 4.000e-04, It/sec 30.559, Tokens/sec 6646.585, Trained Tokens 2573843, Peak mem 19.940 GB\n",
      "Iter 15560: Train loss 0.993, Learning Rate 4.000e-04, It/sec 1.368, Tokens/sec 342.689, Trained Tokens 2576348, Peak mem 19.940 GB\n",
      "Iter 15570: Train loss 0.888, Learning Rate 4.000e-04, It/sec 2.048, Tokens/sec 374.378, Trained Tokens 2578176, Peak mem 19.940 GB\n",
      "Iter 15580: Train loss 0.891, Learning Rate 4.000e-04, It/sec 1.296, Tokens/sec 336.096, Trained Tokens 2580770, Peak mem 19.940 GB\n",
      "Iter 15590: Train loss 0.686, Learning Rate 4.000e-04, It/sec 2.480, Tokens/sec 342.513, Trained Tokens 2582151, Peak mem 19.940 GB\n",
      "Iter 15600: Val loss 2.534, Val took 4.151s\n",
      "Iter 15600: Train loss 0.705, Learning Rate 4.000e-04, It/sec 13.150, Tokens/sec 2152.726, Trained Tokens 2583788, Peak mem 19.940 GB\n",
      "Iter 15600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015600_adapters.safetensors.\n",
      "Iter 15610: Train loss 0.688, Learning Rate 4.000e-04, It/sec 2.444, Tokens/sec 366.807, Trained Tokens 2585289, Peak mem 19.940 GB\n",
      "Iter 15620: Train loss 0.683, Learning Rate 4.000e-04, It/sec 2.046, Tokens/sec 388.798, Trained Tokens 2587189, Peak mem 19.940 GB\n",
      "Iter 15630: Train loss 0.736, Learning Rate 4.000e-04, It/sec 2.798, Tokens/sec 365.669, Trained Tokens 2588496, Peak mem 19.940 GB\n",
      "Iter 15640: Train loss 0.674, Learning Rate 4.000e-04, It/sec 2.136, Tokens/sec 367.334, Trained Tokens 2590216, Peak mem 19.940 GB\n",
      "Iter 15650: Val loss 2.579, Val took 4.130s\n",
      "Iter 15650: Train loss 0.644, Learning Rate 4.000e-04, It/sec 15.819, Tokens/sec 2341.199, Trained Tokens 2591696, Peak mem 19.940 GB\n",
      "Iter 15660: Train loss 0.737, Learning Rate 4.000e-04, It/sec 2.012, Tokens/sec 370.204, Trained Tokens 2593536, Peak mem 19.940 GB\n",
      "Iter 15670: Train loss 0.633, Learning Rate 4.000e-04, It/sec 2.030, Tokens/sec 364.153, Trained Tokens 2595330, Peak mem 19.940 GB\n",
      "Iter 15680: Train loss 0.622, Learning Rate 4.000e-04, It/sec 2.467, Tokens/sec 389.310, Trained Tokens 2596908, Peak mem 19.940 GB\n",
      "Iter 15690: Train loss 0.827, Learning Rate 4.000e-04, It/sec 1.735, Tokens/sec 378.478, Trained Tokens 2599089, Peak mem 19.940 GB\n",
      "Iter 15700: Val loss 2.501, Val took 3.318s\n",
      "Iter 15700: Train loss 0.612, Learning Rate 4.000e-04, It/sec 13.525, Tokens/sec 1856.977, Trained Tokens 2600462, Peak mem 19.940 GB\n",
      "Iter 15700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015700_adapters.safetensors.\n",
      "Iter 15710: Train loss 0.649, Learning Rate 4.000e-04, It/sec 2.295, Tokens/sec 361.882, Trained Tokens 2602039, Peak mem 19.940 GB\n",
      "Iter 15720: Train loss 0.684, Learning Rate 4.000e-04, It/sec 2.007, Tokens/sec 301.011, Trained Tokens 2603539, Peak mem 19.940 GB\n",
      "Iter 15730: Train loss 0.596, Learning Rate 4.000e-04, It/sec 2.710, Tokens/sec 378.794, Trained Tokens 2604937, Peak mem 19.940 GB\n",
      "Iter 15740: Train loss 0.706, Learning Rate 4.000e-04, It/sec 1.873, Tokens/sec 346.829, Trained Tokens 2606789, Peak mem 19.940 GB\n",
      "Iter 15750: Val loss 2.568, Val took 4.285s\n",
      "Iter 15750: Train loss 0.675, Learning Rate 4.000e-04, It/sec 19.352, Tokens/sec 3121.421, Trained Tokens 2608402, Peak mem 19.940 GB\n",
      "Iter 15760: Train loss 0.626, Learning Rate 4.000e-04, It/sec 2.473, Tokens/sec 361.999, Trained Tokens 2609866, Peak mem 19.940 GB\n",
      "Iter 15770: Train loss 0.673, Learning Rate 4.000e-04, It/sec 2.008, Tokens/sec 352.443, Trained Tokens 2611621, Peak mem 19.940 GB\n",
      "Iter 15780: Train loss 0.596, Learning Rate 4.000e-04, It/sec 2.180, Tokens/sec 358.418, Trained Tokens 2613265, Peak mem 19.940 GB\n",
      "Iter 15790: Train loss 0.628, Learning Rate 4.000e-04, It/sec 2.903, Tokens/sec 366.913, Trained Tokens 2614529, Peak mem 19.940 GB\n",
      "Iter 15800: Val loss 2.454, Val took 3.875s\n",
      "Iter 15800: Train loss 0.790, Learning Rate 4.000e-04, It/sec 17.333, Tokens/sec 2242.884, Trained Tokens 2615823, Peak mem 19.940 GB\n",
      "Iter 15800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015800_adapters.safetensors.\n",
      "Iter 15810: Train loss 0.790, Learning Rate 4.000e-04, It/sec 1.445, Tokens/sec 384.873, Trained Tokens 2618487, Peak mem 19.940 GB\n",
      "Iter 15820: Train loss 0.878, Learning Rate 4.000e-04, It/sec 1.750, Tokens/sec 391.197, Trained Tokens 2620723, Peak mem 19.940 GB\n",
      "Iter 15830: Train loss 0.803, Learning Rate 4.000e-04, It/sec 2.077, Tokens/sec 376.506, Trained Tokens 2622536, Peak mem 19.940 GB\n",
      "Iter 15840: Train loss 0.718, Learning Rate 4.000e-04, It/sec 2.392, Tokens/sec 373.414, Trained Tokens 2624097, Peak mem 19.940 GB\n",
      "Iter 15850: Val loss 2.422, Val took 3.578s\n",
      "Iter 15850: Train loss 0.754, Learning Rate 4.000e-04, It/sec 41.916, Tokens/sec 5608.395, Trained Tokens 2625435, Peak mem 19.940 GB\n",
      "Iter 15860: Train loss 0.697, Learning Rate 4.000e-04, It/sec 2.733, Tokens/sec 369.991, Trained Tokens 2626789, Peak mem 19.940 GB\n",
      "Iter 15870: Train loss 0.826, Learning Rate 4.000e-04, It/sec 2.501, Tokens/sec 376.433, Trained Tokens 2628294, Peak mem 19.940 GB\n",
      "Iter 15880: Train loss 0.762, Learning Rate 4.000e-04, It/sec 2.630, Tokens/sec 342.702, Trained Tokens 2629597, Peak mem 19.940 GB\n",
      "Iter 15890: Train loss 0.770, Learning Rate 4.000e-04, It/sec 2.284, Tokens/sec 369.547, Trained Tokens 2631215, Peak mem 19.940 GB\n",
      "Iter 15900: Val loss 2.648, Val took 3.600s\n",
      "Iter 15900: Train loss 0.738, Learning Rate 4.000e-04, It/sec 28.500, Tokens/sec 3080.830, Trained Tokens 2632296, Peak mem 19.940 GB\n",
      "Iter 15900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0015900_adapters.safetensors.\n",
      "Iter 15910: Train loss 1.017, Learning Rate 4.000e-04, It/sec 1.795, Tokens/sec 351.956, Trained Tokens 2634257, Peak mem 19.940 GB\n",
      "Iter 15920: Train loss 0.768, Learning Rate 4.000e-04, It/sec 1.930, Tokens/sec 318.226, Trained Tokens 2635906, Peak mem 19.940 GB\n",
      "Iter 15930: Train loss 0.768, Learning Rate 4.000e-04, It/sec 2.155, Tokens/sec 359.266, Trained Tokens 2637573, Peak mem 19.940 GB\n",
      "Iter 15940: Train loss 0.791, Learning Rate 4.000e-04, It/sec 2.527, Tokens/sec 365.119, Trained Tokens 2639018, Peak mem 19.940 GB\n",
      "Iter 15950: Val loss 2.396, Val took 3.184s\n",
      "Iter 15950: Train loss 0.758, Learning Rate 4.000e-04, It/sec 23.382, Tokens/sec 3675.691, Trained Tokens 2640590, Peak mem 19.940 GB\n",
      "Iter 15960: Train loss 0.819, Learning Rate 4.000e-04, It/sec 2.003, Tokens/sec 346.109, Trained Tokens 2642318, Peak mem 19.940 GB\n",
      "Iter 15970: Train loss 0.874, Learning Rate 4.000e-04, It/sec 2.082, Tokens/sec 383.296, Trained Tokens 2644159, Peak mem 19.940 GB\n",
      "Iter 15980: Train loss 1.035, Learning Rate 4.000e-04, It/sec 1.474, Tokens/sec 389.086, Trained Tokens 2646799, Peak mem 19.940 GB\n",
      "Iter 15990: Train loss 0.836, Learning Rate 4.000e-04, It/sec 1.873, Tokens/sec 353.246, Trained Tokens 2648685, Peak mem 19.940 GB\n",
      "Iter 16000: Val loss 2.268, Val took 3.172s\n",
      "Iter 16000: Train loss 0.586, Learning Rate 4.000e-04, It/sec 23.892, Tokens/sec 3323.435, Trained Tokens 2650076, Peak mem 19.940 GB\n",
      "Iter 16000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016000_adapters.safetensors.\n",
      "Iter 16010: Train loss 0.640, Learning Rate 4.000e-04, It/sec 2.273, Tokens/sec 362.011, Trained Tokens 2651669, Peak mem 19.940 GB\n",
      "Iter 16020: Train loss 0.655, Learning Rate 4.000e-04, It/sec 2.493, Tokens/sec 358.986, Trained Tokens 2653109, Peak mem 19.940 GB\n",
      "Iter 16030: Train loss 0.849, Learning Rate 4.000e-04, It/sec 1.479, Tokens/sec 370.479, Trained Tokens 2655614, Peak mem 19.940 GB\n",
      "Iter 16040: Train loss 0.658, Learning Rate 4.000e-04, It/sec 2.015, Tokens/sec 342.924, Trained Tokens 2657316, Peak mem 19.940 GB\n",
      "Iter 16050: Val loss 2.566, Val took 3.980s\n",
      "Iter 16050: Train loss 0.594, Learning Rate 4.000e-04, It/sec 32.802, Tokens/sec 6078.169, Trained Tokens 2659169, Peak mem 19.940 GB\n",
      "Iter 16060: Train loss 0.759, Learning Rate 4.000e-04, It/sec 2.092, Tokens/sec 381.574, Trained Tokens 2660993, Peak mem 19.940 GB\n",
      "Iter 16070: Train loss 0.564, Learning Rate 4.000e-04, It/sec 2.793, Tokens/sec 369.200, Trained Tokens 2662315, Peak mem 19.940 GB\n",
      "Iter 16080: Train loss 0.578, Learning Rate 4.000e-04, It/sec 2.459, Tokens/sec 361.242, Trained Tokens 2663784, Peak mem 19.940 GB\n",
      "Iter 16090: Train loss 0.671, Learning Rate 4.000e-04, It/sec 2.107, Tokens/sec 394.837, Trained Tokens 2665658, Peak mem 19.940 GB\n",
      "Iter 16100: Val loss 2.367, Val took 3.077s\n",
      "Iter 16100: Train loss 0.731, Learning Rate 4.000e-04, It/sec 12.138, Tokens/sec 2756.518, Trained Tokens 2667929, Peak mem 19.940 GB\n",
      "Iter 16100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016100_adapters.safetensors.\n",
      "Iter 16110: Train loss 0.666, Learning Rate 4.000e-04, It/sec 2.451, Tokens/sec 352.389, Trained Tokens 2669367, Peak mem 19.940 GB\n",
      "Iter 16120: Train loss 0.690, Learning Rate 4.000e-04, It/sec 2.596, Tokens/sec 366.744, Trained Tokens 2670780, Peak mem 19.940 GB\n",
      "Iter 16130: Train loss 0.608, Learning Rate 4.000e-04, It/sec 2.484, Tokens/sec 355.467, Trained Tokens 2672211, Peak mem 19.940 GB\n",
      "Iter 16140: Train loss 0.675, Learning Rate 4.000e-04, It/sec 2.044, Tokens/sec 354.186, Trained Tokens 2673944, Peak mem 19.940 GB\n",
      "Iter 16150: Val loss 2.164, Val took 3.909s\n",
      "Iter 16150: Train loss 0.629, Learning Rate 4.000e-04, It/sec 34.028, Tokens/sec 5842.533, Trained Tokens 2675661, Peak mem 19.940 GB\n",
      "Iter 16160: Train loss 0.733, Learning Rate 4.000e-04, It/sec 2.329, Tokens/sec 340.217, Trained Tokens 2677122, Peak mem 19.940 GB\n",
      "Iter 16170: Train loss 0.690, Learning Rate 4.000e-04, It/sec 2.515, Tokens/sec 379.203, Trained Tokens 2678630, Peak mem 19.940 GB\n",
      "Iter 16180: Train loss 0.698, Learning Rate 4.000e-04, It/sec 2.563, Tokens/sec 379.893, Trained Tokens 2680112, Peak mem 19.940 GB\n",
      "Iter 16190: Train loss 0.719, Learning Rate 4.000e-04, It/sec 2.513, Tokens/sec 375.378, Trained Tokens 2681606, Peak mem 19.940 GB\n",
      "Iter 16200: Val loss 2.337, Val took 3.170s\n",
      "Iter 16200: Train loss 0.695, Learning Rate 4.000e-04, It/sec 12.398, Tokens/sec 1879.513, Trained Tokens 2683122, Peak mem 19.940 GB\n",
      "Iter 16200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016200_adapters.safetensors.\n",
      "Iter 16210: Train loss 0.655, Learning Rate 4.000e-04, It/sec 2.451, Tokens/sec 330.395, Trained Tokens 2684470, Peak mem 19.940 GB\n",
      "Iter 16220: Train loss 0.771, Learning Rate 4.000e-04, It/sec 2.478, Tokens/sec 369.964, Trained Tokens 2685963, Peak mem 19.940 GB\n",
      "Iter 16230: Train loss 0.760, Learning Rate 4.000e-04, It/sec 2.644, Tokens/sec 356.710, Trained Tokens 2687312, Peak mem 19.940 GB\n",
      "Iter 16240: Train loss 0.675, Learning Rate 4.000e-04, It/sec 2.882, Tokens/sec 336.383, Trained Tokens 2688479, Peak mem 19.940 GB\n",
      "Iter 16250: Val loss 2.467, Val took 2.828s\n",
      "Iter 16250: Train loss 0.865, Learning Rate 4.000e-04, It/sec 22.613, Tokens/sec 4649.333, Trained Tokens 2690535, Peak mem 19.940 GB\n",
      "Iter 16260: Train loss 0.894, Learning Rate 4.000e-04, It/sec 1.440, Tokens/sec 407.445, Trained Tokens 2693364, Peak mem 19.940 GB\n",
      "Iter 16270: Train loss 0.838, Learning Rate 4.000e-04, It/sec 1.940, Tokens/sec 402.650, Trained Tokens 2695440, Peak mem 19.940 GB\n",
      "Iter 16280: Train loss 0.768, Learning Rate 4.000e-04, It/sec 2.357, Tokens/sec 378.954, Trained Tokens 2697048, Peak mem 19.940 GB\n",
      "Iter 16290: Train loss 0.712, Learning Rate 4.000e-04, It/sec 1.933, Tokens/sec 303.164, Trained Tokens 2698616, Peak mem 19.940 GB\n",
      "Iter 16300: Val loss 2.546, Val took 3.467s\n",
      "Iter 16300: Train loss 0.734, Learning Rate 4.000e-04, It/sec 24.311, Tokens/sec 2885.659, Trained Tokens 2699803, Peak mem 19.940 GB\n",
      "Iter 16300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016300_adapters.safetensors.\n",
      "Iter 16310: Train loss 0.749, Learning Rate 4.000e-04, It/sec 2.112, Tokens/sec 386.759, Trained Tokens 2701634, Peak mem 19.940 GB\n",
      "Iter 16320: Train loss 0.695, Learning Rate 4.000e-04, It/sec 3.295, Tokens/sec 357.785, Trained Tokens 2702720, Peak mem 19.940 GB\n",
      "Iter 16330: Train loss 0.745, Learning Rate 4.000e-04, It/sec 2.783, Tokens/sec 374.530, Trained Tokens 2704066, Peak mem 19.940 GB\n",
      "Iter 16340: Train loss 0.799, Learning Rate 4.000e-04, It/sec 2.260, Tokens/sec 375.451, Trained Tokens 2705727, Peak mem 19.940 GB\n",
      "Iter 16350: Val loss 2.595, Val took 3.255s\n",
      "Iter 16350: Train loss 0.751, Learning Rate 4.000e-04, It/sec 17.919, Tokens/sec 3370.507, Trained Tokens 2707608, Peak mem 19.940 GB\n",
      "Iter 16360: Train loss 0.876, Learning Rate 4.000e-04, It/sec 1.565, Tokens/sec 385.073, Trained Tokens 2710068, Peak mem 19.940 GB\n",
      "Iter 16370: Train loss 0.820, Learning Rate 4.000e-04, It/sec 1.898, Tokens/sec 334.666, Trained Tokens 2711831, Peak mem 19.940 GB\n",
      "Iter 16380: Train loss 0.812, Learning Rate 4.000e-04, It/sec 2.330, Tokens/sec 365.766, Trained Tokens 2713401, Peak mem 19.940 GB\n",
      "Iter 16390: Train loss 0.695, Learning Rate 4.000e-04, It/sec 2.136, Tokens/sec 353.454, Trained Tokens 2715056, Peak mem 19.940 GB\n",
      "Iter 16400: Val loss 2.246, Val took 3.319s\n",
      "Iter 16400: Train loss 0.780, Learning Rate 4.000e-04, It/sec 15.019, Tokens/sec 2318.936, Trained Tokens 2716600, Peak mem 19.940 GB\n",
      "Iter 16400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016400_adapters.safetensors.\n",
      "Iter 16410: Train loss 0.668, Learning Rate 4.000e-04, It/sec 2.567, Tokens/sec 346.075, Trained Tokens 2717948, Peak mem 19.940 GB\n",
      "Iter 16420: Train loss 0.678, Learning Rate 4.000e-04, It/sec 2.692, Tokens/sec 364.248, Trained Tokens 2719301, Peak mem 19.940 GB\n",
      "Iter 16430: Train loss 0.635, Learning Rate 4.000e-04, It/sec 3.137, Tokens/sec 380.569, Trained Tokens 2720514, Peak mem 19.940 GB\n",
      "Iter 16440: Train loss 0.700, Learning Rate 4.000e-04, It/sec 3.084, Tokens/sec 375.303, Trained Tokens 2721731, Peak mem 19.940 GB\n",
      "Iter 16450: Val loss 2.701, Val took 3.868s\n",
      "Iter 16450: Train loss 0.745, Learning Rate 4.000e-04, It/sec 8.596, Tokens/sec 1646.228, Trained Tokens 2723646, Peak mem 19.940 GB\n",
      "Iter 16460: Train loss 0.668, Learning Rate 4.000e-04, It/sec 2.835, Tokens/sec 337.696, Trained Tokens 2724837, Peak mem 19.940 GB\n",
      "Iter 16470: Train loss 0.660, Learning Rate 4.000e-04, It/sec 3.688, Tokens/sec 365.487, Trained Tokens 2725828, Peak mem 19.940 GB\n",
      "Iter 16480: Train loss 0.724, Learning Rate 4.000e-04, It/sec 2.321, Tokens/sec 388.122, Trained Tokens 2727500, Peak mem 19.940 GB\n",
      "Iter 16490: Train loss 0.784, Learning Rate 4.000e-04, It/sec 1.864, Tokens/sec 389.412, Trained Tokens 2729589, Peak mem 19.940 GB\n",
      "Iter 16500: Val loss 2.086, Val took 2.924s\n",
      "Iter 16500: Train loss 0.660, Learning Rate 4.000e-04, It/sec 23.872, Tokens/sec 3650.057, Trained Tokens 2731118, Peak mem 19.940 GB\n",
      "Iter 16500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016500_adapters.safetensors.\n",
      "Iter 16510: Train loss 0.679, Learning Rate 4.000e-04, It/sec 2.110, Tokens/sec 338.026, Trained Tokens 2732720, Peak mem 19.940 GB\n",
      "Iter 16520: Train loss 0.658, Learning Rate 4.000e-04, It/sec 2.118, Tokens/sec 388.041, Trained Tokens 2734552, Peak mem 19.940 GB\n",
      "Iter 16530: Train loss 0.681, Learning Rate 4.000e-04, It/sec 3.293, Tokens/sec 348.116, Trained Tokens 2735609, Peak mem 19.940 GB\n",
      "Iter 16540: Train loss 0.658, Learning Rate 4.000e-04, It/sec 3.057, Tokens/sec 352.178, Trained Tokens 2736761, Peak mem 19.940 GB\n",
      "Iter 16550: Val loss 2.178, Val took 3.402s\n",
      "Iter 16550: Train loss 0.721, Learning Rate 4.000e-04, It/sec 17.391, Tokens/sec 2786.114, Trained Tokens 2738363, Peak mem 19.940 GB\n",
      "Iter 16560: Train loss 0.756, Learning Rate 4.000e-04, It/sec 2.015, Tokens/sec 368.600, Trained Tokens 2740192, Peak mem 19.940 GB\n",
      "Iter 16570: Train loss 0.768, Learning Rate 4.000e-04, It/sec 1.367, Tokens/sec 375.466, Trained Tokens 2742938, Peak mem 19.940 GB\n",
      "Iter 16580: Train loss 0.733, Learning Rate 4.000e-04, It/sec 1.862, Tokens/sec 335.956, Trained Tokens 2744742, Peak mem 19.940 GB\n",
      "Iter 16590: Train loss 0.794, Learning Rate 4.000e-04, It/sec 2.148, Tokens/sec 371.032, Trained Tokens 2746469, Peak mem 19.940 GB\n",
      "Iter 16600: Val loss 2.535, Val took 3.427s\n",
      "Iter 16600: Train loss 0.782, Learning Rate 4.000e-04, It/sec 5.065, Tokens/sec 938.492, Trained Tokens 2748322, Peak mem 19.940 GB\n",
      "Iter 16600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016600_adapters.safetensors.\n",
      "Iter 16610: Train loss 0.833, Learning Rate 4.000e-04, It/sec 1.488, Tokens/sec 365.371, Trained Tokens 2750778, Peak mem 19.940 GB\n",
      "Iter 16620: Train loss 0.758, Learning Rate 4.000e-04, It/sec 1.594, Tokens/sec 394.780, Trained Tokens 2753255, Peak mem 19.940 GB\n",
      "Iter 16630: Train loss 0.637, Learning Rate 4.000e-04, It/sec 2.667, Tokens/sec 371.479, Trained Tokens 2754648, Peak mem 19.940 GB\n",
      "Iter 16640: Train loss 0.604, Learning Rate 4.000e-04, It/sec 2.749, Tokens/sec 351.607, Trained Tokens 2755927, Peak mem 19.940 GB\n",
      "Iter 16650: Val loss 2.081, Val took 2.857s\n",
      "Iter 16650: Train loss 0.693, Learning Rate 4.000e-04, It/sec 19.443, Tokens/sec 2578.089, Trained Tokens 2757253, Peak mem 19.940 GB\n",
      "Iter 16660: Train loss 0.754, Learning Rate 4.000e-04, It/sec 2.288, Tokens/sec 365.859, Trained Tokens 2758852, Peak mem 19.940 GB\n",
      "Iter 16670: Train loss 0.717, Learning Rate 4.000e-04, It/sec 2.338, Tokens/sec 385.457, Trained Tokens 2760501, Peak mem 19.940 GB\n",
      "Iter 16680: Train loss 0.792, Learning Rate 4.000e-04, It/sec 1.858, Tokens/sec 387.143, Trained Tokens 2762585, Peak mem 19.940 GB\n",
      "Iter 16690: Train loss 0.697, Learning Rate 4.000e-04, It/sec 1.774, Tokens/sec 397.256, Trained Tokens 2764824, Peak mem 19.940 GB\n",
      "Iter 16700: Val loss 2.432, Val took 3.612s\n",
      "Iter 16700: Train loss 0.692, Learning Rate 4.000e-04, It/sec 25.379, Tokens/sec 3337.356, Trained Tokens 2766139, Peak mem 19.940 GB\n",
      "Iter 16700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016700_adapters.safetensors.\n",
      "Iter 16710: Train loss 0.733, Learning Rate 4.000e-04, It/sec 2.053, Tokens/sec 335.933, Trained Tokens 2767775, Peak mem 19.940 GB\n",
      "Iter 16720: Train loss 0.720, Learning Rate 4.000e-04, It/sec 2.548, Tokens/sec 368.409, Trained Tokens 2769221, Peak mem 19.940 GB\n",
      "Iter 16730: Train loss 0.698, Learning Rate 4.000e-04, It/sec 2.191, Tokens/sec 336.054, Trained Tokens 2770755, Peak mem 19.940 GB\n",
      "Iter 16740: Train loss 0.904, Learning Rate 4.000e-04, It/sec 1.591, Tokens/sec 358.213, Trained Tokens 2773006, Peak mem 19.940 GB\n",
      "Iter 16750: Val loss 2.216, Val took 3.311s\n",
      "Iter 16750: Train loss 0.752, Learning Rate 4.000e-04, It/sec 44.643, Tokens/sec 5660.774, Trained Tokens 2774274, Peak mem 19.940 GB\n",
      "Iter 16760: Train loss 0.751, Learning Rate 4.000e-04, It/sec 2.102, Tokens/sec 347.669, Trained Tokens 2775928, Peak mem 19.940 GB\n",
      "Iter 16770: Train loss 0.927, Learning Rate 4.000e-04, It/sec 1.612, Tokens/sec 367.227, Trained Tokens 2778206, Peak mem 19.940 GB\n",
      "Iter 16780: Train loss 0.745, Learning Rate 4.000e-04, It/sec 1.992, Tokens/sec 346.849, Trained Tokens 2779947, Peak mem 19.940 GB\n",
      "Iter 16790: Train loss 0.791, Learning Rate 4.000e-04, It/sec 2.042, Tokens/sec 334.888, Trained Tokens 2781587, Peak mem 19.940 GB\n",
      "Iter 16800: Val loss 2.506, Val took 3.616s\n",
      "Iter 16800: Train loss 0.725, Learning Rate 4.000e-04, It/sec 21.227, Tokens/sec 3721.070, Trained Tokens 2783340, Peak mem 19.940 GB\n",
      "Iter 16800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016800_adapters.safetensors.\n",
      "Iter 16810: Train loss 0.747, Learning Rate 4.000e-04, It/sec 2.913, Tokens/sec 342.252, Trained Tokens 2784515, Peak mem 19.940 GB\n",
      "Iter 16820: Train loss 0.689, Learning Rate 4.000e-04, It/sec 2.597, Tokens/sec 374.711, Trained Tokens 2785958, Peak mem 19.940 GB\n",
      "Iter 16830: Train loss 0.657, Learning Rate 4.000e-04, It/sec 2.546, Tokens/sec 376.079, Trained Tokens 2787435, Peak mem 19.940 GB\n",
      "Iter 16840: Train loss 0.655, Learning Rate 4.000e-04, It/sec 2.625, Tokens/sec 380.637, Trained Tokens 2788885, Peak mem 19.940 GB\n",
      "Iter 16850: Val loss 2.585, Val took 3.353s\n",
      "Iter 16850: Train loss 0.690, Learning Rate 4.000e-04, It/sec 26.138, Tokens/sec 4493.159, Trained Tokens 2790604, Peak mem 19.940 GB\n",
      "Iter 16860: Train loss 0.845, Learning Rate 4.000e-04, It/sec 1.718, Tokens/sec 389.751, Trained Tokens 2792872, Peak mem 19.940 GB\n",
      "Iter 16870: Train loss 0.684, Learning Rate 4.000e-04, It/sec 2.152, Tokens/sec 368.815, Trained Tokens 2794586, Peak mem 19.940 GB\n",
      "Iter 16880: Train loss 0.741, Learning Rate 4.000e-04, It/sec 2.099, Tokens/sec 355.917, Trained Tokens 2796282, Peak mem 19.940 GB\n",
      "Iter 16890: Train loss 0.665, Learning Rate 4.000e-04, It/sec 1.906, Tokens/sec 351.878, Trained Tokens 2798128, Peak mem 19.940 GB\n",
      "Iter 16900: Val loss 2.559, Val took 3.193s\n",
      "Iter 16900: Train loss 0.639, Learning Rate 4.000e-04, It/sec 16.040, Tokens/sec 1875.041, Trained Tokens 2799297, Peak mem 19.940 GB\n",
      "Iter 16900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0016900_adapters.safetensors.\n",
      "Iter 16910: Train loss 0.703, Learning Rate 4.000e-04, It/sec 2.543, Tokens/sec 378.582, Trained Tokens 2800786, Peak mem 19.940 GB\n",
      "Iter 16920: Train loss 0.585, Learning Rate 4.000e-04, It/sec 2.779, Tokens/sec 349.649, Trained Tokens 2802044, Peak mem 19.940 GB\n",
      "Iter 16930: Train loss 0.712, Learning Rate 4.000e-04, It/sec 3.419, Tokens/sec 361.078, Trained Tokens 2803100, Peak mem 19.940 GB\n",
      "Iter 16940: Train loss 0.877, Learning Rate 4.000e-04, It/sec 1.556, Tokens/sec 365.591, Trained Tokens 2805450, Peak mem 19.940 GB\n",
      "Iter 16950: Val loss 2.901, Val took 4.193s\n",
      "Iter 16950: Train loss 0.657, Learning Rate 4.000e-04, It/sec 41.245, Tokens/sec 5679.410, Trained Tokens 2806827, Peak mem 19.940 GB\n",
      "Iter 16960: Train loss 0.745, Learning Rate 4.000e-04, It/sec 2.197, Tokens/sec 311.072, Trained Tokens 2808243, Peak mem 19.940 GB\n",
      "Iter 16970: Train loss 0.943, Learning Rate 4.000e-04, It/sec 1.504, Tokens/sec 383.887, Trained Tokens 2810796, Peak mem 19.940 GB\n",
      "Iter 16980: Train loss 0.675, Learning Rate 4.000e-04, It/sec 2.164, Tokens/sec 345.598, Trained Tokens 2812393, Peak mem 19.940 GB\n",
      "Iter 16990: Train loss 0.757, Learning Rate 4.000e-04, It/sec 2.169, Tokens/sec 371.739, Trained Tokens 2814107, Peak mem 19.940 GB\n",
      "Iter 17000: Val loss 2.237, Val took 3.969s\n",
      "Iter 17000: Train loss 0.748, Learning Rate 4.000e-04, It/sec 38.819, Tokens/sec 6704.037, Trained Tokens 2815834, Peak mem 19.940 GB\n",
      "Iter 17000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017000_adapters.safetensors.\n",
      "Iter 17010: Train loss 0.651, Learning Rate 4.000e-04, It/sec 1.961, Tokens/sec 357.695, Trained Tokens 2817658, Peak mem 19.940 GB\n",
      "Iter 17020: Train loss 0.629, Learning Rate 4.000e-04, It/sec 2.383, Tokens/sec 347.220, Trained Tokens 2819115, Peak mem 19.940 GB\n",
      "Iter 17030: Train loss 0.751, Learning Rate 4.000e-04, It/sec 1.940, Tokens/sec 372.652, Trained Tokens 2821036, Peak mem 19.940 GB\n",
      "Iter 17040: Train loss 0.712, Learning Rate 4.000e-04, It/sec 2.167, Tokens/sec 368.443, Trained Tokens 2822736, Peak mem 19.940 GB\n",
      "Iter 17050: Val loss 2.597, Val took 3.803s\n",
      "Iter 17050: Train loss 0.671, Learning Rate 4.000e-04, It/sec 21.466, Tokens/sec 2586.655, Trained Tokens 2823941, Peak mem 19.940 GB\n",
      "Iter 17060: Train loss 0.605, Learning Rate 4.000e-04, It/sec 2.393, Tokens/sec 324.915, Trained Tokens 2825299, Peak mem 19.940 GB\n",
      "Iter 17070: Train loss 0.840, Learning Rate 4.000e-04, It/sec 1.364, Tokens/sec 371.840, Trained Tokens 2828025, Peak mem 19.940 GB\n",
      "Iter 17080: Train loss 0.743, Learning Rate 4.000e-04, It/sec 2.256, Tokens/sec 382.689, Trained Tokens 2829721, Peak mem 19.940 GB\n",
      "Iter 17090: Train loss 0.627, Learning Rate 4.000e-04, It/sec 2.650, Tokens/sec 361.177, Trained Tokens 2831084, Peak mem 19.940 GB\n",
      "Iter 17100: Val loss 2.214, Val took 3.658s\n",
      "Iter 17100: Train loss 0.618, Learning Rate 4.000e-04, It/sec 18.662, Tokens/sec 2942.980, Trained Tokens 2832661, Peak mem 19.940 GB\n",
      "Iter 17100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017100_adapters.safetensors.\n",
      "Iter 17110: Train loss 0.716, Learning Rate 4.000e-04, It/sec 2.079, Tokens/sec 368.136, Trained Tokens 2834432, Peak mem 19.940 GB\n",
      "Iter 17120: Train loss 0.860, Learning Rate 4.000e-04, It/sec 1.150, Tokens/sec 337.241, Trained Tokens 2837364, Peak mem 19.940 GB\n",
      "Iter 17130: Train loss 0.739, Learning Rate 4.000e-04, It/sec 1.977, Tokens/sec 323.987, Trained Tokens 2839003, Peak mem 19.940 GB\n",
      "Iter 17140: Train loss 0.625, Learning Rate 4.000e-04, It/sec 3.089, Tokens/sec 378.054, Trained Tokens 2840227, Peak mem 19.940 GB\n",
      "Iter 17150: Val loss 2.437, Val took 3.224s\n",
      "Iter 17150: Train loss 0.656, Learning Rate 4.000e-04, It/sec 15.708, Tokens/sec 2337.356, Trained Tokens 2841715, Peak mem 19.940 GB\n",
      "Iter 17160: Train loss 0.739, Learning Rate 4.000e-04, It/sec 2.280, Tokens/sec 355.423, Trained Tokens 2843274, Peak mem 19.940 GB\n",
      "Iter 17170: Train loss 0.733, Learning Rate 4.000e-04, It/sec 2.051, Tokens/sec 368.550, Trained Tokens 2845071, Peak mem 19.940 GB\n",
      "Iter 17180: Train loss 0.673, Learning Rate 4.000e-04, It/sec 2.418, Tokens/sec 321.095, Trained Tokens 2846399, Peak mem 19.940 GB\n",
      "Iter 17190: Train loss 0.757, Learning Rate 4.000e-04, It/sec 2.096, Tokens/sec 352.494, Trained Tokens 2848081, Peak mem 19.940 GB\n",
      "Iter 17200: Val loss 2.541, Val took 3.533s\n",
      "Iter 17200: Train loss 0.700, Learning Rate 4.000e-04, It/sec 33.727, Tokens/sec 4522.833, Trained Tokens 2849422, Peak mem 19.940 GB\n",
      "Iter 17200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017200_adapters.safetensors.\n",
      "Iter 17210: Train loss 0.796, Learning Rate 4.000e-04, It/sec 2.427, Tokens/sec 351.947, Trained Tokens 2850872, Peak mem 19.940 GB\n",
      "Iter 17220: Train loss 0.748, Learning Rate 4.000e-04, It/sec 2.464, Tokens/sec 383.847, Trained Tokens 2852430, Peak mem 19.940 GB\n",
      "Iter 17230: Train loss 0.688, Learning Rate 4.000e-04, It/sec 2.450, Tokens/sec 373.063, Trained Tokens 2853953, Peak mem 19.940 GB\n",
      "Iter 17240: Train loss 0.744, Learning Rate 4.000e-04, It/sec 1.158, Tokens/sec 368.577, Trained Tokens 2857135, Peak mem 19.940 GB\n",
      "Iter 17250: Val loss 2.725, Val took 4.814s\n",
      "Iter 17250: Train loss 0.679, Learning Rate 4.000e-04, It/sec 34.030, Tokens/sec 5883.766, Trained Tokens 2858864, Peak mem 19.940 GB\n",
      "Iter 17260: Train loss 0.583, Learning Rate 4.000e-04, It/sec 2.098, Tokens/sec 326.383, Trained Tokens 2860420, Peak mem 19.940 GB\n",
      "Iter 17270: Train loss 0.535, Learning Rate 4.000e-04, It/sec 2.129, Tokens/sec 360.627, Trained Tokens 2862114, Peak mem 19.940 GB\n",
      "Iter 17280: Train loss 0.741, Learning Rate 4.000e-04, It/sec 2.104, Tokens/sec 389.021, Trained Tokens 2863963, Peak mem 19.940 GB\n",
      "Iter 17290: Train loss 0.605, Learning Rate 4.000e-04, It/sec 2.367, Tokens/sec 349.299, Trained Tokens 2865439, Peak mem 19.940 GB\n",
      "Iter 17300: Val loss 2.288, Val took 3.183s\n",
      "Iter 17300: Train loss 0.688, Learning Rate 4.000e-04, It/sec 12.562, Tokens/sec 2020.045, Trained Tokens 2867047, Peak mem 19.940 GB\n",
      "Iter 17300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017300_adapters.safetensors.\n",
      "Iter 17310: Train loss 0.569, Learning Rate 4.000e-04, It/sec 3.240, Tokens/sec 335.941, Trained Tokens 2868084, Peak mem 19.940 GB\n",
      "Iter 17320: Train loss 0.652, Learning Rate 4.000e-04, It/sec 3.113, Tokens/sec 357.037, Trained Tokens 2869231, Peak mem 19.940 GB\n",
      "Iter 17330: Train loss 0.631, Learning Rate 4.000e-04, It/sec 1.866, Tokens/sec 335.808, Trained Tokens 2871031, Peak mem 19.940 GB\n",
      "Iter 17340: Train loss 0.670, Learning Rate 4.000e-04, It/sec 1.921, Tokens/sec 361.920, Trained Tokens 2872915, Peak mem 19.940 GB\n",
      "Iter 17350: Val loss 2.619, Val took 4.741s\n",
      "Iter 17350: Train loss 0.622, Learning Rate 4.000e-04, It/sec 14.449, Tokens/sec 2428.898, Trained Tokens 2874596, Peak mem 19.940 GB\n",
      "Iter 17360: Train loss 0.688, Learning Rate 4.000e-04, It/sec 1.808, Tokens/sec 319.853, Trained Tokens 2876365, Peak mem 19.940 GB\n",
      "Iter 17370: Train loss 0.618, Learning Rate 4.000e-04, It/sec 2.071, Tokens/sec 345.505, Trained Tokens 2878033, Peak mem 19.940 GB\n",
      "Iter 17380: Train loss 0.679, Learning Rate 4.000e-04, It/sec 1.807, Tokens/sec 372.985, Trained Tokens 2880097, Peak mem 19.940 GB\n",
      "Iter 17390: Train loss 0.663, Learning Rate 4.000e-04, It/sec 2.414, Tokens/sec 372.897, Trained Tokens 2881642, Peak mem 19.940 GB\n",
      "Iter 17400: Val loss 2.335, Val took 4.749s\n",
      "Iter 17400: Train loss 0.684, Learning Rate 4.000e-04, It/sec 24.564, Tokens/sec 4288.916, Trained Tokens 2883388, Peak mem 19.940 GB\n",
      "Iter 17400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017400_adapters.safetensors.\n",
      "Iter 17410: Train loss 0.589, Learning Rate 4.000e-04, It/sec 1.941, Tokens/sec 349.892, Trained Tokens 2885191, Peak mem 19.940 GB\n",
      "Iter 17420: Train loss 0.713, Learning Rate 4.000e-04, It/sec 2.370, Tokens/sec 365.528, Trained Tokens 2886733, Peak mem 19.940 GB\n",
      "Iter 17430: Train loss 0.650, Learning Rate 4.000e-04, It/sec 2.140, Tokens/sec 320.743, Trained Tokens 2888232, Peak mem 19.940 GB\n",
      "Iter 17440: Train loss 0.599, Learning Rate 4.000e-04, It/sec 2.752, Tokens/sec 353.406, Trained Tokens 2889516, Peak mem 19.940 GB\n",
      "Iter 17450: Val loss 2.252, Val took 2.837s\n",
      "Iter 17450: Train loss 0.652, Learning Rate 4.000e-04, It/sec 40.141, Tokens/sec 5366.828, Trained Tokens 2890853, Peak mem 19.940 GB\n",
      "Iter 17460: Train loss 0.678, Learning Rate 4.000e-04, It/sec 2.989, Tokens/sec 364.971, Trained Tokens 2892074, Peak mem 19.940 GB\n",
      "Iter 17470: Train loss 0.657, Learning Rate 4.000e-04, It/sec 3.074, Tokens/sec 360.930, Trained Tokens 2893248, Peak mem 19.940 GB\n",
      "Iter 17480: Train loss 0.786, Learning Rate 4.000e-04, It/sec 1.783, Tokens/sec 372.592, Trained Tokens 2895338, Peak mem 19.940 GB\n",
      "Iter 17490: Train loss 0.831, Learning Rate 4.000e-04, It/sec 1.779, Tokens/sec 362.530, Trained Tokens 2897376, Peak mem 19.940 GB\n",
      "Iter 17500: Val loss 2.159, Val took 4.406s\n",
      "Iter 17500: Train loss 0.790, Learning Rate 4.000e-04, It/sec 6.687, Tokens/sec 1049.183, Trained Tokens 2898945, Peak mem 19.940 GB\n",
      "Iter 17500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017500_adapters.safetensors.\n",
      "Iter 17510: Train loss 0.689, Learning Rate 4.000e-04, It/sec 2.624, Tokens/sec 343.184, Trained Tokens 2900253, Peak mem 19.940 GB\n",
      "Iter 17520: Train loss 0.694, Learning Rate 4.000e-04, It/sec 2.769, Tokens/sec 373.523, Trained Tokens 2901602, Peak mem 19.940 GB\n",
      "Iter 17530: Train loss 0.760, Learning Rate 4.000e-04, It/sec 2.148, Tokens/sec 356.594, Trained Tokens 2903262, Peak mem 19.940 GB\n",
      "Iter 17540: Train loss 0.701, Learning Rate 4.000e-04, It/sec 1.855, Tokens/sec 337.256, Trained Tokens 2905080, Peak mem 19.940 GB\n",
      "Iter 17550: Val loss 2.135, Val took 3.373s\n",
      "Iter 17550: Train loss 0.787, Learning Rate 4.000e-04, It/sec 18.083, Tokens/sec 3470.188, Trained Tokens 2906999, Peak mem 19.940 GB\n",
      "Iter 17560: Train loss 1.044, Learning Rate 4.000e-04, It/sec 1.203, Tokens/sec 373.210, Trained Tokens 2910102, Peak mem 19.940 GB\n",
      "Iter 17570: Train loss 0.730, Learning Rate 4.000e-04, It/sec 2.100, Tokens/sec 341.802, Trained Tokens 2911730, Peak mem 19.940 GB\n",
      "Iter 17580: Train loss 0.744, Learning Rate 4.000e-04, It/sec 2.941, Tokens/sec 321.706, Trained Tokens 2912824, Peak mem 19.940 GB\n",
      "Iter 17590: Train loss 0.800, Learning Rate 4.000e-04, It/sec 2.418, Tokens/sec 341.702, Trained Tokens 2914237, Peak mem 19.940 GB\n",
      "Iter 17600: Val loss 2.218, Val took 3.006s\n",
      "Iter 17600: Train loss 0.815, Learning Rate 4.000e-04, It/sec 17.101, Tokens/sec 3847.824, Trained Tokens 2916487, Peak mem 19.940 GB\n",
      "Iter 17600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017600_adapters.safetensors.\n",
      "Iter 17610: Train loss 0.766, Learning Rate 4.000e-04, It/sec 2.418, Tokens/sec 359.858, Trained Tokens 2917975, Peak mem 19.940 GB\n",
      "Iter 17620: Train loss 0.712, Learning Rate 4.000e-04, It/sec 2.163, Tokens/sec 321.647, Trained Tokens 2919462, Peak mem 19.940 GB\n",
      "Iter 17630: Train loss 0.708, Learning Rate 4.000e-04, It/sec 3.541, Tokens/sec 312.670, Trained Tokens 2920345, Peak mem 19.940 GB\n",
      "Iter 17640: Train loss 0.718, Learning Rate 4.000e-04, It/sec 1.669, Tokens/sec 295.246, Trained Tokens 2922114, Peak mem 19.940 GB\n",
      "Iter 17650: Val loss 2.369, Val took 4.307s\n",
      "Iter 17650: Train loss 0.658, Learning Rate 4.000e-04, It/sec 23.357, Tokens/sec 3772.087, Trained Tokens 2923729, Peak mem 19.940 GB\n",
      "Iter 17660: Train loss 0.704, Learning Rate 4.000e-04, It/sec 1.990, Tokens/sec 274.092, Trained Tokens 2925106, Peak mem 19.940 GB\n",
      "Iter 17670: Train loss 0.619, Learning Rate 4.000e-04, It/sec 2.127, Tokens/sec 326.706, Trained Tokens 2926642, Peak mem 19.940 GB\n",
      "Iter 17680: Train loss 0.834, Learning Rate 4.000e-04, It/sec 1.757, Tokens/sec 363.305, Trained Tokens 2928710, Peak mem 19.940 GB\n",
      "Iter 17690: Train loss 0.644, Learning Rate 4.000e-04, It/sec 2.662, Tokens/sec 361.288, Trained Tokens 2930067, Peak mem 19.940 GB\n",
      "Iter 17700: Val loss 2.161, Val took 3.282s\n",
      "Iter 17700: Train loss 0.710, Learning Rate 4.000e-04, It/sec 23.184, Tokens/sec 4078.033, Trained Tokens 2931826, Peak mem 19.940 GB\n",
      "Iter 17700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017700_adapters.safetensors.\n",
      "Iter 17710: Train loss 0.711, Learning Rate 4.000e-04, It/sec 1.779, Tokens/sec 347.540, Trained Tokens 2933780, Peak mem 19.940 GB\n",
      "Iter 17720: Train loss 0.768, Learning Rate 4.000e-04, It/sec 1.849, Tokens/sec 378.826, Trained Tokens 2935829, Peak mem 19.940 GB\n",
      "Iter 17730: Train loss 0.680, Learning Rate 4.000e-04, It/sec 2.959, Tokens/sec 376.064, Trained Tokens 2937100, Peak mem 19.940 GB\n",
      "Iter 17740: Train loss 0.716, Learning Rate 4.000e-04, It/sec 1.888, Tokens/sec 371.759, Trained Tokens 2939069, Peak mem 19.940 GB\n",
      "Iter 17750: Val loss 2.458, Val took 4.198s\n",
      "Iter 17750: Train loss 0.699, Learning Rate 4.000e-04, It/sec 58.559, Tokens/sec 8315.345, Trained Tokens 2940489, Peak mem 19.940 GB\n",
      "Iter 17760: Train loss 0.760, Learning Rate 4.000e-04, It/sec 2.352, Tokens/sec 353.743, Trained Tokens 2941993, Peak mem 19.940 GB\n",
      "Iter 17770: Train loss 0.711, Learning Rate 4.000e-04, It/sec 2.532, Tokens/sec 382.344, Trained Tokens 2943503, Peak mem 19.940 GB\n",
      "Iter 17780: Train loss 0.726, Learning Rate 4.000e-04, It/sec 3.185, Tokens/sec 340.207, Trained Tokens 2944571, Peak mem 19.940 GB\n",
      "Iter 17790: Train loss 0.726, Learning Rate 4.000e-04, It/sec 2.432, Tokens/sec 385.409, Trained Tokens 2946156, Peak mem 19.940 GB\n",
      "Iter 17800: Val loss 2.450, Val took 3.623s\n",
      "Iter 17800: Train loss 0.926, Learning Rate 4.000e-04, It/sec 26.013, Tokens/sec 5241.710, Trained Tokens 2948171, Peak mem 19.940 GB\n",
      "Iter 17800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017800_adapters.safetensors.\n",
      "Iter 17810: Train loss 0.675, Learning Rate 4.000e-04, It/sec 2.417, Tokens/sec 338.555, Trained Tokens 2949572, Peak mem 19.940 GB\n",
      "Iter 17820: Train loss 0.678, Learning Rate 4.000e-04, It/sec 2.980, Tokens/sec 341.175, Trained Tokens 2950717, Peak mem 19.940 GB\n",
      "Iter 17830: Train loss 0.763, Learning Rate 4.000e-04, It/sec 1.764, Tokens/sec 316.903, Trained Tokens 2952513, Peak mem 19.940 GB\n",
      "Iter 17840: Train loss 0.844, Learning Rate 4.000e-04, It/sec 2.283, Tokens/sec 377.393, Trained Tokens 2954166, Peak mem 19.940 GB\n",
      "Iter 17850: Val loss 2.351, Val took 3.389s\n",
      "Iter 17850: Train loss 0.871, Learning Rate 4.000e-04, It/sec 42.878, Tokens/sec 12640.552, Trained Tokens 2957114, Peak mem 19.940 GB\n",
      "Iter 17860: Train loss 0.667, Learning Rate 4.000e-04, It/sec 2.232, Tokens/sec 346.666, Trained Tokens 2958667, Peak mem 19.940 GB\n",
      "Iter 17870: Train loss 0.855, Learning Rate 4.000e-04, It/sec 1.629, Tokens/sec 333.496, Trained Tokens 2960714, Peak mem 19.940 GB\n",
      "Iter 17880: Train loss 0.876, Learning Rate 4.000e-04, It/sec 1.505, Tokens/sec 341.138, Trained Tokens 2962980, Peak mem 19.940 GB\n",
      "Iter 17890: Train loss 0.756, Learning Rate 4.000e-04, It/sec 2.489, Tokens/sec 362.846, Trained Tokens 2964438, Peak mem 19.940 GB\n",
      "Iter 17900: Val loss 2.385, Val took 4.510s\n",
      "Iter 17900: Train loss 0.805, Learning Rate 4.000e-04, It/sec 17.350, Tokens/sec 3006.765, Trained Tokens 2966171, Peak mem 19.940 GB\n",
      "Iter 17900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0017900_adapters.safetensors.\n",
      "Iter 17910: Train loss 0.773, Learning Rate 4.000e-04, It/sec 2.170, Tokens/sec 291.031, Trained Tokens 2967512, Peak mem 19.940 GB\n",
      "Iter 17920: Train loss 0.834, Learning Rate 4.000e-04, It/sec 1.772, Tokens/sec 378.276, Trained Tokens 2969647, Peak mem 19.940 GB\n",
      "Iter 17930: Train loss 0.733, Learning Rate 4.000e-04, It/sec 2.407, Tokens/sec 341.039, Trained Tokens 2971064, Peak mem 19.940 GB\n",
      "Iter 17940: Train loss 0.752, Learning Rate 4.000e-04, It/sec 2.190, Tokens/sec 337.969, Trained Tokens 2972607, Peak mem 19.940 GB\n",
      "Iter 17950: Val loss 2.392, Val took 4.019s\n",
      "Iter 17950: Train loss 0.691, Learning Rate 4.000e-04, It/sec 19.609, Tokens/sec 3176.731, Trained Tokens 2974227, Peak mem 19.940 GB\n",
      "Iter 17960: Train loss 0.752, Learning Rate 4.000e-04, It/sec 1.793, Tokens/sec 344.224, Trained Tokens 2976147, Peak mem 19.940 GB\n",
      "Iter 17970: Train loss 0.776, Learning Rate 4.000e-04, It/sec 2.650, Tokens/sec 365.697, Trained Tokens 2977527, Peak mem 19.940 GB\n",
      "Iter 17980: Train loss 0.723, Learning Rate 4.000e-04, It/sec 2.849, Tokens/sec 350.958, Trained Tokens 2978759, Peak mem 19.940 GB\n",
      "Iter 17990: Train loss 0.958, Learning Rate 4.000e-04, It/sec 1.688, Tokens/sec 399.299, Trained Tokens 2981125, Peak mem 19.940 GB\n",
      "Iter 18000: Val loss 2.513, Val took 3.285s\n",
      "Iter 18000: Train loss 0.694, Learning Rate 4.000e-04, It/sec 18.430, Tokens/sec 2401.383, Trained Tokens 2982428, Peak mem 19.940 GB\n",
      "Iter 18000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018000_adapters.safetensors.\n",
      "Iter 18010: Train loss 0.752, Learning Rate 4.000e-04, It/sec 2.264, Tokens/sec 326.482, Trained Tokens 2983870, Peak mem 19.940 GB\n",
      "Iter 18020: Train loss 0.777, Learning Rate 4.000e-04, It/sec 2.855, Tokens/sec 353.766, Trained Tokens 2985109, Peak mem 19.940 GB\n",
      "Iter 18030: Train loss 0.862, Learning Rate 4.000e-04, It/sec 1.710, Tokens/sec 268.642, Trained Tokens 2986680, Peak mem 19.940 GB\n",
      "Iter 18040: Train loss 0.850, Learning Rate 4.000e-04, It/sec 2.209, Tokens/sec 349.009, Trained Tokens 2988260, Peak mem 19.940 GB\n",
      "Iter 18050: Val loss 2.605, Val took 3.971s\n",
      "Iter 18050: Train loss 0.821, Learning Rate 4.000e-04, It/sec 11.452, Tokens/sec 2365.901, Trained Tokens 2990326, Peak mem 19.940 GB\n",
      "Iter 18060: Train loss 0.616, Learning Rate 4.000e-04, It/sec 1.743, Tokens/sec 282.914, Trained Tokens 2991949, Peak mem 19.940 GB\n",
      "Iter 18070: Train loss 0.658, Learning Rate 4.000e-04, It/sec 2.781, Tokens/sec 354.321, Trained Tokens 2993223, Peak mem 19.940 GB\n",
      "Iter 18080: Train loss 0.881, Learning Rate 4.000e-04, It/sec 1.562, Tokens/sec 346.427, Trained Tokens 2995441, Peak mem 19.940 GB\n",
      "Iter 18090: Train loss 0.702, Learning Rate 4.000e-04, It/sec 2.382, Tokens/sec 358.915, Trained Tokens 2996948, Peak mem 19.940 GB\n",
      "Iter 18100: Val loss 2.437, Val took 3.981s\n",
      "Iter 18100: Train loss 0.749, Learning Rate 4.000e-04, It/sec 18.732, Tokens/sec 4504.956, Trained Tokens 2999353, Peak mem 19.940 GB\n",
      "Iter 18100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018100_adapters.safetensors.\n",
      "Iter 18110: Train loss 0.819, Learning Rate 4.000e-04, It/sec 1.434, Tokens/sec 362.476, Trained Tokens 3001880, Peak mem 19.940 GB\n",
      "Iter 18120: Train loss 0.584, Learning Rate 4.000e-04, It/sec 1.783, Tokens/sec 356.697, Trained Tokens 3003881, Peak mem 19.940 GB\n",
      "Iter 18130: Train loss 0.875, Learning Rate 4.000e-04, It/sec 1.673, Tokens/sec 356.718, Trained Tokens 3006013, Peak mem 19.940 GB\n",
      "Iter 18140: Train loss 0.609, Learning Rate 4.000e-04, It/sec 2.329, Tokens/sec 343.051, Trained Tokens 3007486, Peak mem 19.940 GB\n",
      "Iter 18150: Val loss 2.309, Val took 2.896s\n",
      "Iter 18150: Train loss 0.693, Learning Rate 4.000e-04, It/sec 17.976, Tokens/sec 2293.709, Trained Tokens 3008762, Peak mem 19.940 GB\n",
      "Iter 18160: Train loss 0.647, Learning Rate 4.000e-04, It/sec 2.630, Tokens/sec 347.640, Trained Tokens 3010084, Peak mem 19.940 GB\n",
      "Iter 18170: Train loss 0.660, Learning Rate 4.000e-04, It/sec 3.374, Tokens/sec 364.009, Trained Tokens 3011163, Peak mem 19.940 GB\n",
      "Iter 18180: Train loss 0.821, Learning Rate 4.000e-04, It/sec 1.815, Tokens/sec 372.997, Trained Tokens 3013218, Peak mem 19.940 GB\n",
      "Iter 18190: Train loss 0.609, Learning Rate 4.000e-04, It/sec 2.502, Tokens/sec 366.049, Trained Tokens 3014681, Peak mem 19.940 GB\n",
      "Iter 18200: Val loss 2.720, Val took 3.801s\n",
      "Iter 18200: Train loss 0.771, Learning Rate 4.000e-04, It/sec 8.600, Tokens/sec 1356.277, Trained Tokens 3016258, Peak mem 19.940 GB\n",
      "Iter 18200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018200_adapters.safetensors.\n",
      "Iter 18210: Train loss 0.648, Learning Rate 4.000e-04, It/sec 2.797, Tokens/sec 373.904, Trained Tokens 3017595, Peak mem 19.940 GB\n",
      "Iter 18220: Train loss 0.754, Learning Rate 4.000e-04, It/sec 1.793, Tokens/sec 385.556, Trained Tokens 3019745, Peak mem 19.940 GB\n",
      "Iter 18230: Train loss 0.716, Learning Rate 4.000e-04, It/sec 2.274, Tokens/sec 374.913, Trained Tokens 3021394, Peak mem 19.940 GB\n",
      "Iter 18240: Train loss 0.661, Learning Rate 4.000e-04, It/sec 2.462, Tokens/sec 339.535, Trained Tokens 3022773, Peak mem 19.940 GB\n",
      "Iter 18250: Val loss 2.465, Val took 4.524s\n",
      "Iter 18250: Train loss 0.686, Learning Rate 4.000e-04, It/sec 26.433, Tokens/sec 4142.120, Trained Tokens 3024340, Peak mem 19.940 GB\n",
      "Iter 18260: Train loss 0.618, Learning Rate 4.000e-04, It/sec 2.618, Tokens/sec 361.559, Trained Tokens 3025721, Peak mem 19.940 GB\n",
      "Iter 18270: Train loss 0.777, Learning Rate 4.000e-04, It/sec 1.815, Tokens/sec 370.430, Trained Tokens 3027762, Peak mem 19.940 GB\n",
      "Iter 18280: Train loss 0.734, Learning Rate 4.000e-04, It/sec 2.065, Tokens/sec 355.107, Trained Tokens 3029482, Peak mem 19.940 GB\n",
      "Iter 18290: Train loss 0.648, Learning Rate 4.000e-04, It/sec 2.778, Tokens/sec 347.203, Trained Tokens 3030732, Peak mem 19.940 GB\n",
      "Iter 18300: Val loss 2.239, Val took 3.392s\n",
      "Iter 18300: Train loss 0.821, Learning Rate 4.000e-04, It/sec 18.699, Tokens/sec 3719.175, Trained Tokens 3032721, Peak mem 19.940 GB\n",
      "Iter 18300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018300_adapters.safetensors.\n",
      "Iter 18310: Train loss 0.823, Learning Rate 4.000e-04, It/sec 2.068, Tokens/sec 365.773, Trained Tokens 3034490, Peak mem 19.940 GB\n",
      "Iter 18320: Train loss 0.801, Learning Rate 4.000e-04, It/sec 1.991, Tokens/sec 360.174, Trained Tokens 3036299, Peak mem 19.940 GB\n",
      "Iter 18330: Train loss 0.721, Learning Rate 4.000e-04, It/sec 2.664, Tokens/sec 396.637, Trained Tokens 3037788, Peak mem 19.940 GB\n",
      "Iter 18340: Train loss 0.692, Learning Rate 4.000e-04, It/sec 2.526, Tokens/sec 381.903, Trained Tokens 3039300, Peak mem 19.940 GB\n",
      "Iter 18350: Val loss 2.142, Val took 3.460s\n",
      "Iter 18350: Train loss 0.698, Learning Rate 4.000e-04, It/sec 14.375, Tokens/sec 2242.578, Trained Tokens 3040860, Peak mem 19.940 GB\n",
      "Iter 18360: Train loss 0.748, Learning Rate 4.000e-04, It/sec 2.081, Tokens/sec 369.837, Trained Tokens 3042637, Peak mem 19.940 GB\n",
      "Iter 18370: Train loss 0.640, Learning Rate 4.000e-04, It/sec 2.952, Tokens/sec 367.809, Trained Tokens 3043883, Peak mem 19.940 GB\n",
      "Iter 18380: Train loss 0.828, Learning Rate 4.000e-04, It/sec 1.649, Tokens/sec 387.697, Trained Tokens 3046234, Peak mem 19.940 GB\n",
      "Iter 18390: Train loss 0.675, Learning Rate 4.000e-04, It/sec 2.654, Tokens/sec 339.433, Trained Tokens 3047513, Peak mem 19.940 GB\n",
      "Iter 18400: Val loss 2.674, Val took 3.729s\n",
      "Iter 18400: Train loss 0.687, Learning Rate 4.000e-04, It/sec 40.420, Tokens/sec 4850.448, Trained Tokens 3048713, Peak mem 19.940 GB\n",
      "Iter 18400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018400_adapters.safetensors.\n",
      "Iter 18410: Train loss 0.774, Learning Rate 4.000e-04, It/sec 2.145, Tokens/sec 366.079, Trained Tokens 3050420, Peak mem 19.940 GB\n",
      "Iter 18420: Train loss 0.765, Learning Rate 4.000e-04, It/sec 2.895, Tokens/sec 357.841, Trained Tokens 3051656, Peak mem 19.940 GB\n",
      "Iter 18430: Train loss 0.844, Learning Rate 4.000e-04, It/sec 2.770, Tokens/sec 374.550, Trained Tokens 3053008, Peak mem 19.940 GB\n",
      "Iter 18440: Train loss 0.730, Learning Rate 4.000e-04, It/sec 2.222, Tokens/sec 359.245, Trained Tokens 3054625, Peak mem 19.940 GB\n",
      "Iter 18450: Val loss 2.394, Val took 3.285s\n",
      "Iter 18450: Train loss 0.715, Learning Rate 4.000e-04, It/sec 41.567, Tokens/sec 6442.848, Trained Tokens 3056175, Peak mem 19.940 GB\n",
      "Iter 18460: Train loss 0.645, Learning Rate 4.000e-04, It/sec 2.206, Tokens/sec 365.724, Trained Tokens 3057833, Peak mem 19.940 GB\n",
      "Iter 18470: Train loss 0.859, Learning Rate 4.000e-04, It/sec 1.403, Tokens/sec 378.221, Trained Tokens 3060528, Peak mem 19.940 GB\n",
      "Iter 18480: Train loss 0.694, Learning Rate 4.000e-04, It/sec 1.756, Tokens/sec 344.769, Trained Tokens 3062491, Peak mem 19.940 GB\n",
      "Iter 18490: Train loss 0.645, Learning Rate 4.000e-04, It/sec 2.516, Tokens/sec 324.797, Trained Tokens 3063782, Peak mem 19.940 GB\n",
      "Iter 18500: Val loss 2.485, Val took 5.083s\n",
      "Iter 18500: Train loss 0.656, Learning Rate 4.000e-04, It/sec 41.254, Tokens/sec 7392.702, Trained Tokens 3065574, Peak mem 19.940 GB\n",
      "Iter 18500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018500_adapters.safetensors.\n",
      "Iter 18510: Train loss 0.632, Learning Rate 4.000e-04, It/sec 2.560, Tokens/sec 353.038, Trained Tokens 3066953, Peak mem 19.940 GB\n",
      "Iter 18520: Train loss 0.625, Learning Rate 4.000e-04, It/sec 2.385, Tokens/sec 365.400, Trained Tokens 3068485, Peak mem 19.940 GB\n",
      "Iter 18530: Train loss 0.647, Learning Rate 4.000e-04, It/sec 2.438, Tokens/sec 360.818, Trained Tokens 3069965, Peak mem 19.940 GB\n",
      "Iter 18540: Train loss 0.621, Learning Rate 4.000e-04, It/sec 2.166, Tokens/sec 384.505, Trained Tokens 3071740, Peak mem 19.940 GB\n",
      "Iter 18550: Val loss 2.566, Val took 3.497s\n",
      "Iter 18550: Train loss 0.636, Learning Rate 4.000e-04, It/sec 41.113, Tokens/sec 4053.762, Trained Tokens 3072726, Peak mem 19.940 GB\n",
      "Iter 18560: Train loss 0.590, Learning Rate 4.000e-04, It/sec 2.169, Tokens/sec 308.838, Trained Tokens 3074150, Peak mem 19.940 GB\n",
      "Iter 18570: Train loss 0.653, Learning Rate 4.000e-04, It/sec 2.464, Tokens/sec 358.807, Trained Tokens 3075606, Peak mem 19.940 GB\n",
      "Iter 18580: Train loss 0.749, Learning Rate 4.000e-04, It/sec 2.238, Tokens/sec 362.816, Trained Tokens 3077227, Peak mem 19.940 GB\n",
      "Iter 18590: Train loss 0.895, Learning Rate 4.000e-04, It/sec 2.010, Tokens/sec 377.763, Trained Tokens 3079106, Peak mem 19.940 GB\n",
      "Iter 18600: Val loss 2.367, Val took 3.610s\n",
      "Iter 18600: Train loss 0.665, Learning Rate 4.000e-04, It/sec 13.933, Tokens/sec 1556.295, Trained Tokens 3080223, Peak mem 19.940 GB\n",
      "Iter 18600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018600_adapters.safetensors.\n",
      "Iter 18610: Train loss 0.685, Learning Rate 4.000e-04, It/sec 2.179, Tokens/sec 329.260, Trained Tokens 3081734, Peak mem 19.940 GB\n",
      "Iter 18620: Train loss 0.776, Learning Rate 4.000e-04, It/sec 1.416, Tokens/sec 378.604, Trained Tokens 3084407, Peak mem 19.940 GB\n",
      "Iter 18630: Train loss 0.669, Learning Rate 4.000e-04, It/sec 2.150, Tokens/sec 361.796, Trained Tokens 3086090, Peak mem 19.940 GB\n",
      "Iter 18640: Train loss 0.629, Learning Rate 4.000e-04, It/sec 2.136, Tokens/sec 334.920, Trained Tokens 3087658, Peak mem 19.940 GB\n",
      "Iter 18650: Val loss 2.640, Val took 3.969s\n",
      "Iter 18650: Train loss 0.739, Learning Rate 4.000e-04, It/sec 17.221, Tokens/sec 2838.041, Trained Tokens 3089306, Peak mem 19.940 GB\n",
      "Iter 18660: Train loss 0.702, Learning Rate 4.000e-04, It/sec 2.186, Tokens/sec 361.822, Trained Tokens 3090961, Peak mem 19.940 GB\n",
      "Iter 18670: Train loss 0.760, Learning Rate 4.000e-04, It/sec 2.694, Tokens/sec 333.763, Trained Tokens 3092200, Peak mem 19.940 GB\n",
      "Iter 18680: Train loss 0.732, Learning Rate 4.000e-04, It/sec 2.200, Tokens/sec 368.576, Trained Tokens 3093875, Peak mem 19.940 GB\n",
      "Iter 18690: Train loss 0.738, Learning Rate 4.000e-04, It/sec 3.479, Tokens/sec 356.568, Trained Tokens 3094900, Peak mem 19.940 GB\n",
      "Iter 18700: Val loss 2.408, Val took 3.919s\n",
      "Iter 18700: Train loss 0.791, Learning Rate 4.000e-04, It/sec 32.786, Tokens/sec 4606.493, Trained Tokens 3096305, Peak mem 19.940 GB\n",
      "Iter 18700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018700_adapters.safetensors.\n",
      "Iter 18710: Train loss 0.758, Learning Rate 4.000e-04, It/sec 2.553, Tokens/sec 367.091, Trained Tokens 3097743, Peak mem 19.940 GB\n",
      "Iter 18720: Train loss 0.834, Learning Rate 4.000e-04, It/sec 2.297, Tokens/sec 390.432, Trained Tokens 3099443, Peak mem 19.940 GB\n",
      "Iter 18730: Train loss 0.750, Learning Rate 4.000e-04, It/sec 2.294, Tokens/sec 351.151, Trained Tokens 3100974, Peak mem 19.940 GB\n",
      "Iter 18740: Train loss 0.922, Learning Rate 4.000e-04, It/sec 1.992, Tokens/sec 377.968, Trained Tokens 3102871, Peak mem 19.940 GB\n",
      "Iter 18750: Val loss 2.333, Val took 2.720s\n",
      "Iter 18750: Train loss 0.760, Learning Rate 4.000e-04, It/sec 26.492, Tokens/sec 4002.970, Trained Tokens 3104382, Peak mem 19.940 GB\n",
      "Iter 18760: Train loss 0.809, Learning Rate 4.000e-04, It/sec 1.992, Tokens/sec 346.627, Trained Tokens 3106122, Peak mem 19.940 GB\n",
      "Iter 18770: Train loss 0.730, Learning Rate 4.000e-04, It/sec 2.836, Tokens/sec 340.355, Trained Tokens 3107322, Peak mem 19.940 GB\n",
      "Iter 18780: Train loss 0.691, Learning Rate 4.000e-04, It/sec 2.574, Tokens/sec 359.902, Trained Tokens 3108720, Peak mem 19.940 GB\n",
      "Iter 18790: Train loss 0.772, Learning Rate 4.000e-04, It/sec 2.235, Tokens/sec 362.595, Trained Tokens 3110342, Peak mem 19.940 GB\n",
      "Iter 18800: Val loss 2.040, Val took 2.937s\n",
      "Iter 18800: Train loss 0.833, Learning Rate 4.000e-04, It/sec 18.551, Tokens/sec 3585.840, Trained Tokens 3112275, Peak mem 19.940 GB\n",
      "Iter 18800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018800_adapters.safetensors.\n",
      "Iter 18810: Train loss 0.798, Learning Rate 4.000e-04, It/sec 2.162, Tokens/sec 352.389, Trained Tokens 3113905, Peak mem 19.940 GB\n",
      "Iter 18820: Train loss 0.763, Learning Rate 4.000e-04, It/sec 1.751, Tokens/sec 383.018, Trained Tokens 3116093, Peak mem 19.940 GB\n",
      "Iter 18830: Train loss 0.679, Learning Rate 4.000e-04, It/sec 1.952, Tokens/sec 345.113, Trained Tokens 3117861, Peak mem 19.940 GB\n",
      "Iter 18840: Train loss 0.809, Learning Rate 4.000e-04, It/sec 2.032, Tokens/sec 356.204, Trained Tokens 3119614, Peak mem 19.940 GB\n",
      "Iter 18850: Val loss 2.316, Val took 3.863s\n",
      "Iter 18850: Train loss 0.824, Learning Rate 4.000e-04, It/sec 4.198, Tokens/sec 971.064, Trained Tokens 3121927, Peak mem 19.940 GB\n",
      "Iter 18860: Train loss 0.805, Learning Rate 4.000e-04, It/sec 1.803, Tokens/sec 389.938, Trained Tokens 3124090, Peak mem 19.940 GB\n",
      "Iter 18870: Train loss 0.661, Learning Rate 4.000e-04, It/sec 1.817, Tokens/sec 371.829, Trained Tokens 3126136, Peak mem 19.940 GB\n",
      "Iter 18880: Train loss 0.584, Learning Rate 4.000e-04, It/sec 2.381, Tokens/sec 360.713, Trained Tokens 3127651, Peak mem 19.940 GB\n",
      "Iter 18890: Train loss 0.615, Learning Rate 4.000e-04, It/sec 2.440, Tokens/sec 333.128, Trained Tokens 3129016, Peak mem 19.940 GB\n",
      "Iter 18900: Val loss 2.659, Val took 4.005s\n",
      "Iter 18900: Train loss 0.659, Learning Rate 4.000e-04, It/sec 17.391, Tokens/sec 3349.430, Trained Tokens 3130942, Peak mem 19.940 GB\n",
      "Iter 18900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0018900_adapters.safetensors.\n",
      "Iter 18910: Train loss 0.544, Learning Rate 4.000e-04, It/sec 2.382, Tokens/sec 356.815, Trained Tokens 3132440, Peak mem 19.940 GB\n",
      "Iter 18920: Train loss 0.649, Learning Rate 4.000e-04, It/sec 2.489, Tokens/sec 375.650, Trained Tokens 3133949, Peak mem 19.940 GB\n",
      "Iter 18930: Train loss 0.589, Learning Rate 4.000e-04, It/sec 2.044, Tokens/sec 375.600, Trained Tokens 3135787, Peak mem 19.940 GB\n",
      "Iter 18940: Train loss 0.596, Learning Rate 4.000e-04, It/sec 2.946, Tokens/sec 350.281, Trained Tokens 3136976, Peak mem 19.940 GB\n",
      "Iter 18950: Val loss 2.843, Val took 2.948s\n",
      "Iter 18950: Train loss 0.576, Learning Rate 4.000e-04, It/sec 20.605, Tokens/sec 3000.109, Trained Tokens 3138432, Peak mem 19.940 GB\n",
      "Iter 18960: Train loss 0.607, Learning Rate 4.000e-04, It/sec 1.634, Tokens/sec 360.304, Trained Tokens 3140637, Peak mem 19.940 GB\n",
      "Iter 18970: Train loss 0.540, Learning Rate 4.000e-04, It/sec 2.866, Tokens/sec 371.377, Trained Tokens 3141933, Peak mem 19.940 GB\n",
      "Iter 18980: Train loss 0.594, Learning Rate 4.000e-04, It/sec 2.450, Tokens/sec 369.959, Trained Tokens 3143443, Peak mem 19.940 GB\n",
      "Iter 18990: Train loss 0.603, Learning Rate 4.000e-04, It/sec 2.606, Tokens/sec 395.398, Trained Tokens 3144960, Peak mem 19.940 GB\n",
      "Iter 19000: Val loss 2.665, Val took 3.797s\n",
      "Iter 19000: Train loss 0.618, Learning Rate 4.000e-04, It/sec 31.357, Tokens/sec 4239.510, Trained Tokens 3146312, Peak mem 19.940 GB\n",
      "Iter 19000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019000_adapters.safetensors.\n",
      "Iter 19010: Train loss 0.624, Learning Rate 4.000e-04, It/sec 2.315, Tokens/sec 356.214, Trained Tokens 3147851, Peak mem 19.940 GB\n",
      "Iter 19020: Train loss 0.634, Learning Rate 4.000e-04, It/sec 2.338, Tokens/sec 351.142, Trained Tokens 3149353, Peak mem 19.940 GB\n",
      "Iter 19030: Train loss 0.687, Learning Rate 4.000e-04, It/sec 2.099, Tokens/sec 355.588, Trained Tokens 3151047, Peak mem 19.940 GB\n",
      "Iter 19040: Train loss 0.734, Learning Rate 4.000e-04, It/sec 2.500, Tokens/sec 353.732, Trained Tokens 3152462, Peak mem 19.940 GB\n",
      "Iter 19050: Val loss 2.789, Val took 4.264s\n",
      "Iter 19050: Train loss 0.809, Learning Rate 4.000e-04, It/sec 26.095, Tokens/sec 5735.591, Trained Tokens 3154660, Peak mem 19.940 GB\n",
      "Iter 19060: Train loss 0.744, Learning Rate 4.000e-04, It/sec 1.366, Tokens/sec 392.157, Trained Tokens 3157531, Peak mem 19.940 GB\n",
      "Iter 19070: Train loss 0.637, Learning Rate 4.000e-04, It/sec 2.512, Tokens/sec 318.522, Trained Tokens 3158799, Peak mem 19.940 GB\n",
      "Iter 19080: Train loss 0.644, Learning Rate 4.000e-04, It/sec 1.992, Tokens/sec 354.023, Trained Tokens 3160576, Peak mem 19.940 GB\n",
      "Iter 19090: Train loss 0.595, Learning Rate 4.000e-04, It/sec 2.212, Tokens/sec 349.570, Trained Tokens 3162156, Peak mem 19.940 GB\n",
      "Iter 19100: Val loss 2.329, Val took 3.001s\n",
      "Iter 19100: Train loss 0.620, Learning Rate 4.000e-04, It/sec 41.111, Tokens/sec 4037.092, Trained Tokens 3163138, Peak mem 19.940 GB\n",
      "Iter 19100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019100_adapters.safetensors.\n",
      "Iter 19110: Train loss 0.753, Learning Rate 4.000e-04, It/sec 2.875, Tokens/sec 355.323, Trained Tokens 3164374, Peak mem 19.940 GB\n",
      "Iter 19120: Train loss 0.670, Learning Rate 4.000e-04, It/sec 2.093, Tokens/sec 371.631, Trained Tokens 3166150, Peak mem 19.940 GB\n",
      "Iter 19130: Train loss 0.648, Learning Rate 4.000e-04, It/sec 2.396, Tokens/sec 352.441, Trained Tokens 3167621, Peak mem 19.940 GB\n",
      "Iter 19140: Train loss 0.697, Learning Rate 4.000e-04, It/sec 2.175, Tokens/sec 363.387, Trained Tokens 3169292, Peak mem 19.940 GB\n",
      "Iter 19150: Val loss 2.666, Val took 3.650s\n",
      "Iter 19150: Train loss 0.706, Learning Rate 4.000e-04, It/sec 42.805, Tokens/sec 9271.536, Trained Tokens 3171458, Peak mem 19.940 GB\n",
      "Iter 19160: Train loss 0.719, Learning Rate 4.000e-04, It/sec 1.635, Tokens/sec 356.075, Trained Tokens 3173636, Peak mem 19.940 GB\n",
      "Iter 19170: Train loss 0.652, Learning Rate 4.000e-04, It/sec 1.835, Tokens/sec 356.446, Trained Tokens 3175579, Peak mem 19.940 GB\n",
      "Iter 19180: Train loss 0.839, Learning Rate 4.000e-04, It/sec 1.624, Tokens/sec 363.424, Trained Tokens 3177817, Peak mem 19.940 GB\n",
      "Iter 19190: Train loss 0.729, Learning Rate 4.000e-04, It/sec 2.972, Tokens/sec 334.350, Trained Tokens 3178942, Peak mem 19.940 GB\n",
      "Iter 19200: Val loss 2.368, Val took 3.026s\n",
      "Iter 19200: Train loss 0.917, Learning Rate 4.000e-04, It/sec 25.270, Tokens/sec 6964.541, Trained Tokens 3181698, Peak mem 19.940 GB\n",
      "Iter 19200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019200_adapters.safetensors.\n",
      "Iter 19210: Train loss 0.771, Learning Rate 4.000e-04, It/sec 1.854, Tokens/sec 345.404, Trained Tokens 3183561, Peak mem 19.940 GB\n",
      "Iter 19220: Train loss 0.743, Learning Rate 4.000e-04, It/sec 2.440, Tokens/sec 348.920, Trained Tokens 3184991, Peak mem 19.940 GB\n",
      "Iter 19230: Train loss 0.679, Learning Rate 4.000e-04, It/sec 2.873, Tokens/sec 344.441, Trained Tokens 3186190, Peak mem 19.940 GB\n",
      "Iter 19240: Train loss 0.685, Learning Rate 4.000e-04, It/sec 3.001, Tokens/sec 357.747, Trained Tokens 3187382, Peak mem 19.940 GB\n",
      "Iter 19250: Val loss 2.580, Val took 3.850s\n",
      "Iter 19250: Train loss 0.724, Learning Rate 4.000e-04, It/sec 34.026, Tokens/sec 4889.496, Trained Tokens 3188819, Peak mem 19.940 GB\n",
      "Iter 19260: Train loss 0.830, Learning Rate 4.000e-04, It/sec 1.974, Tokens/sec 368.620, Trained Tokens 3190686, Peak mem 19.940 GB\n",
      "Iter 19270: Train loss 0.680, Learning Rate 4.000e-04, It/sec 2.757, Tokens/sec 363.692, Trained Tokens 3192005, Peak mem 19.940 GB\n",
      "Iter 19280: Train loss 0.620, Learning Rate 4.000e-04, It/sec 2.325, Tokens/sec 363.670, Trained Tokens 3193569, Peak mem 19.940 GB\n",
      "Iter 19290: Train loss 0.552, Learning Rate 4.000e-04, It/sec 1.695, Tokens/sec 364.008, Trained Tokens 3195716, Peak mem 19.940 GB\n",
      "Iter 19300: Val loss 3.145, Val took 4.087s\n",
      "Iter 19300: Train loss 0.598, Learning Rate 4.000e-04, It/sec 39.691, Tokens/sec 5286.777, Trained Tokens 3197048, Peak mem 19.940 GB\n",
      "Iter 19300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019300_adapters.safetensors.\n",
      "Iter 19310: Train loss 0.627, Learning Rate 4.000e-04, It/sec 1.669, Tokens/sec 346.565, Trained Tokens 3199125, Peak mem 19.940 GB\n",
      "Iter 19320: Train loss 0.583, Learning Rate 4.000e-04, It/sec 2.972, Tokens/sec 346.847, Trained Tokens 3200292, Peak mem 19.940 GB\n",
      "Iter 19330: Train loss 0.594, Learning Rate 4.000e-04, It/sec 2.138, Tokens/sec 365.012, Trained Tokens 3201999, Peak mem 19.940 GB\n",
      "Iter 19340: Train loss 0.863, Learning Rate 4.000e-04, It/sec 1.479, Tokens/sec 367.290, Trained Tokens 3204483, Peak mem 19.940 GB\n",
      "Iter 19350: Val loss 2.359, Val took 3.507s\n",
      "Iter 19350: Train loss 0.659, Learning Rate 4.000e-04, It/sec 38.276, Tokens/sec 9718.383, Trained Tokens 3207022, Peak mem 19.940 GB\n",
      "Iter 19360: Train loss 0.729, Learning Rate 4.000e-04, It/sec 1.551, Tokens/sec 384.283, Trained Tokens 3209499, Peak mem 19.940 GB\n",
      "Iter 19370: Train loss 0.536, Learning Rate 4.000e-04, It/sec 2.338, Tokens/sec 365.897, Trained Tokens 3211064, Peak mem 19.940 GB\n",
      "Iter 19380: Train loss 0.607, Learning Rate 4.000e-04, It/sec 2.631, Tokens/sec 356.013, Trained Tokens 3212417, Peak mem 19.940 GB\n",
      "Iter 19390: Train loss 0.595, Learning Rate 4.000e-04, It/sec 2.306, Tokens/sec 355.111, Trained Tokens 3213957, Peak mem 19.940 GB\n",
      "Iter 19400: Val loss 2.583, Val took 3.328s\n",
      "Iter 19400: Train loss 0.581, Learning Rate 4.000e-04, It/sec 17.234, Tokens/sec 2703.970, Trained Tokens 3215526, Peak mem 19.940 GB\n",
      "Iter 19400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019400_adapters.safetensors.\n",
      "Iter 19410: Train loss 0.636, Learning Rate 4.000e-04, It/sec 2.296, Tokens/sec 331.011, Trained Tokens 3216968, Peak mem 19.940 GB\n",
      "Iter 19420: Train loss 0.599, Learning Rate 4.000e-04, It/sec 2.630, Tokens/sec 368.778, Trained Tokens 3218370, Peak mem 19.940 GB\n",
      "Iter 19430: Train loss 0.592, Learning Rate 4.000e-04, It/sec 2.498, Tokens/sec 344.437, Trained Tokens 3219749, Peak mem 19.940 GB\n",
      "Iter 19440: Train loss 0.656, Learning Rate 4.000e-04, It/sec 2.843, Tokens/sec 365.581, Trained Tokens 3221035, Peak mem 19.940 GB\n",
      "Iter 19450: Val loss 2.572, Val took 3.404s\n",
      "Iter 19450: Train loss 0.736, Learning Rate 4.000e-04, It/sec 18.790, Tokens/sec 3198.061, Trained Tokens 3222737, Peak mem 19.940 GB\n",
      "Iter 19460: Train loss 0.737, Learning Rate 4.000e-04, It/sec 2.084, Tokens/sec 347.136, Trained Tokens 3224403, Peak mem 19.940 GB\n",
      "Iter 19470: Train loss 0.740, Learning Rate 4.000e-04, It/sec 2.263, Tokens/sec 374.081, Trained Tokens 3226056, Peak mem 19.940 GB\n",
      "Iter 19480: Train loss 0.800, Learning Rate 4.000e-04, It/sec 1.563, Tokens/sec 365.029, Trained Tokens 3228391, Peak mem 19.940 GB\n",
      "Iter 19490: Train loss 0.676, Learning Rate 4.000e-04, It/sec 2.647, Tokens/sec 368.773, Trained Tokens 3229784, Peak mem 19.940 GB\n",
      "Iter 19500: Val loss 2.734, Val took 3.895s\n",
      "Iter 19500: Train loss 0.764, Learning Rate 4.000e-04, It/sec 7.885, Tokens/sec 1420.086, Trained Tokens 3231585, Peak mem 19.940 GB\n",
      "Iter 19500: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019500_adapters.safetensors.\n",
      "Iter 19510: Train loss 0.733, Learning Rate 4.000e-04, It/sec 2.103, Tokens/sec 350.328, Trained Tokens 3233251, Peak mem 19.940 GB\n",
      "Iter 19520: Train loss 0.751, Learning Rate 4.000e-04, It/sec 2.335, Tokens/sec 371.889, Trained Tokens 3234844, Peak mem 19.940 GB\n",
      "Iter 19530: Train loss 0.778, Learning Rate 4.000e-04, It/sec 2.041, Tokens/sec 354.263, Trained Tokens 3236580, Peak mem 19.940 GB\n",
      "Iter 19540: Train loss 0.691, Learning Rate 4.000e-04, It/sec 2.427, Tokens/sec 342.633, Trained Tokens 3237992, Peak mem 19.940 GB\n",
      "Iter 19550: Val loss 2.449, Val took 3.044s\n",
      "Iter 19550: Train loss 0.731, Learning Rate 4.000e-04, It/sec 40.782, Tokens/sec 8038.048, Trained Tokens 3239963, Peak mem 19.940 GB\n",
      "Iter 19560: Train loss 0.705, Learning Rate 4.000e-04, It/sec 2.336, Tokens/sec 343.580, Trained Tokens 3241434, Peak mem 19.940 GB\n",
      "Iter 19570: Train loss 0.780, Learning Rate 4.000e-04, It/sec 2.126, Tokens/sec 367.558, Trained Tokens 3243163, Peak mem 19.940 GB\n",
      "Iter 19580: Train loss 0.721, Learning Rate 4.000e-04, It/sec 2.888, Tokens/sec 346.842, Trained Tokens 3244364, Peak mem 19.940 GB\n",
      "Iter 19590: Train loss 0.681, Learning Rate 4.000e-04, It/sec 2.037, Tokens/sec 364.748, Trained Tokens 3246155, Peak mem 19.940 GB\n",
      "Iter 19600: Val loss 2.167, Val took 2.659s\n",
      "Iter 19600: Train loss 0.708, Learning Rate 4.000e-04, It/sec 26.081, Tokens/sec 3810.478, Trained Tokens 3247616, Peak mem 19.940 GB\n",
      "Iter 19600: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019600_adapters.safetensors.\n",
      "Iter 19610: Train loss 0.855, Learning Rate 4.000e-04, It/sec 1.556, Tokens/sec 367.163, Trained Tokens 3249975, Peak mem 19.940 GB\n",
      "Iter 19620: Train loss 0.766, Learning Rate 4.000e-04, It/sec 2.693, Tokens/sec 361.409, Trained Tokens 3251317, Peak mem 19.940 GB\n",
      "Iter 19630: Train loss 0.718, Learning Rate 4.000e-04, It/sec 3.030, Tokens/sec 361.723, Trained Tokens 3252511, Peak mem 19.940 GB\n",
      "Iter 19640: Train loss 0.783, Learning Rate 4.000e-04, It/sec 2.249, Tokens/sec 378.333, Trained Tokens 3254193, Peak mem 19.940 GB\n",
      "Iter 19650: Val loss 2.640, Val took 3.537s\n",
      "Iter 19650: Train loss 0.749, Learning Rate 4.000e-04, It/sec 22.628, Tokens/sec 3111.398, Trained Tokens 3255568, Peak mem 19.940 GB\n",
      "Iter 19660: Train loss 0.793, Learning Rate 4.000e-04, It/sec 2.637, Tokens/sec 374.458, Trained Tokens 3256988, Peak mem 19.940 GB\n",
      "Iter 19670: Train loss 0.725, Learning Rate 4.000e-04, It/sec 2.809, Tokens/sec 368.812, Trained Tokens 3258301, Peak mem 19.940 GB\n",
      "Iter 19680: Train loss 0.792, Learning Rate 4.000e-04, It/sec 2.229, Tokens/sec 360.821, Trained Tokens 3259920, Peak mem 19.940 GB\n",
      "Iter 19690: Train loss 0.733, Learning Rate 4.000e-04, It/sec 2.897, Tokens/sec 346.782, Trained Tokens 3261117, Peak mem 19.940 GB\n",
      "Iter 19700: Val loss 2.442, Val took 3.585s\n",
      "Iter 19700: Train loss 0.954, Learning Rate 4.000e-04, It/sec 15.421, Tokens/sec 4072.569, Trained Tokens 3263758, Peak mem 19.940 GB\n",
      "Iter 19700: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019700_adapters.safetensors.\n",
      "Iter 19710: Train loss 0.691, Learning Rate 4.000e-04, It/sec 1.662, Tokens/sec 362.786, Trained Tokens 3265941, Peak mem 19.940 GB\n",
      "Iter 19720: Train loss 0.623, Learning Rate 4.000e-04, It/sec 2.461, Tokens/sec 312.804, Trained Tokens 3267212, Peak mem 19.940 GB\n",
      "Iter 19730: Train loss 0.671, Learning Rate 4.000e-04, It/sec 2.639, Tokens/sec 353.613, Trained Tokens 3268552, Peak mem 19.940 GB\n",
      "Iter 19740: Train loss 0.660, Learning Rate 4.000e-04, It/sec 1.939, Tokens/sec 366.994, Trained Tokens 3270445, Peak mem 19.940 GB\n",
      "Iter 19750: Val loss 2.529, Val took 3.290s\n",
      "Iter 19750: Train loss 0.613, Learning Rate 4.000e-04, It/sec 33.629, Tokens/sec 4734.954, Trained Tokens 3271853, Peak mem 19.940 GB\n",
      "Iter 19760: Train loss 0.737, Learning Rate 4.000e-04, It/sec 1.605, Tokens/sec 368.558, Trained Tokens 3274149, Peak mem 19.940 GB\n",
      "Iter 19770: Train loss 0.766, Learning Rate 4.000e-04, It/sec 1.678, Tokens/sec 381.133, Trained Tokens 3276420, Peak mem 19.940 GB\n",
      "Iter 19780: Train loss 0.801, Learning Rate 4.000e-04, It/sec 1.224, Tokens/sec 368.637, Trained Tokens 3279432, Peak mem 19.940 GB\n",
      "Iter 19790: Train loss 0.601, Learning Rate 4.000e-04, It/sec 2.898, Tokens/sec 376.785, Trained Tokens 3280732, Peak mem 19.940 GB\n",
      "Iter 19800: Val loss 2.547, Val took 3.827s\n",
      "Iter 19800: Train loss 0.662, Learning Rate 4.000e-04, It/sec 25.351, Tokens/sec 3582.028, Trained Tokens 3282145, Peak mem 19.940 GB\n",
      "Iter 19800: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019800_adapters.safetensors.\n",
      "Iter 19810: Train loss 0.613, Learning Rate 4.000e-04, It/sec 2.654, Tokens/sec 349.592, Trained Tokens 3283462, Peak mem 19.940 GB\n",
      "Iter 19820: Train loss 0.800, Learning Rate 4.000e-04, It/sec 1.907, Tokens/sec 356.521, Trained Tokens 3285332, Peak mem 19.940 GB\n",
      "Iter 19830: Train loss 0.621, Learning Rate 4.000e-04, It/sec 2.475, Tokens/sec 359.633, Trained Tokens 3286785, Peak mem 19.940 GB\n",
      "Iter 19840: Train loss 0.677, Learning Rate 4.000e-04, It/sec 2.077, Tokens/sec 371.090, Trained Tokens 3288572, Peak mem 19.940 GB\n",
      "Iter 19850: Val loss 2.050, Val took 3.135s\n",
      "Iter 19850: Train loss 0.748, Learning Rate 4.000e-04, It/sec 9.628, Tokens/sec 1450.885, Trained Tokens 3290079, Peak mem 19.940 GB\n",
      "Iter 19860: Train loss 0.673, Learning Rate 4.000e-04, It/sec 2.145, Tokens/sec 347.016, Trained Tokens 3291697, Peak mem 19.940 GB\n",
      "Iter 19870: Train loss 0.645, Learning Rate 4.000e-04, It/sec 2.503, Tokens/sec 392.034, Trained Tokens 3293263, Peak mem 19.940 GB\n",
      "Iter 19880: Train loss 0.761, Learning Rate 4.000e-04, It/sec 1.814, Tokens/sec 373.373, Trained Tokens 3295321, Peak mem 19.940 GB\n",
      "Iter 19890: Train loss 0.643, Learning Rate 4.000e-04, It/sec 2.588, Tokens/sec 365.454, Trained Tokens 3296733, Peak mem 19.940 GB\n",
      "Iter 19900: Val loss 2.409, Val took 3.441s\n",
      "Iter 19900: Train loss 0.690, Learning Rate 4.000e-04, It/sec 40.872, Tokens/sec 7454.974, Trained Tokens 3298557, Peak mem 19.940 GB\n",
      "Iter 19900: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0019900_adapters.safetensors.\n",
      "Iter 19910: Train loss 0.818, Learning Rate 4.000e-04, It/sec 1.714, Tokens/sec 384.346, Trained Tokens 3300800, Peak mem 19.940 GB\n",
      "Iter 19920: Train loss 0.688, Learning Rate 4.000e-04, It/sec 3.127, Tokens/sec 352.699, Trained Tokens 3301928, Peak mem 19.940 GB\n",
      "Iter 19930: Train loss 0.717, Learning Rate 4.000e-04, It/sec 3.023, Tokens/sec 358.220, Trained Tokens 3303113, Peak mem 19.940 GB\n",
      "Iter 19940: Train loss 0.703, Learning Rate 4.000e-04, It/sec 3.638, Tokens/sec 367.832, Trained Tokens 3304124, Peak mem 19.940 GB\n",
      "Iter 19950: Val loss 2.276, Val took 3.569s\n",
      "Iter 19950: Train loss 0.683, Learning Rate 4.000e-04, It/sec 41.701, Tokens/sec 8498.730, Trained Tokens 3306162, Peak mem 19.940 GB\n",
      "Iter 19960: Train loss 0.774, Learning Rate 4.000e-04, It/sec 2.437, Tokens/sec 364.839, Trained Tokens 3307659, Peak mem 19.940 GB\n",
      "Iter 19970: Train loss 0.675, Learning Rate 4.000e-04, It/sec 2.479, Tokens/sec 370.043, Trained Tokens 3309152, Peak mem 19.940 GB\n",
      "Iter 19980: Train loss 0.626, Learning Rate 4.000e-04, It/sec 3.028, Tokens/sec 349.730, Trained Tokens 3310307, Peak mem 19.940 GB\n",
      "Iter 19990: Train loss 0.717, Learning Rate 4.000e-04, It/sec 2.328, Tokens/sec 362.749, Trained Tokens 3311865, Peak mem 19.940 GB\n",
      "Iter 20000: Val loss 2.662, Val took 4.176s\n",
      "Iter 20000: Train loss 0.674, Learning Rate 4.000e-04, It/sec 20.313, Tokens/sec 3623.796, Trained Tokens 3313649, Peak mem 19.940 GB\n",
      "Iter 20000: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0020000_adapters.safetensors.\n",
      "Iter 20010: Train loss 0.687, Learning Rate 4.000e-04, It/sec 2.669, Tokens/sec 351.776, Trained Tokens 3314967, Peak mem 19.940 GB\n",
      "Iter 20020: Train loss 0.805, Learning Rate 4.000e-04, It/sec 2.156, Tokens/sec 372.929, Trained Tokens 3316697, Peak mem 19.940 GB\n",
      "Iter 20030: Train loss 0.749, Learning Rate 4.000e-04, It/sec 1.822, Tokens/sec 377.962, Trained Tokens 3318771, Peak mem 19.940 GB\n",
      "Iter 20040: Train loss 0.724, Learning Rate 4.000e-04, It/sec 2.656, Tokens/sec 374.254, Trained Tokens 3320180, Peak mem 19.940 GB\n",
      "Iter 20050: Val loss 2.696, Val took 3.467s\n",
      "Iter 20050: Train loss 0.711, Learning Rate 4.000e-04, It/sec 33.048, Tokens/sec 4183.876, Trained Tokens 3321446, Peak mem 19.940 GB\n",
      "Iter 20060: Train loss 0.744, Learning Rate 4.000e-04, It/sec 1.860, Tokens/sec 357.309, Trained Tokens 3323367, Peak mem 19.940 GB\n",
      "Iter 20070: Train loss 0.720, Learning Rate 4.000e-04, It/sec 2.697, Tokens/sec 367.274, Trained Tokens 3324729, Peak mem 19.940 GB\n",
      "Iter 20080: Train loss 0.721, Learning Rate 4.000e-04, It/sec 2.482, Tokens/sec 369.026, Trained Tokens 3326216, Peak mem 19.940 GB\n",
      "Iter 20090: Train loss 0.766, Learning Rate 4.000e-04, It/sec 2.265, Tokens/sec 366.672, Trained Tokens 3327835, Peak mem 19.940 GB\n",
      "Iter 20100: Val loss 2.771, Val took 4.292s\n",
      "Iter 20100: Train loss 0.589, Learning Rate 4.000e-04, It/sec 6.839, Tokens/sec 907.551, Trained Tokens 3329162, Peak mem 19.940 GB\n",
      "Iter 20100: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0020100_adapters.safetensors.\n",
      "Iter 20110: Train loss 0.688, Learning Rate 4.000e-04, It/sec 2.288, Tokens/sec 394.020, Trained Tokens 3330884, Peak mem 19.940 GB\n",
      "Iter 20120: Train loss 0.790, Learning Rate 4.000e-04, It/sec 1.754, Tokens/sec 366.571, Trained Tokens 3332974, Peak mem 19.940 GB\n",
      "Iter 20130: Train loss 0.634, Learning Rate 4.000e-04, It/sec 2.376, Tokens/sec 354.784, Trained Tokens 3334467, Peak mem 19.940 GB\n",
      "Iter 20140: Train loss 0.689, Learning Rate 4.000e-04, It/sec 2.709, Tokens/sec 365.707, Trained Tokens 3335817, Peak mem 19.940 GB\n",
      "Iter 20150: Val loss 2.225, Val took 3.394s\n",
      "Iter 20150: Train loss 0.628, Learning Rate 4.000e-04, It/sec 30.824, Tokens/sec 4269.163, Trained Tokens 3337202, Peak mem 19.940 GB\n",
      "Iter 20160: Train loss 0.732, Learning Rate 4.000e-04, It/sec 1.604, Tokens/sec 372.823, Trained Tokens 3339527, Peak mem 19.940 GB\n",
      "Iter 20170: Train loss 0.643, Learning Rate 4.000e-04, It/sec 2.558, Tokens/sec 348.700, Trained Tokens 3340890, Peak mem 19.940 GB\n",
      "Iter 20180: Train loss 0.659, Learning Rate 4.000e-04, It/sec 1.978, Tokens/sec 355.402, Trained Tokens 3342687, Peak mem 19.940 GB\n",
      "Iter 20190: Train loss 0.618, Learning Rate 4.000e-04, It/sec 2.814, Tokens/sec 325.561, Trained Tokens 3343844, Peak mem 19.940 GB\n",
      "Iter 20200: Val loss 2.325, Val took 3.058s\n",
      "Iter 20200: Train loss 0.696, Learning Rate 4.000e-04, It/sec 37.692, Tokens/sec 3678.744, Trained Tokens 3344820, Peak mem 19.940 GB\n",
      "Iter 20200: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0020200_adapters.safetensors.\n",
      "Iter 20210: Train loss 0.632, Learning Rate 4.000e-04, It/sec 2.369, Tokens/sec 336.882, Trained Tokens 3346242, Peak mem 19.940 GB\n",
      "Iter 20220: Train loss 0.723, Learning Rate 4.000e-04, It/sec 2.199, Tokens/sec 352.067, Trained Tokens 3347843, Peak mem 19.940 GB\n",
      "Iter 20230: Train loss 0.668, Learning Rate 4.000e-04, It/sec 2.071, Tokens/sec 382.548, Trained Tokens 3349690, Peak mem 19.940 GB\n",
      "Iter 20240: Train loss 0.700, Learning Rate 4.000e-04, It/sec 2.690, Tokens/sec 358.087, Trained Tokens 3351021, Peak mem 19.940 GB\n",
      "Iter 20250: Val loss 2.761, Val took 3.921s\n",
      "Iter 20250: Train loss 0.729, Learning Rate 4.000e-04, It/sec 23.304, Tokens/sec 3733.357, Trained Tokens 3352623, Peak mem 19.940 GB\n",
      "Iter 20260: Train loss 0.717, Learning Rate 4.000e-04, It/sec 3.259, Tokens/sec 330.103, Trained Tokens 3353636, Peak mem 19.940 GB\n",
      "Iter 20270: Train loss 0.775, Learning Rate 4.000e-04, It/sec 1.901, Tokens/sec 342.713, Trained Tokens 3355439, Peak mem 19.940 GB\n",
      "Iter 20280: Train loss 0.763, Learning Rate 4.000e-04, It/sec 2.137, Tokens/sec 373.388, Trained Tokens 3357186, Peak mem 19.940 GB\n",
      "Iter 20290: Train loss 0.756, Learning Rate 4.000e-04, It/sec 2.002, Tokens/sec 376.736, Trained Tokens 3359068, Peak mem 19.940 GB\n",
      "Iter 20300: Val loss 2.730, Val took 3.710s\n",
      "Iter 20300: Train loss 0.746, Learning Rate 4.000e-04, It/sec 44.057, Tokens/sec 5837.554, Trained Tokens 3360393, Peak mem 19.940 GB\n",
      "Iter 20300: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0020300_adapters.safetensors.\n",
      "Iter 20310: Train loss 0.801, Learning Rate 4.000e-04, It/sec 2.183, Tokens/sec 377.603, Trained Tokens 3362123, Peak mem 19.940 GB\n",
      "Iter 20320: Train loss 0.832, Learning Rate 4.000e-04, It/sec 2.073, Tokens/sec 378.045, Trained Tokens 3363947, Peak mem 19.940 GB\n",
      "Iter 20330: Train loss 0.796, Learning Rate 4.000e-04, It/sec 2.185, Tokens/sec 384.306, Trained Tokens 3365706, Peak mem 19.940 GB\n",
      "Iter 20340: Train loss 0.728, Learning Rate 4.000e-04, It/sec 2.008, Tokens/sec 380.072, Trained Tokens 3367599, Peak mem 19.940 GB\n",
      "Iter 20350: Val loss 2.689, Val took 4.142s\n",
      "Iter 20350: Train loss 0.780, Learning Rate 4.000e-04, It/sec 59.378, Tokens/sec 8645.437, Trained Tokens 3369055, Peak mem 19.940 GB\n",
      "Iter 20360: Train loss 0.746, Learning Rate 4.000e-04, It/sec 2.260, Tokens/sec 370.189, Trained Tokens 3370693, Peak mem 19.940 GB\n",
      "Iter 20370: Train loss 0.968, Learning Rate 4.000e-04, It/sec 1.730, Tokens/sec 395.377, Trained Tokens 3372979, Peak mem 19.940 GB\n",
      "Iter 20380: Train loss 0.817, Learning Rate 4.000e-04, It/sec 1.819, Tokens/sec 362.462, Trained Tokens 3374972, Peak mem 19.940 GB\n",
      "Iter 20390: Train loss 0.693, Learning Rate 4.000e-04, It/sec 2.542, Tokens/sec 386.354, Trained Tokens 3376492, Peak mem 19.940 GB\n",
      "Iter 20400: Val loss 2.358, Val took 3.115s\n",
      "Iter 20400: Train loss 0.774, Learning Rate 4.000e-04, It/sec 24.946, Tokens/sec 4150.964, Trained Tokens 3378156, Peak mem 19.940 GB\n",
      "Iter 20400: Saved adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors and ../trained_models/adapters_40_2_4e4/0020400_adapters.safetensors.\n",
      "Iter 20410: Train loss 0.696, Learning Rate 4.000e-04, It/sec 2.704, Tokens/sec 372.347, Trained Tokens 3379533, Peak mem 19.940 GB\n",
      "Iter 20420: Train loss 0.837, Learning Rate 4.000e-04, It/sec 1.848, Tokens/sec 372.128, Trained Tokens 3381547, Peak mem 19.940 GB\n",
      "Iter 20430: Train loss 0.831, Learning Rate 4.000e-04, It/sec 1.347, Tokens/sec 366.359, Trained Tokens 3384267, Peak mem 19.940 GB\n",
      "Iter 20440: Train loss 0.720, Learning Rate 4.000e-04, It/sec 2.123, Tokens/sec 327.168, Trained Tokens 3385808, Peak mem 19.940 GB\n",
      "Iter 20450: Val loss 2.263, Val took 4.040s\n",
      "Iter 20450: Train loss 0.928, Learning Rate 4.000e-04, It/sec 3.264, Tokens/sec 1014.451, Trained Tokens 3388916, Peak mem 19.940 GB\n",
      "Iter 20460: Train loss 0.722, Learning Rate 4.000e-04, It/sec 2.083, Tokens/sec 327.107, Trained Tokens 3390486, Peak mem 19.940 GB\n",
      "Iter 20470: Train loss 0.618, Learning Rate 4.000e-04, It/sec 3.016, Tokens/sec 364.886, Trained Tokens 3391696, Peak mem 19.940 GB\n",
      "Iter 20480: Val loss 2.300, Val took 2.930s\n",
      "Iter 20480: Train loss 0.690, Learning Rate 4.000e-04, It/sec 35.187, Tokens/sec 4760.735, Trained Tokens 3393049, Peak mem 19.940 GB\n",
      "Saved final adapter weights to ../trained_models/adapters_40_2_4e4/adapters.safetensors.\n",
      "\n",
      " Starting Evaluation\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [36:21<00:00, 10.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on 205 test samples:\n",
      "Accuracy: 0.000\n",
      "F1 Score: 0.000\n",
      "Perplexity: 7.522\n",
      "\n",
      "ROUGE Scores:\n",
      "rouge1: 0.093\n",
      "rouge2: 0.030\n",
      "rougeL: 0.072\n",
      "🏃 View run MLX-40_2_4e4 at: http://127.0.0.1:5000/#/experiments/880645134898555871/runs/f84d60f79a2a4c06864a5dd89d44adbd\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/880645134898555871\n"
     ]
    }
   ],
   "source": [
    "# Checl if the output folder exists, if not, create it:\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Output folder {output_path} created\")\n",
    "else:\n",
    "    print(f\"Output folder {output_path} already exists\")\n",
    "\n",
    "\n",
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "if optimizer == \"adam\":\n",
    "    opt = optim.Adam(learning_rate=learning_rate_value)\n",
    "else:\n",
    "    opt = optim.AdamW(learning_rate=learning_rate_value, weight_decay=weight_decay_value)\n",
    "\n",
    "# Make a class to record the training stats:\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "        try:\n",
    "            mlflow.log_metric(\"train_loss\", info[\"train_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log train metric to MLflow: {e}\")\n",
    "            \n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        self.val_accuracies.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        # log validation loss\n",
    "        try:\n",
    "            mlflow.log_metric(\"val_loss\", info[\"val_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log validation metric to MLflow: {e}\")\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "with mlflow.start_run(run_name=corrida_name):\n",
    "    mlflow.log_params({\n",
    "        \"num_train_epoch\": lora_config[\"lora_parameters\"][\"epochs\"],\n",
    "        \"max_steps\": training_args.iters,\n",
    "        \"lora_r\": lora_config[\"lora_parameters\"][\"rank\"],\n",
    "        \"lora_dropout\":lora_config[\"lora_parameters\"][\"dropout\"],\n",
    "        \"lora_layers\":lora_config[\"lora_layers\"],\n",
    "        \"lora_layeres_scale\":lora_config[\"lora_parameters\"][\"scale\"],\n",
    "        \"batch_size\":training_args.batch_size,\n",
    "        \"optimizer\":optimizer,\n",
    "        \"learning_rate\":learning_rate_value,\n",
    "        # \"weight_decay\": weight_decay_value,\n",
    "        \"scheduler\":lr_scheduler\n",
    "    })\n",
    "\n",
    "    # Train model:\n",
    "    train(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        optimizer=opt,\n",
    "        train_dataset=train_set,\n",
    "        val_dataset=valid_set,\n",
    "        training_callback=metrics,\n",
    "    )\n",
    "\n",
    "    print(\"\\n Starting Evaluation\")\n",
    "    # Evaluate model and log metrics in the same run\n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, f1, perplexity, rouge_scores = evaluate_model(model, tokenizer, test_set[:num_test])\n",
    "\n",
    "    # Log final metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": metrics.train_losses[-1][1],\n",
    "        \"final_val_loss\": metrics.val_losses[-1][1],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"rouge1\": rouge_scores['rouge1'],\n",
    "        \"rouge2\": rouge_scores['rouge2'],\n",
    "        \"rougeL\": rouge_scores['rougeL']\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nResults on {num_test} test samples:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\") \n",
    "    print(f\"Perplexity: {perplexity:.3f}\")\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for key, score in rouge_scores.items():\n",
    "        print(f\"{key}: {score:.3f}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved every 100 iterations along with the final adapters in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac329358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000100_adapters.safetensors 0001200_adapters.safetensors\n",
      "0000200_adapters.safetensors 0001300_adapters.safetensors\n",
      "0000300_adapters.safetensors 0001400_adapters.safetensors\n",
      "0000400_adapters.safetensors 0001500_adapters.safetensors\n",
      "0000500_adapters.safetensors 0001600_adapters.safetensors\n",
      "0000600_adapters.safetensors 0001700_adapters.safetensors\n",
      "0000700_adapters.safetensors 0001800_adapters.safetensors\n",
      "0000800_adapters.safetensors 0001900_adapters.safetensors\n",
      "0000900_adapters.safetensors 0002000_adapters.safetensors\n",
      "0001000_adapters.safetensors adapters.safetensors\n",
      "0001100_adapters.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls ../trained_models/adapters2k/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training and validation losses to see how well the adapters fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x33f8b6d50>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC0Z0lEQVR4nOydd3gU1frHvzOppAcSEghFio0igjUKhiKC14JRQESviIIien8CFkBRQKVZAK+KXsGCDQUxYKMIQkCNqPSiIk0gpHdC+pzfH5uZzOzOzM7szpYk7+d59kl2ZnbmTDvnPW/lADAQBEEQBEE0EXhfN4AgCIIgCMJKSLghCIIgCKJJQcINQRAEQRBNChJuCIIgCIJoUpBwQxAEQRBEk4KEG4IgCIIgmhQk3BAEQRAE0aQI9HUDfEHbtm1RVlbm62YQBEEQBGGCyMhInDlzxul2zU64adu2LTIzM33dDIIgCIIgXCApKcmpgNPshBtRY5OUlETaG4IgCIJoJERGRiIzM9PQ2N3shBuRsrIyEm4IgiAIoglCDsUEQRAEQTQpSLghCIIgCKJJQcINQRAEQRBNimbrc0MQBEEQVhIYGIg2bdqA50lv4AqCICArKwu1tbVu74uEG4IgCIJwk9atW+PFF19EaGior5vSqKmsrMQzzzyDvLw8t/ZDwg1BEARBuAHHcRg3bhzOnj2LV155BVVVVb5uUqMkJCQEEyZMwPjx4zFv3jwwxlzeFwk3BEEQBOEGMTExuOiii7BkyRIcPnzY181p1KxcuRITJ05EdHQ0iouLXd4PGQYJgiAIwg0iIyMBALm5uT5uSeNHvIZRUVFu7YeEG4IgCIJwA47jAAB1dXU+bknjR7yG4jV1FTJLWURAYCBG3n8LOnZui3+OncHK975GnQUe3wRBEARBmIOEGwuYNHMcZk67FdGhDZLmktfGYvb8r7B49jIftowgCIIgmh9klnKTSTPH4dWZwxAVolShRYVweHXmMEyaOc5HLSMIgiAaExzPo8vlvdH7xsHocnlvcI0wX87x48fx2GOP+boZpLlxh4DAQMycdisAwN48yHEAY8BzU2/F63M+IBMVQRAEoUnPQSm4bdpkxCQmSMuKs3OwZv4i7N+cbvnxnIVZz5o1C7Nnzza93yuuuALl5eWuNssyGp9Y6EeMvP8WRIdyDoKNCMcBMS04jLz/Fu82jCAIgmg09ByUgjEL5yG6dbxieXTreIxZOA89B6VYfszExETp89hjj6GkpESx7JVXXlFsHxAQYGi/+fn5qKiosLy9ZiHhxg06dm5r6XYEQRBE0yC4RaihT0h4GG6bPgUAHMxQtu8Mt02bjJDwMEP7M0pOTo70KSkpAWNM+n7RRRfh7NmzGDp0KH7//XdUVVWhb9++6Ny5M9asWYPs7GyUlZXh119/xaBBgxT7tTdLMcbwwAMP4Msvv0R5eTkOHz6MW27x/ISfzFJu8M+xM5ZuRxAEQTR+gluEYt6vWyzZF8fziElMwNxfNhvafvqVA1BdUWnJsefPn48nnngCx44dQ1FREdq3b4/vvvsOzzzzDKqqqnDvvffi66+/xoUXXohTp05p7mfmzJl46qmn8OSTT+I///kPPvnkE3Ts2BFFRUWWtFMN0ty4wcr3vkZJJYOW6ZIxoLiCYeV7X3u3YQRBEAThJs899xw2bdokCTf79u3DO++8g4MHD+LIkSN47rnncPToUdx66626+/nggw/w2Wef4ejRo3j66acRGRmJK6+80qNtJ82NG9TV1uLNFbsx/b4+YEzpVCwKPEs+203OxARBEM2I6opKTL9ygKFtO/XphQffXux0u3cmTMLxXXsNHdsqfv/9d8X38PBwzJo1CzfddBPatGmDwMBAtGjRAh06dNDdz759+6T/z507h5KSErRu3dqydqrhU83NhAkTsHfvXpSUlKCkpAQ///wzhg4dqrn9mDFjwBhTfHzpuMTxPKra98LX/0TgbI3yUpbV8Pj6nwhUtrukUYbzEQRBEK5TXVFp6HM44zcUZ+eACYLqfpggoCgrG4czfjO0Pyuxj3p65ZVXkJqaiqeffhr9+vXDpZdeiv379yM4OFh3PzU1NcpzYgy8h8dFn466p0+fxrRp03DZZZfh8ssvxw8//IC1a9eiW7dumr+x9+ju2LGjF1uspHOfXohJTMDRs6F493AsauufzW9PRuC9w7E4ejYUsW0S0blPL5+1kSAIgvBfmCBgzfxFADgHAcf2ncPaBYs1hR9vcu211+KDDz7AmjVrcODAAWRnZ+O8887zdbNU8alw880332DdunU4cuQI/v77b8yYMQNnz57F1VdfrfkbuUd3Tk6O00JlwcHBiIyMVHysIio+rqFd4FAt2OxSBVWBYOBUtyMIgiAIOfs3p2P5lOkoyc1TLC/OycXyKdM9kufGFf7++2/cfvvt6NWrFy655BJ8+umnHtfAuIrf+NzwPI8RI0YgPDwcGRkZmttFRETgxIkT4Hkeu3btwtNPP41Dhw5pbj99+nTMmjXLAy0GSvPyFd9rBA4AQxDPdLcjCIIgCDn7N6fjwJbt6NynF6Li41Cal49ju/b6hcZGZMqUKXjvvffw888/Iz8/HwsWLHC7eren8Llw06NHD2RkZCA0NBRnz55Famoq/vjjD9Vt//rrL9x///3Yt28foqOj8cQTT+Dnn39G9+7dkZmZqfqbefPmYeHChdL3yMhIzW3NcmzXXhRn5yC6dTw4npc0N8H1wg0TBBTn5OKYAScwgiAIonnDBAFHf9/t9eMuX74cy5cvl76np6erVuX+559/HPLaLFmyRPG9U6dOiu9q+4mNjXWnuYbwuT7pr7/+wqWXXoqrrroKb731FpYvX46LL75YddtffvkFH330Efbu3Ytt27bh9ttvR15eHh566CHN/VdXV6OsrEzxsQp7W2lNvXATxDO/s5USBEEQRHPB58JNTU0Njh49KpmY9u7da7joVm1tLXbv3o2uXbt6uJXayG2lcuHG32ylBEEQBNFc8LlwYw/P8wgJCTG8bc+ePZGVleXhVumzf3M6XhxyuyTcVBUVYs7QO0iwIQiCIAgf4FOfm7lz52LdunU4efIkIiMjMXr0aPTv3x9DhgwBYLMDZmZm4umnnwYAPPvss/jll19w5MgRxMTE4Mknn0THjh2xbNkyX54GACjMUrxQQ6YogiAIgvARPhVuWrdujQ8//BBt2rRBSUkJ9u3bhyFDhmDTpk0AgA4dOkCQCQmxsbFYunQpEhMTUVRUhJ07d+Kaa67RdED2NtV19Q7FPnfTJgiCIIjmi0+H4XHjxumuHzBAmb56ypQpmDJliieb5Bai5iYk0NE7nCAIgiAI7+B3PjeNGVG4CSbhhiAIgiB8Bgk3FlJdb0EjzQ1BEARB+A4SbiyEzFIEQRBEc2HLli1YtGiR9P348eNOU7kwxjBs2DBPN42EGyupqdfctIoIQEpKD7+tuUEQBEH4HzzPIyWlB0aNus7jY8hXX32FdevWqa7r27cvGGPo2bOnqX1eccUVeOedd6xontvQ6GsRqanJ6N/mHACgbUwgtmydh+MnliE1NdnHLSMIgiD8ndTUZBw/sQxbts7Dpyue9PgY8u6772Lw4MFISkpyWDd27Fj89ttv2L9/v6l95ufno6KiwqomugUJNxaQmpqMVV9MR2iAsmBmUlIrrPpiOgk4BEEQhCbiGJKUFKdY7skx5JtvvkFeXh7uu+8+xfLw8HCMGDECa9aswaefforTp0+jvLwc+/btw6hRo3T3aW+W6tq1K9LT01FRUYGDBw/i+uuvt/w8tCDhxk14nsfi18YDAOzrg9lUigyLFo8nExVBEEQzIiwsxNAnIqIFXvvvgwAAnlcOIuIYsvi1BxER0cLQ/oxSV1eHDz/80EG4GTFiBAICAvDxxx9j586duOmmm9CjRw+88847+Oijj3DFFVcY2j/Hcfjyyy9RXV2Nq666ChMmTMCCBQsMt89dKN2cm/Tr1w3t28drrud5Hh06xKNfv25ITz/gxZYRBEEQviAsLARny7+wZF88z6N9+ziUlq00tH1E+HCcO1dlaNv33nsPTz31FFJSUpCebisXNHbsWKxevRonT57Eq6++Km37xhtvYMiQIRg5ciR+++03p/u+/vrrcdFFF2HIkCFSiaSnn34a69evN9Q2dyHhxk3atGlp6XYEQRAE4Q3++usv/PTTT7j//vuRnp6OLl264LrrrkP//v3B8zyefvppjBw5EklJSQgODkZISAjOnTtnaN8XX3wxTp06paj9mJGR4alTcYCEGzfJyiq0dDuCIAiicXPuXBUiwocb2rZfv+5Yt3620+1uHDoT27cfNHRsM7z77rt4/fXX8cgjj2Ds2LE4cuQI0tPTMXXqVDz22GOYNGkS9u/fj/LycixevBjBwcGm9u8rSLhxk+3bD+HUqTwkJbVS9asRBAGnTxdg+/ZDPmgdQRAE4QuMChnff7/H0Bjy/fd7FLUWrWLlypV47bXXMHr0aNx777146623AADXXnst1q5di08++QSAzYfmggsuwKFDxsayP/74A+3bt0diYiKys7MBAFdffbXl7deCvFzdRBAETHpsKQAOjDmuAzhMnrTUIw8lQRAE0biRjyH244Q3xpDy8nJ8/vnnmDdvHtq0aYMPPvgAAPD3339j8ODBSE5OxkUXXYT//e9/SEhIMLzfTZs24fDhw1i+fDkuueQS9O3bF3PmzPHIOahBwo0FpKVlYMTweThbo7ycp08XYMTweUhL856dkSAIgmhciGNIZmaBYrm3xpB3330XLVu2xIYNGyQfmRdffBG7du3Chg0bsHXrVmRnZ2PNmjWG98kYQ2pqKlq0aIFff/0Vy5YtwzPPPOOhM9BoQ3P6REZGMsYYi4yMtHzfC/f/zKrrvmYC+5qNGnUd43ne5+dLH/rQhz708eynY8eO7MMPP2QdO3Z0az88z7OUlB5s1KjrWEpKj2Y5huhdSzPjN/ncWAgDhxqBQyDPUFQXCubrBhEEQRCNBkEQKGWIRZBZyiJ6DkoBANQyWxKme+Y+ixkbvpSWEwRBEAThHUi4sYCeg1IwZuE8AEBtvc9XIM8Q3ToeYxbOIwGHIAiCILwICTduwvE8RsycBtRnzRY1N4EcA1efOnvY1En1/xMEQRAE4WloxHWT68ePQXhsDLj6wlK1Qr1ww9s8bjieR2ybRHTu08tnbSQIgiA8B6vPAxIQEODjljR+xGvI7HOrmISEGzfgeB797rlTsay2/n4E2hXRjIpXVnslCIIgmgZlZWUAgNatW/u4JY0f8RqWlpa6tR+KlnKDzn16ITwmWrHMXnMjUpqX77V2EQRBEN6juLgYf/75J0aOHInCwkJUVZkrgUDYCAkJwciRI/Hnn3+ipKTErX2RcOMGatoYSbjhGoSbyvJyHNu112vtIgiCILwHYwxLly7FnDlzMGPGDF83p1FTWVmJefPmuW2WIuHGDdS0MaJDcYDM4Meo9AJBEESTJi8vDxMnTkRiYiL53rhIXV0dsrOzUVtb6/a+SLhxg2O79uJsYSEiWraUltWJPjcys1SLyEh07tMLR3/f7e0mEgRBEF6itrYWp0+f9nUzCJBDsVswQcDObzYolqmZpQByKCYIgiAIb0HCjZsc3LJd8b3BoVi5HTkUEwRBEIR3IOHGTY7t2ovi7BzJr6YhFNz2DxMEFGVlk0MxQRAEQXgJEm7chAkC1sxfBIADY0wRCm7z9uawdsFiciomCIIgfAbP80hJ6YFRo65DSkoP8E08az45FFuFQ/kFgOM4MDB0uKQ79m9O92HjCIIgiOZKamoyFr82Hu3bx0vLTp3Kw6THliItLcOHLfMcTVt08wIcz+O2aZMBxsBxHOpkhTNFBoy9Bz0HD/BRCwmCIIjmSmpqMlZ9MR1JScqglqSkVlj1xXSkpib7qGWehYQbN+ncpxdiEhOkwpjywpmATXvDcRzueOYJKp5JEARBeA2e57H4tfH1/3MO6wCGRYvHN0kTVdM7Iy9jH+KtVX4hslVLKp5JEARBeI1+/bqhfft4B8FGhOd5dOgQj379unm5ZZ6HhBs3sQ/xFqOlAlSeJcp1QxAEQXiLNm1aOt/IxHaNCRJu3ETMUiyipbkBKNcNQRAE4T2ysgqdb2Riu8YECTduwgQBq194GYzZQr/tfW4AW1E1ynVDEARBeJPt2w/h1Kk8CBqpSARBwMmTedi+/ZCXW+Z5SLixgH2btuLAD7ZQbzFaKjSAoV14NcBsC3av+55y3RAEQRBeQxAETHpsKcQ8bPbrAA6TJy3VFH4aMyTcWADH82jf/WJ0jarCsI6lAIAWgQwjOpVi3IXF6BpVhd43DqZoKYIgCMKrpKVlYMTweSgvr1IsP326ACOGz2uyeW4oiZ8FdO7TC5dfEIOb25c5rIsIEnBLh7PguFiqDE4QhFvwPI9+/bqhTZuWyMoqxPbth5rkrJuwlrS0DNy8chvG3n8DAGBA/+lN/tkh4cYCYhNbo39iOQCAs4uS4jiAMSAlsRyxia190DqCIJoCzTHLLGElDYNTevoBH7bDO5CdxAKSr74AkcGCg2AjwnFAVLCA5Ksv8G7DCIJoEjTXLLOEhWgNUE0UEm4sIDzImGrP6HYEQRAizTnLLGEdWon8mir0NljAP0fPWLodQRCESHPOMktYB0eaG8IsK9/7GiWVDMwxb5+EIDBUF+Z5r1EEQTQJmnOWWcI6SLghTFNXW4vZ878CAE0Bh+OAzz9/imzjBEGYojlnmSWso5nJNr4VbiZMmIC9e/eipKQEJSUl+PnnnzF06FDd3wwfPhx//PEHKioqsG/fPtx4441eaq0+i2cvw5JvT0JLeWOTmsk2ThCEOZpzllnCOprbuOPTsz19+jSmTZuGyy67DJdffjl++OEHrF27Ft26qduOk5OTsWLFCrz77rvo3bs31qxZgzVr1qB79+5ebrkjN02eiPhuPaDns0W2cYIgzNKcs8wS1kGaGy/yzTffYN26dThy5Aj+/vtvzJgxA2fPnsXVV1+tuv1jjz2G9evX45VXXsGff/6J5557Drt27cKjjz7q5ZYr4QMD0X/MaIQHGutcyDZOEIQZxCyzpSXnFMubepZZwjrI58ZH8DyPO++8E+Hh4cjIUH9Rk5OTsWnTJsWyDRs2IDlZ248lODgYkZGRio/VXHvn7eADAnCuztjlJNs4QRBmSUvLwMKFawAAhw6dxID+09G50zgSbAhDkHDjZXr06IGysjJUVVXh7bffRmpqKv744w/VbRMTE5GTk6NYlpOTg8TERM39T58+HaWlpdInMzPT0vYDQFyHdgCAzPIglFXzmk7FjAFnskvINk4QhEsEBNi67IKCMqSnHyBTFGEYynPjZf766y9ceumluOqqq/DWW29h+fLluPjiiy3b/7x58xAVFSV9kpKSLNu3SP7J0wAABg5bs8NVtxEFnve+PkwdEkEQLiEKN+JfgjCKXHPTHJ4fn9eWqqmpwdGjRwEAu3btwhVXXIHHHnsMEyZMcNg2OzsbCQkJimUJCQnIzs7W3H91dTWqq6utbbQdpbl5YIyB4zgcKQ3B8bJKdI6qUWxTVsMjPTscP+3N8mhbCIJougQGBgBoHoMTYS1y4SYwMAB1dU17ku13bwjP8wgJCVFdl5GRgUGDBimWDR48WNNHxxtwPI9bn3pMkeAmr1IpM/5ZHIz3DsfiSGkIjv62y9tNJAiiiSAKNaKQQxBGkbvcNAfh2Keam7lz52LdunU4efIkIiMjMXr0aPTv3x9DhgwBACxfvhyZmZl4+umnAQCvvfYa0tPTMWXKFHz77bcYNWoULr/8cjz44IM+O4fOfXohJlGpTaoRlLbNyjoeAgPKi4tx9Pfd3mweQRBNCE9qbnieR79+3dCmTUtkZRVi+/ZDZEJvQsjz3JBw42Fat26NDz/8EG3atEFJSQn27duHIUOGSBFRHTp0ULxcGRkZGD16NF588UXMnTsXf//9N2677TYcPHjQV6eAqPg4h2X2wk0wb9PqfDF7Phh1FgRBuIinfG5SU5Ox+LXxaN8+Xlp26lQeJj22lKKxmgikufEi48aN010/YMAAh2VffPEFvvjiC081yTSlefkOyxyEmwCGDW8uxf7N6d5qFkEQTRBPCDepqclY9cV0h+VJSa2w6ovpzTaPTlPTZMk1N83BrOlzh+LGzrFde1GcnYPo1vHg6h8ee+GGq6nEpqXLfdE8giCaEOKgZNXgxPM8Fr82vv5/zmGdIAhYtHg81q7d0agHdrM0RU2WXCBuDpqbpn+GHoYJAtbMXwSAk0xODsLN2SLcObIvUlJ6NLv6HgRBWIfVmpt+/bqhfft4zRwozbFkjKjJSkpSuhyImqzGWvxYLhCTcEMYYv/mdCyfMh0luXkAgGo74aZzx1b4dMWT2LJ1Ho6fWNZoXw6CIHxLgMUOxUZLwTSXkjHONFmNufhxUJBcuGn6ZqnGd4f8lP2b0/HikNuxZOxEHPz5N83tGrv0TxCE77A6FNxoKZjmUjKmKWuygoIavFACA5v+0N/0z9CLMEHA0d93I/PIcc1tGrv0TxCE77DaLLV9+yGcOpWn6U8jCAJOnsxrNiVjmrImSy7QkFmKME1qajJefaK/7jaNWfonCMJ3WJ3nRhAETHpsKQAOzK4onk3g4TB50tJm40zcmDVZPM8jJaUHRo26TtW/U665IbMUYQrRES0qPNjQ9o1R+icIwnd4IhQ8LS0DI4bPQ1lZhWL56dMFzS4MvLFqslJTk3H8xDJs2TpP07+THIoJl5A7ohktLe+P0j9BEP6LaFqwOk9JWloGPvpws/T93nsXonOncc1KsAGUmixBaByaLKPRXUqH4qY/9Df9M/QSzhzR5Pir9E8QhH8jmhM8MTiFhbeQ/v/78Bm/GsC9iajJys0tViz3R02WmeguuUBMSfwIwxg1Mdns2v4n/RME4f94qvwCAERGNgg3sbERlu+/MZGWloHTp/Ox49eFAIC7R7+Mzz//0e/6bHFSrYXcv1Ppc9P09Rok3FiEcRMTh3c//hFr1u7waHsIgmh6iGYpTws3MTHhlu9fDX8ucSAXBnbuPOo37ZJjJrqruUVLkXBjEdu3H0JWTgkSW0fDmcvNnaOuQ2bbZKTNX0T1pgiiCeKpQdvqPDdyIiJCpf+9obnx9xIHoaFB0v/+KgyYie5qbpqbpn+GXkIQBLy67Cen23EcEBUsoFunGIxZOA89B6V4oXUEQXgLI5ErZpCH+LZqFSUtsxpvam4aQ4mDkJAG4cZffVScRXcBQG1tHVrFRTU7nxsSbizktz8KsKsg1PmGACKCAYBh2NRJUsFNgiAaN1YP2vaCUq9enaR1Vs++IyK8I9w0lhIHoaENKT38VRjQi+4S4XkeK1dOQ4sWDefj6rPjLJeOP+G/LWuElBcV41iZsRw35bUcOJ5HbJtEdO7Ty8MtIwjC01g9aGsJSiK3336NW+21R665uezyrh4bvBpLiQOl5sZ/h8q0tAzcOXK+QxJGEdt1ZmjRIkRa5opwo6WRnDHjTr8UdsjnxkJatU9CZnkQyqp5RAQJqr43jAFlNTwyyxtenKh49c6LIIjGg9HIlZSU7hAEpuuPoycoibzy6gNYvfpnyxxdo6PDpP8HDuyFgQN7ecQHprGUOGgMZimR/PxSXYHFXugwK9yIgrY97drF4fkX7pG++5PPlP+IWY0cjudx9fBhEBiwNdum0rUXpMXv6dnhYGjosErz8r3VTIKQaEwq5saA0cF45appTv1xjOTNat8+zjLtxvDh1yI4OMhhuSd8YBpLiQO5Q7G/CzdmBUEz56MnaNsnrPUnnynqzSyic59eiElMAMdxOFIagm9OReJsjePl/eZUBCrrOFwYXYWkFtUozsrGiT37aZAhvIrVTq+E8cG4ZctIxXe1AcGb2g2e57Fo8XjNdVb7wDSWEgdyzY2/RxeZFQQDAnjDkxszCWr9yWeKzFIWYW9aOlIagqOlwUgKr0FMcB0GJ5WD44D+iecQGdzwUhfF1eA/WcsRFxclLfMn1R7R9NBSMYuDrC+zsPpz3hNniIN2UlIr1Y5d9Imwn+3yPA9BELBo8XisXbsDgiB4VbvRr183JCW10lwv94FJTz/g9vFEJ9hVX0yHIAiKa+VPJQ70HIr97Tndvv0QiovPIiZGPYTf/jpfc83FeP2NhwyF4ZsVoK1+XlzFv8XRRoSaaYmBw+nyYPxR3BBBFRGkfAFiokLRqpXzmRxBWIE/R6p4SpvkLfObIAhY8ek21dpygsDAcZxm3Tl7J1ojIb6ZmQWWaDd84QMjljjIzCxQLPenEgdaPjf++JwKgoDvN+7RXAcon7snn7rdcESfqwK0r32mSLixiGO79qK8uER1ncCY5G9j37epdXi+HmSIpou/Rqp4Ku+JN81vqanJeOLJ2x187QDH914LcUBQhviqCzizZn5iibaga9c2hraz2gcmLS0Dnc4bp1jmT8U6lT43tn7Yn5/TM2cKVJefPl2AMWMWOiw3OrkxImirYfS58hQ0cloEEwRs/2Sl6ror4ysMd24i/hIOSTQt/DFSxVPaJG8minMW3aQVpmuPXIDQ0m6IbNSYqZshNTUZs2bfrds+V31gjGgi7AdMT5t2zGhH7DU3/v6cykP5RRa+mobOncZhw/pdiuVGNYiAsVw69jDGMG78EJ9Ozkm4sZDju/Y6LOPA0LtVpcv79LVqj2ha+GOkiie0SZ4aiLQGRyPnAEBzcNASINS0GyLuOrnKr5HWYOdqoV9/dFg32yZ7nxt/f04jo8Iclp04kQtBEExHe6mNO4WFpYacigHb8+TryTkJNxYSGefolJcUXoMWgcakXTV8HQ5JNC38MVLFE9okTwxEeoOj0bZxnJa2QluA0LpX7iaWMxIFw3EcZs38xJSpyB9LK7jSJvtoKX9/TqNUhJvgYFvMkLyulBHk44547Vq2jNL5hTq+nJyTcGMhEbExDsvCXRRs/CUckmhayFXM9qYI+0HWW464ntAmWT0QORscjfoXzHzuE8ucaAMC3Mu9YvTcjxzJMrxPf3RYd7VNIXZ5bnz5nA4c2MvpNVMTbkShxl4Q1jJD2o87RpJJ6uFLvxsSbiykvKjYcVmt+QcCsM2YPluxzefhkETTQ/TlqKysViyXD7KeLP5oLyh5Qptk5UBkZHAcN34ITp3Kd3oOc+euUpiZfv75D5edaN01S3lisHZFE1FXV2d4/67gqnbE3ufGl8/ps8+Ncvr+qQk3F16YhFGjrsO11zpqfoxoEM3kuLHH1343JNxYSElunsOyzPIgVNRyqhEUImrrGAOeePJ2CgdvZnhLW5KWloENG3ZL3wf0ny4Nsp4u/mgvKAmCgMmTl4HjnGuTjGLlQGR0cFz6znoY0YjJ25SfX+ryBMZd4cYTg7UrGrPaWs9O4FzV4tkLN2a0nkYxE4mk9/7xPI927RzdIu4bez0+XfEkPvxoimL5u8s2IidHGd2rpkF0x6zka78bEm4s5NiuvSjOzlE8+F2iqhEaYN40JRY7o3Dw5oNV2hKjAlJNTa30f3r6AckU5Y3ij/KOOjU1GYsWjVNNi+CqycbKgciM+WbE8Hk4d65KsVzvHIxEUWk5g1pREmDpOxtUhUob5gdrV7RBtbWua26MPOuuaqiUDsW2/YpaT3s88Zzao/X+paYmIzvnQ0RHG6/kfuDAPxh+x1zp+0sLvlDVIFphVvKV3w2NmhbCBAFr5i+SvnNg6J9YbvtfR6untY7Cwf0fqzQtVmlLzAhINTUNg4ooVFjp4GhEUHr7f49oVr5mjOHxKcukDtfstU5Ly8DIkfN1zW96bReP1Toh2um5ArbBMS0tA+vW7ZSWFRaWYex9ixESEqTaZiOhtWFhIarL3dHciM/J8y/co5lc8InH3zU9WBvRRNTW1qGVLCO7q8KN0WfdVQ2VVhK/tLQMxfWSaz1dQRSYjGQLEN+/mTPvQkpKD9x+xzVY9cV0tGplztk3MDBAETp+6lQ++vXrpni3eJ7H+AeHGE5joIWvgmKo/ILF7N+cjm0ff46Uf49CUniNotSCq1A4uH+SmpqMxa+Nd0hhvvSdDThyJMtwWnZnQoB9an699pgpqyDX3LRoEYxz56osdcQ1UiU7Pj4ajDHVwZUx4NWF45CW9guGDbtK9VrrlSkRNUItWjQIBwUFpQqBSet39seqra2r7/DVsg8LOH26IVuwGKECANHR4dj8wxyHNjeco/eFG63nRGxPcXE5YmMj8NNP5oMZ5KUVtO4rz/NYuXKa9DzW1ZnvI808666We5An8dO71laUGEhLy8DZsxWqfjNqPPvcKDz73CjU1taB47RD+bU4//y2OH26Iav+Cy/eoyjdIPZjeu+vM+zfC29DmhuL6TkoBVem3gzA9Ugpe1onRDudrVKFZ++ipWlp1y4Oz79wjymzkhltidZ9NqIlWfLWRNw1OkX6XZ1sxhwebisRkpNTbOj8jczGjIdHa523zWb/9NMjTGu1tO5PbGwkPl85TfOeaP2O5zlwnKMwojY4ymf89vdCbLOIkUmxlcKN7Tl5EBynlWwQiIiwPQvy8zBDWloG7hw5X6cNSpO7XHMTGBjotB9zxXQqakfsn289LZ6W5sYeq/pdVzRYgYEBpgUbAIiOCVeYsexNWklJrTBr9t2G92eVD5KVkObGQnoOSsGYhfOkMh6uRkqJCIIAxoDFix+UlqnNVrU0CFR80zPoda72HY2RYpRGhYBbb70KH340RfU+FxaWOdWSJCTE4JNPnpB+9/ffDSG+YWEh0nOkh5nZmFXq6Mcm3QrAuFZLf/DjNDVhRjRoaj5Bkycp3zP5oKhVJFPsJIx0/FrCjSs+N08/PQLt2zuaABvax4HnbcOC3OfELPn5pbqDrlxglw/qx44vRbt2De1T68eMaATVCjempWXg77/PYN/+NwAAd416GatW/ah5D/QKZ8rZsrXBB8edfld+Hew1TFZTXl6J2NgGTY2z59QZpSXnEB3TICCpvRfehqb3FsHxPG6bNhlAgyo2szwIZdW8odmZPWJHaj87S0qKU8xW/TFhVlPHTHikESdco0LApMnDNO/zrbdeZWgf8t8NGNBT+j5s2FWavi8iopnB6GzMiK+DEVq1inKq1UpJ6S7NoB/9z00u+Q0Z0aDJB4Fbb3le1dciLk7f/0H+HHhTcyOWWjCK3CxjFjPmTXm0VNu2yogftX7MHdOpXEjZufNv3WdQLqSef35bQ9oZM/2uvRZWqbkxXurAFU6fykfPnh2dtg/Q9gtjjEnO888++7G0XOu98DYk3FhE5z69EJOYAE7ecYHD1mybNKsfCu64Uk9VDwCLFo9HYGCg3yXMag6Y9YFy5oRrRAiora0DY9r3efTd/U23Sc70p0fWL9cX2PLzS7F27Q5Dx9CrSSPOCvVCoQWBIT+/1NCxVq6aJjmWyjWdetjfR7P39eDBk6pt1xJG1HBHc2NGuJFrpYzijubG1QglI/2YK/sWBYmbbr5CWuZM8yUX7h4Yd4PC1KyF0X5XzRm6desYaf2I4fOQl1us2z53CAjgES87nh5qWbVFDh06BcCmCRKFs99/P+IX+dlo1LOIqHj1Ge+R0hB8cyoSZ2u0L3VZWQUAoKTEFllVXV2rGcEANPgiTHzkRr+s8NzUMeqXYo/W4KlXAVoUAmyF+7Tvc0JCDIqKzppqj/z5SkiIcSrYcByH+PhoU8+TGLFUoREe/dCDb0ArDJbjgP++9pWh47RsGWm4TSL2g6RZM5rcAVOOGRcIbzkUG9U2CgJDRYUtusxVnxvAJrDbR6kpj9MQoRQeri8Miv3Yo/+5CaNGXQee5wwlTBRNp3JB4oUX7pG2GzKkj+bxUlJ6ICZGPbQ6Kckxn4xae7XeEy1tu/x+2vJNzbH/qWV07pyINm1iDW2rllUbAHbuPIrCwjIAtsjLqqoaAO4JxVZCwo1FlObla647UhqCdw/HYtXxKHx3KhKrjkci+5zt0v+aF4ovVv0IADh2LBuAMtpCjy5djOUg8Gl9j0bk6Gykrampyfhg+SSX9q83eGpVgD59ugCvLV5raP8ZP//hUrvMYuZ5EiOWwuudVAGbMC9GLInnXVxc7vDbV175EnPnrtLVajHGdKKtzBWpNGtGa9kyQvV5kYfYG9mPM6zwuTFT++rAgRMA3BukBEGQ+jO1dXJnU6N1jxYvfhCfrngSm3+YixYtgsBx2hpBcd9aggRgi8SzNx/JBSF5lJ0co32Y2jU36q8XGhqsKK+Rk1Os+1zaR5yJE2YtRt/dHz17nqe7jTyr9vldH5KWv/7frwHYIg/Fsaq6ukYSZt0xZ1qJ/440jQwpgZ9WJwwOp8uD8VdJCE6Xh6CwyvYAVNVx6HWlTcK3T/7ljKNHjdV88UaeATXBwB8rA2thpK16HaUeRjO92leAPnkyD507jcNXXxkzA238frfzjSzA6POkdb0iIkIVEUtpaRlShyln186jTpPx6Wk41ZbbBkN1vyEjZjQ5Kz57SvV50dN42O/HyMCuJdwEBQUYnjgY1TZyHIfu3W2+GO4OUuXl6v2ZfYSSK6n9W7aMBGNw0A7l5ZXitcVrUVhYpmu2F5Gbj1x9v7VQe0+MatAGDeqFoKAG4WbJkm91hbl5c1cCAE6cyMGA/tPxyMS3TLXVWbSTvC179x4HYBOuxeXV1bWorLRpbtzR+FkJCTcW0ZDAj9MUcOSIkVTJrStwWc92AKBa/0OL4uJyHDxwEgUFpZoOX94qvqkmGGTnfIgvVvvO0dmMxsiIU7bRAnLuhkTKtwkJCYQgCIaTkC15cx1OncoznHRLvl1hYZnT9pl5npzPUJV+CWomAFEzIWp3RHNJQ5vNmeFsvynTjVwTj1Vg5+tTUFAmmY1FWrVSmsLE50UvV8np0wV49JGGgUc+aGihJdy8s/Q/hiYOZrWNLVrYNDaXXd5Vdb3Rd0tNOHr++RWmnU3Vnmcx55D82hQUlCEhIQaTp9yGLVvn4XTm+07M9pwixYI7BSLl6L0nRjVoHTrEKzRzjz56MziOc2ibKCj+9tvfAIDs7CKkpx9AdXWNG2fgKIDKrQnl5ZUAbM9ucLDtHldX15JZqimzf3M6lk+ZrlpjSk7XqCr0aml7QAJV7gBjTNdTnjGG6OgwfL/pRSmSxFd5BrQEAzFjpi8cnc1ojIzmzEhJ6WFoxlVdXav4bjYtu3wm37JlJFJSegCAU43C5ElLUVtbq0gQJ0f9+Wjgiy9+Ut2/8vfGnyezmY6jVYQb+cCflpaBNWsaruGA/tNx58gFTtthz1NPvo+1a3foDs5paRl48sn3pO8/bj+IVq2iHAQwtfBZwPZuqjFs2Ivo3Gkctm7dLztH1zU3WsKVu9pG8bxuuy3Z4dqYebfUBrnCAkchWn4NzARX2NOypdIHylnUmkibNi3dKhApx1m/a1TrWVJSrnj+tc5FNO+K2pKqKlv/Y98POcP+Gnft8iAKC8ukd0QUeAVBkCYZQUGBMrNULZmlmjr7N6dj7cv/lXwB7OkaVYWb25chSOfK21Tt2j4DRl52V2udmMHZ7FzbIdpzjs5mQ+ONDsL9+/dUXW/PH3+ckv53JS378BHXSv8HBQUqojNGDJ+HvDz9YndpaRnY9P0eh/3a2+RPny5QmDWP/H0GI4bP03RKrq6uxYjh85wKBiJmw3XlOTdE7H1K5L4sP/30B9LTDzjVaNmfz5VXnu90cOZ5Htdce7H0/dLenQEYe+/EtPVq7N1zDIIgKGbBQUEBTjUhWsKNlnAlThzc1UbExIQr3lGz75baIDdg4CUO5xgom+G5E/6sfj2cc/HF7TFwYC+XjyvHWb9rtFDmoUMndfMliby6cBx4npcESVHAcOb35Yyjx95RvCO//mYrK1RVVSNlNg8MDLATbkhz06TheB53PPO46uButNaUbb22cCCu1/p+802zvZJnwN3ZjtWOzq5kLrW6DfJOZdu2g5qdmJaP0kcfPe6wrTyrrTzz6/x5q1Tvc7adf0XqbXOQ0Lohv8kbr3+Nzp3G4ezZSmlZWFgI0tIyMHfOStX2/vnnaQAwPGs3G66rZ5YSsc+obCTKbOPGXYrlD0240WkRz+MnluHBB4dK6yMiWrg9oxfbDEBS5QO258/+mubkfoQZM+6UBJSLLmpn+BjyiYMV2gjx/XDl3VIzzw0bdrXiubHl8mq4z/L76G5NI2eI+xdLGZj9nT3/unGW035Xz69Lvt/Q0GD0vOQ8p20R73WD5sYmYJjV3NgjT6QIAImJtsgqQWBSXiKbWcom3NTUNJilyOemidK5Ty9EtFQfMMVaUy5kyzZFixbBUoZLT0YquSsYWO3obFQL8/TTI0y3YevWfU5nXJWVNQq/DK0ZjJZq/3/vPKrZbnHwkKvwtSIoRBWyyO7dRxEa2jD7P3z4jEOUijjwJmgUiWzVKtLQrF185tq2bYXcXP0ID7lfgprmxt5kI88hJRb904syGzF8Hk6ddIxidKWIpxWI11je+Xfv3kHVpPv8C/egtGwlCgo/NZ3DCLC9m1YI7uL7YdbMmJqarOt7JCYjvWP4tYrlShOVu603h7vCVEJijCGTrfjM2msV5ccPCwsxZVYTtWSi9kReN84VHLVgtu+hocGoq7NNMoKCAqX7RWapZoBWvhvAulpTzujQsbVXIpVcFU4EQUB+fqmuCt8VjHbms5+/W7oORp1109MPamoJxE4pMzNf0fmrmRP0VPtxcc4z8fbu00VappXbxf64YWEhilwioi1fbh7pdWlnDBjQE70u7aS6T3Hmpjdrv/2Oa6Rn7pNPn0Dr1jGqER6i/86ypRukZepmKeWzIe80I2Sh5fZRZgCkGbQoUIjomUrj46M1ay4ZRf5siI6XIuI9sE/1oHW8sLAQh5o/RsnKKnR78lBRUYW2Sa1swqqT3C4it99xDQYM6InFr+knUhTrS7366gNOtvEcetpvM78TmTPn34b7s7Vrd+DL1T9J39ev3ylpPgDbvS8r1Q/nFsnKKrREc2NEuAsI4NGtWwcAtveTzFLNCL18N2ZrTbk6k7jg/LZeKcngTDBQ8ztijIHnecTFRWHzD3NMC1x62iijnTljDSGgeqYNwNaRfbZiGwRBkGZcWVlFqvutqqpR5MawFzKcq/ad0zq+YTYXG6s+8Nlrblq0CFEM8qJZRO74esMNvbH5h7kYMuQy1X06SyLYoUM8Vq2a5vDM2XLQKLcXoz6ef+EeHD+xDHfccY2Dc6x4TDnyTjMiooVinbpZCghzkiDOHleKEIqIQpvI2bPKwanBLNUg3LhzPDXkGjGj/h32iO9sixYh+OSTJ7Bl6zwsWjTOya9s/Oc/t2DzD3PRvn2c03PjeV63zlVjo23bVob8CMWJ53iZ6fO663oonouwsBDNPEFyxHstvhvVLgo3ZsYa0YRs71BMZqkmjl6+G3dqTZnh9juuAeD5SCVnOUgYs4Vn6mFG4HKmjTLamctDQAHbzP+Vl7/USAQHPPHk7YqcLNckPyGt/+jDH6TftWgRotAo2As3VvhAJMls4TEq2g6149o0Nw3tCgoKQGpqMuLj1U1Q7qD2zOl1nElJrbBy1TTVhGl6wo1olnKGeN7uqumNwHEcXn55tfT93Dll6LqacOMOWhGSy5ZuwMiRfdGvXzdMnrwMVtQpiouL0o3i9LR/jBH8oQ3OtMdamtsWLZRJ+8LCQhTPv9a5iVFZokDR4FCs/rzrlTMxKmiLOZPkPjfKJH6kucG0adPw66+/orS0FDk5OUhLS8MFF1yg+5sxY8ZIGgHxU1FhTH3nDfTy3RitNSViZlYnf/jj46MN28bdRdRmyFWqQIPPQ2LCvxVtdBbhoYWRSA25sGUEubPkXaOvU70novpc3j75zEQ+Q2rRIlihUbAXMtz1gWCMYfDg3tJ3LbOUo+YmWCHchIQEGa4zZGbA0DP5aKG37hI7h0ots5QaQ4dehlGjrpM0A6KQrZe12ApOnbJpbuVRJSJaZilXsZ+dFxSUobCwDM+/cI8k/C9aNA6vvPwlCgv0a3TdPfpl3D36FeRq1DMShVS1KE6tDNHeprT0nOJ7bW2dR4tPqqGnPTaanRiw9R3yUHCt1/C7734HYMuJBTg3S8nbl5HxJx4c/7rq8bWora3Db78dBqAXLUWaG6SkpODNN9/E1VdfjcGDByMoKAgbN25EWJi2IxoAlJSUIDExUfp07Khf3dTb6OW7MVJryhXMdi5WRgmlpWXgxx8bElbJQ6Dlg4aroeFmIjXS0jIwa+Ynhtpt1lny0f/cBJ7nFVqGNm0brqNNuAlVfFc7nqtwHKdwMuzatY2h0GF7n5tOnRPRvn28oWdG3MY2a3evurezY9gz9MbLFOemZ5ayny1+t24WPl3xJK666kIAypmsVmTVuXNVbs/+RSdRm3CjDMdVcyh253h79hyT/l+y5DvExUWp5r554snb8eFHP0jLvv32N4d9bdt2EGfOFEh+UmqIVdHtkyn6UrCRX7/33/te+v+jj37AqFEvOWxj5fHsyc0t1k1waUZza6+50fqN+Ew1hILrOxSXlDQIgCdP5uGTT9KdtkVOVlahlEvHZpYKqj9enWQSI7MUgBtvvBHLly/HoUOHsG/fPtx3333o2LEjLrtM3e4vwhhDTk6O9MnNzdXcNjg4GJGRkYqPN9i/OR1zbhyOs4VFDi+EWGvqSInvHgKrI5XkeVTS0w+4NOBpCVxmIzXmzl2lm25eEATk5BSbdpZcvPhBHD+xDEOHNhTca6sj3NgLGWZrFznjgguSVB3FReFLHGjtNUoxGknm9LANYNrJIj1BdLQyz4qe5sZekLRHDG39/vs9DpFVNj8wW7ZbVwdqxhhOnszD/n0nAGhpbtTNUq4OvnJt6UMPDVVNHyEK/2PGXC8tUzvHqqoawxOeH7cfBABFKgFfIT+XmNiGvr2o8Cy+XP0zRo6cr5PB3bXrrnW/li3dqPsOm5lQXnnlBYp+Rgt7gdmZ5kYezXmuvFJRvkLvORTXnTiRKz3X8v6NzFJOiI62+QAUFuoPvBEREThx4gROnjyJNWvWoFs3bRPL9OnTUVpaKn0yMzMtbbMenS7tiYiWseq+HOCQW2mNetqekuJywyG4VmGF+ldL4DKbEE4QBM1q0mI9ooSEGNPOkoBtJjx/wX2yYzZU1g0ODlJoda688gKF47MzHyVXhQR7vyVxoC+oN0XYa27sTYhGufvuVxxq+Zw+XYA7R853yXHVCPJ7r+dzYx9SrMWZMwXodN44HD+eA6DBwd0KJk9aqgiN1dLcyPPcFBefdfl+yAcXvQrhPM8rNDpqVa2rqmoMT3gaJjK+93GRIz/HyMgWSE1NxuLF4zWvTXGx+fIdX3/1q+Y6uSZNjhgEcfHF7Q0fZ/iIvhgyVH+SDwCRkfXCTf27IT5LWkn85AVqz56tBGNMeqe1BHv5e11VVSPluVEKNxQKrgnHcVi8eDF+/PFHHDx4UHO7v/76C/fffz+GDRuGe+65BzzP4+eff0ZSUpLq9vPmzUNUVJT00drOE+iFhXeNqkKfVs5nPowBpdUc6nTGDcaYQnPyww97oZfYzGxJBiP5crT2J+9YzFZpFjGbEA4Afv31sOo2ai+w6CxpZAZt8z1o+J6QEKu57Ytz/u3g+Cz6KKllDL5z5Hxdhz+tNtqb5sROR6y9ZO9zk59filOn9EuEyI8psmH9Tuze3dCBD7nhOXTuNA6rV/9sytfJDPJ7Kld3yzU3qanJePvtR3T3I973pKS4emd3Ji3X6tTNaFSysgpRWFgmdexGfW7q6hi+V8kobQT7MHejqAk31dW1Ms2i/ntaUmobIIODA50KtbW1dRg08BncPdpRMLaarl3bSP9fcGE7rPpiOtq21dbKzpr5qeljVFbVgOM41NXVOWgA1bSH8iAIMVGglWaya/vaJvaiz414jbU0N6Uys1R5eSV4nneazfj06QK8+eY30n7VTF62aCnbctLc2PHmm2+iR48eGDVKP1PkL7/8go8++gh79+7Ftm3bcPvttyMvLw8PPfSQ6vbV1dUoKytTfLyFVli4WIIhJMDeMU99P7vOAFnn9Avs1cqyt2ZnF+smNlPLoMnzPAYM6InZs+/G7Nl3Y8CAS6SsuUby5cg7RHkiLvks1XaO5mtgGYmCqq2tQyuZP4qa3be09BwYU0/TbqbDcSXaSa5dSUvLUNwb0Udp9eqf8eknWwGoXycjJS2uu6671LmITrT20VKBgQF4/PH3VPejR4sWwYrMpb/9dlhKFllYWIbVX/yk82t19K57WVmFQuBVmqVsmhu5P5YRxJluVJTzaCs9E5V9YcK2bVthy9Z5SFvzDAB9nxv78gvOIr+0rpGWQ7kz1JLDVVfXSppFtdMW39N3390oJYQMDg7CFCkay34iZWtzbm4JtmzZhxUr0qXn0VNOvvJMzn3q80Hpvatyn5by8kpDE74ePWw5XoqKzjo4X9tH/GkFQXCco+bWVcR925ultH1uGjQ3HTvG4/iJZbrPX0VFFTp3GoeDB05K+1cThmpr6yTBinxuZLz++uu4+eabMWDAANNmo9raWuzevRtdu6pXsPUlamHheiUY1DoVjgP6n8fQMkA7Iiwzs0DxQPXo2RGFhWXo0rkhkZYgCBh732KsXbtDWiZqZF599QEUFH6KzT/MlVKRb/5hDgqLVuhW9r6jPuRc3L+IfAajnKUKqmYNZzWwnOWiEc9l5cppktCl9oJFRYXp+u140jHSXrsiP5bcR2n//n8AwOE6Ga2A3bFjvPS/aJay5blp6HgDgwLxnYpTqRo//fQHKiqqAAC//b4YHTo07H//gTcxf/59kvA7YmRf3X1pdehayzN+/kNxv+Uzwkt7d0ZKSg/DBU1FRI2Zu7PLv/8+o7o8Li66fv9BjpqbCEeH4sDAAN3IrxnPfKT5XIaFGTsHxphDJJGc6uoa6R6kpWVg+vTlDtuIVbhnz74b//rX5dLyDRt2q06k8vNtNdAKZFFaYlJDjrMuOk2LFi2CnT4T51/QVvp/584jqkKHvSAmJrCLi4uW/pcfU8TKKuN6nKu/pvYOxdo+Nw3Pwai7Upxm4w4NDcawYVcphCf5RFpcZjt2vXBDZikbr7/+OlJTUzFw4ECcOHHC9O95nkfPnj2RlZXlfGMvwwQBaxcsBmQvjaslGFroPDD2L2S/ft2xZes8nMlq6KR4nlckzZNrZCZPuU01E6oo0atFKPE8h88+f0oq9Chvgvwlt+/I9+49DgDIyioyVVgyLS0Dd46cD6Yx67MP2fYX1agcueOz1v0X1cvffPMbht36grR89OhXDB2jsKhhZlYo09zIHYqDgwNVq1E/MvEtAMoOvW/fhro1YpZikTZtYvHkU7c71KExSklJOUaMmK/wA5BzzbXdFBpCufbp+usvxZat87By1VRDxxLfkbKyCqSmJiMszDWTjkiXLm1Ul4vvStu2rSzR3NgXSpVjZoac8fMfmuvkA2FqajImTRqmuy95iHKLFsFIS8vAhRdMkJat/Hw7xj3wXwANgy3QINy8+OLnDsKQEZxpO8xOTmJjGvJEXXZZV1XNaIFOCL29Y7i833MWBCGPQnSH4ydsvmOuaG4AY4LXqi+mS5owNbOU+F08tr/0vT4Vbt58803cc889GD16NMrKypCQkICEhASEhjZ0PMuXL8fcuXOl788++ywGDx6MTp06oXfv3vj444/RsWNHLFu2zBen4JTy4hLFS+NqCQa9F7d16xjV5WpZX0Wti5pGRu2YescNCAjA559Pxfz59yk6Wrl61r4DaBgImemoqvz8UgQEapvn5MKDKCR4Anc7pDZtWmpe11CZY+A33/wmdRwXXtAWubnFutqPvLwS7K13aqyqqpGiWVq0CEaYXRI/+X05d86mmRF9MdQyCgPaWZWNDiqHDyu1stHR4fhq7Q6sXfuLdA5ywsNDFAUt1fLDqJVt0EN8/t2dTTvrwENCghQZoAGbQJSS0kPxrgQHB+kKN61b2zRB4j2SY2/y1YLjOLROiNFcLw5KohlFqz+R709E1DrJ701eXonUNrkGsrzcdg579xx3KJfhDJu/mamfOEV+Te0jG8WEhcuWbdT8vf1zr0gRYTAIwl1tcf/+PZGS0kNWW8p2ve19+kTkEwnjqSCY5LBfU+1olhKFY1GQJbMUgIkTJyImJgbp6enIzs6WPnfeeae0TYcOHdCmTcMsKTY2FkuXLsUff/yB7777DlFRUbjmmmvwxx/aMxNfEtU6XvHdbAkGNeyVF1oPk9rDazOJiP9boy61zd4bHPe0NDdAg3DjLAmbGkY7jIkTb1IdfIqKzuoKU0aEFits5VlZhap+PykpPXDFFecDsKVRv+22q6XtXn9jgtNBJy4uCrfcciUAm61cNCfZR0sFBwdJs+/a2jopamTCwzcCMFdzx0gHKZpExAglObfddrWknVE/rk0b99p/H9R8no04g4u/NVNhWw1BEHRNPHLsMyxfcsl52LJ1Hh559CbF8pgYbQEtMTEGAFBWZuyYWlx6aWfNdbZyGK6ZUfpKDq0N73lISJDDYAs0aG7CwkJMm6UqKqoN568CbIO7s2Pk5xdL/2s9exMm3Gj4mPJ+z+pUG1o8/PC/sGXrPKnfcBZ5Z5+jyAg8z0taxzvvvA4333yFYn2DcONf0VKem94awEjHOGDAAMX3KVOmYMqUKZ5qkuVExMYovoslGCKCXK8OXlHHuVWE00rfEnFfF17YMGjIX3K5fV5OeHgoAgIC0LfvxWjTpiWysgqxffsh3Q7JaIcxYmRfXD/4Uofla9f+gnvvHaSZUVWuKta6Ru5cO0EQcPp0AbZvP6S496mpyVj82ni0b98gCN81OgVj7x9seN+2ApUCnnnGNjE4d65Kmpm2CAtW7DsoKEARslxaWoG2bV13UHWGqJlQM32uXDVN97eiNk4PM/dEzRynhfpzwBku/aCVe8c+AaFagVWRsfffAMBmTtOLzHOHqKgwyXfJLOJkRS7cREaFOfiAAA3CTbhKvS9nWY4XLVyLuXNXYfyDQ9CunfO6VbW1dQgKCtTdb9eu+pGzPM+b0gzK7/f27YeQm1vsdEJihv37T6Bnz/NU14nP9RVXnI9163Zq7kOs6O0q4RGh+OxzpSnYXnNDZqlmQnlRseK7XgkGo0oBq7MbW4F90TfANmgvWqweycLzPLJzPjRVtVwM2TaCWNxNzl9/ZmLWzE+cdoxa6+fNXWXo2GrYR4XJj6EWUSF2EGa0KDzPI7E+705FRbU0S0tNvQZXX32htF1KSg/cVD/7qqmpdbDDW43Y5t69tbUH/oj9tWaMobCwzNB7eu5cFTp3TjS0Xz3EAdNZLh5nztp6xwwMDED//j0Nt0lOaZkt0EE+W4+KCtPQ3NiEba0Qdr2JTVFRGQRBwNJ3Nhi6fiEhQbhz5HzdMOeWLc2ZNJ0hF24EQcAjE98ynGLCCFqCDdBwfx+e+C/dZ0VurnIlak3UaMnPSYwcpMKZzQy1cHCtEgxlNRyq6pwLOfmV+mHhviY8PBQDBvTEO0sf1dVO2WsK9Ipo8jyPhYvGGRYA1TrA4OBAHDlizPF87RpHJ+ddu48aO7gKeXmlulFharVm3NESBQUF4vrrLwXgqEEICQnCf/9rS51QU1OniKDwJN6Y0f3229+W7k/eiZeXVyIuLsqQ2SYvr9jlPDRyxGdAngnbKN4oJPl3vR+VfECLimqB7t1tkUTR0eHSYFtxTlu4OXw4U9fJOCExFjzPG35/Advzlp1dBAB4dsZH2L5dmT/N6oj0ULtQ8NWrf8bLL33p0r7kuZjMkJAQq1s3UEzAJz+OWeyjPf3VLEXCjafRGKDEEgyrjkfhu1ORWHU8CunZ4QjWuCPeqo9iBR99/Dg2/zAXrVpFmfLX0CuiaTbkV42goADDpq3ffz/isOzY0SycOpXn0vWbMnmZYcHGCjp0iJeqw+tpf6qrayQfEtFHRwtfVF0WE8fp3TdBECRHyVKLBTX5tTKTSbhDh9aWtsOZYHjypHpSRqMC8tat+wxnmZY/B2K75O274ooL8PBEm1/RDTf0ljSyolnqoova4a67UhT7zMsrRafzxmFA/+n472trHY755JO34/iJZYpEfc5YuGicFFTx6afpOH1aOdF09rwLgiAJR2rYvw/2oeApKT2wZ88x7NplfFJkRNPmDD3fRDFPD2CLjlQrRWIWUTtGZqlmRmQr7QeNgcPp8mD8VRKCzPIg9E+0dczOnuvuscacwrQeVE/kc5EfSy1KyyhqRTRTU5MNh/zqERQUKIWi61Faek5VAxAbG1Gfb8c8XbooTRSeugdGOydRoOJ5HqX1ZimxorVa1WdfYDuuzZQnzoC1MjSLZtGiYs8l6dQKr/UHxIH15Mk8qfSI0WesqqoG6ekHneaSEpHvVzyuXHMjDxUHGjSyV9WbRu8anYJPPn1CsU1NjS2JYMuWkRhz3/VQIympFWbNvhv5+aWGhLD4+GhJS5SfXypl0BXR06zZTDYcFr6aprmNKKyJiIO6feLTyy4znoNNzCHjjuZWbyLw6sKGKLXs7CJ06fwg1q75RVrmyrsuam7E96Nly0jNTPbehIQbD6OVpdgeZ/lvXHnQvVmtV34sK457661XAWgIT42Ndd/ZNSQkUNPBGWh4sTes36VqqmnVKgppaRmm63IxxjBr9t0Kc5u7l0hLAHHl2ovnKkb32GsoxBmZt6s/V1XVSKa8n36yXXOtQU0cZDmOc5ihy2GMuVRTCLA59RrVbnj7WomOq9nZRThw4B9Tvz18OBOCIEilQQoLjQuIvXt3xqhR1yl8urQ0ssnJF2nup2XLCOldj4pSL+wq7kcUeo3CGMP1118qVa0W0RNuCgpsZmSt0hgTH16Cjz/aoljWokWwZlZio/zxx2mXfieSmVlguH9KW/MMjh57R3G/8vNLMWjg0ygoKNUtlyP33amurkFqajLWrJ0BwOYbacSH0tOQcONhju3ai4pS7URQIu5EPzVFJk0ehjvuuMbSLJ8PjBuCjz95QnO9OBPbvfsozp51zAg9ZEhvpKT0kLKvbtt2wNBx5SHN4mzGSBI5NU2M+N1+1ujqTK+yslpyKI6Pt6Xl37btIAb0n47Rd72MAf2nY+PGXaq/1XJIdFXTU1BQhkEDn8GiRTazxG+//S2Z8sRZsSCoa6fEcx8ypA+mTH5Xtw2L6/d//Hi2Q7ZVPSorawxrN/Rw5fqUGQw/LyurwIl/ck3tW26aWLt2Byoqagy38ampw/HpiifxyqsP6G7nLAP4BRckYfFrtozqzpzm4+Oj8cH7mwy1T2TR4vGoqlJqvNWitkT+fc+rSEvL0Hw+7P13AKBdu1Z46+2J9e10rb8qLCxz69maMeMjqSSKEZKSWuGWW6+UvtfU1GHLlv14cPwbqtuLgmVRUcMEISoqTDU/kp4PpTcg4cbDMEHAqlnznZoMrMh/05RgjOGNNx92289Gjl5unRdf+BzffGOr+FtRUY2yMkfh5v4HbsCWrfNw44027c+uncadV+XmNnneCC3EpGX2OVXECKgpk9/FP/WDmCuDpSiYlJScQ1mZTVCKjLTNmKura5GefgCffbYN6ekHHNT5Z87YBkMtDYhWAjFnfPDBJmzZsg/pW/cDUOaJEYWboKBA3cEvIqIFli57VDP9fHV1reRPlZ9fJqnwjVzDwMAArF27AyOGz9MtcOoMV4TQnTuN+W2Ulp7DD5v3mXom5FFDtsy6zkOtraZFixC0bx9n+F3fvHkvTp3KNxTxw3EcOnSIR+/eXRTL9d5BMbLL3gFXvt5egOjatS1at45xq786fTpfSuHgynud9qVtMpCS0sPQ9vbnIL43aWkZqiVfCgvPYsTweYr+8fzz29bvy7gPpTcg4cYL7P1+Cw78kK67jZj/xir3BkFguoOM1X4UVu+P53kk6GRVdYZap6fXYbdsGYEuXWwvaWVljapwIyJGAzw04V+m29WmTct6Acd5OPqsmZ9gyA3PAbCpiwf0n470dJu2qKamVro+ZgciW+4P2//h4SF4ZsZIxfqBAy9RzLbszVSlpbZrM/M59aRqR49mYUD/6dJ3o47K2+rPTZwtK4Ub4xEYUVFhitQEX67+GbffPqd+nw2lOcLCQpxeQ/lzffHF7XH8hC0TerukscjNLTYcTitGkrj6nvQfYCxUu6ysAoIgmBogr7jiAul+G02U6WvOnCnApMfeAWA84kesoC2iJ9yIz6yW5ubs2Qp07pxg6LjOkN8jxhy1smYYMqSPaR9F+bMv9ytTSxz5f//5n4NGKzRUu46Xmg+ltyDhxkt8MGk6tnzwqeZ6vfw3rsBxwMYNu+r3Z26gN4t93hY1vOmUyhjDX3+dMvWbiY/cJGX5fHHOv3H94N6a24rn6kpUQFZWIdq1N2aPP3IkS/J/CA4ORHr6AclZs0uXNi5HJXAchxUr0qX92Ccqa9EiWKFObttGmTxOzPJ71VUXQo26OgHp6Qek/Bdi+K7WM5CTUwygwbdHjL4ICgqQok4GXd/L1PnJycj4E9u32cwIAQEB+Pe9tsSg3bq1N5XUD2hQtd9yyxV4eMISw78Tj+NpjYhovjKbu0icXXsrs66rCALDyZN52L79kOQj5KoWTU+TK2pItSaIFRU1uDr5YpeOa4/8mejUKUHSGrnyrKz47Cl8sdp1H0V5XiC1HEEN9aPMhXv7Qmgm4caLfLtQ3Y4popX/xhXBYO6clfi2vvKzlopebd95uSWmj5WZqewQvSXICIKgOrPiOA4XX9xB5RfGaNkyAitWPOl0OzOdjxjSvH37IamgpTNycoqkEOeoqDAEBPDSIOlK+Qo58s5Pr+zBgpfuw8BB6oLF3ff0V10uRs6InaM4UNgXyBRn3PHxtvpJok+EeE9btoyQok7+7/9uNXpqDufUuXMi/vjzben7sGFXq24nb5eYZ0QvXcHatTvw2mLHsGU1AgK809WKGkexrpgRRLNNv37dsH37IcNO00YRTfK6Zvnyynozk3Nts5gIE7CZT9oljXWqHQQc77Xe5EB8ZtX6F0EQcPnlXdx+B9Xo27ebQyHa4uJywxrChihI14RocUICqEcHVlXZnIfNFsv1hdBMwo0XuWnyRKfbyPPffPtPOD5edxgMxgUG0Q9j797j0stbW1un+/ucnGLcffcrGNB/OkaNesnQceQ8MlE5g7VidursfEXHtrtGvYTnnv3Y0kygVs+uxbBSsVM2mvfig+VTFJlj5eYWd3w+AODGGy8DoH2uojr5iSduN71vsY1i5ygOAtOnLZcclZ979mNpViwO/J+vnIrU1GRpQGnfPt7lqBM5Ex6+UTU9gd656znAylXtcsdKf0AUbsSmm3mW27RpCUEQLHGalsNxHH78UT+CZ8eOw/VmJu3jFuSrJ8Ksra3Fuu+0Sw64gmhGVBNuzp2r8qgmwj6UXgztN3o/zPZfcsFJrq1RmxTX1NRKQR5Gqa2tQ6u4KFO/sQISbrwEHxiI/mNGG9pWyn9TGorcpGR8czISFQYdjrOybEmnwsNDJeEmPDxUt1ZSQkIMzmQWID39gEt+Lm3bttJcJ8/bYAZn21dWVtd3dL9g/INDwJi1QomVifWKis4qOmWj1ZyTklris8+nSrPSmJhwqePbu/cYyk3Mzl1FLwpLa3l4eCh4npc6StG3obKyut5BuQazZt/tUFgyPj4Gq76YjmuvbVD3W3UfPGEOatu2Vf2z5z+RjtOmj8Add1yDujrzbRJn16K5Ry9jsFn69u2ma24/eTJP87j5+aV47tmPkZh4r2YizIyMPy1rK6CvuampqbNME2HEZWDr1v2W3w8Rm0tBw3e5tkbNLHXRRe3Qvn28qfeJ53msXDnN61FTJNx4iRHPPQU+wFzZBI7jAM6mzVn6V0ucq+V0O4icnGKcOmXLVBoeHmLKLirORET/BzPo5cWwH8Cs4sd6m7stusNYRJU3B6HCwrM4XJ+a/rlnP1Z0ykbvi2gCEU1RMTER0v9VVbX48UebH4k/Da6ALYnX8RPLJI2MqLmprq5VVJ92NPnYzGFPPHm76nozyK+Jp/xc4ltHGe7ozWQ3dofIyBZYuWqaZq4YNRhr8GMRSUvLkDIGL1q4Brm5xW63Te8dDQ4OcDiumIogMeHfePHFz3U1F3JNpnZ+FuPviZ5DcU1NLeLionT3p9ZWs0EODcerk67LpEnvON3eDKdPF+D55z+Tvsu1NWpmqUt6dTJ9DPG99nbUFAk3XqDnoBRcMexml34rzpwF8Nh8xub4qTWWJSTESEnq5JobI2RlFSI1NRkfLJ9kqn1VVdVSaLQVGE0gVlJq898wox52NcOsGeFBEATk5BQjMeHfUsixPHIHMOeIzPO8JCDGxoZL+6qurpUSfulFdrmKuyaJpKRWkqOyqLmpqqpxKozyPO92dXJ3hT3Rl0vrGoj+U3l5xk2Doh+Ss7aJvj7MzcJHbeycwJ3x+JRlDucrCDbH8Mcffxdt24xRmBKNIu7T2SDepk2D9lc8rpiKwMiz6KzfsNdQOEMc5NVCwQMCAvD5ymma+xPN0PZkZ7um7RH7LUEQ8Mbr37rsEyU+t4MGPiMJjp07jcMmWaJCZw7F998/2PwJwDdRUyTceBiO53HbtMkA3J9dazkcyxErcl92WVdJQ1Back7zZRBnba3iolzKrBkSEoyx96unS3cFcWBzNggcP5YNwJyj2vHjOairM560TcTozF+s/D3x4SWora1FlVRITinMuFpYzqa5sQk6NTW1kjbgvXc3SokFX355NV588XOX9i9iNgOsGvIZmnj+1dW1XomaKMgvxZYt+136rXgPF766Bmr+H/Lq7mc8YCY4fboAa9b84tblN5vQkeM4LFw0TtdsMGzYVZg1+25TZkIzz1FEhHZCPWekpibjnaX/kb6rnfvp0wV49RXtUgpaqAlz0dFhmscBbM/I//3f/xyWX9rr/ySNlBkNjFx7JPeJMoP8ud2yZZ9CcJQ7Y8s1NzEx4ar7cse/0ZtRUyTceJjOfXohJjEBnEXquCOlIXjvcIymiUp84YbeeJmUkv7rb36FnqPe559tw6JFtpojniji6Cp6L5CYWG779kOGVeYRES0QUG8adDXRnB72lb9F4SMkxHXNjZwxYwYhrt4xr65OkJweQ0KCJc3AO//bgFkzP3Ur4qWk5BzuHDnf6T6cdXD2nX9VVY1HoyZeeXk1Bg18BomJ9+KfEznScr122q87fboAI4bPw7RpH6j6OYjrxTIcrl7ns2crcOjQSen7uAf+iwH9p2PKlGUYNuxqy/JdqaE2OOllk9UzJepRW1uHWTPVcyE5buvasyqWOxAj7tT4v//8D507jcNvvx02vX81s1RgYIBuPxkYGKD6u/LyKkkjZUYDY69BSUvLwJ0j55vKYyZ/bu0R/Ytsx7L1qzzP44ILklT37Y6Z15tRUyTceJioePejPewf1LbhtQgLZLpq1qioMLSvr0y8a+cRvPLyl5oP5eNP3G5pJmA1zEr6zl4gUXAQBAGPTHzL0GyibVvbrKGgoBQFBsOxzWBf+VurSq4Z4cYWlmw7r1tuvRIxMTZTz/oNs6UKyaEtgiXTj5jEzZ2IlwXzv8Dq1T873YeYj8Mo1dW1TgUC0SzjCrt2HcOWLfsgCAKC7EyBevtc8Wm6Qk0v3kM1/w/5eleus/hcl5VVKPzR3nvve2zffshrkwy9EHd7vwgzfm1yfv31MObOXWVoELfPxG0EudCl17bt2w9i2LCr8OYS59Gq9pjx05Hz0ktjHZaJkxHbfrWfHftnVU3bvHr1z3jm6Q9Vj217h2y+fmrPrT1K4cZ2rH79uimqnNuj1j87m0TY+3V5GhJuPIzRwpl62D9IRutQRUW1AGDTctw1+jodTY/nHVLdLfz5zde/KtbJ1aerV/8sVY02sj9BcL14oh5iWQIRUQDr0qUNRo26TqqUa9QsJSZHVLt00dHhGH13fwC2auXigCTWxHIn4iWpXSvdfYiJ4sz6MFVV1dh16srnzkgySD0WLLgPPM8jNTXZrkiptvDBGHDnqOvqK2M7+nc48/9w9TpXVdUoIhN5nndZiDCLkRB3Oa6aEsRn0ogAGB8fZdrZ1Oj1uuffA7Dqi+mq6QD0SE1NxrHjS039RiQysoXie1WVY70urWfHfrvvN72oqlET85jZI2ppXnzxc0N+S0qzlK3PMnPP586xmcH13l2O47Bs6QZL8yc5g4QbD3Ns115UlFk7kBqtQyUOHh06xDl14jSKOw/nqpU/YtHCNS7laAmwi7qyjz6ZNu0DjBgx35CJKj4+Gued15A6nTHm8gwNUCbok3NeJ5vmbMTIvvh0xZNSpdzrrjNW90Vsk1qnIV8mmqrkafcBR83DqpU/Gjqu6Lelto8B/afjiSfeA+Dcd8i+k5bXrRkxfJ6DMJiXV+pyAVAAaN8hHk8/PQKrvpiuOAfA9oyrzSytiOSQX6MXZJEnerRt2wrR0Q0+DcdPLMOtt17l0vGtxn5gM2NKkPcPyckXSeUqnAmAffp0NV1F2ugAPHaszSfQzHOlV93bSH9hfyytftM+Kk0tcWRCQoyqyVDUDIv88sufTrU0aqhpbszc84OHjGWDFzOVewsSbryA1aGoRupQVVXVSLOH8PAW2hvKMDLAm4kQsWf4iGvx44+H0KG9o8rWGR06xCu+q4XWfrn6Z9w16mVD+5Mnynp6+nJkZrqmYZM76sk7sNTUZIwc2c9h+6SkVpjy+G2a+3vjjW8kh8PAwACnsyGg4dqoZaWVax6WLPnW0DnZm+zstRfitW/RQtsJVK0zl9+ztLQMnNfxAUnTlJ9fgilTlhlqnx6PTbJlMlbLumxWY2EG8RrNnr3CkBnGPlFbUlIrPDZpmMvHtxL7gc2Ib5EoONpfY9GXB4A0iO/adUR1H2arSBsdgFu2jDStDXNW3ZvjzE309FJiCIKA7dsPYfiIa1XzdWmZDO2zMh8+fMZwdJkchXBT3eDLaLTGlWhKdYa3sxSTcONhOvfphdAIda9zV9GrQyUKKCEhQejZ8zwAwJj7Bhrar9oLK/p8LFq4BgP6T0f7dmNddqJkzPaCVlfXSoOaUdrb1WPSKilhNAmhvAP5/vs96HTeOAwa+DQKCko1hTy1mb+ao57cF8AeZ9qBA/v/wWefbUNuTomR0wDQENXg7Jpu337I0H07eOAf3fVGcracPl2AA3b7kad2B2zP1qlTNqGyro4hPt79LKatWkW5bNaxIpLDqB+O1gDmLATdykzcavtX00AaOSfxfPQGZsD2DHbt2lZ1H2arSBsRulzVNOtV9xYFZbkPjTMCAwN0hTYjKRLsBXC5UAIAlRXG2yOnpqZW8uuRh53/opEY0f75i4uL0tVmaT1XnoaEGw9jhUOxGlph4WqT04iIFrqdovjwjRgxXzM65PHH30V6+gHU1tZqhiI663TlL2hhoTlTnX1SMq0BVnSyNUO37h0gCAK2bNmPB8fb6n+phwA3cPhwpqYKWOyozGb2BSCZlczMcsRr4yzfjZ6zrny5M+FFa31JSbnCgVHMli3Su3cXxaCVmpqMTp1s5sGEhBgsXvyg01Iheu13tySFVTNLV/1wxJxGaj5CoobQiNmupKRc9123DUTaIe5qAoHWORnxu5K/9/36ddNNMGhGi2ZE6PJk8kQ9zaU9HMfpCm1GBWv5dvZmKblJ2iyioCSPzNIyI6kJsDZzmla/qf1ceRISbjyMFQ7FWoh1qFYei8LOvBBDMzr7beR1j75c/bNudIhIWloGRo6c77Bvo34rbdq0REGBewORmuYmNTUZs2bfbXpmK8/noNWJnz5doMjefPx4jqYK2B0NgNhBibNSMwO9kWKJARpZsuWzQL1Cq4D2gFFRUa1wYLR34Px85VTJr0L0aRBD2EX0tC7i+Wo9Z2INHrN4YmZp76s09akPDP928aK1qs+f0SKd7737PRhzvE7iQPPyS1/qhrhroeZ/tX37QWMnBdt74cogroczQfLcuSrLC4GKMAZdTZs9ekKbUcFavp295shI8VAtxPdfq3DmGic+PGItNvsJhpHnylMEOt+EcIdju/aiODsH0a3jLct1I4eBw5lzQbi5fZkh/4y6OgEBAQ3b5eYW45GJbynCW9PTD+geMzU1WdXOarTycVZWIUpKzId+yundpwu++OIn6buruTgA4PRppQCalpaBtWt3oF+/bmjTpiWysgqxffsh7Ny1GImJtsyveqGr7mgAxMrY4qzU5q/g6GSoJvSYzVQ8oP906fwefGgo7rorpb4N+rNdLeFHvjw1NRl9+nRx2Eb0qxAzyqrNArWipgryS/Hee5tw1+jr0L690gcrK6sQc+euwvgHhyApqZXqDFnNL8STM0v5u5SSYsyJHAC++moHnnzyfYfnr1+/bpg85TZDv//xx0NY/Np4xXU6fboAkyctRVpaBp5++kOH/Rs5f/v+4QkTphAz74WZbdXe1y1b59W3l0nvkSAIlqb/53kOPB8gacKM7FtLaBMnM1rPriAIOH26QCGAM8ZQWVktpZawN1OZQU1zI//faIHYyZOW4cyZAtPPlScg4cbDMEHAmvmLMGbhPI8dIym8BmFBxrQV9gLIv26ciT17jhs+ljjj1kPN419cnp9fiu3bD5myV6vxwAM34NkZH0svjmgKcoX0rY7CnJqQJ/dpOasjSDR0VHGm/T+6yMxq4qz0f+88grg4ZZKygoIyLF60Bi/OuVfWPnOFNOXnd3d9WDnguuZGngBMz+dIEAQpwktrG5G6ujrMnbMKW7fulzRC4sD83HN3YcDASwDYrodcILQfbEQhprCwTHFs+YDvKVJTk/Hue//ndDv5AKb2/JkZAAVBUBXQxffFyCTGCPamEWfnBdhKJWiV2FAbxI0gPx/5tQkI4LF27Q6MGD7PQdizisWL1uLue/qjdesYp9tqCW1Gnl01AdwK4UZ+rLZtW0rvqFxzY9Tke+ZMgSXPlRWQWcoL7N+cjg1LXMuXYASjeW/UOHfO+AthNGkWx3Ga5pS4uCgMG3aV27bw1q2jFSpes6YgefuMRgXINSN6WhKxo3IlSO6qqy5UfE9Ly0Biwr0YNPBpRTsTE/6NjRv32LVPXxtm32HKv8v37Uy40fKzEH/nzOfIzOw5P78UM2d+IiXnE9uenn4Af/55WtpOfJ70zIojhs9DYsK/nZpdrUScDMjDvtUwokHS8zFR+72zHD1WEBennzvGvl2CIODD5T8Y2tYVUlOTpfBzoKGIK2CL1nru2Y8td8j+6qsdaJc01un7l5NTjJ9+0q5e7uzZdZZd2BWzlHi9xIjLsfcPlkzHcs3N/v3/OE2+6QunYT1IuPESm5YuR3F2jtsF8dQwmvdGRP4SmJH2zSQZ08rNwhjD2/97BEOHXmb4uFrIBRqzpiAzPiYics2Is4yqa9fucCnr6qhR1zlEVYjOzmJkkbjMXkCMimyhKTjYd/o8zyvyisizDdtHNdnvJ23NDNV1otBjZf2YoqJyzXXy87cPM9fyHfPGgC8iTgY4zrmp1KhvgisDoKdITU12mrNJrV0//qg+ALp7Dlq5aRpCzK/G+AeHmCptYSQQY/v2Q7jlliuc+hwmJMTg6LF3dKOmxGdX3p8ZzS5s1qHY2fXq0qVBi1xZWW1KsPYHSLjxEqJ5SvzfSozkvQFs9ueTJ/MUkUpmzENWDFo8zyM+PtqhUrYryDsAM3V+7h79MjZs2GX6eHLhxpl/S79+3ZzO1tVgDJpRFXLVcEpKD6SkdFesv+nmK1UToTnv9JMNaW7E/Wip38USEEYFTb3QURE9W79cCLM3j3hTiNHCmQZLztj7Fhke1J2VhfAGeqZHwHb9c3KK0bXLgw7tkk+uPvhgkyXnoKdVFkPM33jzYVMZoG2RZQ3/268TB/Rhw67Cqi+mIzJSOwpMxEguH1FoEunXr5vmpEXef5uZqBq5XkNvbJiAVlXV+JVgbQQSbrzI/s3pWD5lOkpy85xvbAK9vDfSNvUrJk9a6rLmxsokTO4mNhR9CEScq+wb2Lhxj0vHPytTOzsTblxPWc+hQ32mXTmpqcm47LIGB90tW+fhtf8+5PB7+87TSCe2aPF4xaxPTbgxYpJs1y4OPM87jfQSBAF5eSXS//br5GkGOI7T7NjF4qm2/z0X8usqZp6BhIRYU/v2tfBmxPSYkBCDa6+9yGGdvM/5+/AZS87BSJ4YozmwRMRBe/gd2gP62rU7DJnq5e1wlssnNTUZl1xynvRdzGyuJhApzVLWaeF5nldEkcrNvr4WrI1Cwo2X2b85HS8OuR0b337P0v1q5b0RycopkaRr+aBgRnPjThVkV9EaIOfOWenQDr2ZhZzS0nMu+cOYMUu5KwjOfv5uqTMTNSZGCm7ad55Gk4O1TWolLVMTboyYJENCgtCvXzeZoOmIKLxMeOhN1XtVUFAmRVIBwNVXX6jZsWuZpfwFT0UH+QPuhHTL+xyjJmGr2mOEF57/TDFo6w3ortQD08vlo/Wua2l8XBVuzF4v+fvla8HaKBQt5QOYIOCHdz/EDRPut3S/R0pDcLQ0GEnhNQgPFBAWKKCilkdpFTD/0UXYu8EmXcs7l9pax4qzWuh59MvRSsXuCoIgqOZmycsvlbz65WiFcdfWNeQIqampVbQtJaWHoZBFow7FgM1xuq6uTjOvjDNE89TXX/+mGeJupJyA0U5MXidKTVAwO5ilpWXg669+xa3DlPWS7KOT5Peqa9c2mDX7bod9ih27vepbbpbyR+FGnAy0axenea8YYzh1Kt+vHDGN4EpeFhFX/N2sak9ubjHi4tSLdIr3YvbsFaoaRbUoIHeEKvvf6qWzEPu6RYvHY+3aHVL7lGYp4z43ZoVpf3y/nOGS5qZdu3ZISkqSvl9xxRVYtGgRxo/XtsESSoQ6z0i7DBxOlwfjr5JQ7C4Iw58locisCMG/X3oRPQcZy2Oih5Hsq6JviDPzkBFOnMhFfr5jKYLPPntKc0bvbGaRmpqMwYMvlb7rqX3lGNXcpKYm4/OV08BxritGRfPUxEdudLlStCjcGSEnu1j6/7LLujp0/q4MZn/80VBQ7+DBk6oqbPFerVz5I8Y/OASAvvlM3i5/N0uJkwHG1DWQclOxv85+tTBietSKnlEKN9bcN2daZbE9Eye+BbXs6iJm74U7Gjf737pbgsGMQ7GR6yXXoMrftcaCS73vp59+igEDBgAAEhIS8P333+PKK6/EnDlz8Oyzz1rawKaK1U7FethmAQzDpk4Cx/MKaT8lpYfpxFb2atpBA5/GoIHPSCrbNon3atqp8/NLTXUeQUEBiI6OUF1nttAeYMubYkbtK0fudHvBBUmq181ouLzRcFR5xIJZRK2Vs04sL69EKjgJAF+snu4g7BkxSZ47V6UYzOQh48eOZeuqsF3p2OUDY7UfCjdAw2RALSN3QX4pht/hf46YRjBietQSFDyhuTEaJv/l6p8x+q6XVPdx9myl6Xuxffsh5OYWm26rmuDniqnPVbOUkev16SdbpWX+OHlwhkvCTY8ePfDrr78CAEaOHIkDBw7g2muvxd1334377rvPyvY1WeTCjacK4cnheB6xbRIx4fFRSE6+WFpuVGthj1w7smXLfmzZsk+hKdGyUz/04BtwVlRQTvv28QgMVH9MzRbaA4wV+FPbV2pqMuYvuE/6/r93HlW9bq7Y4PU4elS9vose8s7TSCcWFxeFuFbKpHr2wp7efkT27j2uWCfPk+GsqKcrHbu/+9yIyHMVvfD8Z3jh+c8waOAzSEy8t1EKNiJpaRl4841vHJY7i55x1SxupD1GonnS0n6R1h0+nCn978ozJAgCPvl4q6nttQQ/d0swmE3i5+x6/fLLX9Iyf36/tHBJuAkKCkJVlU0Fdv311+Orr2w1Xf7880+0aeP6TLM5YS/QeEOT0zWqCq8vGKXwrwBc04AYQc08pPVC6eWT0PPdMVNoT9zejHYAaHDys6+VpHbdjA7SYs4fZ2r9JW+uM+XErdZ56nVihYVlYAzgDJiCnJkkT5zIUXyXa27KnWRPdqVjV5ql/FttLuYqmjnzE4ekhI0Zec6aHTv+MhQ9Ix+ErfDLk+Msmic1NRlHjr4jbX/BBQ3uFQEBvEvlGb76aofhbfUEP6OmNbnGp9LNJH5610uuVXM3o7wvcEm4OXjwICZMmIC+ffti8ODBWL9+PQCgbdu2KCgwVwmXADYv+xDlxcUePQYHhv6J5QAzr7WwGvkLdffoV5CbW2wbYDUS/xnBymgJ+b6cOfnZXzczNnixwrNWgcPJk5YqqrDbd3pqglFhQZlq56nWiY29b3G9c6VxYU/cz4ED/zhsL9fUAMpZubPSEK507P7uUNwckOcX2rXzqKHoGfm96tatveV9jpbPnVa+J5HY2AiXtNhGnt38/FIMGviMruBnNgM1oBRoXK0KrnW95O9vY3y/XHqqpk6dioceeghbt27FihUrsG/fPgDArbfeKpmrCOOc3HcAO7/73qPHSAqvQWSwYFpr4SnEF+rMmQK0bh3jthnHylBa+b7M+oK4Ei5vX/7BUY3u3IkbsF3TlnbmJfv18k7MaN4Pe8FREARFtmSRWruyDHJhx1mJC1c69sZilmrKyLUwRnxP7DUnT00d7pJAYRajvnCuaLGNPLsPPfiGIW2d2UR5cuHSncKZasgnc43x/XJJuElPT0dcXBzi4uLwwAMPSMvfeecdTJgwwbLGNWXkFcLbXHg+rrz1Ro8ez2j9KSs1IN44nph1WS+UVj4zrKmpNaUdMOsLYsQ3xZ5z56oUDtlqsztRYzJo4NMoKChVLU5qVgPnTjhvSYljWQR751C5WcpIUU+zHbv8eI2x820KyLVn0dHhus+dkUzZnsKoL5yrWmwrs/eaSZQnam5qa+ss9V8CoPB1vOqqC7yi1bcSl1obGhqKkJAQFNebUjp06IDHHnsMF154IfLyrM2+2xTpOSgFMzZ8KX2/8dEH0SJKe8ZtBUbrT3k7mZg7xzMSSmtfUykoKNCpKUi+L1cEAKOaFpGEhBgIguA0KZYgCBAEhlatogzluHGGK6YgkdISx1B4e7OUGYdiETMdO2lufEtqajI++3yq9H3S5GGaWhijmbI9NYCamUS5qsW2Mnuv0UR5ouampqbWpchXLVJTk/HmkonS943fv+gVDZuVuHQl1q5di3vvvRcAEB0djR07duDxxx/HmjVrSHPjhJ6DUjBm4TxEt4736nHF+lN69Xx8UdXVnazHBfmlurMirZmiTeuh3FZrhuWqACB2dKtW/mjoXNzJ+Orqdq6YgkTUNDf21cLNam7kxzbSsZNw4zvEdys+PlqxXEsL40qov5W4MolyRavszey9qanJmDptOACgRYsQlyNf1fZrNIDCn3FJuOnTpw+2b98OABg+fDhycnLQsWNH3Hvvvfi///s/SxvYlOB4HrdNmwyAKcxS3kBgwPojADiVSC2bN69PkonJB1ij5OeX4rlnP9YNpXU2U2SMISenGHff/YruDMsdAUAQBCxZ8q2hc3LHROTOdq6q00tUNDeOZinzmhszNPZojsaKK1oYq4V3s7gyifLnkhiiACKv/wS4L4CYDaDwZ1xqYVhYGMrKbNkLb7jhBnz55ZdgjOGXX35Bx44dLW1gU6Jzn16ISUzwumDDGAMY8OGyb/HNP471p8pqeHzzTySOlDqvXeQJ0tIy8Nritc43hK3uS2LCv/Hii5/rdlRGC+mdySxwOsNyx57ujunHG/sDXFOnq2tu7M1SrmlujKLU3Ph3KHhTwhUtjNVCuVnM+ML5SottFE8KIL7WsFmJS6PskSNHcNttt6Fdu3YYMmQINm7cCABo3bo1SksdM3ESNqLi1UMQPQ3Hcdjw1jL0+dcNOFIajHcPt8Sq41H47lQkVh2PwnuHW+JIabCUwdgXGM0V8cMPew3NvqyeKbpqT3dH8+ON/cl/a0adrqa5cTRLGQ8FdwVyKPYNrrxbnhDKzWLEF86dd8hbeFIA8bWGzUpcGsmef/55vPLKKzhx4gR+/fVX/PKLLePjDTfcgN27d1vawKZEaZ5j+Ky3CAgIkLRGDfWnQnC6PBgMnJTBuHOfXj5pn9Wdnydmiq7a062MpPDE/lzBfLSU9WYp8rnxDa68W54Sys0in6QsWrjGIXzdm++Qq3hSAPG1hs1KXKoKvnr1anTo0AFt2rTB3r17peWbN29GWlqa4f1MmzYNt99+Oy666CJUVFTg559/xtSpU3H48GHd3w0fPhwvvPACzjvvPPz999+YOnUq1q1b58qpeJVju/aiODsH0a3jVTUkauG9VtG6kzFzoa+0S2Lnp1Zx3JXOTxSWkpJaqapnBUHA6dMFXlM9a1Urd7Uzt3p/ZlGriu5tzQ0JN77B1XdLFMoXvzYe7ds3BFTYV4r3NOIkJT39AJ588n2fvUOu4kkBxN/6TXdw2QaRk5ODPXv2oG3btlKF8N9++w1//fWXk182kJKSgjfffBNXX301Bg8ejKCgIGzcuBFhYWGav0lOTsaKFSvw7rvvonfv3lizZg3WrFmD7t27u3oqXoMJAtbMXwSAcyi3wATBI4INYwxni4pwyeCBhrb3pXbJSo2Ev8wU7Y9rZSSFNyMz5KSmJuPTFU86LL+4WwfF97q6BuGm5yUdLXdClOf1uPhi6zPdEuq4825ZGS5tBb56h9zBkyY+f+w3XcVWLtrsjzgOM2bMwOOPP46ICFvF5rKyMrz66quYM2eOy4Ug4+LikJeXh+uuu06KxrLns88+Q3h4OG655RZpWUZGBvbs2YOHH37Y6TEiIyNRWlqKqKgoySna2/QclILbpk1GTGKCtOxsUTEiYmMsPxZjDOdKShEWFanrT8MEAcU5uZgz9A6vVixXg+d5y2ZTqanJDjPFkyfzvDpTbEqIURqAY6QMYwwjR8zH6tU/IzU1GW//7xFFqPCpU3mY9Jg11z01NRmv/fdBtGvXoGm0cv+Ec+jd8h0N7yFT1XK7a1rz13trZvx2SbiZO3cuHnjgAcycORM//fQTAKBv376YNWsWli5dihkzZrjU8C5duuDIkSPo0aMHDh48qLrNP//8g4ULF+K1116Tls2aNQu33XYbLr30Uoftg4ODERISIn2PjIxEZmamT4UbwBYW3rlPL0TFx6E0Lx8cz+Phd9/wWXsYY1g+eTr2b073WRs8hZXCUnOG53kcP7EMSUlxms6MtbV1WLRoDR5//HZwnDKaw8qOV03Asmr/hHHo3fIdnhZA/PHeely4yczMxIQJE/D1118rlt96661YsmQJ2rVrZ3aX4DgOX331FWJiYtCvXz/N7aqqqjBmzBh89tln0rKHH34YM2fORGJiosP2M2fOxKxZsxyW+1q4sYcPDMSC37eC43mP+d3oUXm2HDOuvcHnWhvCf0lJ6YEtW+c53U7U3Ko9x6LNvnOncS51lM4ELHf3TxCNCX8UQDyJGeHGJSN1y5Yt8eeffzos//PPP9GypWshYm+++SZ69OiBUaNGufR7LebNm4eoqCjpI/oH+RudLu0JPiDAJ4INAIRGhPssUopoHBiNvhCrnavhbp6MppSHgyDcpTH6DHkLl4SbvXv34tFHH3VY/uijj0oVws3w+uuv4+abb8aAAQOQmZmpu212djYSEhIUyxISEpCdna26fXV1NcrKyhQffyTKy+UYVNvgo0gponFgZfinq3kymlIeDoIgPIdLoeBPPfUUvv32W1x//fXIyLDZ9pKTk9G+fXv861//MrWv119/Hampqejfvz9OnDjhdPuMjAwMGjRI4XMzePBgqR2NFU84E5slroN5cyLRfHAWJmoGVwWlppSHgyAIz+FSD7Vt2zZccMEFSEtLQ0xMDGJiYvDll1+ie/fu+Pe//214P2+++SbuuecejB49GmVlZUhISEBCQgJCQ0OlbZYvX465c+dK31977TUMHToUU6ZMwYUXXoiZM2fi8ssvxxtv+M4Z1wrKi4p9enzGGK66Y5jPMhQT/o8rdcDU9uFOJlp/yHRLEIT/45JDsRaXXHIJdu3ahcBAYwohrZDx++67D8uXLwcAbNmyBSdOnMDYsWOl9cOHD8eLL74oJfF76qmnDCfx84dQcDW6XN4bE99f4utmYMnYiTj6O2WZJrS5445rsOKzpxAYGKC6XqjP2cSY58JUPRkGSxCEf+LxaCktzAo3vsBfhRuO5zFjw5eITmjtsWR+Rvb78VPPYfe67y0/vn3o+7FdeykyqxFz+x3XYNWqaWBMPRz7lZe/xF2jr/NYmKq/5uEgCMJzmBm//VcKaWYwQcAvX6zF0Ecf9Mj+jQpMYoZiK4URtaSFxdk5WDN/UZPMq9Mc+HL1zxh+h34q/aef/tBjYaq+Lj9BEIR/Q8KNH5F/Sj9SzNNUV1QgqnU8Bj80FlcPH2aJMNJzUArGLJwHewVhdOt4jFk4D8unNM3Egc0BZwKGGKbqKTy9f4IgGi+mzFKrV6/WXR8TE4OUlBQyS7lIv7tH4rZpk33dDACOZixWb24wI4xIpjatQqF+VPKBIAiC8G88ZpYqKSlxuv7DDz80s0tChq8jpuTYm7E4ngcTBAybOgkHtmw3JIx07tNLof1xOAbPI7ZNIjr36UVOzARBEIRlmBJu7r//fk+1gwBQkpvn6yboIgojNzz8AI7s+N2pH47RpICUPJAgCIKwEkpq4kcc27UXxdk5fm+iuWHC/Zj4/hLM2PAleg5K0dxOdE52htHtCIIgCMIIJNz4EUwQsGb+IgCc3ws4QINTsJaA40xYY4KAoqxsHNu115PNJAiCIJoZJNz4Gfs3p2P5lOmqJqryomK/EnpsTsIMw6ZO0nQYloQ1u4SNooPy2gWL/eqcCIIgiMYPCTd+yP7N6XhxyO1YMnYizpWUSstXzZ4Pf9PqyJ2C1RCFtcqz5YrlxTm5FAZOEARBeAQSbvwUJgg4+vtu1FRWScv0tDq+Rs8peP/mdPzw3sfS9yVjJ2LO0DtIsCEIgiA8Agk3jYz9m9Mx96aRvm6GA06dgmVmqaO/7/Yr7RNBEATRtPDfbHsEAICp5Fi8ZsRt3m+IBmIiPudOwZaVMCMIgiAIXUhz0wi54NqrfHJcd5yCtSrAEwRBEITVkOamkXHT5Im4uG+yT45tn7W4OCcXaxcsNuY7Q8INQRAE4SVIuGlEDJ5wPwaMvcfXzQBgcwp2p1I4QRAEQXgKEm78mJ6DUhAV10r6PvSR8T5sjRKztaBIcUMQBEF4CxJu/JSeg1IwZuE8W912giAIgiAMQw7FfgjH87ht2mQAzMHPpdFCqhuCIAjCS5Dmxg/p3KcXYhITfN0MS6FoKYIgCMJbkObGD9HL9ttYUcvXQxAEQRCegIQbP8Rptl+CIAiCIDQh4cYPObZrL4qzczTDrBuliacRNpkgCIJonJBw44cwQcCa+YugVgGcCQLAgIqys75pnAocz6PL5b3R+8bB6HJ5b3A8PVYEQRCE7yCHYj9FrAB+27TJCufi4pxc7Fj9FYY++qAPWwf0vnEwSvPyERYbg9ueekzZxuwcrJm/SJG5uFFqmwiCIIhGCQk3fsz+zek4sGU7Ovfphaj4OJTm5ePYrr24dMggn7SHsYbQ9Hteel5aZk9063iMWTgPy6dMN1aagSAIgiAshIQbP4cJgkM2YH9zOLbPxcPxPJggYNjUSTiwZXu9KY00NwRBEIR3IOeIRojocOwPaCUZ5HgesW0S0blPLwBkliIIgiC8Bwk3jZAGh2Pv4kq25IacPSTcEARBEN6BzFKER4loGYveNw5GXId2mttwPO/gV0TVxgmCIAhXIeGmESIV1fRjGGNgglBfI0ubnoNSHCPCVKKtCIIgCMIoZJZqZHA8jxEzp/l1tXDRv0Yr381NkycCaBDSolvHK9aL0VY9B6V4tqEEQRBEk4SEm0bG9ePHIDw2xq+rhXMcJ33UGDD2Hlxyw8CGyud2QpDtO8OwqZMoISBBEARhGho5GhEcz6PfPXf6uhluw3EcRjw3FTGJCZrCi320FUEQBEEYhYSbRkTnPr0QHhPt62ZYQlh0lKHtmmKFdIIgCMKzkHDTiDAz0DeVvDL+lrCQIAiC8H8oWqoRYWag92efHBGhrs7mm6NimmKCgOKcXBzbtdcHLSMIgiAaM6S5aUSImYm1csAwxlB59iy+/9/7Xm6ZYzuMwAcEABznsL3t/DisXbCY8t0QBEEQpiHhphHRkJmYcxj0bfWbgM9mvIi/f/nNNw2sx4zWaNtHn6GitEyxrDgnl4puEgRBEC5Dwk0jY//mdCyfMh0luXmK5XKB4Niuvag4e9ZHLTRH0ZlsrHtzqfR9ydiJmDP0DhJsCIIgCJfh0MyK/kRGRqK0tBRRUVEoKytz/gM/xVnJgsET7sfQR8b7sIXGOVdahrCoSADA4z2TfdwagiAIwh8xM36TQ3EjhQkCjv6+W3P9pnc+aDTCTYvICF83gSAIgmhCkFmqidKYHHHlPjqUkZggCIJwFxpJCL+CMhITBEEQ7kLCTTNhz4bN+Pip51BWUOjrpuhyyeAB6HJ5b49rcDieR5fLe6P3jYO9cjyCIAjCe/i0R+/Xrx+++uorZGZmgjGGYcOG6W6fkpICxpjDJyEhwUstbrwUZ+dg97rvUV1R4eum6NJ39AhMfH8JZmz40mNVwXsOSsGMDV9i4vtLcM9Lz3v8eARBEIR38alwEx4ejr179+KRRx4x9bsLLrgAiYmJ0ic3N9dDLWy8OGgiGkHGYjnRreMxZuE8ywWOnoNSMGbhPES3jvfK8QiCIAjv49NoqfXr12P9+vWmf5ebm4uSkhIPtKhp0HNQCm6bNlmx7Oo7bsWJXXvBB/hfgBxjzCHxH8fzYIKAYVMn4cCW7ZY4SHM8X39dmIPw54njEQRBEL6hUToa7NmzB2fOnMHGjRtxzTXX6G4bHByMyMhIxacpo6WZCAkLq1/uf1W2tTIaczyP2DaJljkZd+7TCzGJCZr+NVYfjyAIgvANjUq4ycrKwkMPPYQ77rgDd9xxB06dOoWtW7eid+/emr+ZPn06SktLpU9mZqYXW+xddDUTHAdwjTPU2kw1dD1HYaP7MXM8giAIwv/wPxuFDocPH8bhw4el7xkZGejSpQsmT56Me++9V/U38+bNw8KFC6XvkZGRTVbAETUTWjSGSuFqGK2GLprj5NegODsHa+Yvwv7N6Yb3I27nLAs0QRAE4Z80KuFGjV9//RV9+/bVXF9dXY3q6movtsh3WKFxOP3HX2h38YUWtMZ9mCCgOCcXx3btBVCvlbnsUnS98jIwAEd/24Wjv+8GEwTJHGdfTUR0FF4+ZToObNmO4uwcRLeOV9VgyY/nTFAiCIIg/JdGL9xceumlyMrK8nUz/AKjmgk9Dqb/5BfCDWMMAIe1CxZLwsuImdMQHhvTsNGE+1FeVIwvnl+AYVMnwYij8Jr5izBm4TwHJ2abRsZ2vB4D+jkVlEjAIQiC8F98KtyEh4eja9eu0vdOnTqhV69eKCwsxKlTpzB37lwkJSVhzJgxAIDHHnsMx48fx8GDBxEaGopx48Zh4MCBuOGGG3x1Cn7FsV17dTUTRhgw5i6LW+UaHMehsrwcQIOTNFSsamEx0bh34Vxdk5vcUfjAlu3YsGQprn9oLAKDgqRtinNysXbBYhzYsh0zNnwJiqgiCIJovPhUuLn88suxdetW6fuiRYsAAB988AHGjh2LNm3aoEOHDtL64OBgvPrqq0hKSsK5c+ewb98+XH/99Yp9NGeYIGDtgsW4d+Fc1fBqIwSFhnqgZa4REm6L8DpXUmJzhlY5H47j6rU8zkkZcxdGz5vp4Je0/o13sGnpcjBBQJfLe+v7LckEJb3CpQRBEITv8Klwk56erjsAjx07VvH95Zdfxssvv+zpZjVqyotL3HIc9ienY47jwMCUpiiN7YzQLaUvoCIIDZk4HtlHjmH/5vRGHVFFDtAEQRA2Gr3PDaHEHwddd7BS2LKFw6vtj0mmJrMRVf4COUATBEE00PiSnhC6+Nug2xiQm5pEvyUtjQcTBBRlZeP4nv1+U3iTSkoQBEEoIc1NE8OZUzETBFSWn0OLyAgftM4ziD437mp5ouLjwATBaUTV7nXf45l1X/iFloRKShAEQThCmpsmhjg4A5zDYCYOzukfrvBJ2zyJFeYrUeslRlTV1dQo1hfn5GLLBx9jwH33+I2WhEpKEARBOELCTRNk/+Z0LJ8yHSW5eYrlxTm5WD5lOja98wHKi4oNRxn5O+4KNqKpSUzeN2PDlxj66IMIDA6Wtln/xjuY+68R6POvG6ClJRF9d7xpomrMDtAEQRCegsxSTZT9m9NxYMt2zeiZVbPn20wvcC1kvGlhl7xP5XIMmTgeAPwuTLyxOkATBEF4EtLcNGGYIODo77uxe933UpkCEUm7k5Prwxb6B9s+/gwVpWUYMWuaZj4dgKHv3SMN7a/7gH7WNlAHow7QYgkLgiCI5gAH+xzzTZzIyEiUlpYiKioKZWVlvm6OzxFzo3Qf0A/X/XsUwBxNLoQ5GGPY8OZS5J887ZV8M/K6WvJ7J/pYUbkIgiCaAmbGbxJuCAm1XCmEkvLiEoRFReoKgPZRVmcLC7Hzmw04uGW7xwQdtXtXlJWNtQsWk2BDEESTgIQbHUi40UfU5Jx/1eUYPOF+XzfH71j/xjsYMnE8ON41PyVPhoxzPI9X9v4EAMhYlYbVL75C4d8EQTQZzIzfZH8gFIh+OhveelfXl6M5Ul5UjE1Ll2Pbx5+5vI/ohNYYs8gzIePye1Vw+gzdO4Igmi0k3BCqKPLlNJGQcXfZ/slKMEHAwS3bXd6HaK4aPnOaR32bKAKOIIjmDAk3hCZiRFVtdY3zjZs4FWfPYtPS5QAaIpRcheM4RMTGoMvlva1qHkEQBCGDhBvCKc1JCaClpQoND0eP+hDvBq2We3S5oo/b+9CCtG0EQTRnSLghNBFDjAOCghTLm+rAqXteTJl9WMsh2My1aUYyI0EQhFch4YZQRVGQ0U5101T9OTiO0zw3+xpNzkLBjXDk153mG2mQpnqPCIIgjEDCDaGKs4KMTY3cEycNbRcVH4eeg1Iwe+u3Lh+LMYazRUU4unOPy/sgCIIgtKHaUoQqza3QYnzH9oa2i+vQzlZnSkMxwnH60WWMMYABP36yCpcOGeSVDMYEQRDNDRJuCFWMFlq0z8bbVGGCgOKcXFw9fBhspjptjZbe9SgvLgYHDkMffVBa5onEfk3VL4ogCMIIzcPmQJjGSEFGoOn4djg9D47DjtVfuWWqK8g8g/DoGIRFRymWR7eOx5iF1ib2S7rofHS5vHezMSsSBEHIoZ6PUEWRxM9OwBELMjYntn30GfJPnnZrHy3btrFVHbcTOGzfldFYriAXjnrfeAMmvr8EMzZ86ZFsyARBEP4MCTeEJmISv5LcPMXy4pxcbFiy1Eet8g1FZ7IRZ9AvRwsz0VhmaagMrsSsVojjeXS5vDd63ziYND8EQTRayOeG0GX/5nQc2LIdnfv0QlR8nOQACwBXDx+G6ITWTcY0pcdt0yaDMWbIx8gdPyRXHLnlYfv2GjWO58EEAcOmTsKB+rIR9vdS1MypVRbX8gcSC6yq7YcgCMLXkHBDOEUspmnPmvmLMGbhPDCVXDhNFU8KNgBQll9g+jdi2L4Wolbo+vFjcPXwYarCC4B6zY/SEVnU/CyfMl0ScMwIQQRBEL6Ag31v1sQxUzKdcE7PQSkY9eKzCI0I93VTmgRGhQS55qR15/Nww4T7ne7bFobOFKYm0X/qXEkJwqKjVM1QYqTYnKF3oMeAfpIQpLYfuRBEEARhJWbGbxJuCLfpeuVlePjdN3zdjCaBmAdHT0hQ05wY3beaVokJgiHfmrfufwR3zX0O0a3jnQpBZKIiCMJqzIzf5C1IuI2ayUoO5VwxDsdxAAfNyCnRcTi6dbxiuegPpIazsH2jTsPJd96uGwrvrlO0q5ATNEEQ9pDPDeE28lm6vXZAChtvHi45lsBxnCQkyAVHRb0v+3ByjczIVobtd7vuGkPbeTO7Nfn/EAShBk1xCLdRDLR2A2xxTi4+fPzp+oSApMExg712xlm9LzXNjJVh+8EtWhjarjQv3yvaFC0tlieSIhIE0bggzQ3hFuLMWYTjeZwtLMTObzbg4JbtUogwY0w1DwuhzbCpk1BTVSVpIMxqRI7u3IO37n8EHM+j7+gRCI+N0fS5qTp3DqEREW63uaygEGGxMZix4UvLtClqYecAtLVYduHv5P9DEM0P0twQLqM1cw6PicF194xCWHSUNLCICQGbA1b5GIXHRCs0EHEd2pn6/bniEvQY0A/PrPsCES1jNQUbgMPWDz61osk4uf8gxrwyx6k2xahmp+egFMzY8CUmvr8E97z0vJR1+frxY/zS/4cgCP+ANDeES+j6f2jMnJuLD4RVOX/k1/Fg+k+4evgw3Tw69uui4uNUc9fIOVtUjC9ffBn7f9iGfnePRFhMtLoQZDB/T8dLesDZMwGex21PPeZUs9OQddkx986QieOdtgUwp+2ixIQE0XQg4YZwCaOJ4+ROsXyg88etuVQZN4p4HUc895TT0G/769bmgi5QEzTkRLZqaRM4DCAIAjioR1cxQcDZomJEtmqp3b76cxnz6hwH3yz7ZIFGhGcjjtJGq9uTYzJBNC3ILEW4hNEZsbhdz0EpmLl5rdPtSbBR54rbbjb9m+DQUEOOvNGt43Hvq3M1fXIA233heR5Qicqyfeew69sNhtvmrHioU+fp+uVaTupMEFCUlS355+hBjskE0fQg4YZwCaMz4tK8fGnwCI+J8WyjCJfgeN5wtPi+jT+guqJCsay6ogLLp0zHwfraVU6PZ6B4aPcB/Yw1iHP0cRK1OmsXLHZqVnKmIbKiWjtBEN6H3ljCJY7t2lsf3q0+eIgz5+N79msOHoRxnNa0EhiKsrI9tn+RXkMGOSzb+c167N+c7vSZMEr3Af1w3T2jDG274c2lqKutVSwrLynFhiVLpUKhehjREJFjMkE0Pmi0IVyCCUJ9wUXOYTCTz5w7XdpTd/Bw/B3hEpzNDCVHL2uxFka2t893I5qGFM+EGxFjl908FM6qwojC86aly5H191HFuojYGAx99EHM2PClU5OSWfMqQRCNAxJuCJcRw7tLcvMUy4tzciXHUHODAvnbuENYTLTqcqvLX+hpefZvTseGJUtd8p1igoCygkJb2LozYZhrMDtpFW014jNjxrxKEETjgaKlCLfYvzkdB7Zs1wyhNTMoHPn1d5x/9RWeamqTx16g4DhOSqDIBQS4tA8j2AtP+SdPm99HvbZv17cbkHLvXU633/bRZ1JUVWxiouo2RpL5iaY0Z8VAjTgmEwSlE/AfSLgh3IYJgmbxzGO79qK8uAThGloFOV2vupxCwV1Ez0nX81dTKdy4ouUozsnF2gWLca6k1JBwIzovd+7TC4HBQZrbqaUkkCOa0sYsnKf67HE8j+DQUPQY0I9Cwv0YfxAqKJ2Af0FmKcKjMEHA9o8/N7w9CTaNj6g4penx+J79EOrqdH8j1/ase/0dzBl6hyGnZPsQbyt8ZkTzqr1jskhYdJSmeYsqkvserSzW3gzhp3QC/ge9iYTH2bR0OcqLip36fpBg0zgJaqF0ZO7cpxd4J2Yw+b3OPnJUEmb0HNXrf6kI8S7LLzDURmfb2cxW6s+nVki4PwyqzZ2egwdgzKJ5iE5orVjuTaGC0gn4J3S1CY/DBAGrZs8HmHbSNSMIZLv2S4JCQqSOu+egFIx9bYGp39sLvVqO6gAkR3UJowKxk+069+mFoJBg7Z/bhYQbmakb0epYtU1z5JLr++Pel18Ax3Gq5kRvCRWUTsA/IZ8bwiuIA5a9TdoMG996F0MmjiMNj0k87cfU9Yo+mL31W+xI+xoD7rvHdNCb2uaio/ore39yWC4nMq6VsTZeeRkiW7XU9MeIshNStIiKjzNUGmL4zKlO/S+M+GhY7cdhxDfFH/xXnNFzUArufXUuOF77YXPmb2UVlE7APyHhhvAa4oDV967h9YODMRhjKC8uweZlH6L/mNGaob+EOt4QBsNiojFg7D0uHe/ul57HnnXfY9XzL0GQ+b0YGVAjYmMMHWPwQ2Ol/1UFiKmPGdpPaV6+obpqEbGxDhopef0sAJpFQaVteN5Wh8sO+zpcRvGFMOUufGAgrr3zdsR1aIf8k6fx0+dfggmCJFwakaQ9LVRQOgH/xKf6zX79+uGrr75CZmYmGGMYNmyY09+kpKRg586dqKysxN9//40xY8Z4oaWEVTBBwNnCIld+ic59epFg46eIpgEjgo39oB8cGoorU2/Bgp3puP/1l3RNL/bryouKTbdVFA4uub6/4dIgYg6eqNbxuHbUHYaOo2cqceajMXzmVEtNLkbMaP7mFHvT5IlY8PtW3DZtMvqOHoHbpk3Ggt+3YvS8mYYTgwKeFyrMOsET3sGnmpvw8HDs3bsX7733HtLS0pxuf9555+Hbb7/F22+/jbvvvhuDBg3CsmXLkJWVhY0bN3qhxYQVmO1sOI5DRGwsulzRx0MtIryJlgDE8zy69++H7v37SdoCeya+v0ShSWjVPsn88esHxX+/8iIqysrgrDQIYwzgOES2aol7Fsw2fTz7Y8e2Uc/LI98mIjbW0H7kJhctc5IRM9qwqZPq74v+Nlr5gqzmpskTJU2gfVt63zjY0D4YYyjOzvG4UKFIJ1B/veXrjNY5I6zFp8LN+vXrsX79esPbT5gwAcePH8cTTzwBAPjzzz/Rt29fTJ48WVO4CQ4ORkhIiPQ9MjLSvUYTbuMscZoW5GnTfBC1BXrrtnzwMQbcd4/LPkV8QECjL+Yqmlz0zEnnSkqdmtGMCFze8F8BbKao/mNG246rlpjSRMZtbwkVWj6FYv6mpp7nxh/9tBqVz01ycjI2bdqkWLZhwwYsXrxY8zfTp0/HrFmzPNswwhR6Mx096urqINTVOQ0zJho/HM9rDmKiJqH/mNEA53mfIn92YC/Ny5fMSVr+O9s+/syy44kO1VYNZGr7uvbO23XfcfF+6Am1Ql0dPnzyWa8KFfZO8NWVlZgz9A7Va+OPwoCr7fI3Py2RRiXcJCYmIicnR7EsJycH0dHRCA0NRWVlpcNv5s2bh4ULF0rfIyMjkZmZ6fG2EvqYiZ5igoDykhIMeXgcnBVUJJoOekKFdzIvW4/NVJILjuPqNZeunYVocjm+Zz+eWfcF9MxJfW4aYkHLbcR1aIcZG760ZCDTGhTPHD6q8yslapMjJjB89MQM7N+01VR7rEAuCDBBUBUM/FUYcKVdzgRrs07vVtLkEyZUV1ejrKxM8SH8g/2b0/HikNuxZOxEpH+4wlbFWqPCOAd1fwCCaExwHIcTe/YhJDxMVbAxM3tfu2AxOl3a02mOlchWLXG2sMipw6szp9izRUUYMnG8JQnz9JyXL+6bbGgfv6Z9o5kLaZ8Fgo27+YXUNI/+5rTtTrv8PXlho9LcZGdnIyFBOctPSEhASUmJqtaG8H/EulRHf9+N47v2qtqsd6z+CkMffdD4Pp34YIidjj+bGwjv4s2aZpcOvV7zWOUlpeAAhEVHa2p15CYXo861gSHBQL2/ivzY4uThq5deQ0KXThjyyHiH39reFw6BQcGqZkCzDsdGHJxFuUDtOomToC9eeAlMEJzmQjKK3CQT16Edrh4+zFLtilHHbm85bbvbLiMpEbzlp6VGoxJuMjIy8K9//UuxbPDgwcjIyPBRiwgr0aowfumQQcZ+v2krel7fn4SWZoSeUGJUYBHq6lBdVYXQsDCrm2cKmxBhEyR0zVUchw49Lsb+77cYjjwMCQsDGFBXW4PA4IZMzMU5udi97nsMmzpJ3zzMQTcNg7OBTC44RLSMdTooAvVCjL0wVi/1bF3+qSInkruomWT08hQZEnBkP+d4Hn3vGm6ZMGClz46rQorR/EHdB/RrfsJNeHg4unbtKn3v1KkTevXqhcLCQpw6dQpz585FUlKSlMvm7bffxqOPPooFCxbgvffew8CBAzFy5EjcdNNNvjoFwmLUKowb7cCzjhxDz+v7O92OhJ+mg65fjsH7zAcE4FD6T+hjUAuihXww1tI46OUCElMeGKnBJoZJf/fa24YiDzmOAzggILChy18ydiLCYmMw5pU50PNlMxOhdP5VlzsMtGqCgxEOpf+Ii/tdA07mXMwEAVuXf4pvFy0xtS89tPxG3NVQyfdv5vydCQ1mfWOcCUKuZlg22i9f9+9ROL5rr9d9b3zqwHD55Zdjz5492LNnDwBg0aJF2LNnD55//nkAQJs2bdChQwdp+xMnTuCmm27C4MGDsXfvXjz++OMYN24c5bhp4hhNknX0t11ebhnRVOjRv5/uejPhx1oYFbacbSeu7z9mNDiebyg0aqCNcgHo2K69uO2px2DEl81o2wdPuF9RPFTLl8MI6ctXYOrl/aXvOcdOYOrl/S0VbPRMMlrbm6kT5cr56wkNZn1jjBR3dTXDsrN+WYL5xvfGp8JNenq6IrOp+Bk71pYqfezYsRgwYIDDb/r06YPQ0FB07doVy5cv90XTCS+iVylaniTr6O+7UV5c4pM2Eo2bYLvK5nKYIBgK0uM4DjzPe0UzyHEc+IAAXDvqDpwrKcXe738wfdwul/c2lenXKPIM0HqCg5YwJs/oKzc9CXV16HRpT3O5sZw4BTsreqmFEW0HY8yU4OQskzEfGIjhz021+T0ZcOA1Kgi5mmFZ3i/r4avCoRR6QjQKtCpFF+fkSjZwJgjY/vHnPmoh0WThOJTm+2ddoBsffRAT319i2C/N/reeQBxob5/xpH4kl5rpTjZZAZQ1wdqc38VB86An0BnRWnQfoK+x08KItoMPCDAsODnLZNxzUApmbl6LiJax2mZNmRBhJpJJMXm0EzidtWv/5nTDeZS8XTi0UTkUE80bLYdj+Uu3aely9Lt7JMJiojX9HgDyuyHM4a8VnYPDWrj8246X9rSwJUrEEHSzlJeU4ovZ8wEAL/60UdWJOTqhNcYsmocNby5FwekzqsfuMaCfbv6VDx9/Gvt/2IbLbjaXA4gJAopzcg2VdDDTx1SWn8Phn3egouysJHCIaPkEaREVH2faSVicPI5ZpMwKbiTD8sEt25Fy711O2+XtwqEk3BCNCjWHY/v1Xzy/APcunKsaLWM2fTtBGB2kvCk4W3EsfxPwGWMIj45Gh0u6Y8B992haO8R3WCs9xIwNXyIoJATaWgtbXbGNb7+HiJbGBTCxxpjRkg5mopdaREag15BB6DVkEMqLirFq9nzs35xu2icIsAkRrjgJ79+cDqG2DgFBNrFgydiJhqKwRLOWljBlRiC0EjJLEU2O8uISp1EpBOEJvCXYGK3A7mv0kgfaI0Z0GSmrobcuunU8wmNjdIUBPiAAQyaOM9QuBQzo1KeXoaR+Ql2dMYdbO8JiojFmkc0nxoxflNw3xqiWpCy/AECDb5I8BcHR33cbFuLUityK63xVOJSEG6LJ4U0TAmmBCJHGIGx4E6GuDr+u/Q5GI7mABkdptzRSHorK4Thb/qGUe+9S9d9RY+2CxVLyRDPHAYDhM6fh3lfnGPqNmGhRFCKMRjLdNfc53DR5ouSbJK/pZSZbspbZSu4T6W1IuCGaHN627RKEN2ksQhTH8xgwZjS2fvAJ6mpqfN0cB6QCnIJrExRnJRMYGM6VlrmkZbPlPIpBWHSUoe3PFhYphAg9J2H7cxgw9h6HkhoA3C4HsWb+IswZegfVliIIq3Aa2qgTgnq2qBhrX3rNk80jiGaBpOkYcxcKz2T7ujnacK5pYO2jjkTTjpzLh7mXYNZQhm1BwNevvoFzJaUOWqtzJSVOC9BqH8dYfhqO5xVRbSI3TByHngOvc9p+T2GrRtiMiIyMRGlpKaKioqiIZhNGHmEgfzmZIAD1L7JajZ3lU6bjwJbtmLHhS6dZX40gVjSPiI11az8E0ZjxZu0uX7D+jXccalH54pzPFhZi9QsvgzGm2v+5wpKxEzWDOHoOSsGImdMQHhujup4xhi3vf2xZ4kUz4zcJN0STRS1NeVFWNnav+x59/nWDw3J5yKMoHOnW+DGAmJKfp2rmRDPGH4UbK6PbGGMAs5tI+eicGWOoqaxCUGiIJcf//u33kHPshEPqDWkCqeP8LV7j5Y8/g/3fb3G7LSTc6EDCTfNCq66KkcJzgx8aa6oauRr+2KkTBGHtu+lP77kn23K2sBA7v9mAQ1t/xF1znzOs3S4rKMTsgbe4HTFFwo0OJNwQRuF4vt481dplDY4/dXoEQRD2eKuP0jNvGcXM+E26coLQQJ6/wdWQb3c7DdGsRRAE4Qm8NfnydpZvEm4IQgcxLfk5LxXkdBBkSLAhCKIJ4O0UHSTcEIQT9m9Ox8z+N2H9G+94tOq4arkIngcYQ1lBIT6eOhN/bP/ZY8cnHCGtGWEEek70KS8q9nr5BaotRRAGYIKA7//3PjYtXS45Ike0aonbpk6y7Bh61X4jW7VEaW4e3vu/qVjw+1ZFJlHCc5C/FGEEek702f7JSiq/QBD+jFi4c/e67xHbRrvqrieIio+DUFuLjC/WePW4BEEQrsAYg1BXh83vfuT1Y5NwQxAu0HNQCq67Z5RXj1mal4+eg1LQY4D7WT9JjU4QhKcRa4V1urSn149NZimCMAnH87ht2mTYsihoq6OZIKA4Jxccx9fng3DcljGG8uJi1FZVa+aMEPcTFhuDMa84FtITsyubCVcnNTpBEN7C25FSAGluCMI0nfv0QkxigvPkVZytSu+a+QsBwMHmzAQBYMAXsxc0FLlT2wYcvnrpNdz21GOwpVNXcTqGTf2rWTeLMVRXVZk4S4IgCGvwRTFjEm4IwiRGZyHbPvoM+zenS+HkJbl5ivXFOblSJV9n25QXl+gKVBzPO3UyDgoONtRugiAIq6iqqPB6pBRAZimCMI3RWcjBLdul//dvTseBLdt1Sz7obdP7xsGGjqkZceVDMxRlaSaI5ktwaCj+9dgEy4pnGoWEG4IwybFde1GcnePUR8Z+tiJGWumhtY0v1LpWQYINQTRv+o8ZjXWvvwOhttZrxySzFEGYpKEsg7aPzNoFiy3N6yAKVFr79EQOibKCQrz1wKNI/3AFzhYWWb5/giCaPmLE1LV33u7V45JwQxAuYMSPxkqMCFQVZWetOx5jWD3nFRz5dSe+evm/mDXgZiwZOxEfP/Uc1ixYbNlxjLSDIIjGT1yHdl49HpmlCMJFjPjRWH285VOm47ZpkxGT2JBAsDgnF2sXLEbi+V0w9JHxlhxrz7rvsf/7LdJ3ubmM43kMfvA+hMVEe9zkRCYtgjCOP/u3BYWGevV4JNwQhBsY8aOxEj2BKtBgNFRleTlCwsJUO0Ex784n02dr/p4JAlbNno8xC+eBwX87Uy38eQAgiKYIYwwXXHMlOJ73WhkGMksRRCNDXgLi6O+7pc7CqNPx1vc/ARjABKXJR553x1kHJJnlcnJdOwmTMMYsMVHlnzxtQWsIwj/xV6Gd4zjEJiagc59eXjsmaW4IoolgNIpr09LlyD5yTNO8ZdRfyF6LdLagEG0uPB+t2rVF/qlM9B8zGtEJ8ZodLmMMVeXnUFtdjfCYaM02i5oWp0kTDdAiKhJgDNDQWvnr4EAQTQFvZiom4YYgmgii0/GYhfPABEEhDNhHcVnlL2Rvlvv7153S/8VZ2ZqmK1EL89mMFwDAtp2dcCG2mQ+wTsEcHhOtuY4EG4LwLN5MaUFmKYJoQpiJ4tIyb1ndlnPFJQ7ryotLsHxyQ3bmLR98bNOoyGCM4dC2Hw0dq7qyUtdsxRiD4CVbP0EQSpggoCgr26uZijnYqv81GyIjI1FaWoqoqCiUlZX5ujkE4RE4nvdaFJeRtnS57FJ0vfIyMABHf9ulEKZ6DkrBmIXzAA6qmhsjBUF/TfsaV6be4qEz8DxVFRUIDg3VdPJuzlql5n7+jR3xPbYiRYaZ8ZvMUgTRBPF2FJceTBBw5LddOPLbLod18grrHMc7rGOCAEGo97nRjO4qweFffjck3JQXlyAsKlLXv8dZjS6rYIxh20ef4eCW7Ti2ay84nsfw555C7xsHI9jLYbP+DAk2jRvGGLYu/8Ty3F/OILMUQRA+w1mFdY7nwTt1JGYoyy8wdLztH38OvUSI1RXOzVtGcBbdJQgClj/+DL56+b+SFkuorcXK5+bi6asGYcnYifj+7fcA0ODeVGkuCSo5jsOA++5Bz0EpXj0uCTcEQfgMo9ETegVBI2JjAcaclqcoysrGpqXLNX2SNixZitCIcF1hguNsmaCdDUzievvtGGNgAsNHjz+jSJJo39ajv+9GzrETusdQOx7ReGguQqtt4sIwbOokSyIejUJmKYIgfIZV0RORca3cjhS7dMggQ8f66+cdTrfleR7r33gHVw8fpgy3z84xHG5v5to0l4GSaJxwPI/YNono3KeX18zlJNwQBOEzjOTmMTLbK83Lx9Hfd+uWp1CLFLPfhxFyj/9jaLv8k6fx4pDbXXbsdnZtfEHh6Uy0bJfk62YQjRTKc0MQRLPASG6e8qJihEVH6SYmFENM3cnfYzQJ4tHfdgET7ne6v9K8fLccuxXXxksRQ1rHEc89JCLC423wJBR55Vsozw1BEM0GZ7l5Vs2eDz0nYNHcJF/uSv4eI5XX1y5YjKO/7zbk32NZTg8DY7GVPjda575j9Ve6SRDlVJRal2bDyvxE5UXFlu3LFZggQKir81laBk+i50Tvizw3JNwQBOFz9m9Ox4tDbseSsRPx8VPPYcnYiZgz9A4pyZ/RxIRWtMPZsYwKQe4OYFKYvBNtg3gcNedl++/OBp/ljz+jee5G63KVFxVj5ez5+sczUSss9+hxQ9s5o6ygELMH3Yq37n8E5cUlDrXVPI34bGxd/ikMSayNFE++E2YgsxRBEH6BngnHqnIRRjByLFEIcrc+lx5imLwzzhYV47e136LPv25QbM8EAZwsZ095cTHCY2J0Ha73b07Hgc3pqufe5fLehtq9/ZOV2LfxB2x5/2MMGHuPw3rGmC11rMHxfd8P2xAZ1wphMdFumZR2fbsRQm0tjvy2C6tmzVM1hXoS+bNRfa4CQx990CvHNYurprsNby61pGadVVCGYoIgCBfxZCbo3jcOxj0vPe90u0+mzsSu7zY6tOX4nv3odGlPRdt6DOjnMPgUZWUbGnw4nseMDV9q+yQxhvLiYszqf3ND9unBAzB8xpOIaBnrcLzErp0NDfBvPfAoWkRGqGaxFo9rZDBeM38Rtn+yUvrec1CKw7UQ6urA8bzbfjmiVmrDkmXI/+eUw7PRcC1bq2bgZoyh8uxZVJVXILp1nNeSTlaUlSE0PKK+jbJs4fXno+ePNWfoHZKw6Kl3wsz4TcINQRCEH9Ll8t6Y+P4Sp9stGTvRlNOyO4OPVCoDTFX7o2Ym1Doex/OYnf4twqLVNTJi9ulZ/W8CEwT0HJSCETOnITw2xmE7I8KIKATqtS0sNgZjXpmjeX41NdUIDglxeizGGLa8/zG+XaR9/4xcSwC621SdO4eQ8DDLhLHljz8DCIKD0He2qAjhMTE2E6nB++4JSLjRgYQbgiAaA041JXYzZm+hpvEwqv1R29eYRfMA2NUVEwfbycpBU61OWac+vTD0kfFOj2VUCNQ6v69eeg3Dn5vq1Dwm1NXho6eew76NP7h8LPm11Npmx+qvLDNtMcaw5YNP8e3CNwCoC6TuaP2sgoQbHUi4IQiiseCKpsQbWGl6UB+8c7B2wSJD5+bUxOOCEKh2fp379DKkSVv/5lKpdIarx7Jvp9o2lw4ZZMhsaRQjwp+vC/I2usKZEydOxJNPPonExETs3bsX//nPf/Dbb7+pbjtmzBh88MEHimWVlZVo0aKFF1pKEAThPbzhuOwKVhZmdddZ3EiuJLOROmrnZzQBXf4/pwwfR+tYRraxOmeMkfPzp4K8zvC5cDNy5EgsXLgQEyZMwI4dOzBp0iRs2LABF154IfLy8lR/U1JSggsvvFD6TnVVCIJoqngzUsxXuDtoekMINCpMeCtRnZR0MqG1ppnM5phcjhaRzpMvejPBnjfwuVnql19+wW+//Yb//Oc/tgZxHE6dOoXXX38dCxYscNh+zJgxWLx4MWJjYx3WGYHMUgRBEE0TT5pN/NEHSjJbakSRAcCHU57W9RXyle+WK5gZv32axC8oKAiXXXYZNm3aJC1jjGHTpk1ITk7W/F1ERAROnDiBkydPYs2aNejWrZvmtsHBwYiMjFR8CIIgiKaHq9mpje7bG8kbzSBqrM4VlzisKy8uwfLJ07Fv01Zblm8Gh8SFvmq3N/CpWSouLg6BgYHIyclRLM/JycFFF12k+pu//voL999/P/bt24fo6Gg88cQT+Pnnn9G9e3dkZmY6bD99+nTMmjXLE80nCIIgmhH+6AMlmi3to8jkwp0/ttsbMF992rRpwxhj7Oqrr1YsX7BgAfvll18M7SMwMJD9/fff7Pnnn1ddHxwczCIjI6VP27ZtGWOMRUZG+uy86UMf+tCHPo33w/E863J5b9b7xsGsy+W9GcfzPm9TU263+ImMjDQ8fvtUc5Ofn4/a2lokJChTjCckJCA7O9vQPmpra7F792507dpVdX11dTWqq6vdbitBEARBAGhUUUNyGmu7XcGnPjc1NTXYuXMnBg0aJC3jOA6DBg1CRkaGoX3wPI+ePXsiKyvLU80kCIIgCKKR4VM108iRI1lFRQW799572UUXXcTefvttVlhYyFq3bs0AsOXLl7O5c+dK2z/77LNs8ODBrFOnTqx3797s008/ZefOnWMXX3yx5Wot+tCHPvShD33o4x+fRmOWAoCVK1ciPj4ezz//PBITE7Fnzx4MHToUubm5AIAOHTpAkHlxx8bGYunSpUhMTERRURF27tyJa665Bn/88YevToEgCIIgCD/C53luvA3luSEIgiCIxkejyXNDEARBEARhNSTcEARBEATRpCDhhiAIgiCIJgUJNwRBEARBNClIuCEIgiAIoknh81BwX0EFNAmCIAii8WBm3G52wo14cdSKbBIEQRAE4d9ERkY6DQVvdnluAKBt27YeyXETGRmJzMxMJCUlUQ6dRgTdt8YJ3bfGCd23xom/3LfIyEicOXPG6XbNTnMDwNCFcYeysjJ6aRshdN8aJ3TfGid03xonvr5vRo9NDsUEQRAEQTQpSLghCIIgCKJJQcKNhVRVVWHWrFmoqqrydVMIE9B9a5zQfWuc0H1rnDS2+9YsHYoJgiAIgmi6kOaGIAiCIIgmBQk3BEEQBEE0KUi4IQiCIAiiSUHCDUEQBEEQTQoSbixi4sSJOH78OCoqKvDLL7/giiuu8HWTmg0zZ84EY0zx+eOPP6T1ISEheOONN5Cfn4+ysjJ88cUXaN26tWIf7du3xzfffIPy8nLk5OTgpZdeQkBAgGKblJQU7Ny5E5WVlfj7778xZswYr5xfU6Jfv3746quvkJmZCcYYhg0b5rDN7NmzcebMGZw7dw7ff/89unbtqlgfGxuLjz/+GCUlJSgqKsKyZcsQHh6u2KZnz57Ytm0bKioqcPLkSTz55JMOxxk+fDj++OMPVFRUYN++fbjxxhutPdkmhLP79v777zu8g+vWrVNsQ/fNu0ybNg2//vorSktLkZOTg7S0NFxwwQWKbbzZN/pijGT0ce8zcuRIVllZye677z528cUXs//973+ssLCQxcfH+7xtzeEzc+ZMtn//fpaQkCB9WrVqJa1fsmQJ++eff9iAAQNYnz592M8//8x+/PFHaT3P82zfvn1s48aNrFevXmzo0KEsNzeXzZkzR9rmvPPOY2fPnmWvvPIKu+iii9gjjzzCampq2A033ODz829Mn6FDh7IXXniB3XbbbYwxxoYNG6ZY/9RTT7GioiJ26623sp49e7I1a9awo0ePspCQEGmb7777ju3evZtdeeWV7Nprr2WHDx9mn3zyibQ+MjKSZWVlsY8++oh169aN3Xnnnay8vJyNHz9e2iY5OZnV1NSwJ554gl100UXs+eefZ1VVVax79+4+v0b++HF2395//3323XffKd7BmJgYxTZ037z7WbduHRszZgzr1q0bu+SSS9g333zDTpw4wcLCwqRtvNU3+miM9P1NaOyfX375hb3++uvSd47j2OnTp9nUqVN93rbm8Jk5cybbvXu36rqoqChWVVXF7rjjDmnZhRdeyBhj7KqrrmKAreOura1lrVu3lrZ56KGHWHFxMQsKCmIA2Pz589n+/fsV+16xYgVbt26dz8+/sX7UBskzZ86wxx9/XHH/Kioq2J133skAsIsuuogxxthll10mbTNkyBBWV1fH2rRpwwCwCRMmsIKCAuneAWDz5s1jf/zxh/T9s88+Y19//bXi2BkZGeytt97y+XXx94+WcJOWlqb5G7pvvv/ExcUxxhjr168fA7zbN/pijCSzlJsEBQXhsssuw6ZNm6RljDFs2rQJycnJPmxZ8+L8889HZmYmjh49io8//hjt27cHAFx22WUIDg5W3J+//voL//zzj3R/kpOTsX//fuTm5krbbNiwAdHR0ejevbu0jXwf4jZ0j62jU6dOaNOmjeI6l5aWYseOHYp7VVRUhJ07d0rbbNq0CYIg4KqrrpK22bZtG2pqaqRtNmzYgIsuuggxMTHSNnQ/raV///7IycnBn3/+iSVLlqBly5bSOrpvvic6OhoAUFhYCMB7faOvxkgSbtwkLi4OgYGByMnJUSzPyclBYmKij1rVvNixYwfuu+8+DB06FA8//DA6deqE7du3IyIiAomJiaiqqkJJSYniN/L7k5iYqHr/xHV620RHRyM0NNRTp9asEK+13ruUmJio6GgBoK6uDoWFhZbcT3pnXWP9+vW49957MWjQIEydOhUpKSlYt24deN42xNB98y0cx2Hx4sX48ccfcfDgQQDwWt/oqzGyWVYFJ5oW69evl/7fv38/duzYgX/++QcjR45ERUWFD1tGEM2Dzz//XPr/wIED2LdvH44dO4b+/fvjhx9+8GHLCAB488030aNHD/Tt29fXTfEapLlxk/z8fNTW1iIhIUGxPCEhAdnZ2T5qVfOmpKQEhw8fRteuXZGdnY2QkBBJJSsivz/Z2dmq909cp7dNSUkJKisrPXUqzQrxWuu9S9nZ2Q7RHAEBAWjZsqUl95PeWWs4fvw48vLypEg3um++4/XXX8fNN9+MAQMGIDMzU1rurb7RV2MkCTduUlNTg507d2LQoEHSMo7jMGjQIGRkZPiwZc2X8PBwdOnSBVlZWdi5cyeqq6sV9+eCCy5Ax44dpfuTkZGBnj17Ij4+Xtpm8ODBKCkpwaFDh6Rt5PsQt6F7bB3Hjx9HVlaW4jpHRkbiqquuUtyr2NhY9OnTR9pm4MCB4HkeO3bskLa57rrrEBjYoJgePHgw/vzzTxQXF0vb0P30HElJSWjVqhWysrIA0H3zFa+//jpSU1MxcOBAnDhxQrHOW32jL8dIn3txN/bPyJEjWUVFBbv33nvZRRddxN5++21WWFio8DCnj+c+L7/8MrvuuutYx44dWXJyMtu4cSPLzc1lcXFxDLCFO544cYL179+f9enTh/3000/sp59+kn4vhjuuX7+eXXLJJeyGG25gOTk5quGOCxYsYBdeeCF7+OGHKRTchU94eDjr1asX69WrF2OMsUmTJrFevXqx9u3bM8AWCl5YWMhuueUW1qNHD5aWlqYaCr5z5052xRVXsGuuuYb99ddfipDiqKgolpWVxZYvX866devGRo4cyc6ePesQUlxdXc2mTJnCLrzwQjZz5kwKKXbxvoWHh7OXXnqJXXXVVaxjx45s4MCB7Pfff2d//fUXCw4Opvvmo8+bb77JioqK2HXXXacI0Q8NDZW28Vbf6KMx0vc3oSl8HnnkEXbixAlWWVnJfvnlF3bllVf6vE3N5bNixQqWmZnJKisr2alTp9iKFStY586dpfUhISHsjTfeYAUFBezs2bNs9erVLCEhQbGPDh06sG+//ZaVl5ez3Nxc9vLLL7OAgADFNikpKWzXrl2ssrKSHTlyhI0ZM8bn597YPikpKUyN999/X9pm9uzZLCsri1VUVLDvv/+enX/++Yp9xMbGsk8++YSVlpay4uJi9u6777Lw8HDFNj179mTbtm1jFRUV7NSpU+ypp55yaMvw4cPZn3/+ySorK9n+/fvZjTfe6PPr468fvfsWGhrK1q9fz3JyclhVVRU7fvw4+9///ucwcNF98+5HC3m/5c2+0dtjJFf/D0EQBEEQRJOAfG4IgiAIgmhSkHBDEARBEESTgoQbgiAIgiCaFCTcEARBEATRpCDhhiAIgiCIJgUJNwRBEARBNClIuCEIgiAIoklBwg1BEARBEE0KEm4Igmh2HD9+HI899pivm0EQhIcg4YYgCI/y/vvvIy0tDQCwZcsWLFq0yGvHHjNmDIqKihyWX3HFFXjnnXe81g6CILxLoPNNCIIg/IugoCDU1NS4/Pv8/HwLW0MQhL9BmhuCILzC+++/j/79+2PSpElgjIExho4dOwIAunfvju+++w5lZWXIzs7Ghx9+iFatWkm/3bJlC/6/vbsJSS6LwwD+WCFI9ilUUCYZQdIiIqNFgWQftBAKgloUuG1TEkEmLlwXQUGE0KIiCLFNRS0qpKKFbrIgIxdFSgUuiiwyAsPOLAZkfH1n3oaZt+Ly/OAPes+551zv6vGeA3d2dhbT09O4u7vDzs4OAGBkZASnp6eIxWK4vr7G3NwcsrOzAQAGgwFLS0vIz89PzudwOACkL0up1Wqsr6/j+fkZT09PcLvdKCoqSrY7HA6cnJxgYGAAoVAIj4+PcLlcUCqVv/2+EdG/x3BDRJ/CYrHA6/Vifn4eJSUlKCkpwc3NDfLy8rC3t4eTkxPo9Xp0dnaiuLgYq6urKeebzWbE43E0NTVhcHAQAPD+/o7h4WHU1NTAbDbDaDRicnISAOD1emGxWPD09JScb2pqKu26ZDIZNjY2UFhYCIPBgPb2dmi1Wrjd7pR+lZWV6O7uhslkgslkgsFgwPj4+G+6W0T0X335q9lZLJZ0a3FxUaytrQkAYn9/X0xPT6e02+12sb29nXKstLRUCCFEVVVV8jy/3//LuXp6esTd3V3yu9lsFtFoNK1fKBQSFotFABBtbW3i7e1NlJWVJdt1Op0QQgi9Xi8ACIfDIWKxmFAqlck+ExMTwufzffn9ZbFY6cU9N0T0pWpra9HS0oLn5+e0tsrKSlxcXAAA/H5/WntraytsNhuqq6uRm5uLrKwsKBQKKBQKvL6+fmh+nU6Hm5sb3N7eJo8Fg0FEo1HodDocHR0BAMLhMGKxWLJPJBJJWboiou+D4YaIvpRSqcTm5iasVmtaWyQSSX5+eXlJadNoNNja2oLT6YTdbsfDwwOam5uxsLAAuVz+4XDzUT9uYBZCICODK/tE3xHDDRF9mng8jszMzJRjx8fH6OnpQTgcRiKR+PBY9fX1yMjIwOjoKIQQAIDe3t5fzvejYDAItVqNsrKy5NMbnU6HgoICnJ+ff/h6iOj74N8OIvo04XAYjY2N0Gg0UKlUkMlkmJubQ2FhIVwuF/R6PbRaLTo6OrCwsPCPT0YuLy8hl8sxNDSEiooKDAwMJDca/3W+nJwcGI1GqFQqKBSKtHE8Hg8CgQBWVlZQV1eHhoYGLC8v4+Dg4KdLYUT0/THcENGnmZqaQiKRwPn5Oe7v71FeXo5IJIKmpiZkZmZid3cXgUAAMzMzeHx8xPv7+9+OdXp6ipGREVitVpydnaG/vx82my2lj8/ng9PphNvtxv39PcbGxn46VldXF6LRKA4PD+HxeHB1dYW+vr7/9bcT0eeR4c+dxURERESSwCc3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpDDdEREQkKQw3REREJCkMN0RERCQpfwDOmQuTZiy7BgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_its, train_losses = zip(*metrics.train_losses)\n",
    "val_its, val_losses = zip(*metrics.val_losses)\n",
    "plt.plot(train_its, train_losses, '-o')\n",
    "plt.plot(val_its, val_losses, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', \"Valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "The adapted weights are: $\\tilde{W} = W + c \\cdot \\mathbf{b}^\\top \\mathbf{a}$. Note, this process can be destructive if the inputs are in low precision and they have very different magnitudes. Tuning the `scale` parameter, $c$, prior to fine-tuning can improve the model performance after fusion.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlx_lm.fuse --model {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `lora_fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = load(\"lora_fused_model\")\n",
    "# num_correct = 0\n",
    "# for prompt, answer in tqdm.tqdm(test_set[:num_test]):\n",
    "#     response = generate(model, tokenizer, prompt, max_tokens=2)\n",
    "#     num_correct += (response==answer)\n",
    "# test_acc = num_correct / num_test\n",
    "# print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your LoRA adapters are not working well it's critical to plot both the trianing loss and validation loss over the duration of fine-tuning. There are really only two cases to consider: underfitting or overfitting. And you can figure out which regime you are in based on the above plot.\n",
    "\n",
    "**Underfitting**: The trianing loss is not low enough and the validation loss closely matches the training loss. You could also measure the accuracy on the training set itself for question-answering style tasks like HellaSwag. If you are in this regime you have a few options to improve the results:\n",
    "\n",
    "- Use more adapters. Increase `lora_layers` or adapt more of the linear layers within a given block by setting `lora_parameters[\"keys\"]`.\n",
    "- Use a higher rank. A higher rank means more parameters per adapter.\n",
    "- If you are using dropout, decrease the droupout rate or turn it off entirely.\n",
    "- Sometimes, underfitting issues are really optimization issues. In these cases it can be helpful to tune the learning rate or learning rate schedule.\n",
    "- If none of the above works, try a bigger model. For example, try Phi-3 medium instead of Phi-3 tiny.\n",
    "\n",
    "**Overfitting**: The trianing loss keeps going down but the validation loss stops going down and even starts to go up. If you are in this regime you also have a few options:\n",
    "\n",
    "- The best thing to do is to use more trianing data if you have it.\n",
    "- Contrary to the underfitting regime decreasing the capacity of the model can help. For example, use fewer adapters, a lower LoRA rank, or a smaller model size.\n",
    "- If you are not using dropout, use it.\n",
    "\n",
    "If you find your adapters work well pre-fusion but stop working post-fusion, try tuning the `scale` parameter, $c$, prior to fine-tuning. Typically the adapters have a smaller magnitude than the weights, so using a larger scale helps.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "Fine-tuning a large LM with LoRA requires a machine with a decent amount of memory. Here are some tips to reduce memory use should you need to do so. \n",
    "\n",
    "- Try quantization (QLoRA). You can use QLoRA by generating a quantized model with `mlx_lm.convert` and the `-q` flag or by using an already quantized model from HuggingFace.\n",
    "\n",
    "- Try using a smaller batch size. You can set the `batch_size` parameter in the `TrainingArgs` or pass `--batch-size` if you are using the CLI. The default is 4 so setting this to 2 or 1 will reduce memory consumption. Note, this may slow things down a little..\n",
    "\n",
    "- Reduce the number of layers to fine-tune with by setting `lora_layers` to a smaller value or passing `--lora-layers` if you are using the CLI. The default is `16`, so you can try `8` or `4`. This reduces the amount of memory needed for back propagation. It may also reduce the quality of the fine-tuned model and you may need to compensate with a larger `rank`.\n",
    "\n",
    "- Longer examples require more memory. If it makes sense for your data, one thing you can do is break your examples into smaller sequences when making the `train`, `valid`, and `test` data sets.\n",
    "\n",
    "- Gradient checkpointing lets you trade-off memory use (less) for computation (more) by recomputing instead of storing intermediate values needed by the backward pass. You can use gradient checkpointing by passing `grad_checkpoint=True` to the `TrainingArgs` or the `--grad-checkpoint` flag if using the CLI. Gradient checkpointing will be more helpful for larger batch sizes or sequence lengths with smaller or quantized models.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project.\n",
    "- We also have an example of [LoRA fine-tuning in MLX Swift](https://github.com/ml-explore/mlx-swift-examples/tree/main/Applications/LoRATrainingExample)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
