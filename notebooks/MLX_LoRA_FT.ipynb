{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to [LoRA fine-tune](https://arxiv.org/abs/2106.09685) an LLM with MLX LM. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it in the right format for MLX LM.\n",
    "2. Setup and run LoRA training. We'll show how to capture the training logs and plot some statistics to visualize the performance.\n",
    "3. Evaluate on the test set. We'll compute the final question-answer accuracy of the fine-tuned model.\n",
    "4. Fuse the resulting adapters into the base model and upload to Hugging Face.\n",
    "5. Discuss tips for debugging accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "664272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install mlx-lm\n",
    "# pip install matplotlib\n",
    "# pip install rouge-score\n",
    "# pip install scikit-learn\n",
    "# pip install tqdm\n",
    "# pip install numpy\n",
    "# pip install json\n",
    "# pip install pathlib\n",
    "# pip install transformers\n",
    "# pip install sentencepiece\n",
    "# pip install datasets\n",
    "# pip install torch\n",
    "# pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1131315",
   "metadata": {},
   "source": [
    "### MLFOW CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4d1cf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae62a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1d438bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_name=\"finetuning_comparativa\"\n",
    "exp_name=\"MLX1.0\"\n",
    "corrida_name=\"MLX-30_2_1e4\"\n",
    "# base_model = \"Llama-3.2-1B-Instruct\"\n",
    "# four_bits = True \n",
    "# dataset_path = \"FAQ_All.jsonl\"\n",
    "# dataset_type = \"alpaca_chat.load_qa\"\n",
    "# output_dir = \"../trained_models/adapters/adapters.safetensors\"\n",
    "output_path = \"../trained_models/adapters_30_2_1e4/\"\n",
    "output_dir = output_path + 'adapters.safetensors'\n",
    "sequence_len = 2048\n",
    "lora_layers = 8\n",
    "lora_layers_scale = 20.0\n",
    "grad_checkpoint_value = True\n",
    "\n",
    "\n",
    "\n",
    "lora_r = 8\n",
    "# lora_alpha = 16\n",
    "lora_dropout = 0.0\n",
    "# gradient_accumulation_steps = 4\n",
    "optimizer = \"adam\"\n",
    "# weight_decay_value = 0.02\n",
    "lr_scheduler = \"linear\"\n",
    "learning_rate_value = 1e-4\n",
    "batch_size = 2\n",
    "\n",
    "epochs = 30\n",
    "steps = 15360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0c0069b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/880645134898555871', creation_time=1733877767863, experiment_id='880645134898555871', last_update_time=1733877767863, lifecycle_stage='active', name='MLX1.0', tags={}>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"https://4z0r6nts-5000.usw3.devtunnels.ms/\") \n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(experiment_name=exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "61698208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello stats: 1024 lenght trainging dataset\n",
      "An example:\n",
      "\n",
      "{\n",
      "  \"pregunta\": \"\\u00bfPuedo presentar una p\\u00f3liza de bonos en formato electr\\u00f3nico?\",\n",
      "  \"respuesta\": \"Este tipo de garant\\u00eda es v\\u00e1lida, \\u00bfno?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# jsonl file path\n",
    "\"\"\" save_path = \"./dataset/FAQ_All.jsonl\"\n",
    "with open(save_path, 'r') as file:\n",
    "\tdataset = [json.loads(line) for line in file]\n",
    "\"\"\"\n",
    "\n",
    "# csv file path\n",
    "save_path = \"./datasets/Parph_Data/FAQs_1000.csv\"\n",
    "dataset = []\n",
    "with open(save_path, 'r', encoding='utf-8') as file:\n",
    "\treader = csv.DictReader(file)\n",
    "\tfor row in reader:\n",
    "\t\tdataset.append(row)\n",
    "\n",
    "print(f\"Hello stats: {len(dataset)} lenght trainging dataset\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9b607237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "valid_set = [dataset[i] for i in perm[:valid_size]]\n",
    "train_set = [dataset[i] for i in perm[valid_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c38c4e",
   "metadata": {},
   "source": [
    "Finally, put the data splits in the MLX LM training format. The format simply expects the data to be in a container which supports random access to the individual examples (e.g. a Python `list`):\n",
    "```\n",
    "[\"An example for the model.\", \"Another example for the model.\", ...]\n",
    "```\n",
    "For more details, see the [documentation on supported formats](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ea738f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\"\"\"def preprocess(dataset):\n",
    "    return [\n",
    "        f\"Question: {clean_text(t['question'])}\\n\"\n",
    "        f\"Answer: {clean_text(t['answer'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\"\"\"\n",
    "def preprocess(dataset):\n",
    "    return [\n",
    "        f\"pregunta: {clean_text(t['pregunta'])}\\n\"\n",
    "        f\"respuesta: {clean_text(t['respuesta'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\n",
    "\n",
    "train_set, valid_set = map(preprocess, (train_set, valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [Phi-3 mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). At 3.8 billion parameters, Phi-3 mini is a high-quality model that is also fast to fine-tune on most Apple silicon machines. Also, it has a [permissive MIT License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE).\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c3ff309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm.utils import load, generate\n",
    "from mlx_lm.tuner.trainer import train, evaluate, TrainingArgs\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters and make the training arguments. See the [training argument class](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/trainer.py#L31-L63) for a more detailed list of training parameters. \n",
    "\n",
    "Recall the LoRA update is $W^\\top \\mathbf{x} + c \\cdot \\mathbf{a} \\mathbf{b}^\\top \\mathbf{x}$ where $\\mathbf{a}$ has shape `(D, rank)`.\n",
    "\n",
    "With that in mind, the LoRA parameters to attend to are:\n",
    "- `lora_layers`: The number of Transformer blocks from the top of the model to adapt.\n",
    "- `rank`: The rank of the low-rank adapters. A larger rank implies more adapter parameters per linear layer.\n",
    "- `scale`: This is the constant $c$ that scales the low-rank update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f0851dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save the adapter config and weights\n",
    "adapter_path = Path(\"../trained_models/adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"lora_layers\": lora_layers,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": lora_r,\n",
    "    \"scale\": lora_layers_scale,\n",
    "    \"dropout\": lora_dropout,\n",
    "    \"epochs\": epochs\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)    \n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=output_dir,\n",
    "    iters=steps,\n",
    "    steps_per_eval=50,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_length=sequence_len,\n",
    "    grad_checkpoint=grad_checkpoint_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the Phi-3 mini model. Note this may take a few minutes to download from HuggingFace if you haven't downloaded it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fb0b16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../original_models/Llama-3.2-1B-Instruct-bf16\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the model, freeze it's parameters so we don't train them. Then convert linear layers to LoRA layers using the MLX LM utility `linear_to_lora_layers`. The adapters in the `LoRA` layers are not frozen, so they will be included in the model's `trainable_parameters`. Check-out the [LoRA layer implementation](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/lora.py#L72-L104) to see how it all works.\n",
    "\n",
    "By default, MLX LM only adapts the query, key, and value projection matrices for Phi-3. You can specify the layers to adapt by setting `lora_parameters[\"keys\"]` to a list of layer names. In this case it defaults to `[\"attn.qkv_proj\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "50e1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 425984\n"
     ]
    }
   ],
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"lora_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34ee27",
   "metadata": {},
   "source": [
    "### Evaluate Functions\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each example in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "37e55f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > 0.8\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    # Adjust generation parameters\n",
    "    generation_params = {\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    for example in tqdm(dataset):\n",
    "        # Get prompt and true answer\n",
    "        prompt = example[\"prompt\"]\n",
    "        true_answer = example[\"response\"]\n",
    "        \n",
    "        # Generate prediction\n",
    "        response = generate(model, tokenizer, prompt, **generation_params)\n",
    "        \n",
    "        # Store prediction and true label\n",
    "        all_preds.append(response)\n",
    "        all_labels.append(true_answer)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(true_answer, response)\n",
    "        for key, score in scores.items():\n",
    "            rouge_scores[key].append(score.fmeasure)\n",
    "\n",
    "        # Calculate loss/perplexity\n",
    "        tokens = tokenizer.encode(prompt + true_answer)\n",
    "        tokens = mx.array(tokens)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(tokens[None])[0]\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        targets = tokens[1:]\n",
    "        logits = logits[:-1]\n",
    "        loss = mx.mean(nn.losses.cross_entropy(logits, targets))\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = sum(1 for x,y in zip(all_preds, all_labels) if similar(x.strip(), y.strip())) / len(all_preds)\n",
    "    \n",
    "    # Convert predictions and labels to match format for F1\n",
    "    pred_labels = [1 if similar(p.strip(), l.strip()) else 0 for p,l in zip(all_preds, all_labels)]\n",
    "    true_labels = [1] * len(all_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='binary')\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge_scores = {key: np.mean(scores) for key, scores in rouge_scores.items()}\n",
    "\n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    perplexity = float(mx.exp(mx.array(avg_loss)))\n",
    "\n",
    "    return accuracy, f1, perplexity, avg_rouge_scores\n",
    "\n",
    "# Load test dataset\n",
    "test_set_path = \"./datasets/Parph_Data/FAQs_200_testing.jsonl\" \n",
    "with open(test_set_path, 'r') as file:\n",
    "    test_set = [json.loads(line) for line in file]\n",
    "\n",
    "# Define number of test samples\n",
    "num_test = len(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer, but you can specify any [optimizer](https://ml-explore.github.io/mlx/build/html/python/optimizers/common_optimizers.html) with any [scheduler](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html). We also added a custom class to capture the training and validation loss to plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "984516d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder ../trained_models/adapters_30_2_1e4/ created\n",
      "Starting training..., iters: 15360\n",
      "Iter 1: Val loss 3.567, Val took 5.640s\n",
      "Iter 10: Train loss 3.057, Learning Rate 1.000e-04, It/sec 1.585, Tokens/sec 235.460, Trained Tokens 1486, Peak mem 19.940 GB\n",
      "Iter 20: Train loss 2.792, Learning Rate 1.000e-04, It/sec 1.524, Tokens/sec 262.507, Trained Tokens 3208, Peak mem 19.940 GB\n",
      "Iter 30: Train loss 2.581, Learning Rate 1.000e-04, It/sec 1.506, Tokens/sec 260.445, Trained Tokens 4937, Peak mem 19.940 GB\n",
      "Iter 40: Train loss 2.373, Learning Rate 1.000e-04, It/sec 1.587, Tokens/sec 252.543, Trained Tokens 6528, Peak mem 19.940 GB\n",
      "Iter 50: Val loss 2.616, Val took 4.010s\n",
      "Iter 50: Train loss 2.586, Learning Rate 1.000e-04, It/sec 28.170, Tokens/sec 4279.063, Trained Tokens 8047, Peak mem 19.940 GB\n",
      "Iter 60: Train loss 2.581, Learning Rate 1.000e-04, It/sec 1.585, Tokens/sec 255.913, Trained Tokens 9662, Peak mem 19.940 GB\n",
      "Iter 70: Train loss 2.426, Learning Rate 1.000e-04, It/sec 1.766, Tokens/sec 248.485, Trained Tokens 11069, Peak mem 19.940 GB\n",
      "Iter 80: Train loss 2.634, Learning Rate 1.000e-04, It/sec 1.506, Tokens/sec 259.698, Trained Tokens 12793, Peak mem 19.940 GB\n",
      "Iter 90: Train loss 2.540, Learning Rate 1.000e-04, It/sec 1.790, Tokens/sec 245.807, Trained Tokens 14166, Peak mem 19.940 GB\n",
      "Iter 100: Val loss 2.598, Val took 3.535s\n",
      "Iter 100: Train loss 2.629, Learning Rate 1.000e-04, It/sec 11.019, Tokens/sec 1800.438, Trained Tokens 15800, Peak mem 19.940 GB\n",
      "Iter 100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 2.620, Learning Rate 1.000e-04, It/sec 1.783, Tokens/sec 253.059, Trained Tokens 17219, Peak mem 19.940 GB\n",
      "Iter 120: Train loss 2.594, Learning Rate 1.000e-04, It/sec 1.422, Tokens/sec 245.490, Trained Tokens 18945, Peak mem 19.940 GB\n",
      "Iter 130: Train loss 2.342, Learning Rate 1.000e-04, It/sec 1.025, Tokens/sec 267.303, Trained Tokens 21553, Peak mem 19.940 GB\n",
      "Iter 140: Train loss 2.268, Learning Rate 1.000e-04, It/sec 1.234, Tokens/sec 271.414, Trained Tokens 23753, Peak mem 19.940 GB\n",
      "Iter 150: Val loss 2.505, Val took 3.990s\n",
      "Iter 150: Train loss 2.600, Learning Rate 1.000e-04, It/sec 17.713, Tokens/sec 4272.491, Trained Tokens 26165, Peak mem 19.940 GB\n",
      "Iter 160: Train loss 2.662, Learning Rate 1.000e-04, It/sec 1.725, Tokens/sec 245.125, Trained Tokens 27586, Peak mem 19.940 GB\n",
      "Iter 170: Train loss 2.481, Learning Rate 1.000e-04, It/sec 1.411, Tokens/sec 266.359, Trained Tokens 29474, Peak mem 19.940 GB\n",
      "Iter 180: Train loss 2.434, Learning Rate 1.000e-04, It/sec 1.976, Tokens/sec 230.375, Trained Tokens 30640, Peak mem 19.940 GB\n",
      "Iter 190: Train loss 2.390, Learning Rate 1.000e-04, It/sec 1.699, Tokens/sec 236.509, Trained Tokens 32032, Peak mem 19.940 GB\n",
      "Iter 200: Val loss 2.606, Val took 3.786s\n",
      "Iter 200: Train loss 2.330, Learning Rate 1.000e-04, It/sec 11.593, Tokens/sec 2014.916, Trained Tokens 33770, Peak mem 19.940 GB\n",
      "Iter 200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 2.391, Learning Rate 1.000e-04, It/sec 1.883, Tokens/sec 251.995, Trained Tokens 35108, Peak mem 19.940 GB\n",
      "Iter 220: Train loss 2.496, Learning Rate 1.000e-04, It/sec 1.837, Tokens/sec 246.939, Trained Tokens 36452, Peak mem 19.940 GB\n",
      "Iter 230: Train loss 2.188, Learning Rate 1.000e-04, It/sec 2.074, Tokens/sec 243.084, Trained Tokens 37624, Peak mem 19.940 GB\n",
      "Iter 240: Train loss 2.392, Learning Rate 1.000e-04, It/sec 1.557, Tokens/sec 260.349, Trained Tokens 39296, Peak mem 19.940 GB\n",
      "Iter 250: Val loss 2.379, Val took 3.404s\n",
      "Iter 250: Train loss 2.344, Learning Rate 1.000e-04, It/sec 8.225, Tokens/sec 1301.230, Trained Tokens 40878, Peak mem 19.940 GB\n",
      "Iter 260: Train loss 2.419, Learning Rate 1.000e-04, It/sec 1.975, Tokens/sec 254.325, Trained Tokens 42166, Peak mem 19.940 GB\n",
      "Iter 270: Train loss 2.243, Learning Rate 1.000e-04, It/sec 1.414, Tokens/sec 263.384, Trained Tokens 44029, Peak mem 19.940 GB\n",
      "Iter 280: Train loss 2.426, Learning Rate 1.000e-04, It/sec 1.567, Tokens/sec 258.285, Trained Tokens 45677, Peak mem 19.940 GB\n",
      "Iter 290: Train loss 2.453, Learning Rate 1.000e-04, It/sec 1.368, Tokens/sec 260.241, Trained Tokens 47580, Peak mem 19.940 GB\n",
      "Iter 300: Val loss 2.453, Val took 4.549s\n",
      "Iter 300: Train loss 2.226, Learning Rate 1.000e-04, It/sec 20.773, Tokens/sec 2457.388, Trained Tokens 48763, Peak mem 19.940 GB\n",
      "Iter 300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 2.253, Learning Rate 1.000e-04, It/sec 1.453, Tokens/sec 215.400, Trained Tokens 50245, Peak mem 19.940 GB\n",
      "Iter 320: Train loss 2.333, Learning Rate 1.000e-04, It/sec 1.575, Tokens/sec 215.314, Trained Tokens 51612, Peak mem 19.940 GB\n",
      "Iter 330: Train loss 2.244, Learning Rate 1.000e-04, It/sec 1.794, Tokens/sec 252.474, Trained Tokens 53019, Peak mem 19.940 GB\n",
      "Iter 340: Train loss 2.299, Learning Rate 1.000e-04, It/sec 0.830, Tokens/sec 266.869, Trained Tokens 56236, Peak mem 19.940 GB\n",
      "Iter 350: Val loss 2.292, Val took 3.454s\n",
      "Iter 350: Train loss 2.260, Learning Rate 1.000e-04, It/sec 11.559, Tokens/sec 1837.869, Trained Tokens 57826, Peak mem 19.940 GB\n",
      "Iter 360: Train loss 2.336, Learning Rate 1.000e-04, It/sec 1.833, Tokens/sec 243.825, Trained Tokens 59156, Peak mem 19.940 GB\n",
      "Iter 370: Train loss 2.195, Learning Rate 1.000e-04, It/sec 1.019, Tokens/sec 256.895, Trained Tokens 61677, Peak mem 19.940 GB\n",
      "Iter 380: Train loss 2.459, Learning Rate 1.000e-04, It/sec 1.214, Tokens/sec 232.207, Trained Tokens 63590, Peak mem 19.940 GB\n",
      "Iter 390: Train loss 2.398, Learning Rate 1.000e-04, It/sec 1.424, Tokens/sec 247.775, Trained Tokens 65330, Peak mem 19.940 GB\n",
      "Iter 400: Val loss 2.347, Val took 3.357s\n",
      "Iter 400: Train loss 2.112, Learning Rate 1.000e-04, It/sec 28.488, Tokens/sec 3746.229, Trained Tokens 66645, Peak mem 19.940 GB\n",
      "Iter 400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 2.138, Learning Rate 1.000e-04, It/sec 1.821, Tokens/sec 231.242, Trained Tokens 67915, Peak mem 19.940 GB\n",
      "Iter 420: Train loss 2.078, Learning Rate 1.000e-04, It/sec 1.564, Tokens/sec 237.143, Trained Tokens 69431, Peak mem 19.940 GB\n",
      "Iter 430: Train loss 1.994, Learning Rate 1.000e-04, It/sec 1.447, Tokens/sec 230.347, Trained Tokens 71023, Peak mem 19.940 GB\n",
      "Iter 440: Train loss 1.838, Learning Rate 1.000e-04, It/sec 1.476, Tokens/sec 246.509, Trained Tokens 72693, Peak mem 19.940 GB\n",
      "Iter 450: Val loss 2.401, Val took 3.916s\n",
      "Iter 450: Train loss 2.071, Learning Rate 1.000e-04, It/sec 4.795, Tokens/sec 1068.300, Trained Tokens 74921, Peak mem 19.940 GB\n",
      "Iter 460: Train loss 1.935, Learning Rate 1.000e-04, It/sec 1.604, Tokens/sec 247.262, Trained Tokens 76463, Peak mem 19.940 GB\n",
      "Iter 470: Train loss 2.030, Learning Rate 1.000e-04, It/sec 1.331, Tokens/sec 268.529, Trained Tokens 78481, Peak mem 19.940 GB\n",
      "Iter 480: Train loss 2.115, Learning Rate 1.000e-04, It/sec 1.798, Tokens/sec 226.689, Trained Tokens 79742, Peak mem 19.940 GB\n",
      "Iter 490: Train loss 2.029, Learning Rate 1.000e-04, It/sec 1.680, Tokens/sec 258.438, Trained Tokens 81280, Peak mem 19.940 GB\n",
      "Iter 500: Val loss 2.298, Val took 3.671s\n",
      "Iter 500: Train loss 1.987, Learning Rate 1.000e-04, It/sec 29.295, Tokens/sec 3823.062, Trained Tokens 82585, Peak mem 19.940 GB\n",
      "Iter 500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 1.976, Learning Rate 1.000e-04, It/sec 1.898, Tokens/sec 248.640, Trained Tokens 83895, Peak mem 19.940 GB\n",
      "Iter 520: Train loss 1.963, Learning Rate 1.000e-04, It/sec 1.803, Tokens/sec 237.147, Trained Tokens 85210, Peak mem 19.940 GB\n",
      "Iter 530: Train loss 2.094, Learning Rate 1.000e-04, It/sec 1.769, Tokens/sec 267.333, Trained Tokens 86721, Peak mem 19.940 GB\n",
      "Iter 540: Train loss 2.347, Learning Rate 1.000e-04, It/sec 1.083, Tokens/sec 265.508, Trained Tokens 89172, Peak mem 19.940 GB\n",
      "Iter 550: Val loss 2.292, Val took 3.075s\n",
      "Iter 550: Train loss 1.976, Learning Rate 1.000e-04, It/sec 18.301, Tokens/sec 2662.846, Trained Tokens 90627, Peak mem 19.940 GB\n",
      "Iter 560: Train loss 1.820, Learning Rate 1.000e-04, It/sec 1.717, Tokens/sec 249.139, Trained Tokens 92078, Peak mem 19.940 GB\n",
      "Iter 570: Train loss 2.216, Learning Rate 1.000e-04, It/sec 1.063, Tokens/sec 255.620, Trained Tokens 94483, Peak mem 19.940 GB\n",
      "Iter 580: Train loss 2.172, Learning Rate 1.000e-04, It/sec 1.541, Tokens/sec 249.745, Trained Tokens 96104, Peak mem 19.940 GB\n",
      "Iter 590: Train loss 1.961, Learning Rate 1.000e-04, It/sec 1.583, Tokens/sec 263.614, Trained Tokens 97769, Peak mem 19.940 GB\n",
      "Iter 600: Val loss 2.223, Val took 3.453s\n",
      "Iter 600: Train loss 2.159, Learning Rate 1.000e-04, It/sec 5.800, Tokens/sec 1014.963, Trained Tokens 99519, Peak mem 19.940 GB\n",
      "Iter 600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 1.973, Learning Rate 1.000e-04, It/sec 1.690, Tokens/sec 269.699, Trained Tokens 101115, Peak mem 19.940 GB\n",
      "Iter 620: Train loss 2.220, Learning Rate 1.000e-04, It/sec 1.576, Tokens/sec 256.426, Trained Tokens 102742, Peak mem 19.940 GB\n",
      "Iter 630: Train loss 2.115, Learning Rate 1.000e-04, It/sec 1.544, Tokens/sec 253.364, Trained Tokens 104383, Peak mem 19.940 GB\n",
      "Iter 640: Train loss 2.055, Learning Rate 1.000e-04, It/sec 1.492, Tokens/sec 243.309, Trained Tokens 106014, Peak mem 19.940 GB\n",
      "Iter 650: Val loss 2.280, Val took 3.101s\n",
      "Iter 650: Train loss 2.038, Learning Rate 1.000e-04, It/sec 12.474, Tokens/sec 1781.258, Trained Tokens 107442, Peak mem 19.940 GB\n",
      "Iter 660: Train loss 1.852, Learning Rate 1.000e-04, It/sec 1.953, Tokens/sec 232.456, Trained Tokens 108632, Peak mem 19.940 GB\n",
      "Iter 670: Train loss 1.993, Learning Rate 1.000e-04, It/sec 1.669, Tokens/sec 241.847, Trained Tokens 110081, Peak mem 19.940 GB\n",
      "Iter 680: Train loss 1.939, Learning Rate 1.000e-04, It/sec 1.547, Tokens/sec 245.853, Trained Tokens 111670, Peak mem 19.940 GB\n",
      "Iter 690: Train loss 2.094, Learning Rate 1.000e-04, It/sec 1.711, Tokens/sec 260.275, Trained Tokens 113191, Peak mem 19.940 GB\n",
      "Iter 700: Val loss 2.482, Val took 4.579s\n",
      "Iter 700: Train loss 2.041, Learning Rate 1.000e-04, It/sec 21.820, Tokens/sec 3506.526, Trained Tokens 114798, Peak mem 19.940 GB\n",
      "Iter 700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 2.086, Learning Rate 1.000e-04, It/sec 1.125, Tokens/sec 242.962, Trained Tokens 116957, Peak mem 19.940 GB\n",
      "Iter 720: Train loss 1.971, Learning Rate 1.000e-04, It/sec 1.723, Tokens/sec 251.929, Trained Tokens 118419, Peak mem 19.940 GB\n",
      "Iter 730: Train loss 1.838, Learning Rate 1.000e-04, It/sec 2.603, Tokens/sec 234.543, Trained Tokens 119320, Peak mem 19.940 GB\n",
      "Iter 740: Train loss 2.068, Learning Rate 1.000e-04, It/sec 1.566, Tokens/sec 249.092, Trained Tokens 120911, Peak mem 19.940 GB\n",
      "Iter 750: Val loss 2.425, Val took 4.219s\n",
      "Iter 750: Train loss 2.017, Learning Rate 1.000e-04, It/sec 15.762, Tokens/sec 2558.166, Trained Tokens 122534, Peak mem 19.940 GB\n",
      "Iter 760: Train loss 2.004, Learning Rate 1.000e-04, It/sec 1.153, Tokens/sec 270.216, Trained Tokens 124877, Peak mem 19.940 GB\n",
      "Iter 770: Train loss 2.122, Learning Rate 1.000e-04, It/sec 1.351, Tokens/sec 218.644, Trained Tokens 126495, Peak mem 19.940 GB\n",
      "Iter 780: Train loss 1.942, Learning Rate 1.000e-04, It/sec 1.682, Tokens/sec 220.963, Trained Tokens 127809, Peak mem 19.940 GB\n",
      "Iter 790: Train loss 2.013, Learning Rate 1.000e-04, It/sec 0.937, Tokens/sec 268.182, Trained Tokens 130670, Peak mem 19.940 GB\n",
      "Iter 800: Val loss 2.268, Val took 3.266s\n",
      "Iter 800: Train loss 2.185, Learning Rate 1.000e-04, It/sec 16.327, Tokens/sec 2955.221, Trained Tokens 132480, Peak mem 19.940 GB\n",
      "Iter 800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 1.847, Learning Rate 1.000e-04, It/sec 1.247, Tokens/sec 241.494, Trained Tokens 134416, Peak mem 19.940 GB\n",
      "Iter 820: Train loss 1.813, Learning Rate 1.000e-04, It/sec 1.832, Tokens/sec 258.991, Trained Tokens 135830, Peak mem 19.940 GB\n",
      "Iter 830: Train loss 1.736, Learning Rate 1.000e-04, It/sec 1.780, Tokens/sec 258.489, Trained Tokens 137282, Peak mem 19.940 GB\n",
      "Iter 840: Train loss 1.916, Learning Rate 1.000e-04, It/sec 0.934, Tokens/sec 277.919, Trained Tokens 140259, Peak mem 19.940 GB\n",
      "Iter 850: Val loss 2.111, Val took 3.288s\n",
      "Iter 850: Train loss 1.802, Learning Rate 1.000e-04, It/sec 23.223, Tokens/sec 2795.997, Trained Tokens 141463, Peak mem 19.940 GB\n",
      "Iter 860: Train loss 1.693, Learning Rate 1.000e-04, It/sec 1.310, Tokens/sec 261.142, Trained Tokens 143456, Peak mem 19.940 GB\n",
      "Iter 870: Train loss 1.688, Learning Rate 1.000e-04, It/sec 1.374, Tokens/sec 261.216, Trained Tokens 145357, Peak mem 19.940 GB\n",
      "Iter 880: Train loss 1.753, Learning Rate 1.000e-04, It/sec 1.660, Tokens/sec 239.075, Trained Tokens 146797, Peak mem 19.940 GB\n",
      "Iter 890: Train loss 1.684, Learning Rate 1.000e-04, It/sec 1.723, Tokens/sec 261.518, Trained Tokens 148315, Peak mem 19.940 GB\n",
      "Iter 900: Val loss 2.252, Val took 2.778s\n",
      "Iter 900: Train loss 1.770, Learning Rate 1.000e-04, It/sec 16.610, Tokens/sec 2793.772, Trained Tokens 149997, Peak mem 19.940 GB\n",
      "Iter 900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 1.614, Learning Rate 1.000e-04, It/sec 1.843, Tokens/sec 255.263, Trained Tokens 151382, Peak mem 19.940 GB\n",
      "Iter 920: Train loss 1.721, Learning Rate 1.000e-04, It/sec 1.084, Tokens/sec 276.194, Trained Tokens 153930, Peak mem 19.940 GB\n",
      "Iter 930: Train loss 1.556, Learning Rate 1.000e-04, It/sec 1.852, Tokens/sec 239.141, Trained Tokens 155221, Peak mem 19.940 GB\n",
      "Iter 940: Train loss 1.773, Learning Rate 1.000e-04, It/sec 1.257, Tokens/sec 261.932, Trained Tokens 157305, Peak mem 19.940 GB\n",
      "Iter 950: Val loss 2.365, Val took 4.161s\n",
      "Iter 950: Train loss 1.929, Learning Rate 1.000e-04, It/sec 28.124, Tokens/sec 4412.672, Trained Tokens 158874, Peak mem 19.940 GB\n",
      "Iter 960: Train loss 1.955, Learning Rate 1.000e-04, It/sec 1.048, Tokens/sec 268.345, Trained Tokens 161434, Peak mem 19.940 GB\n",
      "Iter 970: Train loss 1.814, Learning Rate 1.000e-04, It/sec 1.471, Tokens/sec 265.440, Trained Tokens 163239, Peak mem 19.940 GB\n",
      "Iter 980: Train loss 1.828, Learning Rate 1.000e-04, It/sec 1.418, Tokens/sec 249.687, Trained Tokens 165000, Peak mem 19.940 GB\n",
      "Iter 990: Train loss 1.502, Learning Rate 1.000e-04, It/sec 1.769, Tokens/sec 248.534, Trained Tokens 166405, Peak mem 19.940 GB\n",
      "Iter 1000: Val loss 2.333, Val took 4.369s\n",
      "Iter 1000: Train loss 1.798, Learning Rate 1.000e-04, It/sec 17.623, Tokens/sec 2469.043, Trained Tokens 167806, Peak mem 19.940 GB\n",
      "Iter 1000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001000_adapters.safetensors.\n",
      "Iter 1010: Train loss 1.689, Learning Rate 1.000e-04, It/sec 1.723, Tokens/sec 249.823, Trained Tokens 169256, Peak mem 19.940 GB\n",
      "Iter 1020: Train loss 1.763, Learning Rate 1.000e-04, It/sec 2.065, Tokens/sec 245.483, Trained Tokens 170445, Peak mem 19.940 GB\n",
      "Iter 1030: Train loss 1.798, Learning Rate 1.000e-04, It/sec 1.776, Tokens/sec 264.462, Trained Tokens 171934, Peak mem 19.940 GB\n",
      "Iter 1040: Train loss 1.891, Learning Rate 1.000e-04, It/sec 1.353, Tokens/sec 250.711, Trained Tokens 173787, Peak mem 19.940 GB\n",
      "Iter 1050: Val loss 2.298, Val took 3.933s\n",
      "Iter 1050: Train loss 1.849, Learning Rate 1.000e-04, It/sec 17.417, Tokens/sec 2690.878, Trained Tokens 175332, Peak mem 19.940 GB\n",
      "Iter 1060: Train loss 1.848, Learning Rate 1.000e-04, It/sec 1.208, Tokens/sec 250.286, Trained Tokens 177404, Peak mem 19.940 GB\n",
      "Iter 1070: Train loss 1.763, Learning Rate 1.000e-04, It/sec 1.406, Tokens/sec 236.885, Trained Tokens 179089, Peak mem 19.940 GB\n",
      "Iter 1080: Train loss 1.678, Learning Rate 1.000e-04, It/sec 1.595, Tokens/sec 249.585, Trained Tokens 180654, Peak mem 19.940 GB\n",
      "Iter 1090: Train loss 1.962, Learning Rate 1.000e-04, It/sec 1.969, Tokens/sec 268.583, Trained Tokens 182018, Peak mem 19.940 GB\n",
      "Iter 1100: Val loss 2.105, Val took 4.545s\n",
      "Iter 1100: Train loss 1.934, Learning Rate 1.000e-04, It/sec 5.884, Tokens/sec 1167.895, Trained Tokens 184003, Peak mem 19.940 GB\n",
      "Iter 1100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001100_adapters.safetensors.\n",
      "Iter 1110: Train loss 1.757, Learning Rate 1.000e-04, It/sec 1.742, Tokens/sec 256.835, Trained Tokens 185477, Peak mem 19.940 GB\n",
      "Iter 1120: Train loss 1.711, Learning Rate 1.000e-04, It/sec 2.160, Tokens/sec 253.774, Trained Tokens 186652, Peak mem 19.940 GB\n",
      "Iter 1130: Train loss 2.200, Learning Rate 1.000e-04, It/sec 1.286, Tokens/sec 273.992, Trained Tokens 188783, Peak mem 19.940 GB\n",
      "Iter 1140: Train loss 1.682, Learning Rate 1.000e-04, It/sec 1.713, Tokens/sec 252.448, Trained Tokens 190257, Peak mem 19.940 GB\n",
      "Iter 1150: Val loss 2.299, Val took 3.598s\n",
      "Iter 1150: Train loss 1.935, Learning Rate 1.000e-04, It/sec 17.606, Tokens/sec 2764.214, Trained Tokens 191827, Peak mem 19.940 GB\n",
      "Iter 1160: Train loss 1.662, Learning Rate 1.000e-04, It/sec 1.736, Tokens/sec 251.674, Trained Tokens 193277, Peak mem 19.940 GB\n",
      "Iter 1170: Train loss 1.802, Learning Rate 1.000e-04, It/sec 1.868, Tokens/sec 249.556, Trained Tokens 194613, Peak mem 19.940 GB\n",
      "Iter 1180: Train loss 1.622, Learning Rate 1.000e-04, It/sec 1.944, Tokens/sec 247.522, Trained Tokens 195886, Peak mem 19.940 GB\n",
      "Iter 1190: Train loss 1.874, Learning Rate 1.000e-04, It/sec 1.573, Tokens/sec 252.767, Trained Tokens 197493, Peak mem 19.940 GB\n",
      "Iter 1200: Val loss 2.260, Val took 3.757s\n",
      "Iter 1200: Train loss 1.809, Learning Rate 1.000e-04, It/sec 18.155, Tokens/sec 3320.550, Trained Tokens 199322, Peak mem 19.940 GB\n",
      "Iter 1200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001200_adapters.safetensors.\n",
      "Iter 1210: Train loss 1.758, Learning Rate 1.000e-04, It/sec 1.557, Tokens/sec 233.776, Trained Tokens 200823, Peak mem 19.940 GB\n",
      "Iter 1220: Train loss 1.540, Learning Rate 1.000e-04, It/sec 1.679, Tokens/sec 256.017, Trained Tokens 202348, Peak mem 19.940 GB\n",
      "Iter 1230: Train loss 1.852, Learning Rate 1.000e-04, It/sec 1.913, Tokens/sec 267.291, Trained Tokens 203745, Peak mem 19.940 GB\n",
      "Iter 1240: Train loss 1.640, Learning Rate 1.000e-04, It/sec 1.725, Tokens/sec 270.409, Trained Tokens 205313, Peak mem 19.940 GB\n",
      "Iter 1250: Val loss 2.106, Val took 3.336s\n",
      "Iter 1250: Train loss 1.519, Learning Rate 1.000e-04, It/sec 10.530, Tokens/sec 1674.314, Trained Tokens 206903, Peak mem 19.940 GB\n",
      "Iter 1260: Train loss 1.626, Learning Rate 1.000e-04, It/sec 1.409, Tokens/sec 259.997, Trained Tokens 208748, Peak mem 19.940 GB\n",
      "Iter 1270: Train loss 1.675, Learning Rate 1.000e-04, It/sec 1.553, Tokens/sec 269.867, Trained Tokens 210486, Peak mem 19.940 GB\n",
      "Iter 1280: Train loss 1.490, Learning Rate 1.000e-04, It/sec 1.701, Tokens/sec 283.459, Trained Tokens 212152, Peak mem 19.940 GB\n",
      "Iter 1290: Train loss 1.351, Learning Rate 1.000e-04, It/sec 1.935, Tokens/sec 225.260, Trained Tokens 213316, Peak mem 19.940 GB\n",
      "Iter 1300: Val loss 2.337, Val took 3.904s\n",
      "Iter 1300: Train loss 1.780, Learning Rate 1.000e-04, It/sec 12.772, Tokens/sec 3199.491, Trained Tokens 215821, Peak mem 19.940 GB\n",
      "Iter 1300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001300_adapters.safetensors.\n",
      "Iter 1310: Train loss 1.783, Learning Rate 1.000e-04, It/sec 1.372, Tokens/sec 240.168, Trained Tokens 217572, Peak mem 19.940 GB\n",
      "Iter 1320: Train loss 1.691, Learning Rate 1.000e-04, It/sec 1.205, Tokens/sec 243.745, Trained Tokens 219594, Peak mem 19.940 GB\n",
      "Iter 1330: Train loss 1.391, Learning Rate 1.000e-04, It/sec 2.019, Tokens/sec 241.902, Trained Tokens 220792, Peak mem 19.940 GB\n",
      "Iter 1340: Train loss 1.704, Learning Rate 1.000e-04, It/sec 1.349, Tokens/sec 254.597, Trained Tokens 222679, Peak mem 19.940 GB\n",
      "Iter 1350: Val loss 2.041, Val took 3.455s\n",
      "Iter 1350: Train loss 1.507, Learning Rate 1.000e-04, It/sec 16.212, Tokens/sec 2020.067, Trained Tokens 223925, Peak mem 19.940 GB\n",
      "Iter 1360: Train loss 1.529, Learning Rate 1.000e-04, It/sec 1.715, Tokens/sec 249.924, Trained Tokens 225382, Peak mem 19.940 GB\n",
      "Iter 1370: Train loss 1.392, Learning Rate 1.000e-04, It/sec 2.201, Tokens/sec 237.023, Trained Tokens 226459, Peak mem 19.940 GB\n",
      "Iter 1380: Train loss 1.580, Learning Rate 1.000e-04, It/sec 1.594, Tokens/sec 241.146, Trained Tokens 227972, Peak mem 19.940 GB\n",
      "Iter 1390: Train loss 1.587, Learning Rate 1.000e-04, It/sec 1.264, Tokens/sec 233.525, Trained Tokens 229820, Peak mem 19.940 GB\n",
      "Iter 1400: Val loss 2.188, Val took 2.846s\n",
      "Iter 1400: Train loss 1.884, Learning Rate 1.000e-04, It/sec 9.870, Tokens/sec 2793.297, Trained Tokens 232650, Peak mem 19.940 GB\n",
      "Iter 1400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001400_adapters.safetensors.\n",
      "Iter 1410: Train loss 1.480, Learning Rate 1.000e-04, It/sec 1.504, Tokens/sec 243.376, Trained Tokens 234268, Peak mem 19.940 GB\n",
      "Iter 1420: Train loss 1.657, Learning Rate 1.000e-04, It/sec 1.320, Tokens/sec 255.097, Trained Tokens 236201, Peak mem 19.940 GB\n",
      "Iter 1430: Train loss 1.480, Learning Rate 1.000e-04, It/sec 1.247, Tokens/sec 210.159, Trained Tokens 237886, Peak mem 19.940 GB\n",
      "Iter 1440: Train loss 1.611, Learning Rate 1.000e-04, It/sec 1.167, Tokens/sec 265.004, Trained Tokens 240156, Peak mem 19.940 GB\n",
      "Iter 1450: Val loss 2.388, Val took 3.824s\n",
      "Iter 1450: Train loss 1.440, Learning Rate 1.000e-04, It/sec 28.817, Tokens/sec 3483.942, Trained Tokens 241365, Peak mem 19.940 GB\n",
      "Iter 1460: Train loss 1.617, Learning Rate 1.000e-04, It/sec 1.485, Tokens/sec 266.485, Trained Tokens 243160, Peak mem 19.940 GB\n",
      "Iter 1470: Train loss 1.582, Learning Rate 1.000e-04, It/sec 1.398, Tokens/sec 259.395, Trained Tokens 245016, Peak mem 19.940 GB\n",
      "Iter 1480: Train loss 1.360, Learning Rate 1.000e-04, It/sec 1.985, Tokens/sec 245.556, Trained Tokens 246253, Peak mem 19.940 GB\n",
      "Iter 1490: Train loss 1.455, Learning Rate 1.000e-04, It/sec 1.589, Tokens/sec 248.981, Trained Tokens 247820, Peak mem 19.940 GB\n",
      "Iter 1500: Val loss 2.207, Val took 3.546s\n",
      "Iter 1500: Train loss 1.375, Learning Rate 1.000e-04, It/sec 22.639, Tokens/sec 2775.585, Trained Tokens 249046, Peak mem 19.940 GB\n",
      "Iter 1500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001500_adapters.safetensors.\n",
      "Iter 1510: Train loss 1.499, Learning Rate 1.000e-04, It/sec 1.677, Tokens/sec 230.191, Trained Tokens 250419, Peak mem 19.940 GB\n",
      "Iter 1520: Train loss 1.758, Learning Rate 1.000e-04, It/sec 1.556, Tokens/sec 230.920, Trained Tokens 251903, Peak mem 19.940 GB\n",
      "Iter 1530: Train loss 1.704, Learning Rate 1.000e-04, It/sec 1.567, Tokens/sec 245.822, Trained Tokens 253472, Peak mem 19.940 GB\n",
      "Iter 1540: Train loss 1.452, Learning Rate 1.000e-04, It/sec 1.901, Tokens/sec 235.389, Trained Tokens 254710, Peak mem 19.940 GB\n",
      "Iter 1550: Val loss 1.973, Val took 4.606s\n",
      "Iter 1550: Train loss 1.496, Learning Rate 1.000e-04, It/sec 30.464, Tokens/sec 5157.492, Trained Tokens 256403, Peak mem 19.940 GB\n",
      "Iter 1560: Train loss 1.553, Learning Rate 1.000e-04, It/sec 1.814, Tokens/sec 234.725, Trained Tokens 257697, Peak mem 19.940 GB\n",
      "Iter 1570: Train loss 1.549, Learning Rate 1.000e-04, It/sec 1.525, Tokens/sec 261.238, Trained Tokens 259410, Peak mem 19.940 GB\n",
      "Iter 1580: Train loss 1.658, Learning Rate 1.000e-04, It/sec 0.765, Tokens/sec 219.752, Trained Tokens 262284, Peak mem 19.940 GB\n",
      "Iter 1590: Train loss 1.746, Learning Rate 1.000e-04, It/sec 1.366, Tokens/sec 248.278, Trained Tokens 264101, Peak mem 19.940 GB\n",
      "Iter 1600: Val loss 2.006, Val took 3.418s\n",
      "Iter 1600: Train loss 1.671, Learning Rate 1.000e-04, It/sec 9.507, Tokens/sec 1564.821, Trained Tokens 265747, Peak mem 19.940 GB\n",
      "Iter 1600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001600_adapters.safetensors.\n",
      "Iter 1610: Train loss 1.694, Learning Rate 1.000e-04, It/sec 1.915, Tokens/sec 248.749, Trained Tokens 267046, Peak mem 19.940 GB\n",
      "Iter 1620: Train loss 1.749, Learning Rate 1.000e-04, It/sec 1.473, Tokens/sec 251.377, Trained Tokens 268752, Peak mem 19.940 GB\n",
      "Iter 1630: Train loss 1.525, Learning Rate 1.000e-04, It/sec 1.514, Tokens/sec 260.071, Trained Tokens 270470, Peak mem 19.940 GB\n",
      "Iter 1640: Train loss 1.511, Learning Rate 1.000e-04, It/sec 2.022, Tokens/sec 240.666, Trained Tokens 271660, Peak mem 19.940 GB\n",
      "Iter 1650: Val loss 2.063, Val took 3.245s\n",
      "Iter 1650: Train loss 1.440, Learning Rate 1.000e-04, It/sec 15.684, Tokens/sec 2843.588, Trained Tokens 273473, Peak mem 19.940 GB\n",
      "Iter 1660: Train loss 1.185, Learning Rate 1.000e-04, It/sec 2.052, Tokens/sec 253.180, Trained Tokens 274707, Peak mem 19.940 GB\n",
      "Iter 1670: Train loss 1.564, Learning Rate 1.000e-04, It/sec 1.255, Tokens/sec 269.381, Trained Tokens 276853, Peak mem 19.940 GB\n",
      "Iter 1680: Train loss 1.400, Learning Rate 1.000e-04, It/sec 0.846, Tokens/sec 175.417, Trained Tokens 278927, Peak mem 19.940 GB\n",
      "Iter 1690: Train loss 1.396, Learning Rate 1.000e-04, It/sec 1.630, Tokens/sec 244.615, Trained Tokens 280428, Peak mem 19.940 GB\n",
      "Iter 1700: Val loss 2.360, Val took 3.532s\n",
      "Iter 1700: Train loss 1.145, Learning Rate 1.000e-04, It/sec 9.681, Tokens/sec 1174.286, Trained Tokens 281641, Peak mem 19.940 GB\n",
      "Iter 1700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001700_adapters.safetensors.\n",
      "Iter 1710: Train loss 1.330, Learning Rate 1.000e-04, It/sec 1.354, Tokens/sec 249.350, Trained Tokens 283483, Peak mem 19.940 GB\n",
      "Iter 1720: Train loss 1.534, Learning Rate 1.000e-04, It/sec 1.562, Tokens/sec 261.927, Trained Tokens 285160, Peak mem 19.940 GB\n",
      "Iter 1730: Train loss 1.411, Learning Rate 1.000e-04, It/sec 1.898, Tokens/sec 240.637, Trained Tokens 286428, Peak mem 19.940 GB\n",
      "Iter 1740: Train loss 1.308, Learning Rate 1.000e-04, It/sec 1.793, Tokens/sec 262.118, Trained Tokens 287890, Peak mem 19.940 GB\n",
      "Iter 1750: Val loss 2.242, Val took 4.097s\n",
      "Iter 1750: Train loss 1.425, Learning Rate 1.000e-04, It/sec 6.669, Tokens/sec 1248.473, Trained Tokens 289762, Peak mem 19.940 GB\n",
      "Iter 1760: Train loss 1.585, Learning Rate 1.000e-04, It/sec 0.893, Tokens/sec 226.996, Trained Tokens 292305, Peak mem 19.940 GB\n",
      "Iter 1770: Train loss 1.338, Learning Rate 1.000e-04, It/sec 1.717, Tokens/sec 241.378, Trained Tokens 293711, Peak mem 19.940 GB\n",
      "Iter 1780: Train loss 1.290, Learning Rate 1.000e-04, It/sec 1.511, Tokens/sec 234.080, Trained Tokens 295260, Peak mem 19.940 GB\n",
      "Iter 1790: Train loss 1.258, Learning Rate 1.000e-04, It/sec 2.137, Tokens/sec 239.792, Trained Tokens 296382, Peak mem 19.940 GB\n",
      "Iter 1800: Val loss 2.198, Val took 3.674s\n",
      "Iter 1800: Train loss 1.457, Learning Rate 1.000e-04, It/sec 28.364, Tokens/sec 4606.272, Trained Tokens 298006, Peak mem 19.940 GB\n",
      "Iter 1800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001800_adapters.safetensors.\n",
      "Iter 1810: Train loss 1.259, Learning Rate 1.000e-04, It/sec 1.899, Tokens/sec 241.145, Trained Tokens 299276, Peak mem 19.940 GB\n",
      "Iter 1820: Train loss 1.363, Learning Rate 1.000e-04, It/sec 1.601, Tokens/sec 255.152, Trained Tokens 300870, Peak mem 19.940 GB\n",
      "Iter 1830: Train loss 1.403, Learning Rate 1.000e-04, It/sec 1.174, Tokens/sec 243.851, Trained Tokens 302947, Peak mem 19.940 GB\n",
      "Iter 1840: Train loss 1.382, Learning Rate 1.000e-04, It/sec 1.058, Tokens/sec 199.324, Trained Tokens 304831, Peak mem 19.940 GB\n",
      "Iter 1850: Val loss 2.158, Val took 4.010s\n",
      "Iter 1850: Train loss 1.511, Learning Rate 1.000e-04, It/sec 39.666, Tokens/sec 9928.316, Trained Tokens 307334, Peak mem 19.940 GB\n",
      "Iter 1860: Train loss 1.661, Learning Rate 1.000e-04, It/sec 0.894, Tokens/sec 257.987, Trained Tokens 310220, Peak mem 19.940 GB\n",
      "Iter 1870: Train loss 1.351, Learning Rate 1.000e-04, It/sec 1.625, Tokens/sec 259.335, Trained Tokens 311816, Peak mem 19.940 GB\n",
      "Iter 1880: Train loss 1.364, Learning Rate 1.000e-04, It/sec 1.515, Tokens/sec 251.106, Trained Tokens 313474, Peak mem 19.940 GB\n",
      "Iter 1890: Train loss 1.509, Learning Rate 1.000e-04, It/sec 1.679, Tokens/sec 252.353, Trained Tokens 314977, Peak mem 19.940 GB\n",
      "Iter 1900: Val loss 2.082, Val took 2.772s\n",
      "Iter 1900: Train loss 1.178, Learning Rate 1.000e-04, It/sec 29.000, Tokens/sec 2656.376, Trained Tokens 315893, Peak mem 19.940 GB\n",
      "Iter 1900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0001900_adapters.safetensors.\n",
      "Iter 1910: Train loss 1.645, Learning Rate 1.000e-04, It/sec 1.435, Tokens/sec 253.928, Trained Tokens 317663, Peak mem 19.940 GB\n",
      "Iter 1920: Train loss 1.609, Learning Rate 1.000e-04, It/sec 1.669, Tokens/sec 254.810, Trained Tokens 319190, Peak mem 19.940 GB\n",
      "Iter 1930: Train loss 1.314, Learning Rate 1.000e-04, It/sec 1.603, Tokens/sec 252.326, Trained Tokens 320764, Peak mem 19.940 GB\n",
      "Iter 1940: Train loss 1.345, Learning Rate 1.000e-04, It/sec 1.863, Tokens/sec 252.637, Trained Tokens 322120, Peak mem 19.940 GB\n",
      "Iter 1950: Val loss 1.892, Val took 3.423s\n",
      "Iter 1950: Train loss 1.631, Learning Rate 1.000e-04, It/sec 12.388, Tokens/sec 2269.466, Trained Tokens 323952, Peak mem 19.940 GB\n",
      "Iter 1960: Train loss 1.431, Learning Rate 1.000e-04, It/sec 1.723, Tokens/sec 251.274, Trained Tokens 325410, Peak mem 19.940 GB\n",
      "Iter 1970: Train loss 1.466, Learning Rate 1.000e-04, It/sec 1.972, Tokens/sec 254.608, Trained Tokens 326701, Peak mem 19.940 GB\n",
      "Iter 1980: Train loss 1.654, Learning Rate 1.000e-04, It/sec 1.168, Tokens/sec 262.401, Trained Tokens 328947, Peak mem 19.940 GB\n",
      "Iter 1990: Train loss 1.492, Learning Rate 1.000e-04, It/sec 1.798, Tokens/sec 245.743, Trained Tokens 330314, Peak mem 19.940 GB\n",
      "Iter 2000: Val loss 2.215, Val took 3.493s\n",
      "Iter 2000: Train loss 1.497, Learning Rate 1.000e-04, It/sec 30.593, Tokens/sec 4435.923, Trained Tokens 331764, Peak mem 19.940 GB\n",
      "Iter 2000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002000_adapters.safetensors.\n",
      "Iter 2010: Train loss 1.451, Learning Rate 1.000e-04, It/sec 1.818, Tokens/sec 261.436, Trained Tokens 333202, Peak mem 19.940 GB\n",
      "Iter 2020: Train loss 1.353, Learning Rate 1.000e-04, It/sec 1.595, Tokens/sec 273.940, Trained Tokens 334920, Peak mem 19.940 GB\n",
      "Iter 2030: Train loss 1.511, Learning Rate 1.000e-04, It/sec 1.550, Tokens/sec 261.510, Trained Tokens 336607, Peak mem 19.940 GB\n",
      "Iter 2040: Train loss 1.406, Learning Rate 1.000e-04, It/sec 1.653, Tokens/sec 274.284, Trained Tokens 338266, Peak mem 19.940 GB\n",
      "Iter 2050: Val loss 2.164, Val took 4.538s\n",
      "Iter 2050: Train loss 1.302, Learning Rate 1.000e-04, It/sec 29.228, Tokens/sec 3825.911, Trained Tokens 339575, Peak mem 19.940 GB\n",
      "Iter 2060: Train loss 1.237, Learning Rate 1.000e-04, It/sec 1.132, Tokens/sec 261.860, Trained Tokens 341888, Peak mem 19.940 GB\n",
      "Iter 2070: Train loss 1.206, Learning Rate 1.000e-04, It/sec 1.775, Tokens/sec 263.360, Trained Tokens 343372, Peak mem 19.940 GB\n",
      "Iter 2080: Train loss 1.145, Learning Rate 1.000e-04, It/sec 1.652, Tokens/sec 260.084, Trained Tokens 344946, Peak mem 19.940 GB\n",
      "Iter 2090: Train loss 1.331, Learning Rate 1.000e-04, It/sec 1.251, Tokens/sec 269.157, Trained Tokens 347098, Peak mem 19.940 GB\n",
      "Iter 2100: Val loss 2.213, Val took 3.640s\n",
      "Iter 2100: Train loss 1.261, Learning Rate 1.000e-04, It/sec 13.005, Tokens/sec 2153.704, Trained Tokens 348754, Peak mem 19.940 GB\n",
      "Iter 2100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002100_adapters.safetensors.\n",
      "Iter 2110: Train loss 1.400, Learning Rate 1.000e-04, It/sec 1.364, Tokens/sec 265.415, Trained Tokens 350700, Peak mem 19.940 GB\n",
      "Iter 2120: Train loss 1.217, Learning Rate 1.000e-04, It/sec 1.838, Tokens/sec 267.192, Trained Tokens 352154, Peak mem 19.940 GB\n",
      "Iter 2130: Train loss 1.311, Learning Rate 1.000e-04, It/sec 1.528, Tokens/sec 256.575, Trained Tokens 353833, Peak mem 19.940 GB\n",
      "Iter 2140: Train loss 1.189, Learning Rate 1.000e-04, It/sec 1.789, Tokens/sec 251.053, Trained Tokens 355236, Peak mem 19.940 GB\n",
      "Iter 2150: Val loss 2.251, Val took 4.077s\n",
      "Iter 2150: Train loss 1.252, Learning Rate 1.000e-04, It/sec 17.473, Tokens/sec 3250.029, Trained Tokens 357096, Peak mem 19.940 GB\n",
      "Iter 2160: Train loss 1.327, Learning Rate 1.000e-04, It/sec 1.865, Tokens/sec 269.722, Trained Tokens 358542, Peak mem 19.940 GB\n",
      "Iter 2170: Train loss 1.246, Learning Rate 1.000e-04, It/sec 1.891, Tokens/sec 239.571, Trained Tokens 359809, Peak mem 19.940 GB\n",
      "Iter 2180: Train loss 1.368, Learning Rate 1.000e-04, It/sec 1.595, Tokens/sec 261.633, Trained Tokens 361449, Peak mem 19.940 GB\n",
      "Iter 2190: Train loss 1.484, Learning Rate 1.000e-04, It/sec 1.482, Tokens/sec 262.633, Trained Tokens 363221, Peak mem 19.940 GB\n",
      "Iter 2200: Val loss 2.286, Val took 3.484s\n",
      "Iter 2200: Train loss 1.355, Learning Rate 1.000e-04, It/sec 39.687, Tokens/sec 6953.105, Trained Tokens 364973, Peak mem 19.940 GB\n",
      "Iter 2200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002200_adapters.safetensors.\n",
      "Iter 2210: Train loss 1.273, Learning Rate 1.000e-04, It/sec 1.532, Tokens/sec 257.715, Trained Tokens 366655, Peak mem 19.940 GB\n",
      "Iter 2220: Train loss 1.324, Learning Rate 1.000e-04, It/sec 1.426, Tokens/sec 266.777, Trained Tokens 368526, Peak mem 19.940 GB\n",
      "Iter 2230: Train loss 1.271, Learning Rate 1.000e-04, It/sec 1.673, Tokens/sec 256.107, Trained Tokens 370057, Peak mem 19.940 GB\n",
      "Iter 2240: Train loss 1.173, Learning Rate 1.000e-04, It/sec 1.116, Tokens/sec 259.641, Trained Tokens 372384, Peak mem 19.940 GB\n",
      "Iter 2250: Val loss 2.256, Val took 3.489s\n",
      "Iter 2250: Train loss 1.257, Learning Rate 1.000e-04, It/sec 17.715, Tokens/sec 2400.442, Trained Tokens 373739, Peak mem 19.940 GB\n",
      "Iter 2260: Train loss 1.283, Learning Rate 1.000e-04, It/sec 1.691, Tokens/sec 248.516, Trained Tokens 375209, Peak mem 19.940 GB\n",
      "Iter 2270: Train loss 1.022, Learning Rate 1.000e-04, It/sec 2.234, Tokens/sec 257.160, Trained Tokens 376360, Peak mem 19.940 GB\n",
      "Iter 2280: Train loss 1.278, Learning Rate 1.000e-04, It/sec 1.484, Tokens/sec 264.023, Trained Tokens 378139, Peak mem 19.940 GB\n",
      "Iter 2290: Train loss 1.413, Learning Rate 1.000e-04, It/sec 1.430, Tokens/sec 272.831, Trained Tokens 380047, Peak mem 19.940 GB\n",
      "Iter 2300: Val loss 2.120, Val took 3.693s\n",
      "Iter 2300: Train loss 1.311, Learning Rate 1.000e-04, It/sec 22.360, Tokens/sec 3029.797, Trained Tokens 381402, Peak mem 19.940 GB\n",
      "Iter 2300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002300_adapters.safetensors.\n",
      "Iter 2310: Train loss 1.409, Learning Rate 1.000e-04, It/sec 1.455, Tokens/sec 232.619, Trained Tokens 383001, Peak mem 19.940 GB\n",
      "Iter 2320: Train loss 1.116, Learning Rate 1.000e-04, It/sec 2.161, Tokens/sec 241.554, Trained Tokens 384119, Peak mem 19.940 GB\n",
      "Iter 2330: Train loss 1.195, Learning Rate 1.000e-04, It/sec 1.487, Tokens/sec 243.799, Trained Tokens 385758, Peak mem 19.940 GB\n",
      "Iter 2340: Train loss 1.165, Learning Rate 1.000e-04, It/sec 1.893, Tokens/sec 257.305, Trained Tokens 387117, Peak mem 19.940 GB\n",
      "Iter 2350: Val loss 2.216, Val took 3.221s\n",
      "Iter 2350: Train loss 1.511, Learning Rate 1.000e-04, It/sec 17.929, Tokens/sec 4026.904, Trained Tokens 389363, Peak mem 19.940 GB\n",
      "Iter 2360: Train loss 1.419, Learning Rate 1.000e-04, It/sec 1.621, Tokens/sec 239.395, Trained Tokens 390840, Peak mem 19.940 GB\n",
      "Iter 2370: Train loss 1.391, Learning Rate 1.000e-04, It/sec 1.199, Tokens/sec 262.128, Trained Tokens 393026, Peak mem 19.940 GB\n",
      "Iter 2380: Train loss 1.418, Learning Rate 1.000e-04, It/sec 1.278, Tokens/sec 250.383, Trained Tokens 394985, Peak mem 19.940 GB\n",
      "Iter 2390: Train loss 1.263, Learning Rate 1.000e-04, It/sec 1.397, Tokens/sec 266.824, Trained Tokens 396895, Peak mem 19.940 GB\n",
      "Iter 2400: Val loss 2.168, Val took 3.368s\n",
      "Iter 2400: Train loss 1.271, Learning Rate 1.000e-04, It/sec 12.406, Tokens/sec 1966.291, Trained Tokens 398480, Peak mem 19.940 GB\n",
      "Iter 2400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002400_adapters.safetensors.\n",
      "Iter 2410: Train loss 1.096, Learning Rate 1.000e-04, It/sec 2.060, Tokens/sec 256.440, Trained Tokens 399725, Peak mem 19.940 GB\n",
      "Iter 2420: Train loss 1.440, Learning Rate 1.000e-04, It/sec 1.452, Tokens/sec 253.424, Trained Tokens 401470, Peak mem 19.940 GB\n",
      "Iter 2430: Train loss 1.205, Learning Rate 1.000e-04, It/sec 2.181, Tokens/sec 259.775, Trained Tokens 402661, Peak mem 19.940 GB\n",
      "Iter 2440: Train loss 1.189, Learning Rate 1.000e-04, It/sec 1.917, Tokens/sec 247.504, Trained Tokens 403952, Peak mem 19.940 GB\n",
      "Iter 2450: Val loss 2.136, Val took 3.580s\n",
      "Iter 2450: Train loss 1.278, Learning Rate 1.000e-04, It/sec 23.833, Tokens/sec 4289.977, Trained Tokens 405752, Peak mem 19.940 GB\n",
      "Iter 2460: Train loss 1.284, Learning Rate 1.000e-04, It/sec 1.506, Tokens/sec 261.690, Trained Tokens 407490, Peak mem 19.940 GB\n",
      "Iter 2470: Train loss 1.183, Learning Rate 1.000e-04, It/sec 1.642, Tokens/sec 245.099, Trained Tokens 408983, Peak mem 19.940 GB\n",
      "Iter 2480: Train loss 1.347, Learning Rate 1.000e-04, It/sec 1.416, Tokens/sec 269.968, Trained Tokens 410890, Peak mem 19.940 GB\n",
      "Iter 2490: Train loss 1.040, Learning Rate 1.000e-04, It/sec 1.670, Tokens/sec 267.256, Trained Tokens 412490, Peak mem 19.940 GB\n",
      "Iter 2500: Val loss 2.299, Val took 3.828s\n",
      "Iter 2500: Train loss 1.070, Learning Rate 1.000e-04, It/sec 28.315, Tokens/sec 4448.363, Trained Tokens 414061, Peak mem 19.940 GB\n",
      "Iter 2500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002500_adapters.safetensors.\n",
      "Iter 2510: Train loss 1.313, Learning Rate 1.000e-04, It/sec 1.364, Tokens/sec 270.526, Trained Tokens 416044, Peak mem 19.940 GB\n",
      "Iter 2520: Train loss 1.187, Learning Rate 1.000e-04, It/sec 1.505, Tokens/sec 267.943, Trained Tokens 417824, Peak mem 19.940 GB\n",
      "Iter 2530: Train loss 1.140, Learning Rate 1.000e-04, It/sec 1.907, Tokens/sec 265.273, Trained Tokens 419215, Peak mem 19.940 GB\n",
      "Iter 2540: Train loss 1.296, Learning Rate 1.000e-04, It/sec 1.203, Tokens/sec 282.508, Trained Tokens 421564, Peak mem 19.940 GB\n",
      "Iter 2550: Val loss 2.159, Val took 3.418s\n",
      "Iter 2550: Train loss 1.022, Learning Rate 1.000e-04, It/sec 40.758, Tokens/sec 4442.665, Trained Tokens 422654, Peak mem 19.940 GB\n",
      "Iter 2560: Train loss 1.167, Learning Rate 1.000e-04, It/sec 1.267, Tokens/sec 235.701, Trained Tokens 424515, Peak mem 19.940 GB\n",
      "Iter 2570: Train loss 1.057, Learning Rate 1.000e-04, It/sec 1.586, Tokens/sec 268.170, Trained Tokens 426206, Peak mem 19.940 GB\n",
      "Iter 2580: Train loss 1.557, Learning Rate 1.000e-04, It/sec 1.049, Tokens/sec 278.601, Trained Tokens 428861, Peak mem 19.940 GB\n",
      "Iter 2590: Train loss 1.200, Learning Rate 1.000e-04, It/sec 1.878, Tokens/sec 253.191, Trained Tokens 430209, Peak mem 19.940 GB\n",
      "Iter 2600: Val loss 2.327, Val took 3.573s\n",
      "Iter 2600: Train loss 1.101, Learning Rate 1.000e-04, It/sec 6.900, Tokens/sec 1086.035, Trained Tokens 431783, Peak mem 19.940 GB\n",
      "Iter 2600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002600_adapters.safetensors.\n",
      "Iter 2610: Train loss 1.087, Learning Rate 1.000e-04, It/sec 1.776, Tokens/sec 263.628, Trained Tokens 433267, Peak mem 19.940 GB\n",
      "Iter 2620: Train loss 1.155, Learning Rate 1.000e-04, It/sec 1.618, Tokens/sec 254.937, Trained Tokens 434843, Peak mem 19.940 GB\n",
      "Iter 2630: Train loss 1.318, Learning Rate 1.000e-04, It/sec 1.517, Tokens/sec 260.983, Trained Tokens 436563, Peak mem 19.940 GB\n",
      "Iter 2640: Train loss 1.239, Learning Rate 1.000e-04, It/sec 2.002, Tokens/sec 268.490, Trained Tokens 437904, Peak mem 19.940 GB\n",
      "Iter 2650: Val loss 2.272, Val took 3.922s\n",
      "Iter 2650: Train loss 1.177, Learning Rate 1.000e-04, It/sec 22.696, Tokens/sec 3331.799, Trained Tokens 439372, Peak mem 19.940 GB\n",
      "Iter 2660: Train loss 1.336, Learning Rate 1.000e-04, It/sec 1.402, Tokens/sec 258.519, Trained Tokens 441216, Peak mem 19.940 GB\n",
      "Iter 2670: Train loss 1.023, Learning Rate 1.000e-04, It/sec 1.710, Tokens/sec 243.338, Trained Tokens 442639, Peak mem 19.940 GB\n",
      "Iter 2680: Train loss 1.088, Learning Rate 1.000e-04, It/sec 1.898, Tokens/sec 248.436, Trained Tokens 443948, Peak mem 19.940 GB\n",
      "Iter 2690: Train loss 1.213, Learning Rate 1.000e-04, It/sec 1.243, Tokens/sec 278.806, Trained Tokens 446191, Peak mem 19.940 GB\n",
      "Iter 2700: Val loss 2.201, Val took 3.507s\n",
      "Iter 2700: Train loss 1.084, Learning Rate 1.000e-04, It/sec 28.559, Tokens/sec 4186.783, Trained Tokens 447657, Peak mem 19.940 GB\n",
      "Iter 2700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002700_adapters.safetensors.\n",
      "Iter 2710: Train loss 0.989, Learning Rate 1.000e-04, It/sec 2.043, Tokens/sec 253.111, Trained Tokens 448896, Peak mem 19.940 GB\n",
      "Iter 2720: Train loss 1.030, Learning Rate 1.000e-04, It/sec 2.198, Tokens/sec 244.903, Trained Tokens 450010, Peak mem 19.940 GB\n",
      "Iter 2730: Train loss 1.259, Learning Rate 1.000e-04, It/sec 1.211, Tokens/sec 257.965, Trained Tokens 452141, Peak mem 19.940 GB\n",
      "Iter 2740: Train loss 1.122, Learning Rate 1.000e-04, It/sec 1.281, Tokens/sec 261.554, Trained Tokens 454183, Peak mem 19.940 GB\n",
      "Iter 2750: Val loss 2.021, Val took 3.110s\n",
      "Iter 2750: Train loss 1.124, Learning Rate 1.000e-04, It/sec 29.712, Tokens/sec 3990.380, Trained Tokens 455526, Peak mem 19.940 GB\n",
      "Iter 2760: Train loss 1.140, Learning Rate 1.000e-04, It/sec 2.022, Tokens/sec 255.587, Trained Tokens 456790, Peak mem 19.940 GB\n",
      "Iter 2770: Train loss 1.139, Learning Rate 1.000e-04, It/sec 1.853, Tokens/sec 246.578, Trained Tokens 458121, Peak mem 19.940 GB\n",
      "Iter 2780: Train loss 1.176, Learning Rate 1.000e-04, It/sec 1.326, Tokens/sec 272.279, Trained Tokens 460174, Peak mem 19.940 GB\n",
      "Iter 2790: Train loss 1.122, Learning Rate 1.000e-04, It/sec 1.046, Tokens/sec 265.179, Trained Tokens 462709, Peak mem 19.940 GB\n",
      "Iter 2800: Val loss 2.153, Val took 3.055s\n",
      "Iter 2800: Train loss 1.172, Learning Rate 1.000e-04, It/sec 12.815, Tokens/sec 2255.512, Trained Tokens 464469, Peak mem 19.940 GB\n",
      "Iter 2800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002800_adapters.safetensors.\n",
      "Iter 2810: Train loss 0.982, Learning Rate 1.000e-04, It/sec 1.622, Tokens/sec 219.884, Trained Tokens 465825, Peak mem 19.940 GB\n",
      "Iter 2820: Train loss 1.327, Learning Rate 1.000e-04, It/sec 1.569, Tokens/sec 258.247, Trained Tokens 467471, Peak mem 19.940 GB\n",
      "Iter 2830: Train loss 1.186, Learning Rate 1.000e-04, It/sec 1.446, Tokens/sec 273.550, Trained Tokens 469363, Peak mem 19.940 GB\n",
      "Iter 2840: Train loss 1.196, Learning Rate 1.000e-04, It/sec 1.615, Tokens/sec 255.751, Trained Tokens 470947, Peak mem 19.940 GB\n",
      "Iter 2850: Val loss 2.073, Val took 3.414s\n",
      "Iter 2850: Train loss 1.345, Learning Rate 1.000e-04, It/sec 18.496, Tokens/sec 3614.110, Trained Tokens 472901, Peak mem 19.940 GB\n",
      "Iter 2860: Train loss 1.152, Learning Rate 1.000e-04, It/sec 1.651, Tokens/sec 242.899, Trained Tokens 474372, Peak mem 19.940 GB\n",
      "Iter 2870: Train loss 1.038, Learning Rate 1.000e-04, It/sec 2.351, Tokens/sec 242.813, Trained Tokens 475405, Peak mem 19.940 GB\n",
      "Iter 2880: Train loss 0.911, Learning Rate 1.000e-04, It/sec 2.088, Tokens/sec 259.751, Trained Tokens 476649, Peak mem 19.940 GB\n",
      "Iter 2890: Train loss 0.828, Learning Rate 1.000e-04, It/sec 2.162, Tokens/sec 262.515, Trained Tokens 477863, Peak mem 19.940 GB\n",
      "Iter 2900: Val loss 2.459, Val took 4.135s\n",
      "Iter 2900: Train loss 0.983, Learning Rate 1.000e-04, It/sec 18.521, Tokens/sec 3280.133, Trained Tokens 479634, Peak mem 19.940 GB\n",
      "Iter 2900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0002900_adapters.safetensors.\n",
      "Iter 2910: Train loss 1.130, Learning Rate 1.000e-04, It/sec 1.488, Tokens/sec 267.759, Trained Tokens 481434, Peak mem 19.940 GB\n",
      "Iter 2920: Train loss 0.919, Learning Rate 1.000e-04, It/sec 1.943, Tokens/sec 228.448, Trained Tokens 482610, Peak mem 19.940 GB\n",
      "Iter 2930: Train loss 0.976, Learning Rate 1.000e-04, It/sec 1.711, Tokens/sec 253.771, Trained Tokens 484093, Peak mem 19.940 GB\n",
      "Iter 2940: Train loss 1.268, Learning Rate 1.000e-04, It/sec 1.391, Tokens/sec 274.500, Trained Tokens 486067, Peak mem 19.940 GB\n",
      "Iter 2950: Val loss 2.112, Val took 3.484s\n",
      "Iter 2950: Train loss 1.052, Learning Rate 1.000e-04, It/sec 10.540, Tokens/sec 2494.877, Trained Tokens 488434, Peak mem 19.940 GB\n",
      "Iter 2960: Train loss 1.107, Learning Rate 1.000e-04, It/sec 1.527, Tokens/sec 259.188, Trained Tokens 490131, Peak mem 19.940 GB\n",
      "Iter 2970: Train loss 1.046, Learning Rate 1.000e-04, It/sec 1.386, Tokens/sec 269.198, Trained Tokens 492073, Peak mem 19.940 GB\n",
      "Iter 2980: Train loss 0.971, Learning Rate 1.000e-04, It/sec 1.652, Tokens/sec 253.878, Trained Tokens 493610, Peak mem 19.940 GB\n",
      "Iter 2990: Train loss 0.903, Learning Rate 1.000e-04, It/sec 1.595, Tokens/sec 252.864, Trained Tokens 495195, Peak mem 19.940 GB\n",
      "Iter 3000: Val loss 2.104, Val took 3.766s\n",
      "Iter 3000: Train loss 0.957, Learning Rate 1.000e-04, It/sec 16.307, Tokens/sec 1899.737, Trained Tokens 496360, Peak mem 19.940 GB\n",
      "Iter 3000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003000_adapters.safetensors.\n",
      "Iter 3010: Train loss 1.147, Learning Rate 1.000e-04, It/sec 1.117, Tokens/sec 254.914, Trained Tokens 498642, Peak mem 19.940 GB\n",
      "Iter 3020: Train loss 0.878, Learning Rate 1.000e-04, It/sec 1.939, Tokens/sec 250.917, Trained Tokens 499936, Peak mem 19.940 GB\n",
      "Iter 3030: Train loss 1.075, Learning Rate 1.000e-04, It/sec 1.535, Tokens/sec 243.613, Trained Tokens 501523, Peak mem 19.940 GB\n",
      "Iter 3040: Train loss 1.102, Learning Rate 1.000e-04, It/sec 0.960, Tokens/sec 278.063, Trained Tokens 504418, Peak mem 19.940 GB\n",
      "Iter 3050: Val loss 2.270, Val took 3.100s\n",
      "Iter 3050: Train loss 1.280, Learning Rate 1.000e-04, It/sec 4.125, Tokens/sec 877.844, Trained Tokens 506546, Peak mem 19.940 GB\n",
      "Iter 3060: Train loss 1.108, Learning Rate 1.000e-04, It/sec 1.451, Tokens/sec 262.856, Trained Tokens 508357, Peak mem 19.940 GB\n",
      "Iter 3070: Train loss 1.075, Learning Rate 1.000e-04, It/sec 1.360, Tokens/sec 251.866, Trained Tokens 510209, Peak mem 19.940 GB\n",
      "Iter 3080: Train loss 1.136, Learning Rate 1.000e-04, It/sec 1.650, Tokens/sec 250.081, Trained Tokens 511725, Peak mem 19.940 GB\n",
      "Iter 3090: Train loss 1.141, Learning Rate 1.000e-04, It/sec 1.636, Tokens/sec 259.428, Trained Tokens 513311, Peak mem 19.940 GB\n",
      "Iter 3100: Val loss 2.161, Val took 3.872s\n",
      "Iter 3100: Train loss 0.992, Learning Rate 1.000e-04, It/sec 30.749, Tokens/sec 3892.815, Trained Tokens 514577, Peak mem 19.940 GB\n",
      "Iter 3100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003100_adapters.safetensors.\n",
      "Iter 3110: Train loss 1.014, Learning Rate 1.000e-04, It/sec 1.775, Tokens/sec 255.018, Trained Tokens 516014, Peak mem 19.940 GB\n",
      "Iter 3120: Train loss 1.027, Learning Rate 1.000e-04, It/sec 1.671, Tokens/sec 262.395, Trained Tokens 517584, Peak mem 19.940 GB\n",
      "Iter 3130: Train loss 1.257, Learning Rate 1.000e-04, It/sec 1.613, Tokens/sec 263.512, Trained Tokens 519218, Peak mem 19.940 GB\n",
      "Iter 3140: Train loss 1.040, Learning Rate 1.000e-04, It/sec 1.968, Tokens/sec 252.254, Trained Tokens 520500, Peak mem 19.940 GB\n",
      "Iter 3150: Val loss 2.092, Val took 3.452s\n",
      "Iter 3150: Train loss 1.086, Learning Rate 1.000e-04, It/sec 22.617, Tokens/sec 3614.225, Trained Tokens 522098, Peak mem 19.940 GB\n",
      "Iter 3160: Train loss 1.248, Learning Rate 1.000e-04, It/sec 1.243, Tokens/sec 242.227, Trained Tokens 524047, Peak mem 19.940 GB\n",
      "Iter 3170: Train loss 1.136, Learning Rate 1.000e-04, It/sec 1.777, Tokens/sec 262.598, Trained Tokens 525525, Peak mem 19.940 GB\n",
      "Iter 3180: Train loss 1.041, Learning Rate 1.000e-04, It/sec 1.723, Tokens/sec 246.327, Trained Tokens 526955, Peak mem 19.940 GB\n",
      "Iter 3190: Train loss 0.934, Learning Rate 1.000e-04, It/sec 2.022, Tokens/sec 244.675, Trained Tokens 528165, Peak mem 19.940 GB\n",
      "Iter 3200: Val loss 2.396, Val took 4.191s\n",
      "Iter 3200: Train loss 1.161, Learning Rate 1.000e-04, It/sec 16.440, Tokens/sec 2789.952, Trained Tokens 529862, Peak mem 19.940 GB\n",
      "Iter 3200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003200_adapters.safetensors.\n",
      "Iter 3210: Train loss 1.062, Learning Rate 1.000e-04, It/sec 1.716, Tokens/sec 253.848, Trained Tokens 531341, Peak mem 19.940 GB\n",
      "Iter 3220: Train loss 1.102, Learning Rate 1.000e-04, It/sec 1.482, Tokens/sec 282.162, Trained Tokens 533245, Peak mem 19.940 GB\n",
      "Iter 3230: Train loss 0.998, Learning Rate 1.000e-04, It/sec 1.553, Tokens/sec 236.648, Trained Tokens 534769, Peak mem 19.940 GB\n",
      "Iter 3240: Train loss 1.193, Learning Rate 1.000e-04, It/sec 1.707, Tokens/sec 258.990, Trained Tokens 536286, Peak mem 19.940 GB\n",
      "Iter 3250: Val loss 2.324, Val took 3.608s\n",
      "Iter 3250: Train loss 0.980, Learning Rate 1.000e-04, It/sec 28.057, Tokens/sec 3498.691, Trained Tokens 537533, Peak mem 19.940 GB\n",
      "Iter 3260: Train loss 0.960, Learning Rate 1.000e-04, It/sec 1.847, Tokens/sec 240.718, Trained Tokens 538836, Peak mem 19.940 GB\n",
      "Iter 3270: Train loss 1.497, Learning Rate 1.000e-04, It/sec 1.137, Tokens/sec 275.339, Trained Tokens 541258, Peak mem 19.940 GB\n",
      "Iter 3280: Train loss 1.129, Learning Rate 1.000e-04, It/sec 1.216, Tokens/sec 250.728, Trained Tokens 543320, Peak mem 19.940 GB\n",
      "Iter 3290: Train loss 0.863, Learning Rate 1.000e-04, It/sec 1.466, Tokens/sec 247.070, Trained Tokens 545005, Peak mem 19.940 GB\n",
      "Iter 3300: Val loss 2.195, Val took 3.476s\n",
      "Iter 3300: Train loss 1.031, Learning Rate 1.000e-04, It/sec 17.504, Tokens/sec 3367.774, Trained Tokens 546929, Peak mem 19.940 GB\n",
      "Iter 3300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003300_adapters.safetensors.\n",
      "Iter 3310: Train loss 1.069, Learning Rate 1.000e-04, It/sec 1.497, Tokens/sec 252.034, Trained Tokens 548613, Peak mem 19.940 GB\n",
      "Iter 3320: Train loss 0.961, Learning Rate 1.000e-04, It/sec 1.785, Tokens/sec 269.487, Trained Tokens 550123, Peak mem 19.940 GB\n",
      "Iter 3330: Train loss 0.827, Learning Rate 1.000e-04, It/sec 1.901, Tokens/sec 256.131, Trained Tokens 551470, Peak mem 19.940 GB\n",
      "Iter 3340: Train loss 1.095, Learning Rate 1.000e-04, It/sec 0.827, Tokens/sec 269.370, Trained Tokens 554728, Peak mem 19.940 GB\n",
      "Iter 3350: Val loss 2.400, Val took 3.671s\n",
      "Iter 3350: Train loss 0.880, Learning Rate 1.000e-04, It/sec 6.527, Tokens/sec 991.515, Trained Tokens 556247, Peak mem 19.940 GB\n",
      "Iter 3360: Train loss 0.780, Learning Rate 1.000e-04, It/sec 1.876, Tokens/sec 263.367, Trained Tokens 557651, Peak mem 19.940 GB\n",
      "Iter 3370: Train loss 0.891, Learning Rate 1.000e-04, It/sec 1.676, Tokens/sec 259.402, Trained Tokens 559199, Peak mem 19.940 GB\n",
      "Iter 3380: Train loss 0.953, Learning Rate 1.000e-04, It/sec 1.632, Tokens/sec 261.813, Trained Tokens 560803, Peak mem 19.940 GB\n",
      "Iter 3390: Train loss 0.988, Learning Rate 1.000e-04, It/sec 1.835, Tokens/sec 265.730, Trained Tokens 562251, Peak mem 19.940 GB\n",
      "Iter 3400: Val loss 2.104, Val took 3.250s\n",
      "Iter 3400: Train loss 1.043, Learning Rate 1.000e-04, It/sec 12.868, Tokens/sec 1935.302, Trained Tokens 563755, Peak mem 19.940 GB\n",
      "Iter 3400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003400_adapters.safetensors.\n",
      "Iter 3410: Train loss 1.035, Learning Rate 1.000e-04, It/sec 1.620, Tokens/sec 258.331, Trained Tokens 565350, Peak mem 19.940 GB\n",
      "Iter 3420: Train loss 1.180, Learning Rate 1.000e-04, It/sec 1.411, Tokens/sec 283.372, Trained Tokens 567359, Peak mem 19.940 GB\n",
      "Iter 3430: Train loss 0.988, Learning Rate 1.000e-04, It/sec 1.872, Tokens/sec 272.762, Trained Tokens 568816, Peak mem 19.940 GB\n",
      "Iter 3440: Train loss 1.154, Learning Rate 1.000e-04, It/sec 1.600, Tokens/sec 269.829, Trained Tokens 570502, Peak mem 19.940 GB\n",
      "Iter 3450: Val loss 2.011, Val took 3.111s\n",
      "Iter 3450: Train loss 1.106, Learning Rate 1.000e-04, It/sec 3.176, Tokens/sec 710.566, Trained Tokens 572739, Peak mem 19.940 GB\n",
      "Iter 3460: Train loss 0.843, Learning Rate 1.000e-04, It/sec 1.582, Tokens/sec 257.633, Trained Tokens 574368, Peak mem 19.940 GB\n",
      "Iter 3470: Train loss 1.019, Learning Rate 1.000e-04, It/sec 1.592, Tokens/sec 260.529, Trained Tokens 576004, Peak mem 19.940 GB\n",
      "Iter 3480: Train loss 1.099, Learning Rate 1.000e-04, It/sec 1.608, Tokens/sec 269.096, Trained Tokens 577677, Peak mem 19.940 GB\n",
      "Iter 3490: Train loss 0.896, Learning Rate 1.000e-04, It/sec 1.842, Tokens/sec 267.584, Trained Tokens 579130, Peak mem 19.940 GB\n",
      "Iter 3500: Val loss 2.405, Val took 3.592s\n",
      "Iter 3500: Train loss 0.964, Learning Rate 1.000e-04, It/sec 14.540, Tokens/sec 2308.950, Trained Tokens 580718, Peak mem 19.940 GB\n",
      "Iter 3500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003500_adapters.safetensors.\n",
      "Iter 3510: Train loss 0.974, Learning Rate 1.000e-04, It/sec 1.690, Tokens/sec 241.869, Trained Tokens 582149, Peak mem 19.940 GB\n",
      "Iter 3520: Train loss 1.065, Learning Rate 1.000e-04, It/sec 1.685, Tokens/sec 262.324, Trained Tokens 583706, Peak mem 19.940 GB\n",
      "Iter 3530: Train loss 0.773, Learning Rate 1.000e-04, It/sec 2.647, Tokens/sec 227.357, Trained Tokens 584565, Peak mem 19.940 GB\n",
      "Iter 3540: Train loss 1.044, Learning Rate 1.000e-04, It/sec 0.975, Tokens/sec 274.808, Trained Tokens 587383, Peak mem 19.940 GB\n",
      "Iter 3550: Val loss 2.013, Val took 3.616s\n",
      "Iter 3550: Train loss 1.053, Learning Rate 1.000e-04, It/sec 7.351, Tokens/sec 1414.300, Trained Tokens 589307, Peak mem 19.940 GB\n",
      "Iter 3560: Train loss 0.971, Learning Rate 1.000e-04, It/sec 1.864, Tokens/sec 247.216, Trained Tokens 590633, Peak mem 19.940 GB\n",
      "Iter 3570: Train loss 0.789, Learning Rate 1.000e-04, It/sec 1.982, Tokens/sec 258.594, Trained Tokens 591938, Peak mem 19.940 GB\n",
      "Iter 3580: Train loss 1.045, Learning Rate 1.000e-04, It/sec 1.568, Tokens/sec 257.379, Trained Tokens 593579, Peak mem 19.940 GB\n",
      "Iter 3590: Train loss 1.175, Learning Rate 1.000e-04, It/sec 1.476, Tokens/sec 274.131, Trained Tokens 595436, Peak mem 19.940 GB\n",
      "Iter 3600: Val loss 2.207, Val took 3.467s\n",
      "Iter 3600: Train loss 0.944, Learning Rate 1.000e-04, It/sec 8.464, Tokens/sec 1159.591, Trained Tokens 596806, Peak mem 19.940 GB\n",
      "Iter 3600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003600_adapters.safetensors.\n",
      "Iter 3610: Train loss 1.034, Learning Rate 1.000e-04, It/sec 1.545, Tokens/sec 239.833, Trained Tokens 598358, Peak mem 19.940 GB\n",
      "Iter 3620: Train loss 0.816, Learning Rate 1.000e-04, It/sec 1.836, Tokens/sec 241.075, Trained Tokens 599671, Peak mem 19.940 GB\n",
      "Iter 3630: Train loss 1.135, Learning Rate 1.000e-04, It/sec 0.997, Tokens/sec 256.773, Trained Tokens 602247, Peak mem 19.940 GB\n",
      "Iter 3640: Train loss 1.101, Learning Rate 1.000e-04, It/sec 1.410, Tokens/sec 260.684, Trained Tokens 604096, Peak mem 19.940 GB\n",
      "Iter 3650: Val loss 2.171, Val took 3.573s\n",
      "Iter 3650: Train loss 0.958, Learning Rate 1.000e-04, It/sec 19.395, Tokens/sec 2469.001, Trained Tokens 605369, Peak mem 19.940 GB\n",
      "Iter 3660: Train loss 0.875, Learning Rate 1.000e-04, It/sec 1.863, Tokens/sec 253.723, Trained Tokens 606731, Peak mem 19.940 GB\n",
      "Iter 3670: Train loss 1.017, Learning Rate 1.000e-04, It/sec 1.589, Tokens/sec 251.907, Trained Tokens 608316, Peak mem 19.940 GB\n",
      "Iter 3680: Train loss 1.044, Learning Rate 1.000e-04, It/sec 1.555, Tokens/sec 246.570, Trained Tokens 609902, Peak mem 19.940 GB\n",
      "Iter 3690: Train loss 1.005, Learning Rate 1.000e-04, It/sec 1.894, Tokens/sec 252.502, Trained Tokens 611235, Peak mem 19.940 GB\n",
      "Iter 3700: Val loss 1.995, Val took 3.582s\n",
      "Iter 3700: Train loss 1.010, Learning Rate 1.000e-04, It/sec 13.431, Tokens/sec 2929.274, Trained Tokens 613416, Peak mem 19.940 GB\n",
      "Iter 3700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003700_adapters.safetensors.\n",
      "Iter 3710: Train loss 0.957, Learning Rate 1.000e-04, It/sec 1.474, Tokens/sec 261.871, Trained Tokens 615193, Peak mem 19.940 GB\n",
      "Iter 3720: Train loss 0.718, Learning Rate 1.000e-04, It/sec 1.922, Tokens/sec 250.181, Trained Tokens 616495, Peak mem 19.940 GB\n",
      "Iter 3730: Train loss 0.808, Learning Rate 1.000e-04, It/sec 1.803, Tokens/sec 253.701, Trained Tokens 617902, Peak mem 19.940 GB\n",
      "Iter 3740: Train loss 1.012, Learning Rate 1.000e-04, It/sec 1.023, Tokens/sec 259.218, Trained Tokens 620436, Peak mem 19.940 GB\n",
      "Iter 3750: Val loss 2.137, Val took 3.012s\n",
      "Iter 3750: Train loss 1.003, Learning Rate 1.000e-04, It/sec 16.963, Tokens/sec 3804.808, Trained Tokens 622679, Peak mem 19.940 GB\n",
      "Iter 3760: Train loss 0.968, Learning Rate 1.000e-04, It/sec 1.558, Tokens/sec 273.655, Trained Tokens 624435, Peak mem 19.940 GB\n",
      "Iter 3770: Train loss 0.974, Learning Rate 1.000e-04, It/sec 1.764, Tokens/sec 256.766, Trained Tokens 625891, Peak mem 19.940 GB\n",
      "Iter 3780: Train loss 0.819, Learning Rate 1.000e-04, It/sec 1.715, Tokens/sec 260.456, Trained Tokens 627410, Peak mem 19.940 GB\n",
      "Iter 3790: Train loss 0.926, Learning Rate 1.000e-04, It/sec 1.043, Tokens/sec 271.176, Trained Tokens 630010, Peak mem 19.940 GB\n",
      "Iter 3800: Val loss 1.948, Val took 3.513s\n",
      "Iter 3800: Train loss 1.027, Learning Rate 1.000e-04, It/sec 22.795, Tokens/sec 4485.963, Trained Tokens 631978, Peak mem 19.940 GB\n",
      "Iter 3800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003800_adapters.safetensors.\n",
      "Iter 3810: Train loss 0.967, Learning Rate 1.000e-04, It/sec 1.636, Tokens/sec 266.905, Trained Tokens 633609, Peak mem 19.940 GB\n",
      "Iter 3820: Train loss 0.735, Learning Rate 1.000e-04, It/sec 2.441, Tokens/sec 257.499, Trained Tokens 634664, Peak mem 19.940 GB\n",
      "Iter 3830: Train loss 0.936, Learning Rate 1.000e-04, It/sec 1.926, Tokens/sec 270.864, Trained Tokens 636070, Peak mem 19.940 GB\n",
      "Iter 3840: Train loss 0.886, Learning Rate 1.000e-04, It/sec 1.589, Tokens/sec 262.268, Trained Tokens 637721, Peak mem 19.940 GB\n",
      "Iter 3850: Val loss 2.209, Val took 3.061s\n",
      "Iter 3850: Train loss 0.793, Learning Rate 1.000e-04, It/sec 23.915, Tokens/sec 2802.807, Trained Tokens 638893, Peak mem 19.940 GB\n",
      "Iter 3860: Train loss 0.846, Learning Rate 1.000e-04, It/sec 1.616, Tokens/sec 262.469, Trained Tokens 640517, Peak mem 19.940 GB\n",
      "Iter 3870: Train loss 0.905, Learning Rate 1.000e-04, It/sec 1.764, Tokens/sec 259.802, Trained Tokens 641990, Peak mem 19.940 GB\n",
      "Iter 3880: Train loss 0.863, Learning Rate 1.000e-04, It/sec 2.134, Tokens/sec 239.243, Trained Tokens 643111, Peak mem 19.940 GB\n",
      "Iter 3890: Train loss 0.862, Learning Rate 1.000e-04, It/sec 2.076, Tokens/sec 250.741, Trained Tokens 644319, Peak mem 19.940 GB\n",
      "Iter 3900: Val loss 2.111, Val took 3.215s\n",
      "Iter 3900: Train loss 0.790, Learning Rate 1.000e-04, It/sec 16.424, Tokens/sec 2588.496, Trained Tokens 645895, Peak mem 19.940 GB\n",
      "Iter 3900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0003900_adapters.safetensors.\n",
      "Iter 3910: Train loss 0.890, Learning Rate 1.000e-04, It/sec 1.393, Tokens/sec 259.893, Trained Tokens 647761, Peak mem 19.940 GB\n",
      "Iter 3920: Train loss 0.992, Learning Rate 1.000e-04, It/sec 1.588, Tokens/sec 271.985, Trained Tokens 649474, Peak mem 19.940 GB\n",
      "Iter 3930: Train loss 0.887, Learning Rate 1.000e-04, It/sec 1.648, Tokens/sec 260.846, Trained Tokens 651057, Peak mem 19.940 GB\n",
      "Iter 3940: Train loss 0.995, Learning Rate 1.000e-04, It/sec 1.265, Tokens/sec 272.839, Trained Tokens 653213, Peak mem 19.940 GB\n",
      "Iter 3950: Val loss 1.987, Val took 3.246s\n",
      "Iter 3950: Train loss 0.738, Learning Rate 1.000e-04, It/sec 13.361, Tokens/sec 1822.468, Trained Tokens 654577, Peak mem 19.940 GB\n",
      "Iter 3960: Train loss 0.881, Learning Rate 1.000e-04, It/sec 1.609, Tokens/sec 240.402, Trained Tokens 656071, Peak mem 19.940 GB\n",
      "Iter 3970: Train loss 0.969, Learning Rate 1.000e-04, It/sec 1.305, Tokens/sec 269.828, Trained Tokens 658139, Peak mem 19.940 GB\n",
      "Iter 3980: Train loss 1.013, Learning Rate 1.000e-04, It/sec 1.327, Tokens/sec 259.211, Trained Tokens 660093, Peak mem 19.940 GB\n",
      "Iter 3990: Train loss 0.957, Learning Rate 1.000e-04, It/sec 1.531, Tokens/sec 237.840, Trained Tokens 661647, Peak mem 19.940 GB\n",
      "Iter 4000: Val loss 2.237, Val took 4.564s\n",
      "Iter 4000: Train loss 0.976, Learning Rate 1.000e-04, It/sec 18.390, Tokens/sec 2629.714, Trained Tokens 663077, Peak mem 19.940 GB\n",
      "Iter 4000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004000_adapters.safetensors.\n",
      "Iter 4010: Train loss 0.868, Learning Rate 1.000e-04, It/sec 1.836, Tokens/sec 249.925, Trained Tokens 664438, Peak mem 19.940 GB\n",
      "Iter 4020: Train loss 0.939, Learning Rate 1.000e-04, It/sec 1.662, Tokens/sec 252.937, Trained Tokens 665960, Peak mem 19.940 GB\n",
      "Iter 4030: Train loss 1.034, Learning Rate 1.000e-04, It/sec 1.156, Tokens/sec 276.427, Trained Tokens 668351, Peak mem 19.940 GB\n",
      "Iter 4040: Train loss 0.801, Learning Rate 1.000e-04, It/sec 1.873, Tokens/sec 256.245, Trained Tokens 669719, Peak mem 19.940 GB\n",
      "Iter 4050: Val loss 1.972, Val took 3.519s\n",
      "Iter 4050: Train loss 0.903, Learning Rate 1.000e-04, It/sec 23.588, Tokens/sec 2981.526, Trained Tokens 670983, Peak mem 19.940 GB\n",
      "Iter 4060: Train loss 1.030, Learning Rate 1.000e-04, It/sec 1.607, Tokens/sec 264.260, Trained Tokens 672627, Peak mem 19.940 GB\n",
      "Iter 4070: Train loss 0.967, Learning Rate 1.000e-04, It/sec 1.616, Tokens/sec 257.843, Trained Tokens 674223, Peak mem 19.940 GB\n",
      "Iter 4080: Train loss 0.815, Learning Rate 1.000e-04, It/sec 1.678, Tokens/sec 255.197, Trained Tokens 675744, Peak mem 19.940 GB\n",
      "Iter 4090: Train loss 1.008, Learning Rate 1.000e-04, It/sec 1.835, Tokens/sec 245.537, Trained Tokens 677082, Peak mem 19.940 GB\n",
      "Iter 4100: Val loss 2.416, Val took 3.522s\n",
      "Iter 4100: Train loss 1.000, Learning Rate 1.000e-04, It/sec 22.894, Tokens/sec 4734.533, Trained Tokens 679150, Peak mem 19.940 GB\n",
      "Iter 4100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004100_adapters.safetensors.\n",
      "Iter 4110: Train loss 0.692, Learning Rate 1.000e-04, It/sec 1.773, Tokens/sec 248.373, Trained Tokens 680551, Peak mem 19.940 GB\n",
      "Iter 4120: Train loss 0.782, Learning Rate 1.000e-04, It/sec 1.580, Tokens/sec 249.627, Trained Tokens 682131, Peak mem 19.940 GB\n",
      "Iter 4130: Train loss 0.758, Learning Rate 1.000e-04, It/sec 1.485, Tokens/sec 256.209, Trained Tokens 683856, Peak mem 19.940 GB\n",
      "Iter 4140: Train loss 0.912, Learning Rate 1.000e-04, It/sec 1.209, Tokens/sec 275.244, Trained Tokens 686132, Peak mem 19.940 GB\n",
      "Iter 4150: Val loss 2.367, Val took 2.902s\n",
      "Iter 4150: Train loss 0.916, Learning Rate 1.000e-04, It/sec 9.116, Tokens/sec 1937.124, Trained Tokens 688257, Peak mem 19.940 GB\n",
      "Iter 4160: Train loss 0.823, Learning Rate 1.000e-04, It/sec 1.736, Tokens/sec 262.248, Trained Tokens 689768, Peak mem 19.940 GB\n",
      "Iter 4170: Train loss 0.952, Learning Rate 1.000e-04, It/sec 1.508, Tokens/sec 270.919, Trained Tokens 691565, Peak mem 19.940 GB\n",
      "Iter 4180: Train loss 0.896, Learning Rate 1.000e-04, It/sec 1.598, Tokens/sec 265.575, Trained Tokens 693227, Peak mem 19.940 GB\n",
      "Iter 4190: Train loss 0.783, Learning Rate 1.000e-04, It/sec 1.684, Tokens/sec 268.639, Trained Tokens 694822, Peak mem 19.940 GB\n",
      "Iter 4200: Val loss 2.409, Val took 3.532s\n",
      "Iter 4200: Train loss 0.769, Learning Rate 1.000e-04, It/sec 18.418, Tokens/sec 2786.702, Trained Tokens 696335, Peak mem 19.940 GB\n",
      "Iter 4200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004200_adapters.safetensors.\n",
      "Iter 4210: Train loss 0.772, Learning Rate 1.000e-04, It/sec 1.874, Tokens/sec 231.753, Trained Tokens 697572, Peak mem 19.940 GB\n",
      "Iter 4220: Train loss 0.687, Learning Rate 1.000e-04, It/sec 1.985, Tokens/sec 237.588, Trained Tokens 698769, Peak mem 19.940 GB\n",
      "Iter 4230: Train loss 0.768, Learning Rate 1.000e-04, It/sec 2.079, Tokens/sec 242.612, Trained Tokens 699936, Peak mem 19.940 GB\n",
      "Iter 4240: Train loss 0.914, Learning Rate 1.000e-04, It/sec 1.464, Tokens/sec 274.445, Trained Tokens 701810, Peak mem 19.940 GB\n",
      "Iter 4250: Val loss 2.370, Val took 3.869s\n",
      "Iter 4250: Train loss 0.756, Learning Rate 1.000e-04, It/sec 29.732, Tokens/sec 3344.828, Trained Tokens 702935, Peak mem 19.940 GB\n",
      "Iter 4260: Train loss 0.794, Learning Rate 1.000e-04, It/sec 1.689, Tokens/sec 247.667, Trained Tokens 704401, Peak mem 19.940 GB\n",
      "Iter 4270: Train loss 0.823, Learning Rate 1.000e-04, It/sec 1.516, Tokens/sec 271.737, Trained Tokens 706193, Peak mem 19.940 GB\n",
      "Iter 4280: Train loss 0.917, Learning Rate 1.000e-04, It/sec 1.741, Tokens/sec 255.620, Trained Tokens 707661, Peak mem 19.940 GB\n",
      "Iter 4290: Train loss 0.887, Learning Rate 1.000e-04, It/sec 1.536, Tokens/sec 262.572, Trained Tokens 709370, Peak mem 19.940 GB\n",
      "Iter 4300: Val loss 2.329, Val took 3.574s\n",
      "Iter 4300: Train loss 0.858, Learning Rate 1.000e-04, It/sec 17.040, Tokens/sec 3276.887, Trained Tokens 711293, Peak mem 19.940 GB\n",
      "Iter 4300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004300_adapters.safetensors.\n",
      "Iter 4310: Train loss 0.933, Learning Rate 1.000e-04, It/sec 1.311, Tokens/sec 265.530, Trained Tokens 713319, Peak mem 19.940 GB\n",
      "Iter 4320: Train loss 0.931, Learning Rate 1.000e-04, It/sec 1.825, Tokens/sec 262.467, Trained Tokens 714757, Peak mem 19.940 GB\n",
      "Iter 4330: Train loss 0.873, Learning Rate 1.000e-04, It/sec 1.880, Tokens/sec 268.262, Trained Tokens 716184, Peak mem 19.940 GB\n",
      "Iter 4340: Train loss 0.704, Learning Rate 1.000e-04, It/sec 2.490, Tokens/sec 255.226, Trained Tokens 717209, Peak mem 19.940 GB\n",
      "Iter 4350: Val loss 2.300, Val took 4.045s\n",
      "Iter 4350: Train loss 0.690, Learning Rate 1.000e-04, It/sec 16.633, Tokens/sec 2232.203, Trained Tokens 718551, Peak mem 19.940 GB\n",
      "Iter 4360: Train loss 0.959, Learning Rate 1.000e-04, It/sec 1.135, Tokens/sec 255.265, Trained Tokens 720800, Peak mem 19.940 GB\n",
      "Iter 4370: Train loss 0.969, Learning Rate 1.000e-04, It/sec 1.185, Tokens/sec 271.629, Trained Tokens 723093, Peak mem 19.940 GB\n",
      "Iter 4380: Train loss 0.840, Learning Rate 1.000e-04, It/sec 1.846, Tokens/sec 256.735, Trained Tokens 724484, Peak mem 19.940 GB\n",
      "Iter 4390: Train loss 0.838, Learning Rate 1.000e-04, It/sec 1.388, Tokens/sec 265.995, Trained Tokens 726401, Peak mem 19.940 GB\n",
      "Iter 4400: Val loss 2.204, Val took 3.642s\n",
      "Iter 4400: Train loss 1.026, Learning Rate 1.000e-04, It/sec 18.406, Tokens/sec 4498.473, Trained Tokens 728845, Peak mem 19.940 GB\n",
      "Iter 4400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004400_adapters.safetensors.\n",
      "Iter 4410: Train loss 0.868, Learning Rate 1.000e-04, It/sec 1.531, Tokens/sec 249.576, Trained Tokens 730475, Peak mem 19.940 GB\n",
      "Iter 4420: Train loss 0.905, Learning Rate 1.000e-04, It/sec 1.738, Tokens/sec 268.884, Trained Tokens 732022, Peak mem 19.940 GB\n",
      "Iter 4430: Train loss 0.697, Learning Rate 1.000e-04, It/sec 2.579, Tokens/sec 249.169, Trained Tokens 732988, Peak mem 19.940 GB\n",
      "Iter 4440: Train loss 1.031, Learning Rate 1.000e-04, It/sec 0.957, Tokens/sec 266.556, Trained Tokens 735773, Peak mem 19.940 GB\n",
      "Iter 4450: Val loss 2.174, Val took 4.261s\n",
      "Iter 4450: Train loss 0.777, Learning Rate 1.000e-04, It/sec 12.165, Tokens/sec 1742.021, Trained Tokens 737205, Peak mem 19.940 GB\n",
      "Iter 4460: Train loss 0.864, Learning Rate 1.000e-04, It/sec 1.445, Tokens/sec 245.209, Trained Tokens 738902, Peak mem 19.940 GB\n",
      "Iter 4470: Train loss 1.016, Learning Rate 1.000e-04, It/sec 1.309, Tokens/sec 260.700, Trained Tokens 740894, Peak mem 19.940 GB\n",
      "Iter 4480: Train loss 0.883, Learning Rate 1.000e-04, It/sec 2.067, Tokens/sec 248.398, Trained Tokens 742096, Peak mem 19.940 GB\n",
      "Iter 4490: Train loss 0.859, Learning Rate 1.000e-04, It/sec 1.715, Tokens/sec 264.477, Trained Tokens 743638, Peak mem 19.940 GB\n",
      "Iter 4500: Val loss 2.153, Val took 3.599s\n",
      "Iter 4500: Train loss 0.948, Learning Rate 1.000e-04, It/sec 21.823, Tokens/sec 3744.853, Trained Tokens 745354, Peak mem 19.940 GB\n",
      "Iter 4500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004500_adapters.safetensors.\n",
      "Iter 4510: Train loss 0.986, Learning Rate 1.000e-04, It/sec 1.481, Tokens/sec 253.464, Trained Tokens 747065, Peak mem 19.940 GB\n",
      "Iter 4520: Train loss 0.756, Learning Rate 1.000e-04, It/sec 1.509, Tokens/sec 269.305, Trained Tokens 748850, Peak mem 19.940 GB\n",
      "Iter 4530: Train loss 0.697, Learning Rate 1.000e-04, It/sec 1.898, Tokens/sec 268.932, Trained Tokens 750267, Peak mem 19.940 GB\n",
      "Iter 4540: Train loss 0.797, Learning Rate 1.000e-04, It/sec 0.953, Tokens/sec 269.011, Trained Tokens 753091, Peak mem 19.940 GB\n",
      "Iter 4550: Val loss 2.478, Val took 4.076s\n",
      "Iter 4550: Train loss 0.647, Learning Rate 1.000e-04, It/sec 5.713, Tokens/sec 881.525, Trained Tokens 754634, Peak mem 19.940 GB\n",
      "Iter 4560: Train loss 0.748, Learning Rate 1.000e-04, It/sec 2.084, Tokens/sec 258.818, Trained Tokens 755876, Peak mem 19.940 GB\n",
      "Iter 4570: Train loss 0.811, Learning Rate 1.000e-04, It/sec 1.492, Tokens/sec 263.850, Trained Tokens 757644, Peak mem 19.940 GB\n",
      "Iter 4580: Train loss 0.802, Learning Rate 1.000e-04, It/sec 1.501, Tokens/sec 269.987, Trained Tokens 759443, Peak mem 19.940 GB\n",
      "Iter 4590: Train loss 0.955, Learning Rate 1.000e-04, It/sec 1.215, Tokens/sec 285.453, Trained Tokens 761793, Peak mem 19.940 GB\n",
      "Iter 4600: Val loss 2.273, Val took 3.640s\n",
      "Iter 4600: Train loss 0.908, Learning Rate 1.000e-04, It/sec 8.771, Tokens/sec 1762.952, Trained Tokens 763803, Peak mem 19.940 GB\n",
      "Iter 4600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004600_adapters.safetensors.\n",
      "Iter 4610: Train loss 0.655, Learning Rate 1.000e-04, It/sec 2.607, Tokens/sec 233.037, Trained Tokens 764697, Peak mem 19.940 GB\n",
      "Iter 4620: Train loss 0.636, Learning Rate 1.000e-04, It/sec 2.041, Tokens/sec 251.231, Trained Tokens 765928, Peak mem 19.940 GB\n",
      "Iter 4630: Train loss 0.862, Learning Rate 1.000e-04, It/sec 1.347, Tokens/sec 282.966, Trained Tokens 768029, Peak mem 19.940 GB\n",
      "Iter 4640: Train loss 0.647, Learning Rate 1.000e-04, It/sec 2.561, Tokens/sec 249.445, Trained Tokens 769003, Peak mem 19.940 GB\n",
      "Iter 4650: Val loss 2.510, Val took 3.561s\n",
      "Iter 4650: Train loss 0.691, Learning Rate 1.000e-04, It/sec 30.713, Tokens/sec 3783.812, Trained Tokens 770235, Peak mem 19.940 GB\n",
      "Iter 4660: Train loss 0.759, Learning Rate 1.000e-04, It/sec 1.865, Tokens/sec 258.609, Trained Tokens 771622, Peak mem 19.940 GB\n",
      "Iter 4670: Train loss 0.987, Learning Rate 1.000e-04, It/sec 1.074, Tokens/sec 275.688, Trained Tokens 774188, Peak mem 19.940 GB\n",
      "Iter 4680: Train loss 0.688, Learning Rate 1.000e-04, It/sec 1.761, Tokens/sec 250.379, Trained Tokens 775610, Peak mem 19.940 GB\n",
      "Iter 4690: Train loss 0.773, Learning Rate 1.000e-04, It/sec 1.667, Tokens/sec 275.325, Trained Tokens 777262, Peak mem 19.940 GB\n",
      "Iter 4700: Val loss 1.939, Val took 3.059s\n",
      "Iter 4700: Train loss 0.712, Learning Rate 1.000e-04, It/sec 18.638, Tokens/sec 2473.227, Trained Tokens 778589, Peak mem 19.940 GB\n",
      "Iter 4700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004700_adapters.safetensors.\n",
      "Iter 4710: Train loss 0.838, Learning Rate 1.000e-04, It/sec 1.405, Tokens/sec 255.983, Trained Tokens 780411, Peak mem 19.940 GB\n",
      "Iter 4720: Train loss 0.869, Learning Rate 1.000e-04, It/sec 1.386, Tokens/sec 259.739, Trained Tokens 782285, Peak mem 19.940 GB\n",
      "Iter 4730: Train loss 0.937, Learning Rate 1.000e-04, It/sec 1.331, Tokens/sec 271.452, Trained Tokens 784325, Peak mem 19.940 GB\n",
      "Iter 4740: Train loss 0.714, Learning Rate 1.000e-04, It/sec 2.017, Tokens/sec 259.596, Trained Tokens 785612, Peak mem 19.940 GB\n",
      "Iter 4750: Val loss 2.190, Val took 3.478s\n",
      "Iter 4750: Train loss 0.691, Learning Rate 1.000e-04, It/sec 18.073, Tokens/sec 2472.437, Trained Tokens 786980, Peak mem 19.940 GB\n",
      "Iter 4760: Train loss 0.931, Learning Rate 1.000e-04, It/sec 1.469, Tokens/sec 263.858, Trained Tokens 788776, Peak mem 19.940 GB\n",
      "Iter 4770: Train loss 0.795, Learning Rate 1.000e-04, It/sec 1.485, Tokens/sec 272.423, Trained Tokens 790611, Peak mem 19.940 GB\n",
      "Iter 4780: Train loss 0.755, Learning Rate 1.000e-04, It/sec 1.912, Tokens/sec 265.015, Trained Tokens 791997, Peak mem 19.940 GB\n",
      "Iter 4790: Train loss 0.722, Learning Rate 1.000e-04, It/sec 1.828, Tokens/sec 238.708, Trained Tokens 793303, Peak mem 19.940 GB\n",
      "Iter 4800: Val loss 2.278, Val took 3.491s\n",
      "Iter 4800: Train loss 0.816, Learning Rate 1.000e-04, It/sec 29.609, Tokens/sec 4263.758, Trained Tokens 794743, Peak mem 19.940 GB\n",
      "Iter 4800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004800_adapters.safetensors.\n",
      "Iter 4810: Train loss 1.031, Learning Rate 1.000e-04, It/sec 1.076, Tokens/sec 273.388, Trained Tokens 797283, Peak mem 19.940 GB\n",
      "Iter 4820: Train loss 0.779, Learning Rate 1.000e-04, It/sec 1.820, Tokens/sec 250.311, Trained Tokens 798658, Peak mem 19.940 GB\n",
      "Iter 4830: Train loss 0.765, Learning Rate 1.000e-04, It/sec 1.629, Tokens/sec 272.611, Trained Tokens 800331, Peak mem 19.940 GB\n",
      "Iter 4840: Train loss 0.792, Learning Rate 1.000e-04, It/sec 1.458, Tokens/sec 254.937, Trained Tokens 802079, Peak mem 19.940 GB\n",
      "Iter 4850: Val loss 2.457, Val took 3.235s\n",
      "Iter 4850: Train loss 0.750, Learning Rate 1.000e-04, It/sec 30.134, Tokens/sec 4800.373, Trained Tokens 803672, Peak mem 19.940 GB\n",
      "Iter 4860: Train loss 0.789, Learning Rate 1.000e-04, It/sec 1.740, Tokens/sec 242.699, Trained Tokens 805067, Peak mem 19.940 GB\n",
      "Iter 4870: Train loss 0.818, Learning Rate 1.000e-04, It/sec 1.410, Tokens/sec 267.696, Trained Tokens 806966, Peak mem 19.940 GB\n",
      "Iter 4880: Train loss 0.875, Learning Rate 1.000e-04, It/sec 2.084, Tokens/sec 245.739, Trained Tokens 808145, Peak mem 19.940 GB\n",
      "Iter 4890: Train loss 0.845, Learning Rate 1.000e-04, It/sec 1.490, Tokens/sec 252.658, Trained Tokens 809841, Peak mem 19.940 GB\n",
      "Iter 4900: Val loss 2.112, Val took 3.988s\n",
      "Iter 4900: Train loss 1.036, Learning Rate 1.000e-04, It/sec 43.299, Tokens/sec 8218.235, Trained Tokens 811739, Peak mem 19.940 GB\n",
      "Iter 4900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0004900_adapters.safetensors.\n",
      "Iter 4910: Train loss 0.867, Learning Rate 1.000e-04, It/sec 1.469, Tokens/sec 257.107, Trained Tokens 813489, Peak mem 19.940 GB\n",
      "Iter 4920: Train loss 0.797, Learning Rate 1.000e-04, It/sec 1.726, Tokens/sec 257.394, Trained Tokens 814980, Peak mem 19.940 GB\n",
      "Iter 4930: Train loss 0.654, Learning Rate 1.000e-04, It/sec 1.987, Tokens/sec 253.310, Trained Tokens 816255, Peak mem 19.940 GB\n",
      "Iter 4940: Train loss 0.655, Learning Rate 1.000e-04, It/sec 1.587, Tokens/sec 259.666, Trained Tokens 817891, Peak mem 19.940 GB\n",
      "Iter 4950: Val loss 2.424, Val took 3.838s\n",
      "Iter 4950: Train loss 0.681, Learning Rate 1.000e-04, It/sec 30.417, Tokens/sec 4574.692, Trained Tokens 819395, Peak mem 19.940 GB\n",
      "Iter 4960: Train loss 0.666, Learning Rate 1.000e-04, It/sec 1.975, Tokens/sec 246.538, Trained Tokens 820643, Peak mem 19.940 GB\n",
      "Iter 4970: Train loss 1.032, Learning Rate 1.000e-04, It/sec 0.930, Tokens/sec 263.980, Trained Tokens 823482, Peak mem 19.940 GB\n",
      "Iter 4980: Train loss 0.735, Learning Rate 1.000e-04, It/sec 1.360, Tokens/sec 272.355, Trained Tokens 825484, Peak mem 19.940 GB\n",
      "Iter 4990: Train loss 0.608, Learning Rate 1.000e-04, It/sec 1.658, Tokens/sec 257.282, Trained Tokens 827036, Peak mem 19.940 GB\n",
      "Iter 5000: Val loss 2.325, Val took 3.560s\n",
      "Iter 5000: Train loss 0.692, Learning Rate 1.000e-04, It/sec 29.963, Tokens/sec 5000.870, Trained Tokens 828705, Peak mem 19.940 GB\n",
      "Iter 5000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005000_adapters.safetensors.\n",
      "Iter 5010: Train loss 0.577, Learning Rate 1.000e-04, It/sec 1.564, Tokens/sec 262.660, Trained Tokens 830384, Peak mem 19.940 GB\n",
      "Iter 5020: Train loss 0.614, Learning Rate 1.000e-04, It/sec 1.687, Tokens/sec 254.271, Trained Tokens 831891, Peak mem 19.940 GB\n",
      "Iter 5030: Train loss 0.861, Learning Rate 1.000e-04, It/sec 1.358, Tokens/sec 275.822, Trained Tokens 833922, Peak mem 19.940 GB\n",
      "Iter 5040: Train loss 0.703, Learning Rate 1.000e-04, It/sec 1.253, Tokens/sec 251.555, Trained Tokens 835929, Peak mem 19.940 GB\n",
      "Iter 5050: Val loss 2.268, Val took 4.322s\n",
      "Iter 5050: Train loss 0.681, Learning Rate 1.000e-04, It/sec 14.257, Tokens/sec 1923.245, Trained Tokens 837278, Peak mem 19.940 GB\n",
      "Iter 5060: Train loss 0.769, Learning Rate 1.000e-04, It/sec 1.468, Tokens/sec 266.526, Trained Tokens 839093, Peak mem 19.940 GB\n",
      "Iter 5070: Train loss 0.863, Learning Rate 1.000e-04, It/sec 1.415, Tokens/sec 268.220, Trained Tokens 840989, Peak mem 19.940 GB\n",
      "Iter 5080: Train loss 0.763, Learning Rate 1.000e-04, It/sec 1.558, Tokens/sec 263.833, Trained Tokens 842682, Peak mem 19.940 GB\n",
      "Iter 5090: Train loss 0.802, Learning Rate 1.000e-04, It/sec 1.648, Tokens/sec 266.510, Trained Tokens 844299, Peak mem 19.940 GB\n",
      "Iter 5100: Val loss 2.278, Val took 3.356s\n",
      "Iter 5100: Train loss 0.660, Learning Rate 1.000e-04, It/sec 30.778, Tokens/sec 4124.264, Trained Tokens 845639, Peak mem 19.940 GB\n",
      "Iter 5100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005100_adapters.safetensors.\n",
      "Iter 5110: Train loss 0.661, Learning Rate 1.000e-04, It/sec 1.899, Tokens/sec 233.327, Trained Tokens 846868, Peak mem 19.940 GB\n",
      "Iter 5120: Train loss 0.821, Learning Rate 1.000e-04, It/sec 1.423, Tokens/sec 271.290, Trained Tokens 848775, Peak mem 19.940 GB\n",
      "Iter 5130: Train loss 0.801, Learning Rate 1.000e-04, It/sec 1.954, Tokens/sec 265.709, Trained Tokens 850135, Peak mem 19.940 GB\n",
      "Iter 5140: Train loss 0.759, Learning Rate 1.000e-04, It/sec 1.521, Tokens/sec 271.672, Trained Tokens 851921, Peak mem 19.940 GB\n",
      "Iter 5150: Val loss 2.618, Val took 3.343s\n",
      "Iter 5150: Train loss 0.657, Learning Rate 1.000e-04, It/sec 18.502, Tokens/sec 2941.857, Trained Tokens 853511, Peak mem 19.940 GB\n",
      "Iter 5160: Train loss 0.714, Learning Rate 1.000e-04, It/sec 2.360, Tokens/sec 253.952, Trained Tokens 854587, Peak mem 19.940 GB\n",
      "Iter 5170: Train loss 0.754, Learning Rate 1.000e-04, It/sec 1.784, Tokens/sec 258.611, Trained Tokens 856037, Peak mem 19.940 GB\n",
      "Iter 5180: Train loss 0.677, Learning Rate 1.000e-04, It/sec 2.205, Tokens/sec 253.975, Trained Tokens 857189, Peak mem 19.940 GB\n",
      "Iter 5190: Train loss 0.942, Learning Rate 1.000e-04, It/sec 0.793, Tokens/sec 252.508, Trained Tokens 860375, Peak mem 19.940 GB\n",
      "Iter 5200: Val loss 2.445, Val took 3.263s\n",
      "Iter 5200: Train loss 0.666, Learning Rate 1.000e-04, It/sec 22.591, Tokens/sec 3095.019, Trained Tokens 861745, Peak mem 19.940 GB\n",
      "Iter 5200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005200_adapters.safetensors.\n",
      "Iter 5210: Train loss 0.924, Learning Rate 1.000e-04, It/sec 1.293, Tokens/sec 253.904, Trained Tokens 863708, Peak mem 19.940 GB\n",
      "Iter 5220: Train loss 0.846, Learning Rate 1.000e-04, It/sec 1.377, Tokens/sec 261.127, Trained Tokens 865605, Peak mem 19.940 GB\n",
      "Iter 5230: Train loss 0.672, Learning Rate 1.000e-04, It/sec 2.167, Tokens/sec 270.052, Trained Tokens 866851, Peak mem 19.940 GB\n",
      "Iter 5240: Train loss 0.812, Learning Rate 1.000e-04, It/sec 1.532, Tokens/sec 269.363, Trained Tokens 868609, Peak mem 19.940 GB\n",
      "Iter 5250: Val loss 2.446, Val took 2.684s\n",
      "Iter 5250: Train loss 0.813, Learning Rate 1.000e-04, It/sec 28.867, Tokens/sec 5031.536, Trained Tokens 870352, Peak mem 19.940 GB\n",
      "Iter 5260: Train loss 0.774, Learning Rate 1.000e-04, It/sec 1.532, Tokens/sec 238.298, Trained Tokens 871907, Peak mem 19.940 GB\n",
      "Iter 5270: Train loss 0.647, Learning Rate 1.000e-04, It/sec 2.082, Tokens/sec 240.505, Trained Tokens 873062, Peak mem 19.940 GB\n",
      "Iter 5280: Train loss 0.673, Learning Rate 1.000e-04, It/sec 1.856, Tokens/sec 257.778, Trained Tokens 874451, Peak mem 19.940 GB\n",
      "Iter 5290: Train loss 0.746, Learning Rate 1.000e-04, It/sec 1.838, Tokens/sec 262.848, Trained Tokens 875881, Peak mem 19.940 GB\n",
      "Iter 5300: Val loss 2.248, Val took 3.599s\n",
      "Iter 5300: Train loss 0.848, Learning Rate 1.000e-04, It/sec 21.892, Tokens/sec 3498.391, Trained Tokens 877479, Peak mem 19.940 GB\n",
      "Iter 5300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005300_adapters.safetensors.\n",
      "Iter 5310: Train loss 1.042, Learning Rate 1.000e-04, It/sec 1.095, Tokens/sec 270.345, Trained Tokens 879947, Peak mem 19.940 GB\n",
      "Iter 5320: Train loss 0.826, Learning Rate 1.000e-04, It/sec 1.874, Tokens/sec 255.778, Trained Tokens 881312, Peak mem 19.940 GB\n",
      "Iter 5330: Train loss 0.809, Learning Rate 1.000e-04, It/sec 1.635, Tokens/sec 258.838, Trained Tokens 882895, Peak mem 19.940 GB\n",
      "Iter 5340: Train loss 0.742, Learning Rate 1.000e-04, It/sec 1.477, Tokens/sec 270.923, Trained Tokens 884729, Peak mem 19.940 GB\n",
      "Iter 5350: Val loss 2.466, Val took 4.092s\n",
      "Iter 5350: Train loss 0.636, Learning Rate 1.000e-04, It/sec 17.405, Tokens/sec 2208.727, Trained Tokens 885998, Peak mem 19.940 GB\n",
      "Iter 5360: Train loss 0.691, Learning Rate 1.000e-04, It/sec 1.525, Tokens/sec 256.260, Trained Tokens 887678, Peak mem 19.940 GB\n",
      "Iter 5370: Train loss 0.761, Learning Rate 1.000e-04, It/sec 1.336, Tokens/sec 281.010, Trained Tokens 889782, Peak mem 19.940 GB\n",
      "Iter 5380: Train loss 0.681, Learning Rate 1.000e-04, It/sec 1.697, Tokens/sec 251.174, Trained Tokens 891262, Peak mem 19.940 GB\n",
      "Iter 5390: Train loss 0.675, Learning Rate 1.000e-04, It/sec 1.672, Tokens/sec 270.982, Trained Tokens 892883, Peak mem 19.940 GB\n",
      "Iter 5400: Val loss 2.262, Val took 4.239s\n",
      "Iter 5400: Train loss 0.665, Learning Rate 1.000e-04, It/sec 8.032, Tokens/sec 1571.878, Trained Tokens 894840, Peak mem 19.940 GB\n",
      "Iter 5400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005400_adapters.safetensors.\n",
      "Iter 5410: Train loss 0.722, Learning Rate 1.000e-04, It/sec 1.628, Tokens/sec 261.890, Trained Tokens 896449, Peak mem 19.940 GB\n",
      "Iter 5420: Train loss 0.663, Learning Rate 1.000e-04, It/sec 1.371, Tokens/sec 245.398, Trained Tokens 898239, Peak mem 19.940 GB\n",
      "Iter 5430: Train loss 0.653, Learning Rate 1.000e-04, It/sec 2.569, Tokens/sec 241.507, Trained Tokens 899179, Peak mem 19.940 GB\n",
      "Iter 5440: Train loss 0.761, Learning Rate 1.000e-04, It/sec 1.333, Tokens/sec 255.024, Trained Tokens 901092, Peak mem 19.940 GB\n",
      "Iter 5450: Val loss 2.256, Val took 3.226s\n",
      "Iter 5450: Train loss 0.685, Learning Rate 1.000e-04, It/sec 9.918, Tokens/sec 1552.164, Trained Tokens 902657, Peak mem 19.940 GB\n",
      "Iter 5460: Train loss 0.740, Learning Rate 1.000e-04, It/sec 1.582, Tokens/sec 250.161, Trained Tokens 904238, Peak mem 19.940 GB\n",
      "Iter 5470: Train loss 0.648, Learning Rate 1.000e-04, It/sec 1.565, Tokens/sec 262.095, Trained Tokens 905913, Peak mem 19.940 GB\n",
      "Iter 5480: Train loss 0.557, Learning Rate 1.000e-04, It/sec 2.039, Tokens/sec 252.650, Trained Tokens 907152, Peak mem 19.940 GB\n",
      "Iter 5490: Train loss 0.635, Learning Rate 1.000e-04, It/sec 1.739, Tokens/sec 263.036, Trained Tokens 908665, Peak mem 19.940 GB\n",
      "Iter 5500: Val loss 2.041, Val took 3.075s\n",
      "Iter 5500: Train loss 0.680, Learning Rate 1.000e-04, It/sec 17.492, Tokens/sec 2497.875, Trained Tokens 910093, Peak mem 19.940 GB\n",
      "Iter 5500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005500_adapters.safetensors.\n",
      "Iter 5510: Train loss 0.656, Learning Rate 1.000e-04, It/sec 1.587, Tokens/sec 255.324, Trained Tokens 911702, Peak mem 19.940 GB\n",
      "Iter 5520: Train loss 0.625, Learning Rate 1.000e-04, It/sec 1.790, Tokens/sec 259.194, Trained Tokens 913150, Peak mem 19.940 GB\n",
      "Iter 5530: Train loss 0.637, Learning Rate 1.000e-04, It/sec 1.656, Tokens/sec 255.525, Trained Tokens 914693, Peak mem 19.940 GB\n",
      "Iter 5540: Train loss 0.842, Learning Rate 1.000e-04, It/sec 1.252, Tokens/sec 280.217, Trained Tokens 916932, Peak mem 19.940 GB\n",
      "Iter 5550: Val loss 2.351, Val took 3.642s\n",
      "Iter 5550: Train loss 0.810, Learning Rate 1.000e-04, It/sec 6.846, Tokens/sec 1128.932, Trained Tokens 918581, Peak mem 19.940 GB\n",
      "Iter 5560: Train loss 0.614, Learning Rate 1.000e-04, It/sec 1.995, Tokens/sec 233.606, Trained Tokens 919752, Peak mem 19.940 GB\n",
      "Iter 5570: Train loss 0.703, Learning Rate 1.000e-04, It/sec 1.723, Tokens/sec 263.322, Trained Tokens 921280, Peak mem 19.940 GB\n",
      "Iter 5580: Train loss 0.615, Learning Rate 1.000e-04, It/sec 1.871, Tokens/sec 245.478, Trained Tokens 922592, Peak mem 19.940 GB\n",
      "Iter 5590: Train loss 0.675, Learning Rate 1.000e-04, It/sec 1.798, Tokens/sec 273.309, Trained Tokens 924112, Peak mem 19.940 GB\n",
      "Iter 5600: Val loss 2.473, Val took 4.649s\n",
      "Iter 5600: Train loss 0.815, Learning Rate 1.000e-04, It/sec 28.319, Tokens/sec 5355.125, Trained Tokens 926003, Peak mem 19.940 GB\n",
      "Iter 5600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005600_adapters.safetensors.\n",
      "Iter 5610: Train loss 0.679, Learning Rate 1.000e-04, It/sec 1.929, Tokens/sec 241.143, Trained Tokens 927253, Peak mem 19.940 GB\n",
      "Iter 5620: Train loss 0.869, Learning Rate 1.000e-04, It/sec 1.425, Tokens/sec 277.133, Trained Tokens 929198, Peak mem 19.940 GB\n",
      "Iter 5630: Train loss 0.664, Learning Rate 1.000e-04, It/sec 2.091, Tokens/sec 252.125, Trained Tokens 930404, Peak mem 19.940 GB\n",
      "Iter 5640: Train loss 0.721, Learning Rate 1.000e-04, It/sec 1.509, Tokens/sec 269.137, Trained Tokens 932187, Peak mem 19.940 GB\n",
      "Iter 5650: Val loss 2.446, Val took 3.525s\n",
      "Iter 5650: Train loss 0.893, Learning Rate 1.000e-04, It/sec 30.048, Tokens/sec 5907.418, Trained Tokens 934153, Peak mem 19.940 GB\n",
      "Iter 5660: Train loss 0.652, Learning Rate 1.000e-04, It/sec 2.054, Tokens/sec 254.339, Trained Tokens 935391, Peak mem 19.940 GB\n",
      "Iter 5670: Train loss 0.593, Learning Rate 1.000e-04, It/sec 1.977, Tokens/sec 249.860, Trained Tokens 936655, Peak mem 19.940 GB\n",
      "Iter 5680: Train loss 0.795, Learning Rate 1.000e-04, It/sec 1.071, Tokens/sec 271.402, Trained Tokens 939188, Peak mem 19.940 GB\n",
      "Iter 5690: Train loss 0.736, Learning Rate 1.000e-04, It/sec 1.928, Tokens/sec 262.271, Trained Tokens 940548, Peak mem 19.940 GB\n",
      "Iter 5700: Val loss 2.327, Val took 3.677s\n",
      "Iter 5700: Train loss 0.806, Learning Rate 1.000e-04, It/sec 29.067, Tokens/sec 4746.720, Trained Tokens 942181, Peak mem 19.940 GB\n",
      "Iter 5700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005700_adapters.safetensors.\n",
      "Iter 5710: Train loss 0.663, Learning Rate 1.000e-04, It/sec 1.722, Tokens/sec 246.937, Trained Tokens 943615, Peak mem 19.940 GB\n",
      "Iter 5720: Train loss 0.903, Learning Rate 1.000e-04, It/sec 1.045, Tokens/sec 267.500, Trained Tokens 946175, Peak mem 19.940 GB\n",
      "Iter 5730: Train loss 0.830, Learning Rate 1.000e-04, It/sec 1.336, Tokens/sec 245.842, Trained Tokens 948015, Peak mem 19.940 GB\n",
      "Iter 5740: Train loss 0.944, Learning Rate 1.000e-04, It/sec 0.981, Tokens/sec 274.217, Trained Tokens 950810, Peak mem 19.940 GB\n",
      "Iter 5750: Val loss 2.590, Val took 4.681s\n",
      "Iter 5750: Train loss 0.560, Learning Rate 1.000e-04, It/sec 27.144, Tokens/sec 2776.880, Trained Tokens 951833, Peak mem 19.940 GB\n",
      "Iter 5760: Train loss 0.555, Learning Rate 1.000e-04, It/sec 1.987, Tokens/sec 253.795, Trained Tokens 953110, Peak mem 19.940 GB\n",
      "Iter 5770: Train loss 0.650, Learning Rate 1.000e-04, It/sec 2.085, Tokens/sec 243.109, Trained Tokens 954276, Peak mem 19.940 GB\n",
      "Iter 5780: Train loss 0.671, Learning Rate 1.000e-04, It/sec 1.758, Tokens/sec 254.782, Trained Tokens 955725, Peak mem 19.940 GB\n",
      "Iter 5790: Train loss 0.745, Learning Rate 1.000e-04, It/sec 1.091, Tokens/sec 265.228, Trained Tokens 958155, Peak mem 19.940 GB\n",
      "Iter 5800: Val loss 2.313, Val took 3.532s\n",
      "Iter 5800: Train loss 0.573, Learning Rate 1.000e-04, It/sec 23.857, Tokens/sec 3232.627, Trained Tokens 959510, Peak mem 19.940 GB\n",
      "Iter 5800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005800_adapters.safetensors.\n",
      "Iter 5810: Train loss 0.673, Learning Rate 1.000e-04, It/sec 1.610, Tokens/sec 225.785, Trained Tokens 960912, Peak mem 19.940 GB\n",
      "Iter 5820: Train loss 0.637, Learning Rate 1.000e-04, It/sec 1.557, Tokens/sec 263.839, Trained Tokens 962607, Peak mem 19.940 GB\n",
      "Iter 5830: Train loss 0.726, Learning Rate 1.000e-04, It/sec 1.235, Tokens/sec 273.903, Trained Tokens 964825, Peak mem 19.940 GB\n",
      "Iter 5840: Train loss 0.555, Learning Rate 1.000e-04, It/sec 1.988, Tokens/sec 233.945, Trained Tokens 966002, Peak mem 19.940 GB\n",
      "Iter 5850: Val loss 2.411, Val took 3.360s\n",
      "Iter 5850: Train loss 0.622, Learning Rate 1.000e-04, It/sec 16.688, Tokens/sec 2923.704, Trained Tokens 967754, Peak mem 19.940 GB\n",
      "Iter 5860: Train loss 0.653, Learning Rate 1.000e-04, It/sec 1.423, Tokens/sec 245.650, Trained Tokens 969480, Peak mem 19.940 GB\n",
      "Iter 5870: Train loss 0.598, Learning Rate 1.000e-04, It/sec 1.983, Tokens/sec 246.852, Trained Tokens 970725, Peak mem 19.940 GB\n",
      "Iter 5880: Train loss 0.630, Learning Rate 1.000e-04, It/sec 1.966, Tokens/sec 230.214, Trained Tokens 971896, Peak mem 19.940 GB\n",
      "Iter 5890: Train loss 0.653, Learning Rate 1.000e-04, It/sec 1.494, Tokens/sec 264.511, Trained Tokens 973667, Peak mem 19.940 GB\n",
      "Iter 5900: Val loss 2.618, Val took 3.577s\n",
      "Iter 5900: Train loss 0.653, Learning Rate 1.000e-04, It/sec 22.043, Tokens/sec 3271.185, Trained Tokens 975151, Peak mem 19.940 GB\n",
      "Iter 5900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0005900_adapters.safetensors.\n",
      "Iter 5910: Train loss 0.564, Learning Rate 1.000e-04, It/sec 1.686, Tokens/sec 259.077, Trained Tokens 976688, Peak mem 19.940 GB\n",
      "Iter 5920: Train loss 0.730, Learning Rate 1.000e-04, It/sec 1.420, Tokens/sec 272.340, Trained Tokens 978606, Peak mem 19.940 GB\n",
      "Iter 5930: Train loss 0.627, Learning Rate 1.000e-04, It/sec 1.835, Tokens/sec 258.396, Trained Tokens 980014, Peak mem 19.940 GB\n",
      "Iter 5940: Train loss 0.617, Learning Rate 1.000e-04, It/sec 1.445, Tokens/sec 260.184, Trained Tokens 981814, Peak mem 19.940 GB\n",
      "Iter 5950: Val loss 2.488, Val took 3.216s\n",
      "Iter 5950: Train loss 0.825, Learning Rate 1.000e-04, It/sec 29.436, Tokens/sec 4880.542, Trained Tokens 983472, Peak mem 19.940 GB\n",
      "Iter 5960: Train loss 0.627, Learning Rate 1.000e-04, It/sec 1.510, Tokens/sec 212.155, Trained Tokens 984877, Peak mem 19.940 GB\n",
      "Iter 5970: Train loss 0.764, Learning Rate 1.000e-04, It/sec 1.497, Tokens/sec 231.957, Trained Tokens 986426, Peak mem 19.940 GB\n",
      "Iter 5980: Train loss 0.922, Learning Rate 1.000e-04, It/sec 0.959, Tokens/sec 252.575, Trained Tokens 989060, Peak mem 19.940 GB\n",
      "Iter 5990: Train loss 0.692, Learning Rate 1.000e-04, It/sec 1.438, Tokens/sec 207.844, Trained Tokens 990505, Peak mem 19.940 GB\n",
      "Iter 6000: Val loss 2.590, Val took 4.004s\n",
      "Iter 6000: Train loss 0.637, Learning Rate 1.000e-04, It/sec 14.450, Tokens/sec 2293.285, Trained Tokens 992092, Peak mem 19.940 GB\n",
      "Iter 6000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006000_adapters.safetensors.\n",
      "Iter 6010: Train loss 0.657, Learning Rate 1.000e-04, It/sec 1.481, Tokens/sec 212.315, Trained Tokens 993526, Peak mem 19.940 GB\n",
      "Iter 6020: Train loss 0.620, Learning Rate 1.000e-04, It/sec 1.315, Tokens/sec 193.909, Trained Tokens 995001, Peak mem 19.940 GB\n",
      "Iter 6030: Train loss 0.701, Learning Rate 1.000e-04, It/sec 1.191, Tokens/sec 237.413, Trained Tokens 996995, Peak mem 19.940 GB\n",
      "Iter 6040: Train loss 0.814, Learning Rate 1.000e-04, It/sec 1.096, Tokens/sec 244.700, Trained Tokens 999228, Peak mem 19.940 GB\n",
      "Iter 6050: Val loss 2.107, Val took 3.643s\n",
      "Iter 6050: Train loss 0.655, Learning Rate 1.000e-04, It/sec 7.515, Tokens/sec 1083.717, Trained Tokens 1000670, Peak mem 19.940 GB\n",
      "Iter 6060: Train loss 0.653, Learning Rate 1.000e-04, It/sec 1.561, Tokens/sec 208.447, Trained Tokens 1002005, Peak mem 19.940 GB\n",
      "Iter 6070: Train loss 0.696, Learning Rate 1.000e-04, It/sec 1.231, Tokens/sec 220.549, Trained Tokens 1003797, Peak mem 19.940 GB\n",
      "Iter 6080: Train loss 0.874, Learning Rate 1.000e-04, It/sec 0.899, Tokens/sec 220.935, Trained Tokens 1006255, Peak mem 19.940 GB\n",
      "Iter 6090: Train loss 0.659, Learning Rate 1.000e-04, It/sec 1.512, Tokens/sec 203.682, Trained Tokens 1007602, Peak mem 19.940 GB\n",
      "Iter 6100: Val loss 2.262, Val took 3.555s\n",
      "Iter 6100: Train loss 0.619, Learning Rate 1.000e-04, It/sec 29.798, Tokens/sec 4144.962, Trained Tokens 1008993, Peak mem 19.940 GB\n",
      "Iter 6100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006100_adapters.safetensors.\n",
      "Iter 6110: Train loss 0.905, Learning Rate 1.000e-04, It/sec 0.719, Tokens/sec 221.246, Trained Tokens 1012070, Peak mem 19.940 GB\n",
      "Iter 6120: Train loss 0.637, Learning Rate 1.000e-04, It/sec 1.325, Tokens/sec 200.267, Trained Tokens 1013582, Peak mem 19.940 GB\n",
      "Iter 6130: Train loss 0.711, Learning Rate 1.000e-04, It/sec 1.611, Tokens/sec 239.292, Trained Tokens 1015067, Peak mem 19.940 GB\n",
      "Iter 6140: Train loss 0.698, Learning Rate 1.000e-04, It/sec 1.122, Tokens/sec 216.292, Trained Tokens 1016995, Peak mem 19.940 GB\n",
      "Iter 6150: Val loss 2.184, Val took 3.975s\n",
      "Iter 6150: Train loss 0.734, Learning Rate 1.000e-04, It/sec 22.650, Tokens/sec 3918.530, Trained Tokens 1018725, Peak mem 19.940 GB\n",
      "Iter 6160: Train loss 0.527, Learning Rate 1.000e-04, It/sec 2.210, Tokens/sec 238.492, Trained Tokens 1019804, Peak mem 19.940 GB\n",
      "Iter 6170: Train loss 0.690, Learning Rate 1.000e-04, It/sec 1.165, Tokens/sec 238.299, Trained Tokens 1021850, Peak mem 19.940 GB\n",
      "Iter 6180: Train loss 0.610, Learning Rate 1.000e-04, It/sec 1.635, Tokens/sec 248.837, Trained Tokens 1023372, Peak mem 19.940 GB\n",
      "Iter 6190: Train loss 0.594, Learning Rate 1.000e-04, It/sec 1.453, Tokens/sec 230.349, Trained Tokens 1024957, Peak mem 19.940 GB\n",
      "Iter 6200: Val loss 2.183, Val took 3.188s\n",
      "Iter 6200: Train loss 0.604, Learning Rate 1.000e-04, It/sec 20.575, Tokens/sec 3506.044, Trained Tokens 1026661, Peak mem 19.940 GB\n",
      "Iter 6200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006200_adapters.safetensors.\n",
      "Iter 6210: Train loss 0.586, Learning Rate 1.000e-04, It/sec 1.646, Tokens/sec 232.581, Trained Tokens 1028074, Peak mem 19.940 GB\n",
      "Iter 6220: Train loss 0.637, Learning Rate 1.000e-04, It/sec 1.247, Tokens/sec 221.299, Trained Tokens 1029849, Peak mem 19.940 GB\n",
      "Iter 6230: Train loss 0.577, Learning Rate 1.000e-04, It/sec 1.991, Tokens/sec 215.828, Trained Tokens 1030933, Peak mem 19.940 GB\n",
      "Iter 6240: Train loss 0.856, Learning Rate 1.000e-04, It/sec 0.919, Tokens/sec 216.811, Trained Tokens 1033292, Peak mem 19.940 GB\n",
      "Iter 6250: Val loss 2.036, Val took 3.568s\n",
      "Iter 6250: Train loss 0.498, Learning Rate 1.000e-04, It/sec 16.093, Tokens/sec 2129.068, Trained Tokens 1034615, Peak mem 19.940 GB\n",
      "Iter 6260: Train loss 0.575, Learning Rate 1.000e-04, It/sec 1.033, Tokens/sec 201.685, Trained Tokens 1036567, Peak mem 19.940 GB\n",
      "Iter 6270: Train loss 0.598, Learning Rate 1.000e-04, It/sec 1.447, Tokens/sec 218.317, Trained Tokens 1038076, Peak mem 19.940 GB\n",
      "Iter 6280: Train loss 0.537, Learning Rate 1.000e-04, It/sec 1.820, Tokens/sec 234.412, Trained Tokens 1039364, Peak mem 19.940 GB\n",
      "Iter 6290: Train loss 0.685, Learning Rate 1.000e-04, It/sec 1.400, Tokens/sec 246.242, Trained Tokens 1041123, Peak mem 19.940 GB\n",
      "Iter 6300: Val loss 2.275, Val took 3.343s\n",
      "Iter 6300: Train loss 0.619, Learning Rate 1.000e-04, It/sec 27.315, Tokens/sec 4269.279, Trained Tokens 1042686, Peak mem 19.940 GB\n",
      "Iter 6300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006300_adapters.safetensors.\n",
      "Iter 6310: Train loss 0.885, Learning Rate 1.000e-04, It/sec 0.726, Tokens/sec 243.073, Trained Tokens 1046032, Peak mem 19.940 GB\n",
      "Iter 6320: Train loss 0.542, Learning Rate 1.000e-04, It/sec 1.528, Tokens/sec 215.692, Trained Tokens 1047444, Peak mem 19.940 GB\n",
      "Iter 6330: Train loss 0.712, Learning Rate 1.000e-04, It/sec 1.330, Tokens/sec 267.674, Trained Tokens 1049456, Peak mem 19.940 GB\n",
      "Iter 6340: Train loss 0.732, Learning Rate 1.000e-04, It/sec 1.340, Tokens/sec 278.330, Trained Tokens 1051533, Peak mem 19.940 GB\n",
      "Iter 6350: Val loss 2.556, Val took 3.449s\n",
      "Iter 6350: Train loss 0.609, Learning Rate 1.000e-04, It/sec 30.647, Tokens/sec 5059.891, Trained Tokens 1053184, Peak mem 19.940 GB\n",
      "Iter 6360: Train loss 0.608, Learning Rate 1.000e-04, It/sec 1.818, Tokens/sec 253.190, Trained Tokens 1054577, Peak mem 19.940 GB\n",
      "Iter 6370: Train loss 0.667, Learning Rate 1.000e-04, It/sec 1.563, Tokens/sec 275.554, Trained Tokens 1056340, Peak mem 19.940 GB\n",
      "Iter 6380: Train loss 0.641, Learning Rate 1.000e-04, It/sec 1.377, Tokens/sec 258.619, Trained Tokens 1058218, Peak mem 19.940 GB\n",
      "Iter 6390: Train loss 0.624, Learning Rate 1.000e-04, It/sec 1.748, Tokens/sec 251.543, Trained Tokens 1059657, Peak mem 19.940 GB\n",
      "Iter 6400: Val loss 2.181, Val took 2.908s\n",
      "Iter 6400: Train loss 0.643, Learning Rate 1.000e-04, It/sec 17.924, Tokens/sec 2050.557, Trained Tokens 1060801, Peak mem 19.940 GB\n",
      "Iter 6400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006400_adapters.safetensors.\n",
      "Iter 6410: Train loss 0.726, Learning Rate 1.000e-04, It/sec 1.505, Tokens/sec 243.361, Trained Tokens 1062418, Peak mem 19.940 GB\n",
      "Iter 6420: Train loss 0.712, Learning Rate 1.000e-04, It/sec 1.154, Tokens/sec 277.516, Trained Tokens 1064823, Peak mem 19.940 GB\n",
      "Iter 6430: Train loss 0.613, Learning Rate 1.000e-04, It/sec 1.454, Tokens/sec 268.316, Trained Tokens 1066669, Peak mem 19.940 GB\n",
      "Iter 6440: Train loss 0.667, Learning Rate 1.000e-04, It/sec 1.544, Tokens/sec 276.569, Trained Tokens 1068460, Peak mem 19.940 GB\n",
      "Iter 6450: Val loss 2.357, Val took 3.078s\n",
      "Iter 6450: Train loss 0.548, Learning Rate 1.000e-04, It/sec 30.563, Tokens/sec 3322.219, Trained Tokens 1069547, Peak mem 19.940 GB\n",
      "Iter 6460: Train loss 0.816, Learning Rate 1.000e-04, It/sec 1.111, Tokens/sec 261.880, Trained Tokens 1071904, Peak mem 19.940 GB\n",
      "Iter 6470: Train loss 0.742, Learning Rate 1.000e-04, It/sec 1.537, Tokens/sec 265.975, Trained Tokens 1073634, Peak mem 19.940 GB\n",
      "Iter 6480: Train loss 0.618, Learning Rate 1.000e-04, It/sec 1.935, Tokens/sec 241.516, Trained Tokens 1074882, Peak mem 19.940 GB\n",
      "Iter 6490: Train loss 0.595, Learning Rate 1.000e-04, It/sec 1.746, Tokens/sec 250.045, Trained Tokens 1076314, Peak mem 19.940 GB\n",
      "Iter 6500: Val loss 2.153, Val took 3.869s\n",
      "Iter 6500: Train loss 0.603, Learning Rate 1.000e-04, It/sec 6.287, Tokens/sec 1114.108, Trained Tokens 1078086, Peak mem 19.940 GB\n",
      "Iter 6500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006500_adapters.safetensors.\n",
      "Iter 6510: Train loss 0.716, Learning Rate 1.000e-04, It/sec 1.556, Tokens/sec 263.591, Trained Tokens 1079780, Peak mem 19.940 GB\n",
      "Iter 6520: Train loss 0.601, Learning Rate 1.000e-04, It/sec 2.031, Tokens/sec 253.259, Trained Tokens 1081027, Peak mem 19.940 GB\n",
      "Iter 6530: Train loss 0.641, Learning Rate 1.000e-04, It/sec 1.801, Tokens/sec 270.573, Trained Tokens 1082529, Peak mem 19.940 GB\n",
      "Iter 6540: Train loss 0.644, Learning Rate 1.000e-04, It/sec 1.489, Tokens/sec 243.339, Trained Tokens 1084163, Peak mem 19.940 GB\n",
      "Iter 6550: Val loss 2.409, Val took 3.171s\n",
      "Iter 6550: Train loss 0.601, Learning Rate 1.000e-04, It/sec 23.876, Tokens/sec 2824.530, Trained Tokens 1085346, Peak mem 19.940 GB\n",
      "Iter 6560: Train loss 0.694, Learning Rate 1.000e-04, It/sec 1.793, Tokens/sec 232.032, Trained Tokens 1086640, Peak mem 19.940 GB\n",
      "Iter 6570: Train loss 0.512, Learning Rate 1.000e-04, It/sec 1.989, Tokens/sec 246.827, Trained Tokens 1087881, Peak mem 19.940 GB\n",
      "Iter 6580: Train loss 0.780, Learning Rate 1.000e-04, It/sec 1.075, Tokens/sec 272.477, Trained Tokens 1090416, Peak mem 19.940 GB\n",
      "Iter 6590: Train loss 0.587, Learning Rate 1.000e-04, It/sec 1.710, Tokens/sec 253.240, Trained Tokens 1091897, Peak mem 19.940 GB\n",
      "Iter 6600: Val loss 2.354, Val took 3.899s\n",
      "Iter 6600: Train loss 0.636, Learning Rate 1.000e-04, It/sec 18.098, Tokens/sec 3802.335, Trained Tokens 1093998, Peak mem 19.940 GB\n",
      "Iter 6600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006600_adapters.safetensors.\n",
      "Iter 6610: Train loss 0.606, Learning Rate 1.000e-04, It/sec 1.284, Tokens/sec 260.393, Trained Tokens 1096026, Peak mem 19.940 GB\n",
      "Iter 6620: Train loss 0.576, Learning Rate 1.000e-04, It/sec 1.562, Tokens/sec 252.706, Trained Tokens 1097644, Peak mem 19.940 GB\n",
      "Iter 6630: Train loss 0.688, Learning Rate 1.000e-04, It/sec 1.100, Tokens/sec 262.773, Trained Tokens 1100033, Peak mem 19.940 GB\n",
      "Iter 6640: Train loss 0.570, Learning Rate 1.000e-04, It/sec 1.490, Tokens/sec 249.508, Trained Tokens 1101707, Peak mem 19.940 GB\n",
      "Iter 6650: Val loss 2.743, Val took 3.073s\n",
      "Iter 6650: Train loss 0.552, Learning Rate 1.000e-04, It/sec 16.429, Tokens/sec 2349.343, Trained Tokens 1103137, Peak mem 19.940 GB\n",
      "Iter 6660: Train loss 0.644, Learning Rate 1.000e-04, It/sec 1.736, Tokens/sec 248.939, Trained Tokens 1104571, Peak mem 19.940 GB\n",
      "Iter 6670: Train loss 0.549, Learning Rate 1.000e-04, It/sec 1.681, Tokens/sec 259.582, Trained Tokens 1106115, Peak mem 19.940 GB\n",
      "Iter 6680: Train loss 0.598, Learning Rate 1.000e-04, It/sec 1.388, Tokens/sec 271.466, Trained Tokens 1108071, Peak mem 19.940 GB\n",
      "Iter 6690: Train loss 0.580, Learning Rate 1.000e-04, It/sec 1.909, Tokens/sec 268.556, Trained Tokens 1109478, Peak mem 19.940 GB\n",
      "Iter 6700: Val loss 2.639, Val took 2.875s\n",
      "Iter 6700: Train loss 0.505, Learning Rate 1.000e-04, It/sec 18.618, Tokens/sec 2586.027, Trained Tokens 1110867, Peak mem 19.940 GB\n",
      "Iter 6700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006700_adapters.safetensors.\n",
      "Iter 6710: Train loss 0.708, Learning Rate 1.000e-04, It/sec 1.378, Tokens/sec 273.543, Trained Tokens 1112852, Peak mem 19.940 GB\n",
      "Iter 6720: Train loss 0.652, Learning Rate 1.000e-04, It/sec 1.488, Tokens/sec 260.988, Trained Tokens 1114606, Peak mem 19.940 GB\n",
      "Iter 6730: Train loss 0.617, Learning Rate 1.000e-04, It/sec 1.494, Tokens/sec 251.487, Trained Tokens 1116289, Peak mem 19.940 GB\n",
      "Iter 6740: Train loss 0.636, Learning Rate 1.000e-04, It/sec 1.711, Tokens/sec 256.614, Trained Tokens 1117789, Peak mem 19.940 GB\n",
      "Iter 6750: Val loss 2.519, Val took 4.403s\n",
      "Iter 6750: Train loss 0.779, Learning Rate 1.000e-04, It/sec 40.890, Tokens/sec 7421.583, Trained Tokens 1119604, Peak mem 19.940 GB\n",
      "Iter 6760: Train loss 0.635, Learning Rate 1.000e-04, It/sec 1.454, Tokens/sec 272.715, Trained Tokens 1121479, Peak mem 19.940 GB\n",
      "Iter 6770: Train loss 0.541, Learning Rate 1.000e-04, It/sec 1.972, Tokens/sec 268.521, Trained Tokens 1122841, Peak mem 19.940 GB\n",
      "Iter 6780: Train loss 0.606, Learning Rate 1.000e-04, It/sec 2.221, Tokens/sec 261.398, Trained Tokens 1124018, Peak mem 19.940 GB\n",
      "Iter 6790: Train loss 0.750, Learning Rate 1.000e-04, It/sec 1.672, Tokens/sec 269.139, Trained Tokens 1125628, Peak mem 19.940 GB\n",
      "Iter 6800: Val loss 2.346, Val took 3.427s\n",
      "Iter 6800: Train loss 0.611, Learning Rate 1.000e-04, It/sec 10.125, Tokens/sec 1405.344, Trained Tokens 1127016, Peak mem 19.940 GB\n",
      "Iter 6800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006800_adapters.safetensors.\n",
      "Iter 6810: Train loss 0.777, Learning Rate 1.000e-04, It/sec 1.038, Tokens/sec 274.469, Trained Tokens 1129660, Peak mem 19.940 GB\n",
      "Iter 6820: Train loss 0.734, Learning Rate 1.000e-04, It/sec 1.415, Tokens/sec 256.019, Trained Tokens 1131469, Peak mem 19.940 GB\n",
      "Iter 6830: Train loss 0.667, Learning Rate 1.000e-04, It/sec 1.618, Tokens/sec 254.219, Trained Tokens 1133040, Peak mem 19.940 GB\n",
      "Iter 6840: Train loss 0.658, Learning Rate 1.000e-04, It/sec 1.452, Tokens/sec 262.034, Trained Tokens 1134845, Peak mem 19.940 GB\n",
      "Iter 6850: Val loss 2.412, Val took 3.128s\n",
      "Iter 6850: Train loss 0.642, Learning Rate 1.000e-04, It/sec 16.477, Tokens/sec 2534.231, Trained Tokens 1136383, Peak mem 19.940 GB\n",
      "Iter 6860: Train loss 0.616, Learning Rate 1.000e-04, It/sec 1.707, Tokens/sec 258.548, Trained Tokens 1137898, Peak mem 19.940 GB\n",
      "Iter 6870: Train loss 0.579, Learning Rate 1.000e-04, It/sec 1.692, Tokens/sec 275.058, Trained Tokens 1139524, Peak mem 19.940 GB\n",
      "Iter 6880: Train loss 0.571, Learning Rate 1.000e-04, It/sec 2.264, Tokens/sec 267.126, Trained Tokens 1140704, Peak mem 19.940 GB\n",
      "Iter 6890: Train loss 0.553, Learning Rate 1.000e-04, It/sec 1.813, Tokens/sec 257.407, Trained Tokens 1142124, Peak mem 19.940 GB\n",
      "Iter 6900: Val loss 2.230, Val took 3.340s\n",
      "Iter 6900: Train loss 0.680, Learning Rate 1.000e-04, It/sec 11.166, Tokens/sec 1783.191, Trained Tokens 1143721, Peak mem 19.940 GB\n",
      "Iter 6900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0006900_adapters.safetensors.\n",
      "Iter 6910: Train loss 0.672, Learning Rate 1.000e-04, It/sec 1.296, Tokens/sec 268.992, Trained Tokens 1145797, Peak mem 19.940 GB\n",
      "Iter 6920: Train loss 0.635, Learning Rate 1.000e-04, It/sec 1.533, Tokens/sec 272.490, Trained Tokens 1147575, Peak mem 19.940 GB\n",
      "Iter 6930: Train loss 0.601, Learning Rate 1.000e-04, It/sec 1.792, Tokens/sec 257.273, Trained Tokens 1149011, Peak mem 19.940 GB\n",
      "Iter 6940: Train loss 0.649, Learning Rate 1.000e-04, It/sec 1.714, Tokens/sec 262.309, Trained Tokens 1150541, Peak mem 19.940 GB\n",
      "Iter 6950: Val loss 2.221, Val took 3.123s\n",
      "Iter 6950: Train loss 0.723, Learning Rate 1.000e-04, It/sec 42.189, Tokens/sec 6227.099, Trained Tokens 1152017, Peak mem 19.940 GB\n",
      "Iter 6960: Train loss 0.593, Learning Rate 1.000e-04, It/sec 1.733, Tokens/sec 265.015, Trained Tokens 1153546, Peak mem 19.940 GB\n",
      "Iter 6970: Train loss 0.606, Learning Rate 1.000e-04, It/sec 2.594, Tokens/sec 261.745, Trained Tokens 1154555, Peak mem 19.940 GB\n",
      "Iter 6980: Train loss 0.583, Learning Rate 1.000e-04, It/sec 1.565, Tokens/sec 264.268, Trained Tokens 1156244, Peak mem 19.940 GB\n",
      "Iter 6990: Train loss 0.557, Learning Rate 1.000e-04, It/sec 1.676, Tokens/sec 248.343, Trained Tokens 1157726, Peak mem 19.940 GB\n",
      "Iter 7000: Val loss 2.793, Val took 3.203s\n",
      "Iter 7000: Train loss 0.622, Learning Rate 1.000e-04, It/sec 29.627, Tokens/sec 4204.137, Trained Tokens 1159145, Peak mem 19.940 GB\n",
      "Iter 7000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007000_adapters.safetensors.\n",
      "Iter 7010: Train loss 0.543, Learning Rate 1.000e-04, It/sec 1.797, Tokens/sec 262.160, Trained Tokens 1160604, Peak mem 19.940 GB\n",
      "Iter 7020: Train loss 0.509, Learning Rate 1.000e-04, It/sec 1.308, Tokens/sec 249.585, Trained Tokens 1162512, Peak mem 19.940 GB\n",
      "Iter 7030: Train loss 0.530, Learning Rate 1.000e-04, It/sec 1.568, Tokens/sec 242.634, Trained Tokens 1164059, Peak mem 19.940 GB\n",
      "Iter 7040: Train loss 0.554, Learning Rate 1.000e-04, It/sec 1.871, Tokens/sec 250.522, Trained Tokens 1165398, Peak mem 19.940 GB\n",
      "Iter 7050: Val loss 2.638, Val took 3.153s\n",
      "Iter 7050: Train loss 0.470, Learning Rate 1.000e-04, It/sec 29.283, Tokens/sec 4371.902, Trained Tokens 1166891, Peak mem 19.940 GB\n",
      "Iter 7060: Train loss 0.791, Learning Rate 1.000e-04, It/sec 1.061, Tokens/sec 268.174, Trained Tokens 1169419, Peak mem 19.940 GB\n",
      "Iter 7070: Train loss 0.490, Learning Rate 1.000e-04, It/sec 1.720, Tokens/sec 274.626, Trained Tokens 1171016, Peak mem 19.940 GB\n",
      "Iter 7080: Train loss 0.558, Learning Rate 1.000e-04, It/sec 1.814, Tokens/sec 272.092, Trained Tokens 1172516, Peak mem 19.940 GB\n",
      "Iter 7090: Train loss 0.623, Learning Rate 1.000e-04, It/sec 1.962, Tokens/sec 251.907, Trained Tokens 1173800, Peak mem 19.940 GB\n",
      "Iter 7100: Val loss 2.490, Val took 3.622s\n",
      "Iter 7100: Train loss 0.742, Learning Rate 1.000e-04, It/sec 7.202, Tokens/sec 1830.653, Trained Tokens 1176342, Peak mem 19.940 GB\n",
      "Iter 7100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007100_adapters.safetensors.\n",
      "Iter 7110: Train loss 0.640, Learning Rate 1.000e-04, It/sec 1.544, Tokens/sec 251.948, Trained Tokens 1177974, Peak mem 19.940 GB\n",
      "Iter 7120: Train loss 0.640, Learning Rate 1.000e-04, It/sec 1.511, Tokens/sec 270.698, Trained Tokens 1179765, Peak mem 19.940 GB\n",
      "Iter 7130: Train loss 0.533, Learning Rate 1.000e-04, It/sec 2.057, Tokens/sec 256.134, Trained Tokens 1181010, Peak mem 19.940 GB\n",
      "Iter 7140: Train loss 0.543, Learning Rate 1.000e-04, It/sec 2.158, Tokens/sec 258.503, Trained Tokens 1182208, Peak mem 19.940 GB\n",
      "Iter 7150: Val loss 2.126, Val took 3.203s\n",
      "Iter 7150: Train loss 0.507, Learning Rate 1.000e-04, It/sec 18.190, Tokens/sec 2244.614, Trained Tokens 1183442, Peak mem 19.940 GB\n",
      "Iter 7160: Train loss 0.575, Learning Rate 1.000e-04, It/sec 1.625, Tokens/sec 267.076, Trained Tokens 1185086, Peak mem 19.940 GB\n",
      "Iter 7170: Train loss 0.612, Learning Rate 1.000e-04, It/sec 1.789, Tokens/sec 265.527, Trained Tokens 1186570, Peak mem 19.940 GB\n",
      "Iter 7180: Train loss 0.561, Learning Rate 1.000e-04, It/sec 1.866, Tokens/sec 265.328, Trained Tokens 1187992, Peak mem 19.940 GB\n",
      "Iter 7190: Train loss 0.609, Learning Rate 1.000e-04, It/sec 1.570, Tokens/sec 267.278, Trained Tokens 1189694, Peak mem 19.940 GB\n",
      "Iter 7200: Val loss 2.155, Val took 2.862s\n",
      "Iter 7200: Train loss 0.557, Learning Rate 1.000e-04, It/sec 16.999, Tokens/sec 2379.907, Trained Tokens 1191094, Peak mem 19.940 GB\n",
      "Iter 7200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007200_adapters.safetensors.\n",
      "Iter 7210: Train loss 0.618, Learning Rate 1.000e-04, It/sec 1.710, Tokens/sec 254.057, Trained Tokens 1192580, Peak mem 19.940 GB\n",
      "Iter 7220: Train loss 0.621, Learning Rate 1.000e-04, It/sec 1.432, Tokens/sec 263.289, Trained Tokens 1194418, Peak mem 19.940 GB\n",
      "Iter 7230: Train loss 0.616, Learning Rate 1.000e-04, It/sec 1.609, Tokens/sec 257.261, Trained Tokens 1196017, Peak mem 19.940 GB\n",
      "Iter 7240: Train loss 0.676, Learning Rate 1.000e-04, It/sec 1.651, Tokens/sec 267.076, Trained Tokens 1197635, Peak mem 19.940 GB\n",
      "Iter 7250: Val loss 2.402, Val took 3.285s\n",
      "Iter 7250: Train loss 0.509, Learning Rate 1.000e-04, It/sec 29.790, Tokens/sec 4495.299, Trained Tokens 1199144, Peak mem 19.940 GB\n",
      "Iter 7260: Train loss 0.484, Learning Rate 1.000e-04, It/sec 1.668, Tokens/sec 252.713, Trained Tokens 1200659, Peak mem 19.940 GB\n",
      "Iter 7270: Train loss 0.538, Learning Rate 1.000e-04, It/sec 1.821, Tokens/sec 269.298, Trained Tokens 1202138, Peak mem 19.940 GB\n",
      "Iter 7280: Train loss 0.636, Learning Rate 1.000e-04, It/sec 1.510, Tokens/sec 250.280, Trained Tokens 1203796, Peak mem 19.940 GB\n",
      "Iter 7290: Train loss 0.601, Learning Rate 1.000e-04, It/sec 2.011, Tokens/sec 246.143, Trained Tokens 1205020, Peak mem 19.940 GB\n",
      "Iter 7300: Val loss 2.384, Val took 3.404s\n",
      "Iter 7300: Train loss 0.679, Learning Rate 1.000e-04, It/sec 23.841, Tokens/sec 5099.615, Trained Tokens 1207159, Peak mem 19.940 GB\n",
      "Iter 7300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007300_adapters.safetensors.\n",
      "Iter 7310: Train loss 0.598, Learning Rate 1.000e-04, It/sec 1.568, Tokens/sec 255.821, Trained Tokens 1208790, Peak mem 19.940 GB\n",
      "Iter 7320: Train loss 0.591, Learning Rate 1.000e-04, It/sec 1.727, Tokens/sec 247.097, Trained Tokens 1210221, Peak mem 19.940 GB\n",
      "Iter 7330: Train loss 0.982, Learning Rate 1.000e-04, It/sec 0.587, Tokens/sec 274.009, Trained Tokens 1214887, Peak mem 19.940 GB\n",
      "Iter 7340: Train loss 0.584, Learning Rate 1.000e-04, It/sec 1.593, Tokens/sec 257.314, Trained Tokens 1216502, Peak mem 19.940 GB\n",
      "Iter 7350: Val loss 2.424, Val took 3.381s\n",
      "Iter 7350: Train loss 0.570, Learning Rate 1.000e-04, It/sec 22.288, Tokens/sec 2897.420, Trained Tokens 1217802, Peak mem 19.940 GB\n",
      "Iter 7360: Train loss 0.654, Learning Rate 1.000e-04, It/sec 2.057, Tokens/sec 251.923, Trained Tokens 1219027, Peak mem 19.940 GB\n",
      "Iter 7370: Train loss 0.571, Learning Rate 1.000e-04, It/sec 1.983, Tokens/sec 257.961, Trained Tokens 1220328, Peak mem 19.940 GB\n",
      "Iter 7380: Train loss 0.744, Learning Rate 1.000e-04, It/sec 1.266, Tokens/sec 271.197, Trained Tokens 1222470, Peak mem 19.940 GB\n",
      "Iter 7390: Train loss 0.600, Learning Rate 1.000e-04, It/sec 1.544, Tokens/sec 264.199, Trained Tokens 1224181, Peak mem 19.940 GB\n",
      "Iter 7400: Val loss 2.650, Val took 4.011s\n",
      "Iter 7400: Train loss 0.497, Learning Rate 1.000e-04, It/sec 28.085, Tokens/sec 3833.621, Trained Tokens 1225546, Peak mem 19.940 GB\n",
      "Iter 7400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007400_adapters.safetensors.\n",
      "Iter 7410: Train loss 0.485, Learning Rate 1.000e-04, It/sec 1.778, Tokens/sec 230.982, Trained Tokens 1226845, Peak mem 19.940 GB\n",
      "Iter 7420: Train loss 0.714, Learning Rate 1.000e-04, It/sec 1.008, Tokens/sec 272.173, Trained Tokens 1229546, Peak mem 19.940 GB\n",
      "Iter 7430: Train loss 0.567, Learning Rate 1.000e-04, It/sec 1.428, Tokens/sec 261.181, Trained Tokens 1231375, Peak mem 19.940 GB\n",
      "Iter 7440: Train loss 0.566, Learning Rate 1.000e-04, It/sec 1.293, Tokens/sec 268.150, Trained Tokens 1233449, Peak mem 19.940 GB\n",
      "Iter 7450: Val loss 2.479, Val took 4.334s\n",
      "Iter 7450: Train loss 0.493, Learning Rate 1.000e-04, It/sec 16.199, Tokens/sec 2102.566, Trained Tokens 1234747, Peak mem 19.940 GB\n",
      "Iter 7460: Train loss 0.526, Learning Rate 1.000e-04, It/sec 1.895, Tokens/sec 262.059, Trained Tokens 1236130, Peak mem 19.940 GB\n",
      "Iter 7470: Train loss 0.663, Learning Rate 1.000e-04, It/sec 1.334, Tokens/sec 271.112, Trained Tokens 1238162, Peak mem 19.940 GB\n",
      "Iter 7480: Train loss 0.479, Learning Rate 1.000e-04, It/sec 2.141, Tokens/sec 245.991, Trained Tokens 1239311, Peak mem 19.940 GB\n",
      "Iter 7490: Train loss 0.564, Learning Rate 1.000e-04, It/sec 1.942, Tokens/sec 251.426, Trained Tokens 1240606, Peak mem 19.940 GB\n",
      "Iter 7500: Val loss 2.369, Val took 3.018s\n",
      "Iter 7500: Train loss 0.544, Learning Rate 1.000e-04, It/sec 18.658, Tokens/sec 2839.724, Trained Tokens 1242128, Peak mem 19.940 GB\n",
      "Iter 7500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007500_adapters.safetensors.\n",
      "Iter 7510: Train loss 0.546, Learning Rate 1.000e-04, It/sec 1.640, Tokens/sec 257.223, Trained Tokens 1243696, Peak mem 19.940 GB\n",
      "Iter 7520: Train loss 0.582, Learning Rate 1.000e-04, It/sec 1.413, Tokens/sec 263.395, Trained Tokens 1245560, Peak mem 19.940 GB\n",
      "Iter 7530: Train loss 0.523, Learning Rate 1.000e-04, It/sec 1.536, Tokens/sec 267.218, Trained Tokens 1247300, Peak mem 19.940 GB\n",
      "Iter 7540: Train loss 0.569, Learning Rate 1.000e-04, It/sec 1.897, Tokens/sec 260.132, Trained Tokens 1248671, Peak mem 19.940 GB\n",
      "Iter 7550: Val loss 2.335, Val took 3.778s\n",
      "Iter 7550: Train loss 0.572, Learning Rate 1.000e-04, It/sec 17.172, Tokens/sec 3206.100, Trained Tokens 1250538, Peak mem 19.940 GB\n",
      "Iter 7560: Train loss 0.588, Learning Rate 1.000e-04, It/sec 1.340, Tokens/sec 270.776, Trained Tokens 1252558, Peak mem 19.940 GB\n",
      "Iter 7570: Train loss 0.570, Learning Rate 1.000e-04, It/sec 1.685, Tokens/sec 254.836, Trained Tokens 1254070, Peak mem 19.940 GB\n",
      "Iter 7580: Train loss 0.592, Learning Rate 1.000e-04, It/sec 2.366, Tokens/sec 247.977, Trained Tokens 1255118, Peak mem 19.940 GB\n",
      "Iter 7590: Train loss 0.673, Learning Rate 1.000e-04, It/sec 0.862, Tokens/sec 268.867, Trained Tokens 1258236, Peak mem 19.940 GB\n",
      "Iter 7600: Val loss 2.528, Val took 3.708s\n",
      "Iter 7600: Train loss 0.586, Learning Rate 1.000e-04, It/sec 28.445, Tokens/sec 4793.054, Trained Tokens 1259921, Peak mem 19.940 GB\n",
      "Iter 7600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007600_adapters.safetensors.\n",
      "Iter 7610: Train loss 0.721, Learning Rate 1.000e-04, It/sec 1.180, Tokens/sec 284.201, Trained Tokens 1262330, Peak mem 19.940 GB\n",
      "Iter 7620: Train loss 0.591, Learning Rate 1.000e-04, It/sec 2.121, Tokens/sec 264.685, Trained Tokens 1263578, Peak mem 19.940 GB\n",
      "Iter 7630: Train loss 0.671, Learning Rate 1.000e-04, It/sec 1.327, Tokens/sec 257.477, Trained Tokens 1265519, Peak mem 19.940 GB\n",
      "Iter 7640: Train loss 0.606, Learning Rate 1.000e-04, It/sec 2.313, Tokens/sec 247.996, Trained Tokens 1266591, Peak mem 19.940 GB\n",
      "Iter 7650: Val loss 2.714, Val took 3.565s\n",
      "Iter 7650: Train loss 0.509, Learning Rate 1.000e-04, It/sec 10.878, Tokens/sec 1772.020, Trained Tokens 1268220, Peak mem 19.940 GB\n",
      "Iter 7660: Train loss 0.569, Learning Rate 1.000e-04, It/sec 1.752, Tokens/sec 257.238, Trained Tokens 1269688, Peak mem 19.940 GB\n",
      "Iter 7670: Train loss 0.609, Learning Rate 1.000e-04, It/sec 1.455, Tokens/sec 260.770, Trained Tokens 1271480, Peak mem 19.940 GB\n",
      "Iter 7680: Train loss 0.549, Learning Rate 1.000e-04, It/sec 2.014, Tokens/sec 251.161, Trained Tokens 1272727, Peak mem 19.940 GB\n",
      "Iter 7690: Train loss 0.593, Learning Rate 1.000e-04, It/sec 1.919, Tokens/sec 255.196, Trained Tokens 1274057, Peak mem 19.940 GB\n",
      "Iter 7700: Val loss 2.171, Val took 2.736s\n",
      "Iter 7700: Train loss 0.568, Learning Rate 1.000e-04, It/sec 23.625, Tokens/sec 2648.411, Trained Tokens 1275178, Peak mem 19.940 GB\n",
      "Iter 7700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007700_adapters.safetensors.\n",
      "Iter 7710: Train loss 0.637, Learning Rate 1.000e-04, It/sec 1.193, Tokens/sec 232.604, Trained Tokens 1277127, Peak mem 19.940 GB\n",
      "Iter 7720: Train loss 0.531, Learning Rate 1.000e-04, It/sec 1.894, Tokens/sec 251.144, Trained Tokens 1278453, Peak mem 19.940 GB\n",
      "Iter 7730: Train loss 0.550, Learning Rate 1.000e-04, It/sec 1.665, Tokens/sec 259.198, Trained Tokens 1280010, Peak mem 19.940 GB\n",
      "Iter 7740: Train loss 0.762, Learning Rate 1.000e-04, It/sec 1.156, Tokens/sec 258.456, Trained Tokens 1282245, Peak mem 19.940 GB\n",
      "Iter 7750: Val loss 2.362, Val took 3.573s\n",
      "Iter 7750: Train loss 0.633, Learning Rate 1.000e-04, It/sec 8.249, Tokens/sec 1302.461, Trained Tokens 1283824, Peak mem 19.940 GB\n",
      "Iter 7760: Train loss 0.617, Learning Rate 1.000e-04, It/sec 1.800, Tokens/sec 260.832, Trained Tokens 1285273, Peak mem 19.940 GB\n",
      "Iter 7770: Train loss 0.539, Learning Rate 1.000e-04, It/sec 1.662, Tokens/sec 254.152, Trained Tokens 1286802, Peak mem 19.940 GB\n",
      "Iter 7780: Train loss 0.606, Learning Rate 1.000e-04, It/sec 1.243, Tokens/sec 269.091, Trained Tokens 1288966, Peak mem 19.940 GB\n",
      "Iter 7790: Train loss 0.551, Learning Rate 1.000e-04, It/sec 1.883, Tokens/sec 267.220, Trained Tokens 1290385, Peak mem 19.940 GB\n",
      "Iter 7800: Val loss 2.290, Val took 3.074s\n",
      "Iter 7800: Train loss 0.485, Learning Rate 1.000e-04, It/sec 42.434, Tokens/sec 5456.999, Trained Tokens 1291671, Peak mem 19.940 GB\n",
      "Iter 7800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007800_adapters.safetensors.\n",
      "Iter 7810: Train loss 0.502, Learning Rate 1.000e-04, It/sec 2.119, Tokens/sec 246.825, Trained Tokens 1292836, Peak mem 19.940 GB\n",
      "Iter 7820: Train loss 0.531, Learning Rate 1.000e-04, It/sec 1.546, Tokens/sec 261.337, Trained Tokens 1294526, Peak mem 19.940 GB\n",
      "Iter 7830: Train loss 0.491, Learning Rate 1.000e-04, It/sec 1.744, Tokens/sec 259.232, Trained Tokens 1296012, Peak mem 19.940 GB\n",
      "Iter 7840: Train loss 0.500, Learning Rate 1.000e-04, It/sec 1.424, Tokens/sec 267.487, Trained Tokens 1297890, Peak mem 19.940 GB\n",
      "Iter 7850: Val loss 2.904, Val took 3.710s\n",
      "Iter 7850: Train loss 0.559, Learning Rate 1.000e-04, It/sec 30.364, Tokens/sec 3971.640, Trained Tokens 1299198, Peak mem 19.940 GB\n",
      "Iter 7860: Train loss 0.498, Learning Rate 1.000e-04, It/sec 1.890, Tokens/sec 254.201, Trained Tokens 1300543, Peak mem 19.940 GB\n",
      "Iter 7870: Train loss 0.616, Learning Rate 1.000e-04, It/sec 1.314, Tokens/sec 252.208, Trained Tokens 1302463, Peak mem 19.940 GB\n",
      "Iter 7880: Train loss 0.561, Learning Rate 1.000e-04, It/sec 1.580, Tokens/sec 225.753, Trained Tokens 1303892, Peak mem 19.940 GB\n",
      "Iter 7890: Train loss 0.466, Learning Rate 1.000e-04, It/sec 1.835, Tokens/sec 250.413, Trained Tokens 1305257, Peak mem 19.940 GB\n",
      "Iter 7900: Val loss 2.541, Val took 3.668s\n",
      "Iter 7900: Train loss 0.465, Learning Rate 1.000e-04, It/sec 5.605, Tokens/sec 1100.869, Trained Tokens 1307221, Peak mem 19.940 GB\n",
      "Iter 7900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0007900_adapters.safetensors.\n",
      "Iter 7910: Train loss 0.499, Learning Rate 1.000e-04, It/sec 1.967, Tokens/sec 248.807, Trained Tokens 1308486, Peak mem 19.940 GB\n",
      "Iter 7920: Train loss 0.500, Learning Rate 1.000e-04, It/sec 1.957, Tokens/sec 252.800, Trained Tokens 1309778, Peak mem 19.940 GB\n",
      "Iter 7930: Train loss 0.525, Learning Rate 1.000e-04, It/sec 1.870, Tokens/sec 251.466, Trained Tokens 1311123, Peak mem 19.940 GB\n",
      "Iter 7940: Train loss 0.646, Learning Rate 1.000e-04, It/sec 1.345, Tokens/sec 273.899, Trained Tokens 1313159, Peak mem 19.940 GB\n",
      "Iter 7950: Val loss 2.357, Val took 3.688s\n",
      "Iter 7950: Train loss 0.493, Learning Rate 1.000e-04, It/sec 29.460, Tokens/sec 4142.127, Trained Tokens 1314565, Peak mem 19.940 GB\n",
      "Iter 7960: Train loss 0.527, Learning Rate 1.000e-04, It/sec 1.419, Tokens/sec 247.521, Trained Tokens 1316309, Peak mem 19.940 GB\n",
      "Iter 7970: Train loss 0.536, Learning Rate 1.000e-04, It/sec 1.551, Tokens/sec 265.327, Trained Tokens 1318020, Peak mem 19.940 GB\n",
      "Iter 7980: Train loss 0.539, Learning Rate 1.000e-04, It/sec 1.783, Tokens/sec 247.858, Trained Tokens 1319410, Peak mem 19.940 GB\n",
      "Iter 7990: Train loss 0.593, Learning Rate 1.000e-04, It/sec 1.575, Tokens/sec 263.208, Trained Tokens 1321081, Peak mem 19.940 GB\n",
      "Iter 8000: Val loss 2.050, Val took 3.977s\n",
      "Iter 8000: Train loss 0.482, Learning Rate 1.000e-04, It/sec 12.777, Tokens/sec 2091.632, Trained Tokens 1322718, Peak mem 19.940 GB\n",
      "Iter 8000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008000_adapters.safetensors.\n",
      "Iter 8010: Train loss 0.557, Learning Rate 1.000e-04, It/sec 1.779, Tokens/sec 258.901, Trained Tokens 1324173, Peak mem 19.940 GB\n",
      "Iter 8020: Train loss 0.740, Learning Rate 1.000e-04, It/sec 1.374, Tokens/sec 265.370, Trained Tokens 1326105, Peak mem 19.940 GB\n",
      "Iter 8030: Train loss 0.531, Learning Rate 1.000e-04, It/sec 1.562, Tokens/sec 249.507, Trained Tokens 1327702, Peak mem 19.940 GB\n",
      "Iter 8040: Train loss 0.529, Learning Rate 1.000e-04, It/sec 1.703, Tokens/sec 257.155, Trained Tokens 1329212, Peak mem 19.940 GB\n",
      "Iter 8050: Val loss 2.687, Val took 3.153s\n",
      "Iter 8050: Train loss 0.476, Learning Rate 1.000e-04, It/sec 13.019, Tokens/sec 1933.265, Trained Tokens 1330697, Peak mem 19.940 GB\n",
      "Iter 8060: Train loss 0.586, Learning Rate 1.000e-04, It/sec 2.401, Tokens/sec 233.149, Trained Tokens 1331668, Peak mem 19.940 GB\n",
      "Iter 8070: Train loss 0.563, Learning Rate 1.000e-04, It/sec 1.470, Tokens/sec 261.827, Trained Tokens 1333449, Peak mem 19.940 GB\n",
      "Iter 8080: Train loss 0.629, Learning Rate 1.000e-04, It/sec 1.230, Tokens/sec 259.734, Trained Tokens 1335561, Peak mem 19.940 GB\n",
      "Iter 8090: Train loss 0.518, Learning Rate 1.000e-04, It/sec 1.788, Tokens/sec 242.096, Trained Tokens 1336915, Peak mem 19.940 GB\n",
      "Iter 8100: Val loss 2.854, Val took 3.734s\n",
      "Iter 8100: Train loss 0.553, Learning Rate 1.000e-04, It/sec 15.995, Tokens/sec 3144.608, Trained Tokens 1338881, Peak mem 19.940 GB\n",
      "Iter 8100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008100_adapters.safetensors.\n",
      "Iter 8110: Train loss 0.579, Learning Rate 1.000e-04, It/sec 1.857, Tokens/sec 245.859, Trained Tokens 1340205, Peak mem 19.940 GB\n",
      "Iter 8120: Train loss 0.514, Learning Rate 1.000e-04, It/sec 2.011, Tokens/sec 251.922, Trained Tokens 1341458, Peak mem 19.940 GB\n",
      "Iter 8130: Train loss 0.638, Learning Rate 1.000e-04, It/sec 1.491, Tokens/sec 257.273, Trained Tokens 1343184, Peak mem 19.940 GB\n",
      "Iter 8140: Train loss 0.717, Learning Rate 1.000e-04, It/sec 1.227, Tokens/sec 272.232, Trained Tokens 1345402, Peak mem 19.940 GB\n",
      "Iter 8150: Val loss 2.487, Val took 3.325s\n",
      "Iter 8150: Train loss 0.754, Learning Rate 1.000e-04, It/sec 41.805, Tokens/sec 10660.199, Trained Tokens 1347952, Peak mem 19.940 GB\n",
      "Iter 8160: Train loss 0.681, Learning Rate 1.000e-04, It/sec 1.227, Tokens/sec 216.447, Trained Tokens 1349716, Peak mem 19.940 GB\n",
      "Iter 8170: Train loss 0.605, Learning Rate 1.000e-04, It/sec 0.824, Tokens/sec 268.616, Trained Tokens 1352975, Peak mem 19.940 GB\n",
      "Iter 8180: Train loss 0.496, Learning Rate 1.000e-04, It/sec 1.743, Tokens/sec 252.228, Trained Tokens 1354422, Peak mem 19.940 GB\n",
      "Iter 8190: Train loss 0.617, Learning Rate 1.000e-04, It/sec 1.331, Tokens/sec 271.853, Trained Tokens 1356464, Peak mem 19.940 GB\n",
      "Iter 8200: Val loss 2.693, Val took 3.423s\n",
      "Iter 8200: Train loss 0.641, Learning Rate 1.000e-04, It/sec 16.887, Tokens/sec 3100.527, Trained Tokens 1358300, Peak mem 19.940 GB\n",
      "Iter 8200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008200_adapters.safetensors.\n",
      "Iter 8210: Train loss 0.530, Learning Rate 1.000e-04, It/sec 1.290, Tokens/sec 215.184, Trained Tokens 1359968, Peak mem 19.940 GB\n",
      "Iter 8220: Train loss 0.505, Learning Rate 1.000e-04, It/sec 1.299, Tokens/sec 262.587, Trained Tokens 1361989, Peak mem 19.940 GB\n",
      "Iter 8230: Train loss 0.468, Learning Rate 1.000e-04, It/sec 1.790, Tokens/sec 267.599, Trained Tokens 1363484, Peak mem 19.940 GB\n",
      "Iter 8240: Train loss 0.511, Learning Rate 1.000e-04, It/sec 1.741, Tokens/sec 253.657, Trained Tokens 1364941, Peak mem 19.940 GB\n",
      "Iter 8250: Val loss 2.669, Val took 4.572s\n",
      "Iter 8250: Train loss 0.644, Learning Rate 1.000e-04, It/sec 1.946, Tokens/sec 486.284, Trained Tokens 1367440, Peak mem 19.940 GB\n",
      "Iter 8260: Train loss 0.549, Learning Rate 1.000e-04, It/sec 1.827, Tokens/sec 254.432, Trained Tokens 1368833, Peak mem 19.940 GB\n",
      "Iter 8270: Train loss 0.452, Learning Rate 1.000e-04, It/sec 1.814, Tokens/sec 254.451, Trained Tokens 1370236, Peak mem 19.940 GB\n",
      "Iter 8280: Train loss 0.499, Learning Rate 1.000e-04, It/sec 1.966, Tokens/sec 267.028, Trained Tokens 1371594, Peak mem 19.940 GB\n",
      "Iter 8290: Train loss 0.498, Learning Rate 1.000e-04, It/sec 1.970, Tokens/sec 248.409, Trained Tokens 1372855, Peak mem 19.940 GB\n",
      "Iter 8300: Val loss 2.448, Val took 2.718s\n",
      "Iter 8300: Train loss 0.675, Learning Rate 1.000e-04, It/sec 28.467, Tokens/sec 7689.068, Trained Tokens 1375556, Peak mem 19.940 GB\n",
      "Iter 8300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008300_adapters.safetensors.\n",
      "Iter 8310: Train loss 0.473, Learning Rate 1.000e-04, It/sec 1.827, Tokens/sec 252.730, Trained Tokens 1376939, Peak mem 19.940 GB\n",
      "Iter 8320: Train loss 0.479, Learning Rate 1.000e-04, It/sec 1.544, Tokens/sec 238.473, Trained Tokens 1378484, Peak mem 19.940 GB\n",
      "Iter 8330: Train loss 0.461, Learning Rate 1.000e-04, It/sec 1.719, Tokens/sec 256.611, Trained Tokens 1379977, Peak mem 19.940 GB\n",
      "Iter 8340: Train loss 0.622, Learning Rate 1.000e-04, It/sec 1.274, Tokens/sec 278.272, Trained Tokens 1382162, Peak mem 19.940 GB\n",
      "Iter 8350: Val loss 2.444, Val took 3.303s\n",
      "Iter 8350: Train loss 0.584, Learning Rate 1.000e-04, It/sec 4.946, Tokens/sec 1100.027, Trained Tokens 1384386, Peak mem 19.940 GB\n",
      "Iter 8360: Train loss 0.512, Learning Rate 1.000e-04, It/sec 2.116, Tokens/sec 245.188, Trained Tokens 1385545, Peak mem 19.940 GB\n",
      "Iter 8370: Train loss 0.460, Learning Rate 1.000e-04, It/sec 1.682, Tokens/sec 247.747, Trained Tokens 1387018, Peak mem 19.940 GB\n",
      "Iter 8380: Train loss 0.486, Learning Rate 1.000e-04, It/sec 1.881, Tokens/sec 261.135, Trained Tokens 1388406, Peak mem 19.940 GB\n",
      "Iter 8390: Train loss 0.518, Learning Rate 1.000e-04, It/sec 1.835, Tokens/sec 252.614, Trained Tokens 1389783, Peak mem 19.940 GB\n",
      "Iter 8400: Val loss 2.571, Val took 3.242s\n",
      "Iter 8400: Train loss 0.576, Learning Rate 1.000e-04, It/sec 31.742, Tokens/sec 5262.893, Trained Tokens 1391441, Peak mem 19.940 GB\n",
      "Iter 8400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008400_adapters.safetensors.\n",
      "Iter 8410: Train loss 0.514, Learning Rate 1.000e-04, It/sec 1.351, Tokens/sec 258.633, Trained Tokens 1393356, Peak mem 19.940 GB\n",
      "Iter 8420: Train loss 0.475, Learning Rate 1.000e-04, It/sec 1.395, Tokens/sec 253.993, Trained Tokens 1395177, Peak mem 19.940 GB\n",
      "Iter 8430: Train loss 0.513, Learning Rate 1.000e-04, It/sec 1.785, Tokens/sec 246.027, Trained Tokens 1396555, Peak mem 19.940 GB\n",
      "Iter 8440: Train loss 0.506, Learning Rate 1.000e-04, It/sec 1.580, Tokens/sec 244.698, Trained Tokens 1398104, Peak mem 19.940 GB\n",
      "Iter 8450: Val loss 2.766, Val took 2.832s\n",
      "Iter 8450: Train loss 0.498, Learning Rate 1.000e-04, It/sec 15.384, Tokens/sec 2043.007, Trained Tokens 1399432, Peak mem 19.940 GB\n",
      "Iter 8460: Train loss 0.467, Learning Rate 1.000e-04, It/sec 1.657, Tokens/sec 228.677, Trained Tokens 1400812, Peak mem 19.940 GB\n",
      "Iter 8470: Train loss 0.541, Learning Rate 1.000e-04, It/sec 1.642, Tokens/sec 248.222, Trained Tokens 1402324, Peak mem 19.940 GB\n",
      "Iter 8480: Train loss 0.593, Learning Rate 1.000e-04, It/sec 1.709, Tokens/sec 249.287, Trained Tokens 1403783, Peak mem 19.940 GB\n",
      "Iter 8490: Train loss 0.755, Learning Rate 1.000e-04, It/sec 1.019, Tokens/sec 266.319, Trained Tokens 1406396, Peak mem 19.940 GB\n",
      "Iter 8500: Val loss 2.351, Val took 3.059s\n",
      "Iter 8500: Train loss 0.584, Learning Rate 1.000e-04, It/sec 23.172, Tokens/sec 4242.778, Trained Tokens 1408227, Peak mem 19.940 GB\n",
      "Iter 8500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008500_adapters.safetensors.\n",
      "Iter 8510: Train loss 0.542, Learning Rate 1.000e-04, It/sec 2.122, Tokens/sec 234.705, Trained Tokens 1409333, Peak mem 19.940 GB\n",
      "Iter 8520: Train loss 0.520, Learning Rate 1.000e-04, It/sec 1.524, Tokens/sec 261.729, Trained Tokens 1411050, Peak mem 19.940 GB\n",
      "Iter 8530: Train loss 0.528, Learning Rate 1.000e-04, It/sec 1.993, Tokens/sec 265.466, Trained Tokens 1412382, Peak mem 19.940 GB\n",
      "Iter 8540: Train loss 0.641, Learning Rate 1.000e-04, It/sec 1.096, Tokens/sec 275.793, Trained Tokens 1414899, Peak mem 19.940 GB\n",
      "Iter 8550: Val loss 2.003, Val took 3.171s\n",
      "Iter 8550: Train loss 0.567, Learning Rate 1.000e-04, It/sec 14.972, Tokens/sec 2344.689, Trained Tokens 1416465, Peak mem 19.940 GB\n",
      "Iter 8560: Train loss 0.669, Learning Rate 1.000e-04, It/sec 1.001, Tokens/sec 271.501, Trained Tokens 1419178, Peak mem 19.940 GB\n",
      "Iter 8570: Train loss 0.502, Learning Rate 1.000e-04, It/sec 1.472, Tokens/sec 271.216, Trained Tokens 1421020, Peak mem 19.940 GB\n",
      "Iter 8580: Train loss 0.581, Learning Rate 1.000e-04, It/sec 1.829, Tokens/sec 252.783, Trained Tokens 1422402, Peak mem 19.940 GB\n",
      "Iter 8590: Train loss 0.576, Learning Rate 1.000e-04, It/sec 2.429, Tokens/sec 243.624, Trained Tokens 1423405, Peak mem 19.940 GB\n",
      "Iter 8600: Val loss 2.412, Val took 3.911s\n",
      "Iter 8600: Train loss 0.571, Learning Rate 1.000e-04, It/sec 18.459, Tokens/sec 1993.601, Trained Tokens 1424485, Peak mem 19.940 GB\n",
      "Iter 8600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008600_adapters.safetensors.\n",
      "Iter 8610: Train loss 0.616, Learning Rate 1.000e-04, It/sec 1.386, Tokens/sec 239.757, Trained Tokens 1426215, Peak mem 19.940 GB\n",
      "Iter 8620: Train loss 0.515, Learning Rate 1.000e-04, It/sec 1.576, Tokens/sec 239.878, Trained Tokens 1427737, Peak mem 19.940 GB\n",
      "Iter 8630: Train loss 0.450, Learning Rate 1.000e-04, It/sec 1.602, Tokens/sec 256.983, Trained Tokens 1429341, Peak mem 19.940 GB\n",
      "Iter 8640: Train loss 0.519, Learning Rate 1.000e-04, It/sec 1.638, Tokens/sec 259.250, Trained Tokens 1430924, Peak mem 19.940 GB\n",
      "Iter 8650: Val loss 2.127, Val took 3.180s\n",
      "Iter 8650: Train loss 0.506, Learning Rate 1.000e-04, It/sec 15.609, Tokens/sec 2331.982, Trained Tokens 1432418, Peak mem 19.940 GB\n",
      "Iter 8660: Train loss 0.474, Learning Rate 1.000e-04, It/sec 1.994, Tokens/sec 244.466, Trained Tokens 1433644, Peak mem 19.940 GB\n",
      "Iter 8670: Train loss 0.467, Learning Rate 1.000e-04, It/sec 1.674, Tokens/sec 243.287, Trained Tokens 1435097, Peak mem 19.940 GB\n",
      "Iter 8680: Train loss 0.477, Learning Rate 1.000e-04, It/sec 1.624, Tokens/sec 227.982, Trained Tokens 1436501, Peak mem 19.940 GB\n",
      "Iter 8690: Train loss 0.639, Learning Rate 1.000e-04, It/sec 1.366, Tokens/sec 258.922, Trained Tokens 1438396, Peak mem 19.940 GB\n",
      "Iter 8700: Val loss 2.604, Val took 3.073s\n",
      "Iter 8700: Train loss 0.618, Learning Rate 1.000e-04, It/sec 15.978, Tokens/sec 2820.167, Trained Tokens 1440161, Peak mem 19.940 GB\n",
      "Iter 8700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008700_adapters.safetensors.\n",
      "Iter 8710: Train loss 0.455, Learning Rate 1.000e-04, It/sec 1.552, Tokens/sec 252.085, Trained Tokens 1441785, Peak mem 19.940 GB\n",
      "Iter 8720: Train loss 0.550, Learning Rate 1.000e-04, It/sec 1.243, Tokens/sec 273.813, Trained Tokens 1443988, Peak mem 19.940 GB\n",
      "Iter 8730: Train loss 0.545, Learning Rate 1.000e-04, It/sec 1.398, Tokens/sec 251.967, Trained Tokens 1445790, Peak mem 19.940 GB\n",
      "Iter 8740: Train loss 0.447, Learning Rate 1.000e-04, It/sec 1.435, Tokens/sec 246.584, Trained Tokens 1447508, Peak mem 19.940 GB\n",
      "Iter 8750: Val loss 2.413, Val took 3.242s\n",
      "Iter 8750: Train loss 0.514, Learning Rate 1.000e-04, It/sec 17.035, Tokens/sec 2769.840, Trained Tokens 1449134, Peak mem 19.940 GB\n",
      "Iter 8760: Train loss 0.510, Learning Rate 1.000e-04, It/sec 1.378, Tokens/sec 259.964, Trained Tokens 1451021, Peak mem 19.940 GB\n",
      "Iter 8770: Train loss 0.486, Learning Rate 1.000e-04, It/sec 1.874, Tokens/sec 253.128, Trained Tokens 1452372, Peak mem 19.940 GB\n",
      "Iter 8780: Train loss 0.453, Learning Rate 1.000e-04, It/sec 1.742, Tokens/sec 267.411, Trained Tokens 1453907, Peak mem 19.940 GB\n",
      "Iter 8790: Train loss 0.494, Learning Rate 1.000e-04, It/sec 2.091, Tokens/sec 241.666, Trained Tokens 1455063, Peak mem 19.940 GB\n",
      "Iter 8800: Val loss 2.734, Val took 3.952s\n",
      "Iter 8800: Train loss 0.538, Learning Rate 1.000e-04, It/sec 29.793, Tokens/sec 4945.648, Trained Tokens 1456723, Peak mem 19.940 GB\n",
      "Iter 8800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008800_adapters.safetensors.\n",
      "Iter 8810: Train loss 0.608, Learning Rate 1.000e-04, It/sec 1.354, Tokens/sec 249.480, Trained Tokens 1458565, Peak mem 19.940 GB\n",
      "Iter 8820: Train loss 0.468, Learning Rate 1.000e-04, It/sec 2.050, Tokens/sec 249.282, Trained Tokens 1459781, Peak mem 19.940 GB\n",
      "Iter 8830: Train loss 0.495, Learning Rate 1.000e-04, It/sec 1.613, Tokens/sec 264.970, Trained Tokens 1461424, Peak mem 19.940 GB\n",
      "Iter 8840: Train loss 0.579, Learning Rate 1.000e-04, It/sec 1.710, Tokens/sec 252.521, Trained Tokens 1462901, Peak mem 19.940 GB\n",
      "Iter 8850: Val loss 2.620, Val took 3.750s\n",
      "Iter 8850: Train loss 0.563, Learning Rate 1.000e-04, It/sec 4.124, Tokens/sec 899.785, Trained Tokens 1465083, Peak mem 19.940 GB\n",
      "Iter 8860: Train loss 0.556, Learning Rate 1.000e-04, It/sec 1.260, Tokens/sec 254.333, Trained Tokens 1467102, Peak mem 19.940 GB\n",
      "Iter 8870: Train loss 0.492, Learning Rate 1.000e-04, It/sec 1.659, Tokens/sec 267.793, Trained Tokens 1468716, Peak mem 19.940 GB\n",
      "Iter 8880: Train loss 0.462, Learning Rate 1.000e-04, It/sec 1.792, Tokens/sec 257.814, Trained Tokens 1470155, Peak mem 19.940 GB\n",
      "Iter 8890: Train loss 0.511, Learning Rate 1.000e-04, It/sec 1.439, Tokens/sec 259.682, Trained Tokens 1471959, Peak mem 19.940 GB\n",
      "Iter 8900: Val loss 2.517, Val took 3.394s\n",
      "Iter 8900: Train loss 0.659, Learning Rate 1.000e-04, It/sec 3.249, Tokens/sec 620.604, Trained Tokens 1473869, Peak mem 19.940 GB\n",
      "Iter 8900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0008900_adapters.safetensors.\n",
      "Iter 8910: Train loss 0.507, Learning Rate 1.000e-04, It/sec 1.640, Tokens/sec 201.246, Trained Tokens 1475096, Peak mem 19.940 GB\n",
      "Iter 8920: Train loss 0.542, Learning Rate 1.000e-04, It/sec 1.757, Tokens/sec 256.643, Trained Tokens 1476557, Peak mem 19.940 GB\n",
      "Iter 8930: Train loss 0.715, Learning Rate 1.000e-04, It/sec 1.025, Tokens/sec 280.925, Trained Tokens 1479297, Peak mem 19.940 GB\n",
      "Iter 8940: Train loss 0.535, Learning Rate 1.000e-04, It/sec 2.006, Tokens/sec 235.652, Trained Tokens 1480472, Peak mem 19.940 GB\n",
      "Iter 8950: Val loss 1.968, Val took 3.270s\n",
      "Iter 8950: Train loss 0.555, Learning Rate 1.000e-04, It/sec 23.362, Tokens/sec 2541.777, Trained Tokens 1481560, Peak mem 19.940 GB\n",
      "Iter 8960: Train loss 0.518, Learning Rate 1.000e-04, It/sec 1.483, Tokens/sec 253.095, Trained Tokens 1483267, Peak mem 19.940 GB\n",
      "Iter 8970: Train loss 0.525, Learning Rate 1.000e-04, It/sec 1.794, Tokens/sec 266.161, Trained Tokens 1484751, Peak mem 19.940 GB\n",
      "Iter 8980: Train loss 0.564, Learning Rate 1.000e-04, It/sec 1.430, Tokens/sec 259.695, Trained Tokens 1486567, Peak mem 19.940 GB\n",
      "Iter 8990: Train loss 0.544, Learning Rate 1.000e-04, It/sec 1.575, Tokens/sec 255.496, Trained Tokens 1488189, Peak mem 19.940 GB\n",
      "Iter 9000: Val loss 2.284, Val took 3.195s\n",
      "Iter 9000: Train loss 0.677, Learning Rate 1.000e-04, It/sec 16.397, Tokens/sec 3935.269, Trained Tokens 1490589, Peak mem 19.940 GB\n",
      "Iter 9000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009000_adapters.safetensors.\n",
      "Iter 9010: Train loss 0.504, Learning Rate 1.000e-04, It/sec 1.690, Tokens/sec 236.948, Trained Tokens 1491991, Peak mem 19.940 GB\n",
      "Iter 9020: Train loss 0.710, Learning Rate 1.000e-04, It/sec 1.168, Tokens/sec 249.768, Trained Tokens 1494130, Peak mem 19.940 GB\n",
      "Iter 9030: Train loss 0.437, Learning Rate 1.000e-04, It/sec 1.657, Tokens/sec 235.118, Trained Tokens 1495549, Peak mem 19.940 GB\n",
      "Iter 9040: Train loss 0.753, Learning Rate 1.000e-04, It/sec 0.967, Tokens/sec 258.381, Trained Tokens 1498220, Peak mem 19.940 GB\n",
      "Iter 9050: Val loss 2.778, Val took 3.527s\n",
      "Iter 9050: Train loss 0.538, Learning Rate 1.000e-04, It/sec 30.052, Tokens/sec 5192.995, Trained Tokens 1499948, Peak mem 19.940 GB\n",
      "Iter 9060: Train loss 0.530, Learning Rate 1.000e-04, It/sec 1.806, Tokens/sec 249.032, Trained Tokens 1501327, Peak mem 19.940 GB\n",
      "Iter 9070: Train loss 0.444, Learning Rate 1.000e-04, It/sec 2.041, Tokens/sec 253.288, Trained Tokens 1502568, Peak mem 19.940 GB\n",
      "Iter 9080: Train loss 0.438, Learning Rate 1.000e-04, It/sec 1.930, Tokens/sec 259.233, Trained Tokens 1503911, Peak mem 19.940 GB\n",
      "Iter 9090: Train loss 0.474, Learning Rate 1.000e-04, It/sec 1.982, Tokens/sec 242.011, Trained Tokens 1505132, Peak mem 19.940 GB\n",
      "Iter 9100: Val loss 2.849, Val took 3.856s\n",
      "Iter 9100: Train loss 0.489, Learning Rate 1.000e-04, It/sec 15.111, Tokens/sec 2287.765, Trained Tokens 1506646, Peak mem 19.940 GB\n",
      "Iter 9100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009100_adapters.safetensors.\n",
      "Iter 9110: Train loss 0.449, Learning Rate 1.000e-04, It/sec 1.626, Tokens/sec 245.890, Trained Tokens 1508158, Peak mem 19.940 GB\n",
      "Iter 9120: Train loss 0.575, Learning Rate 1.000e-04, It/sec 1.461, Tokens/sec 265.098, Trained Tokens 1509972, Peak mem 19.940 GB\n",
      "Iter 9130: Train loss 0.523, Learning Rate 1.000e-04, It/sec 1.585, Tokens/sec 262.239, Trained Tokens 1511626, Peak mem 19.940 GB\n",
      "Iter 9140: Train loss 0.522, Learning Rate 1.000e-04, It/sec 1.523, Tokens/sec 262.793, Trained Tokens 1513351, Peak mem 19.940 GB\n",
      "Iter 9150: Val loss 2.418, Val took 3.268s\n",
      "Iter 9150: Train loss 0.481, Learning Rate 1.000e-04, It/sec 23.887, Tokens/sec 3255.737, Trained Tokens 1514714, Peak mem 19.940 GB\n",
      "Iter 9160: Train loss 0.503, Learning Rate 1.000e-04, It/sec 1.685, Tokens/sec 258.119, Trained Tokens 1516246, Peak mem 19.940 GB\n",
      "Iter 9170: Train loss 0.473, Learning Rate 1.000e-04, It/sec 1.421, Tokens/sec 254.146, Trained Tokens 1518034, Peak mem 19.940 GB\n",
      "Iter 9180: Train loss 0.508, Learning Rate 1.000e-04, It/sec 1.987, Tokens/sec 240.479, Trained Tokens 1519244, Peak mem 19.940 GB\n",
      "Iter 9190: Train loss 0.451, Learning Rate 1.000e-04, It/sec 1.674, Tokens/sec 265.107, Trained Tokens 1520828, Peak mem 19.940 GB\n",
      "Iter 9200: Val loss 2.611, Val took 3.802s\n",
      "Iter 9200: Train loss 0.462, Learning Rate 1.000e-04, It/sec 17.896, Tokens/sec 3017.253, Trained Tokens 1522514, Peak mem 19.940 GB\n",
      "Iter 9200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009200_adapters.safetensors.\n",
      "Iter 9210: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.584, Tokens/sec 258.790, Trained Tokens 1524148, Peak mem 19.940 GB\n",
      "Iter 9220: Train loss 0.597, Learning Rate 1.000e-04, It/sec 1.759, Tokens/sec 258.230, Trained Tokens 1525616, Peak mem 19.940 GB\n",
      "Iter 9230: Train loss 0.519, Learning Rate 1.000e-04, It/sec 1.697, Tokens/sec 250.605, Trained Tokens 1527093, Peak mem 19.940 GB\n",
      "Iter 9240: Train loss 0.538, Learning Rate 1.000e-04, It/sec 1.259, Tokens/sec 246.303, Trained Tokens 1529050, Peak mem 19.940 GB\n",
      "Iter 9250: Val loss 2.538, Val took 3.470s\n",
      "Iter 9250: Train loss 0.502, Learning Rate 1.000e-04, It/sec 8.692, Tokens/sec 1439.392, Trained Tokens 1530706, Peak mem 19.940 GB\n",
      "Iter 9260: Train loss 0.472, Learning Rate 1.000e-04, It/sec 2.090, Tokens/sec 258.965, Trained Tokens 1531945, Peak mem 19.940 GB\n",
      "Iter 9270: Train loss 0.608, Learning Rate 1.000e-04, It/sec 1.166, Tokens/sec 261.565, Trained Tokens 1534188, Peak mem 19.940 GB\n",
      "Iter 9280: Train loss 0.644, Learning Rate 1.000e-04, It/sec 0.781, Tokens/sec 264.528, Trained Tokens 1537575, Peak mem 19.940 GB\n",
      "Iter 9290: Train loss 0.549, Learning Rate 1.000e-04, It/sec 2.469, Tokens/sec 242.967, Trained Tokens 1538559, Peak mem 19.940 GB\n",
      "Iter 9300: Val loss 2.388, Val took 2.748s\n",
      "Iter 9300: Train loss 0.444, Learning Rate 1.000e-04, It/sec 7.033, Tokens/sec 1357.390, Trained Tokens 1540489, Peak mem 19.940 GB\n",
      "Iter 9300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009300_adapters.safetensors.\n",
      "Iter 9310: Train loss 0.584, Learning Rate 1.000e-04, It/sec 1.127, Tokens/sec 267.131, Trained Tokens 1542860, Peak mem 19.940 GB\n",
      "Iter 9320: Train loss 0.564, Learning Rate 1.000e-04, It/sec 1.786, Tokens/sec 254.911, Trained Tokens 1544287, Peak mem 19.940 GB\n",
      "Iter 9330: Train loss 0.523, Learning Rate 1.000e-04, It/sec 1.602, Tokens/sec 265.947, Trained Tokens 1545947, Peak mem 19.940 GB\n",
      "Iter 9340: Train loss 0.523, Learning Rate 1.000e-04, It/sec 2.198, Tokens/sec 253.405, Trained Tokens 1547100, Peak mem 19.940 GB\n",
      "Iter 9350: Val loss 2.213, Val took 3.769s\n",
      "Iter 9350: Train loss 0.557, Learning Rate 1.000e-04, It/sec 9.245, Tokens/sec 1328.563, Trained Tokens 1548537, Peak mem 19.940 GB\n",
      "Iter 9360: Train loss 0.488, Learning Rate 1.000e-04, It/sec 1.727, Tokens/sec 253.154, Trained Tokens 1550003, Peak mem 19.940 GB\n",
      "Iter 9370: Train loss 0.474, Learning Rate 1.000e-04, It/sec 1.709, Tokens/sec 268.305, Trained Tokens 1551573, Peak mem 19.940 GB\n",
      "Iter 9380: Train loss 0.457, Learning Rate 1.000e-04, It/sec 1.451, Tokens/sec 268.662, Trained Tokens 1553425, Peak mem 19.940 GB\n",
      "Iter 9390: Train loss 0.519, Learning Rate 1.000e-04, It/sec 1.348, Tokens/sec 271.644, Trained Tokens 1555440, Peak mem 19.940 GB\n",
      "Iter 9400: Val loss 2.598, Val took 3.520s\n",
      "Iter 9400: Train loss 0.581, Learning Rate 1.000e-04, It/sec 5.140, Tokens/sec 993.138, Trained Tokens 1557372, Peak mem 19.940 GB\n",
      "Iter 9400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009400_adapters.safetensors.\n",
      "Iter 9410: Train loss 0.553, Learning Rate 1.000e-04, It/sec 1.675, Tokens/sec 245.373, Trained Tokens 1558837, Peak mem 19.940 GB\n",
      "Iter 9420: Train loss 0.542, Learning Rate 1.000e-04, It/sec 1.282, Tokens/sec 242.250, Trained Tokens 1560726, Peak mem 19.940 GB\n",
      "Iter 9430: Train loss 0.522, Learning Rate 1.000e-04, It/sec 1.706, Tokens/sec 224.994, Trained Tokens 1562045, Peak mem 19.940 GB\n",
      "Iter 9440: Train loss 0.499, Learning Rate 1.000e-04, It/sec 1.830, Tokens/sec 248.676, Trained Tokens 1563404, Peak mem 19.940 GB\n",
      "Iter 9450: Val loss 2.809, Val took 3.654s\n",
      "Iter 9450: Train loss 0.670, Learning Rate 1.000e-04, It/sec 21.586, Tokens/sec 4822.362, Trained Tokens 1565638, Peak mem 19.940 GB\n",
      "Iter 9460: Train loss 0.433, Learning Rate 1.000e-04, It/sec 1.219, Tokens/sec 256.324, Trained Tokens 1567740, Peak mem 19.940 GB\n",
      "Iter 9470: Train loss 0.548, Learning Rate 1.000e-04, It/sec 1.495, Tokens/sec 268.263, Trained Tokens 1569534, Peak mem 19.940 GB\n",
      "Iter 9480: Train loss 0.417, Learning Rate 1.000e-04, It/sec 1.610, Tokens/sec 256.309, Trained Tokens 1571126, Peak mem 19.940 GB\n",
      "Iter 9490: Train loss 0.474, Learning Rate 1.000e-04, It/sec 1.915, Tokens/sec 236.508, Trained Tokens 1572361, Peak mem 19.940 GB\n",
      "Iter 9500: Val loss 2.570, Val took 3.696s\n",
      "Iter 9500: Train loss 0.476, Learning Rate 1.000e-04, It/sec 23.057, Tokens/sec 3806.656, Trained Tokens 1574012, Peak mem 19.940 GB\n",
      "Iter 9500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009500_adapters.safetensors.\n",
      "Iter 9510: Train loss 0.480, Learning Rate 1.000e-04, It/sec 1.973, Tokens/sec 249.814, Trained Tokens 1575278, Peak mem 19.940 GB\n",
      "Iter 9520: Train loss 0.499, Learning Rate 1.000e-04, It/sec 1.717, Tokens/sec 263.596, Trained Tokens 1576813, Peak mem 19.940 GB\n",
      "Iter 9530: Train loss 0.483, Learning Rate 1.000e-04, It/sec 2.035, Tokens/sec 245.642, Trained Tokens 1578020, Peak mem 19.940 GB\n",
      "Iter 9540: Train loss 0.446, Learning Rate 1.000e-04, It/sec 1.777, Tokens/sec 256.624, Trained Tokens 1579464, Peak mem 19.940 GB\n",
      "Iter 9550: Val loss 2.983, Val took 3.255s\n",
      "Iter 9550: Train loss 0.410, Learning Rate 1.000e-04, It/sec 7.273, Tokens/sec 1277.870, Trained Tokens 1581221, Peak mem 19.940 GB\n",
      "Iter 9560: Train loss 0.472, Learning Rate 1.000e-04, It/sec 1.503, Tokens/sec 255.536, Trained Tokens 1582921, Peak mem 19.940 GB\n",
      "Iter 9570: Train loss 0.448, Learning Rate 1.000e-04, It/sec 2.024, Tokens/sec 262.888, Trained Tokens 1584220, Peak mem 19.940 GB\n",
      "Iter 9580: Train loss 0.483, Learning Rate 1.000e-04, It/sec 1.908, Tokens/sec 257.377, Trained Tokens 1585569, Peak mem 19.940 GB\n",
      "Iter 9590: Train loss 0.530, Learning Rate 1.000e-04, It/sec 1.507, Tokens/sec 267.905, Trained Tokens 1587347, Peak mem 19.940 GB\n",
      "Iter 9600: Val loss 2.218, Val took 3.416s\n",
      "Iter 9600: Train loss 0.511, Learning Rate 1.000e-04, It/sec 42.004, Tokens/sec 5279.939, Trained Tokens 1588604, Peak mem 19.940 GB\n",
      "Iter 9600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009600_adapters.safetensors.\n",
      "Iter 9610: Train loss 0.531, Learning Rate 1.000e-04, It/sec 0.897, Tokens/sec 275.034, Trained Tokens 1591671, Peak mem 19.940 GB\n",
      "Iter 9620: Train loss 0.431, Learning Rate 1.000e-04, It/sec 1.458, Tokens/sec 271.233, Trained Tokens 1593531, Peak mem 19.940 GB\n",
      "Iter 9630: Train loss 0.542, Learning Rate 1.000e-04, It/sec 1.733, Tokens/sec 254.614, Trained Tokens 1595000, Peak mem 19.940 GB\n",
      "Iter 9640: Train loss 0.522, Learning Rate 1.000e-04, It/sec 2.182, Tokens/sec 245.445, Trained Tokens 1596125, Peak mem 19.940 GB\n",
      "Iter 9650: Val loss 2.454, Val took 2.812s\n",
      "Iter 9650: Train loss 0.493, Learning Rate 1.000e-04, It/sec 10.616, Tokens/sec 2060.533, Trained Tokens 1598066, Peak mem 19.940 GB\n",
      "Iter 9660: Train loss 0.479, Learning Rate 1.000e-04, It/sec 1.748, Tokens/sec 260.634, Trained Tokens 1599557, Peak mem 19.940 GB\n",
      "Iter 9670: Train loss 0.463, Learning Rate 1.000e-04, It/sec 1.757, Tokens/sec 269.306, Trained Tokens 1601090, Peak mem 19.940 GB\n",
      "Iter 9680: Train loss 0.523, Learning Rate 1.000e-04, It/sec 1.584, Tokens/sec 259.783, Trained Tokens 1602730, Peak mem 19.940 GB\n",
      "Iter 9690: Train loss 0.520, Learning Rate 1.000e-04, It/sec 1.516, Tokens/sec 250.824, Trained Tokens 1604384, Peak mem 19.940 GB\n",
      "Iter 9700: Val loss 2.404, Val took 3.343s\n",
      "Iter 9700: Train loss 0.677, Learning Rate 1.000e-04, It/sec 29.657, Tokens/sec 7942.048, Trained Tokens 1607062, Peak mem 19.940 GB\n",
      "Iter 9700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009700_adapters.safetensors.\n",
      "Iter 9710: Train loss 0.587, Learning Rate 1.000e-04, It/sec 1.328, Tokens/sec 259.269, Trained Tokens 1609015, Peak mem 19.940 GB\n",
      "Iter 9720: Train loss 0.585, Learning Rate 1.000e-04, It/sec 1.684, Tokens/sec 255.764, Trained Tokens 1610534, Peak mem 19.940 GB\n",
      "Iter 9730: Train loss 0.489, Learning Rate 1.000e-04, It/sec 1.907, Tokens/sec 239.141, Trained Tokens 1611788, Peak mem 19.940 GB\n",
      "Iter 9740: Train loss 0.461, Learning Rate 1.000e-04, It/sec 1.848, Tokens/sec 261.000, Trained Tokens 1613200, Peak mem 19.940 GB\n",
      "Iter 9750: Val loss 2.443, Val took 3.433s\n",
      "Iter 9750: Train loss 0.468, Learning Rate 1.000e-04, It/sec 20.747, Tokens/sec 2834.022, Trained Tokens 1614566, Peak mem 19.940 GB\n",
      "Iter 9760: Train loss 0.729, Learning Rate 1.000e-04, It/sec 0.952, Tokens/sec 258.988, Trained Tokens 1617286, Peak mem 19.940 GB\n",
      "Iter 9770: Train loss 0.573, Learning Rate 1.000e-04, It/sec 1.188, Tokens/sec 261.328, Trained Tokens 1619486, Peak mem 19.940 GB\n",
      "Iter 9780: Train loss 0.453, Learning Rate 1.000e-04, It/sec 1.519, Tokens/sec 247.483, Trained Tokens 1621115, Peak mem 19.940 GB\n",
      "Iter 9790: Train loss 0.491, Learning Rate 1.000e-04, It/sec 1.576, Tokens/sec 251.779, Trained Tokens 1622713, Peak mem 19.940 GB\n",
      "Iter 9800: Val loss 2.346, Val took 4.430s\n",
      "Iter 9800: Train loss 0.484, Learning Rate 1.000e-04, It/sec 21.264, Tokens/sec 2925.860, Trained Tokens 1624089, Peak mem 19.940 GB\n",
      "Iter 9800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009800_adapters.safetensors.\n",
      "Iter 9810: Train loss 0.477, Learning Rate 1.000e-04, It/sec 1.452, Tokens/sec 225.508, Trained Tokens 1625642, Peak mem 19.940 GB\n",
      "Iter 9820: Train loss 0.544, Learning Rate 1.000e-04, It/sec 1.844, Tokens/sec 257.586, Trained Tokens 1627039, Peak mem 19.940 GB\n",
      "Iter 9830: Train loss 0.538, Learning Rate 1.000e-04, It/sec 1.649, Tokens/sec 253.141, Trained Tokens 1628574, Peak mem 19.940 GB\n",
      "Iter 9840: Train loss 0.515, Learning Rate 1.000e-04, It/sec 1.805, Tokens/sec 250.114, Trained Tokens 1629960, Peak mem 19.940 GB\n",
      "Iter 9850: Val loss 2.756, Val took 4.571s\n",
      "Iter 9850: Train loss 0.490, Learning Rate 1.000e-04, It/sec 29.056, Tokens/sec 5631.101, Trained Tokens 1631898, Peak mem 19.940 GB\n",
      "Iter 9860: Train loss 0.413, Learning Rate 1.000e-04, It/sec 1.824, Tokens/sec 237.884, Trained Tokens 1633202, Peak mem 19.940 GB\n",
      "Iter 9870: Train loss 0.498, Learning Rate 1.000e-04, It/sec 2.062, Tokens/sec 235.527, Trained Tokens 1634344, Peak mem 19.940 GB\n",
      "Iter 9880: Train loss 0.468, Learning Rate 1.000e-04, It/sec 1.692, Tokens/sec 246.004, Trained Tokens 1635798, Peak mem 19.940 GB\n",
      "Iter 9890: Train loss 0.451, Learning Rate 1.000e-04, It/sec 1.624, Tokens/sec 259.912, Trained Tokens 1637398, Peak mem 19.940 GB\n",
      "Iter 9900: Val loss 2.497, Val took 3.695s\n",
      "Iter 9900: Train loss 0.454, Learning Rate 1.000e-04, It/sec 11.944, Tokens/sec 1792.813, Trained Tokens 1638899, Peak mem 19.940 GB\n",
      "Iter 9900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0009900_adapters.safetensors.\n",
      "Iter 9910: Train loss 0.502, Learning Rate 1.000e-04, It/sec 1.220, Tokens/sec 242.451, Trained Tokens 1640886, Peak mem 19.940 GB\n",
      "Iter 9920: Train loss 0.485, Learning Rate 1.000e-04, It/sec 1.116, Tokens/sec 231.988, Trained Tokens 1642965, Peak mem 19.940 GB\n",
      "Iter 9930: Train loss 0.432, Learning Rate 1.000e-04, It/sec 1.679, Tokens/sec 225.555, Trained Tokens 1644308, Peak mem 19.940 GB\n",
      "Iter 9940: Train loss 0.421, Learning Rate 1.000e-04, It/sec 1.828, Tokens/sec 248.090, Trained Tokens 1645665, Peak mem 19.940 GB\n",
      "Iter 9950: Val loss 2.527, Val took 3.229s\n",
      "Iter 9950: Train loss 0.425, Learning Rate 1.000e-04, It/sec 16.565, Tokens/sec 2955.202, Trained Tokens 1647449, Peak mem 19.940 GB\n",
      "Iter 9960: Train loss 0.451, Learning Rate 1.000e-04, It/sec 2.022, Tokens/sec 244.503, Trained Tokens 1648658, Peak mem 19.940 GB\n",
      "Iter 9970: Train loss 0.492, Learning Rate 1.000e-04, It/sec 1.536, Tokens/sec 240.732, Trained Tokens 1650225, Peak mem 19.940 GB\n",
      "Iter 9980: Train loss 0.521, Learning Rate 1.000e-04, It/sec 1.565, Tokens/sec 246.805, Trained Tokens 1651802, Peak mem 19.940 GB\n",
      "Iter 9990: Train loss 0.512, Learning Rate 1.000e-04, It/sec 1.850, Tokens/sec 237.200, Trained Tokens 1653084, Peak mem 19.940 GB\n",
      "Iter 10000: Val loss 2.424, Val took 3.735s\n",
      "Iter 10000: Train loss 0.452, Learning Rate 1.000e-04, It/sec 8.482, Tokens/sec 1388.532, Trained Tokens 1654721, Peak mem 19.940 GB\n",
      "Iter 10000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010000_adapters.safetensors.\n",
      "Iter 10010: Train loss 0.405, Learning Rate 1.000e-04, It/sec 1.478, Tokens/sec 261.962, Trained Tokens 1656493, Peak mem 19.940 GB\n",
      "Iter 10020: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.566, Tokens/sec 252.753, Trained Tokens 1658107, Peak mem 19.940 GB\n",
      "Iter 10030: Train loss 0.505, Learning Rate 1.000e-04, It/sec 1.796, Tokens/sec 254.451, Trained Tokens 1659524, Peak mem 19.940 GB\n",
      "Iter 10040: Train loss 0.488, Learning Rate 1.000e-04, It/sec 1.808, Tokens/sec 255.406, Trained Tokens 1660937, Peak mem 19.940 GB\n",
      "Iter 10050: Val loss 2.008, Val took 3.196s\n",
      "Iter 10050: Train loss 0.434, Learning Rate 1.000e-04, It/sec 22.599, Tokens/sec 4427.148, Trained Tokens 1662896, Peak mem 19.940 GB\n",
      "Iter 10060: Train loss 0.490, Learning Rate 1.000e-04, It/sec 1.370, Tokens/sec 221.610, Trained Tokens 1664514, Peak mem 19.940 GB\n",
      "Iter 10070: Train loss 0.439, Learning Rate 1.000e-04, It/sec 1.890, Tokens/sec 248.872, Trained Tokens 1665831, Peak mem 19.940 GB\n",
      "Iter 10080: Train loss 0.581, Learning Rate 1.000e-04, It/sec 1.613, Tokens/sec 251.974, Trained Tokens 1667393, Peak mem 19.940 GB\n",
      "Iter 10090: Train loss 0.529, Learning Rate 1.000e-04, It/sec 0.901, Tokens/sec 250.727, Trained Tokens 1670177, Peak mem 19.940 GB\n",
      "Iter 10100: Val loss 2.476, Val took 3.626s\n",
      "Iter 10100: Train loss 0.492, Learning Rate 1.000e-04, It/sec 18.112, Tokens/sec 1970.574, Trained Tokens 1671265, Peak mem 19.940 GB\n",
      "Iter 10100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010100_adapters.safetensors.\n",
      "Iter 10110: Train loss 0.527, Learning Rate 1.000e-04, It/sec 1.559, Tokens/sec 260.446, Trained Tokens 1672936, Peak mem 19.940 GB\n",
      "Iter 10120: Train loss 0.772, Learning Rate 1.000e-04, It/sec 1.058, Tokens/sec 267.120, Trained Tokens 1675460, Peak mem 19.940 GB\n",
      "Iter 10130: Train loss 0.540, Learning Rate 1.000e-04, It/sec 1.514, Tokens/sec 262.776, Trained Tokens 1677196, Peak mem 19.940 GB\n",
      "Iter 10140: Train loss 0.500, Learning Rate 1.000e-04, It/sec 1.825, Tokens/sec 250.571, Trained Tokens 1678569, Peak mem 19.940 GB\n",
      "Iter 10150: Val loss 2.431, Val took 4.660s\n",
      "Iter 10150: Train loss 0.460, Learning Rate 1.000e-04, It/sec 29.384, Tokens/sec 4116.643, Trained Tokens 1679970, Peak mem 19.940 GB\n",
      "Iter 10160: Train loss 0.472, Learning Rate 1.000e-04, It/sec 1.340, Tokens/sec 212.255, Trained Tokens 1681554, Peak mem 19.940 GB\n",
      "Iter 10170: Train loss 0.548, Learning Rate 1.000e-04, It/sec 1.776, Tokens/sec 244.665, Trained Tokens 1682932, Peak mem 19.940 GB\n",
      "Iter 10180: Train loss 0.618, Learning Rate 1.000e-04, It/sec 1.465, Tokens/sec 266.419, Trained Tokens 1684751, Peak mem 19.940 GB\n",
      "Iter 10190: Train loss 0.474, Learning Rate 1.000e-04, It/sec 1.224, Tokens/sec 263.471, Trained Tokens 1686904, Peak mem 19.940 GB\n",
      "Iter 10200: Val loss 2.687, Val took 3.627s\n",
      "Iter 10200: Train loss 0.487, Learning Rate 1.000e-04, It/sec 17.253, Tokens/sec 2644.880, Trained Tokens 1688437, Peak mem 19.940 GB\n",
      "Iter 10200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010200_adapters.safetensors.\n",
      "Iter 10210: Train loss 0.447, Learning Rate 1.000e-04, It/sec 1.068, Tokens/sec 177.774, Trained Tokens 1690102, Peak mem 19.940 GB\n",
      "Iter 10220: Train loss 0.660, Learning Rate 1.000e-04, It/sec 1.147, Tokens/sec 258.080, Trained Tokens 1692352, Peak mem 19.940 GB\n",
      "Iter 10230: Train loss 0.528, Learning Rate 1.000e-04, It/sec 2.308, Tokens/sec 248.080, Trained Tokens 1693427, Peak mem 19.940 GB\n",
      "Iter 10240: Train loss 0.533, Learning Rate 1.000e-04, It/sec 1.484, Tokens/sec 260.939, Trained Tokens 1695185, Peak mem 19.940 GB\n",
      "Iter 10250: Val loss 2.577, Val took 4.782s\n",
      "Iter 10250: Train loss 0.590, Learning Rate 1.000e-04, It/sec 11.729, Tokens/sec 3155.092, Trained Tokens 1697875, Peak mem 19.940 GB\n",
      "Iter 10260: Train loss 0.467, Learning Rate 1.000e-04, It/sec 2.040, Tokens/sec 255.845, Trained Tokens 1699129, Peak mem 19.940 GB\n",
      "Iter 10270: Train loss 0.534, Learning Rate 1.000e-04, It/sec 1.422, Tokens/sec 272.493, Trained Tokens 1701045, Peak mem 19.940 GB\n",
      "Iter 10280: Train loss 0.448, Learning Rate 1.000e-04, It/sec 1.896, Tokens/sec 249.761, Trained Tokens 1702362, Peak mem 19.940 GB\n",
      "Iter 10290: Train loss 0.449, Learning Rate 1.000e-04, It/sec 1.863, Tokens/sec 243.131, Trained Tokens 1703667, Peak mem 19.940 GB\n",
      "Iter 10300: Val loss 2.577, Val took 3.846s\n",
      "Iter 10300: Train loss 0.488, Learning Rate 1.000e-04, It/sec 23.018, Tokens/sec 2506.609, Trained Tokens 1704756, Peak mem 19.940 GB\n",
      "Iter 10300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010300_adapters.safetensors.\n",
      "Iter 10310: Train loss 0.472, Learning Rate 1.000e-04, It/sec 1.783, Tokens/sec 247.770, Trained Tokens 1706146, Peak mem 19.940 GB\n",
      "Iter 10320: Train loss 0.416, Learning Rate 1.000e-04, It/sec 1.756, Tokens/sec 238.161, Trained Tokens 1707502, Peak mem 19.940 GB\n",
      "Iter 10330: Train loss 0.494, Learning Rate 1.000e-04, It/sec 2.371, Tokens/sec 232.870, Trained Tokens 1708484, Peak mem 19.940 GB\n",
      "Iter 10340: Train loss 0.409, Learning Rate 1.000e-04, It/sec 1.667, Tokens/sec 241.358, Trained Tokens 1709932, Peak mem 19.940 GB\n",
      "Iter 10350: Val loss 2.434, Val took 3.469s\n",
      "Iter 10350: Train loss 0.551, Learning Rate 1.000e-04, It/sec 28.914, Tokens/sec 5548.507, Trained Tokens 1711851, Peak mem 19.940 GB\n",
      "Iter 10360: Train loss 0.532, Learning Rate 1.000e-04, It/sec 1.527, Tokens/sec 243.870, Trained Tokens 1713448, Peak mem 19.940 GB\n",
      "Iter 10370: Train loss 0.486, Learning Rate 1.000e-04, It/sec 1.678, Tokens/sec 247.859, Trained Tokens 1714925, Peak mem 19.940 GB\n",
      "Iter 10380: Train loss 0.437, Learning Rate 1.000e-04, It/sec 1.593, Tokens/sec 234.497, Trained Tokens 1716397, Peak mem 19.940 GB\n",
      "Iter 10390: Train loss 0.443, Learning Rate 1.000e-04, It/sec 1.516, Tokens/sec 251.231, Trained Tokens 1718054, Peak mem 19.940 GB\n",
      "Iter 10400: Val loss 2.577, Val took 4.209s\n",
      "Iter 10400: Train loss 0.466, Learning Rate 1.000e-04, It/sec 27.322, Tokens/sec 5218.495, Trained Tokens 1719964, Peak mem 19.940 GB\n",
      "Iter 10400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010400_adapters.safetensors.\n",
      "Iter 10410: Train loss 0.484, Learning Rate 1.000e-04, It/sec 1.860, Tokens/sec 244.450, Trained Tokens 1721278, Peak mem 19.940 GB\n",
      "Iter 10420: Train loss 0.383, Learning Rate 1.000e-04, It/sec 1.143, Tokens/sec 253.669, Trained Tokens 1723498, Peak mem 19.940 GB\n",
      "Iter 10430: Train loss 0.457, Learning Rate 1.000e-04, It/sec 1.743, Tokens/sec 244.676, Trained Tokens 1724902, Peak mem 19.940 GB\n",
      "Iter 10440: Train loss 0.603, Learning Rate 1.000e-04, It/sec 1.339, Tokens/sec 264.529, Trained Tokens 1726878, Peak mem 19.940 GB\n",
      "Iter 10450: Val loss 2.614, Val took 5.092s\n",
      "Iter 10450: Train loss 0.418, Learning Rate 1.000e-04, It/sec 27.699, Tokens/sec 4426.236, Trained Tokens 1728476, Peak mem 19.940 GB\n",
      "Iter 10460: Train loss 0.489, Learning Rate 1.000e-04, It/sec 1.240, Tokens/sec 213.781, Trained Tokens 1730200, Peak mem 19.940 GB\n",
      "Iter 10470: Train loss 0.572, Learning Rate 1.000e-04, It/sec 0.929, Tokens/sec 236.375, Trained Tokens 1732744, Peak mem 19.940 GB\n",
      "Iter 10480: Train loss 0.462, Learning Rate 1.000e-04, It/sec 1.728, Tokens/sec 248.000, Trained Tokens 1734179, Peak mem 19.940 GB\n",
      "Iter 10490: Train loss 0.468, Learning Rate 1.000e-04, It/sec 1.761, Tokens/sec 254.527, Trained Tokens 1735624, Peak mem 19.940 GB\n",
      "Iter 10500: Val loss 2.940, Val took 3.600s\n",
      "Iter 10500: Train loss 0.586, Learning Rate 1.000e-04, It/sec 12.586, Tokens/sec 3555.552, Trained Tokens 1738449, Peak mem 19.940 GB\n",
      "Iter 10500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010500_adapters.safetensors.\n",
      "Iter 10510: Train loss 0.398, Learning Rate 1.000e-04, It/sec 1.384, Tokens/sec 232.552, Trained Tokens 1740129, Peak mem 19.940 GB\n",
      "Iter 10520: Train loss 0.495, Learning Rate 1.000e-04, It/sec 1.540, Tokens/sec 250.403, Trained Tokens 1741755, Peak mem 19.940 GB\n",
      "Iter 10530: Train loss 0.469, Learning Rate 1.000e-04, It/sec 1.570, Tokens/sec 253.919, Trained Tokens 1743372, Peak mem 19.940 GB\n",
      "Iter 10540: Train loss 0.399, Learning Rate 1.000e-04, It/sec 1.303, Tokens/sec 257.031, Trained Tokens 1745344, Peak mem 19.940 GB\n",
      "Iter 10550: Val loss 2.494, Val took 4.242s\n",
      "Iter 10550: Train loss 0.595, Learning Rate 1.000e-04, It/sec 27.831, Tokens/sec 5989.222, Trained Tokens 1747496, Peak mem 19.940 GB\n",
      "Iter 10560: Train loss 0.495, Learning Rate 1.000e-04, It/sec 1.938, Tokens/sec 252.128, Trained Tokens 1748797, Peak mem 19.940 GB\n",
      "Iter 10570: Train loss 0.475, Learning Rate 1.000e-04, It/sec 1.781, Tokens/sec 245.223, Trained Tokens 1750174, Peak mem 19.940 GB\n",
      "Iter 10580: Train loss 0.442, Learning Rate 1.000e-04, It/sec 1.676, Tokens/sec 259.872, Trained Tokens 1751725, Peak mem 19.940 GB\n",
      "Iter 10590: Train loss 0.483, Learning Rate 1.000e-04, It/sec 1.261, Tokens/sec 264.593, Trained Tokens 1753824, Peak mem 19.940 GB\n",
      "Iter 10600: Val loss 2.546, Val took 3.191s\n",
      "Iter 10600: Train loss 0.495, Learning Rate 1.000e-04, It/sec 11.412, Tokens/sec 1812.283, Trained Tokens 1755412, Peak mem 19.940 GB\n",
      "Iter 10600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010600_adapters.safetensors.\n",
      "Iter 10610: Train loss 0.529, Learning Rate 1.000e-04, It/sec 1.193, Tokens/sec 264.404, Trained Tokens 1757629, Peak mem 19.940 GB\n",
      "Iter 10620: Train loss 0.541, Learning Rate 1.000e-04, It/sec 1.738, Tokens/sec 252.078, Trained Tokens 1759079, Peak mem 19.940 GB\n",
      "Iter 10630: Train loss 0.452, Learning Rate 1.000e-04, It/sec 1.290, Tokens/sec 254.688, Trained Tokens 1761053, Peak mem 19.940 GB\n",
      "Iter 10640: Train loss 0.472, Learning Rate 1.000e-04, It/sec 1.654, Tokens/sec 249.889, Trained Tokens 1762564, Peak mem 19.940 GB\n",
      "Iter 10650: Val loss 2.412, Val took 3.177s\n",
      "Iter 10650: Train loss 0.481, Learning Rate 1.000e-04, It/sec 3.621, Tokens/sec 591.281, Trained Tokens 1764197, Peak mem 19.940 GB\n",
      "Iter 10660: Train loss 0.524, Learning Rate 1.000e-04, It/sec 1.509, Tokens/sec 240.403, Trained Tokens 1765790, Peak mem 19.940 GB\n",
      "Iter 10670: Train loss 0.484, Learning Rate 1.000e-04, It/sec 1.354, Tokens/sec 263.360, Trained Tokens 1767735, Peak mem 19.940 GB\n",
      "Iter 10680: Train loss 0.460, Learning Rate 1.000e-04, It/sec 1.791, Tokens/sec 250.758, Trained Tokens 1769135, Peak mem 19.940 GB\n",
      "Iter 10690: Train loss 0.395, Learning Rate 1.000e-04, It/sec 1.593, Tokens/sec 231.687, Trained Tokens 1770589, Peak mem 19.940 GB\n",
      "Iter 10700: Val loss 2.716, Val took 3.451s\n",
      "Iter 10700: Train loss 0.499, Learning Rate 1.000e-04, It/sec 35.613, Tokens/sec 4544.233, Trained Tokens 1771865, Peak mem 19.940 GB\n",
      "Iter 10700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010700_adapters.safetensors.\n",
      "Iter 10710: Train loss 0.471, Learning Rate 1.000e-04, It/sec 2.118, Tokens/sec 223.495, Trained Tokens 1772920, Peak mem 19.940 GB\n",
      "Iter 10720: Train loss 0.520, Learning Rate 1.000e-04, It/sec 1.572, Tokens/sec 258.459, Trained Tokens 1774564, Peak mem 19.940 GB\n",
      "Iter 10730: Train loss 0.427, Learning Rate 1.000e-04, It/sec 1.569, Tokens/sec 249.910, Trained Tokens 1776157, Peak mem 19.940 GB\n",
      "Iter 10740: Train loss 0.407, Learning Rate 1.000e-04, It/sec 1.526, Tokens/sec 248.777, Trained Tokens 1777787, Peak mem 19.940 GB\n",
      "Iter 10750: Val loss 2.692, Val took 3.328s\n",
      "Iter 10750: Train loss 0.460, Learning Rate 1.000e-04, It/sec 14.704, Tokens/sec 1868.906, Trained Tokens 1779058, Peak mem 19.940 GB\n",
      "Iter 10760: Train loss 0.442, Learning Rate 1.000e-04, It/sec 1.208, Tokens/sec 255.839, Trained Tokens 1781175, Peak mem 19.940 GB\n",
      "Iter 10770: Train loss 0.624, Learning Rate 1.000e-04, It/sec 1.502, Tokens/sec 253.347, Trained Tokens 1782862, Peak mem 19.940 GB\n",
      "Iter 10780: Train loss 0.437, Learning Rate 1.000e-04, It/sec 1.133, Tokens/sec 265.849, Trained Tokens 1785209, Peak mem 19.940 GB\n",
      "Iter 10790: Train loss 0.463, Learning Rate 1.000e-04, It/sec 1.373, Tokens/sec 259.435, Trained Tokens 1787099, Peak mem 19.940 GB\n",
      "Iter 10800: Val loss 2.453, Val took 3.154s\n",
      "Iter 10800: Train loss 0.444, Learning Rate 1.000e-04, It/sec 16.308, Tokens/sec 2736.480, Trained Tokens 1788777, Peak mem 19.940 GB\n",
      "Iter 10800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010800_adapters.safetensors.\n",
      "Iter 10810: Train loss 0.472, Learning Rate 1.000e-04, It/sec 1.968, Tokens/sec 245.613, Trained Tokens 1790025, Peak mem 19.940 GB\n",
      "Iter 10820: Train loss 0.454, Learning Rate 1.000e-04, It/sec 1.640, Tokens/sec 250.421, Trained Tokens 1791552, Peak mem 19.940 GB\n",
      "Iter 10830: Train loss 0.446, Learning Rate 1.000e-04, It/sec 1.532, Tokens/sec 251.742, Trained Tokens 1793195, Peak mem 19.940 GB\n",
      "Iter 10840: Train loss 0.481, Learning Rate 1.000e-04, It/sec 1.718, Tokens/sec 245.864, Trained Tokens 1794626, Peak mem 19.940 GB\n",
      "Iter 10850: Val loss 2.658, Val took 3.227s\n",
      "Iter 10850: Train loss 0.486, Learning Rate 1.000e-04, It/sec 14.836, Tokens/sec 1989.498, Trained Tokens 1795967, Peak mem 19.940 GB\n",
      "Iter 10860: Train loss 0.463, Learning Rate 1.000e-04, It/sec 1.531, Tokens/sec 236.976, Trained Tokens 1797515, Peak mem 19.940 GB\n",
      "Iter 10870: Train loss 0.486, Learning Rate 1.000e-04, It/sec 1.958, Tokens/sec 240.268, Trained Tokens 1798742, Peak mem 19.940 GB\n",
      "Iter 10880: Train loss 0.531, Learning Rate 1.000e-04, It/sec 1.976, Tokens/sec 245.249, Trained Tokens 1799983, Peak mem 19.940 GB\n",
      "Iter 10890: Train loss 0.432, Learning Rate 1.000e-04, It/sec 1.530, Tokens/sec 250.384, Trained Tokens 1801619, Peak mem 19.940 GB\n",
      "Iter 10900: Val loss 2.581, Val took 4.946s\n",
      "Iter 10900: Train loss 0.424, Learning Rate 1.000e-04, It/sec 21.362, Tokens/sec 3368.808, Trained Tokens 1803196, Peak mem 19.940 GB\n",
      "Iter 10900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0010900_adapters.safetensors.\n",
      "Iter 10910: Train loss 0.617, Learning Rate 1.000e-04, It/sec 1.240, Tokens/sec 256.637, Trained Tokens 1805265, Peak mem 19.940 GB\n",
      "Iter 10920: Train loss 0.446, Learning Rate 1.000e-04, It/sec 1.844, Tokens/sec 237.837, Trained Tokens 1806555, Peak mem 19.940 GB\n",
      "Iter 10930: Train loss 0.449, Learning Rate 1.000e-04, It/sec 1.285, Tokens/sec 253.587, Trained Tokens 1808529, Peak mem 19.940 GB\n",
      "Iter 10940: Train loss 0.635, Learning Rate 1.000e-04, It/sec 1.005, Tokens/sec 243.311, Trained Tokens 1810949, Peak mem 19.940 GB\n",
      "Iter 10950: Val loss 2.510, Val took 5.584s\n",
      "Iter 10950: Train loss 0.478, Learning Rate 1.000e-04, It/sec 41.096, Tokens/sec 5650.735, Trained Tokens 1812324, Peak mem 19.940 GB\n",
      "Iter 10960: Train loss 0.431, Learning Rate 1.000e-04, It/sec 1.272, Tokens/sec 194.296, Trained Tokens 1813852, Peak mem 19.940 GB\n",
      "Iter 10970: Train loss 0.471, Learning Rate 1.000e-04, It/sec 1.484, Tokens/sec 239.100, Trained Tokens 1815463, Peak mem 19.940 GB\n",
      "Iter 10980: Train loss 0.386, Learning Rate 1.000e-04, It/sec 1.479, Tokens/sec 240.406, Trained Tokens 1817088, Peak mem 19.940 GB\n",
      "Iter 10990: Train loss 0.550, Learning Rate 1.000e-04, It/sec 1.348, Tokens/sec 246.081, Trained Tokens 1818913, Peak mem 19.940 GB\n",
      "Iter 11000: Val loss 2.510, Val took 3.236s\n",
      "Iter 11000: Train loss 0.498, Learning Rate 1.000e-04, It/sec 13.170, Tokens/sec 1876.789, Trained Tokens 1820338, Peak mem 19.940 GB\n",
      "Iter 11000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011000_adapters.safetensors.\n",
      "Iter 11010: Train loss 0.450, Learning Rate 1.000e-04, It/sec 1.652, Tokens/sec 218.096, Trained Tokens 1821658, Peak mem 19.940 GB\n",
      "Iter 11020: Train loss 0.594, Learning Rate 1.000e-04, It/sec 1.019, Tokens/sec 251.323, Trained Tokens 1824125, Peak mem 19.940 GB\n",
      "Iter 11030: Train loss 0.505, Learning Rate 1.000e-04, It/sec 1.712, Tokens/sec 262.583, Trained Tokens 1825659, Peak mem 19.940 GB\n",
      "Iter 11040: Train loss 0.504, Learning Rate 1.000e-04, It/sec 1.550, Tokens/sec 247.958, Trained Tokens 1827259, Peak mem 19.940 GB\n",
      "Iter 11050: Val loss 2.829, Val took 3.180s\n",
      "Iter 11050: Train loss 0.571, Learning Rate 1.000e-04, It/sec 17.924, Tokens/sec 3669.102, Trained Tokens 1829306, Peak mem 19.940 GB\n",
      "Iter 11060: Train loss 0.572, Learning Rate 1.000e-04, It/sec 0.925, Tokens/sec 261.173, Trained Tokens 1832130, Peak mem 19.940 GB\n",
      "Iter 11070: Train loss 0.481, Learning Rate 1.000e-04, It/sec 1.680, Tokens/sec 264.526, Trained Tokens 1833705, Peak mem 19.940 GB\n",
      "Iter 11080: Train loss 0.543, Learning Rate 1.000e-04, It/sec 0.992, Tokens/sec 261.478, Trained Tokens 1836341, Peak mem 19.940 GB\n",
      "Iter 11090: Train loss 0.414, Learning Rate 1.000e-04, It/sec 1.695, Tokens/sec 238.989, Trained Tokens 1837751, Peak mem 19.940 GB\n",
      "Iter 11100: Val loss 2.798, Val took 3.401s\n",
      "Iter 11100: Train loss 0.445, Learning Rate 1.000e-04, It/sec 20.141, Tokens/sec 2332.306, Trained Tokens 1838909, Peak mem 19.940 GB\n",
      "Iter 11100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011100_adapters.safetensors.\n",
      "Iter 11110: Train loss 0.451, Learning Rate 1.000e-04, It/sec 1.465, Tokens/sec 223.413, Trained Tokens 1840434, Peak mem 19.940 GB\n",
      "Iter 11120: Train loss 0.509, Learning Rate 1.000e-04, It/sec 2.194, Tokens/sec 237.572, Trained Tokens 1841517, Peak mem 19.940 GB\n",
      "Iter 11130: Train loss 0.463, Learning Rate 1.000e-04, It/sec 1.745, Tokens/sec 254.660, Trained Tokens 1842976, Peak mem 19.940 GB\n",
      "Iter 11140: Train loss 0.433, Learning Rate 1.000e-04, It/sec 1.617, Tokens/sec 241.349, Trained Tokens 1844469, Peak mem 19.940 GB\n",
      "Iter 11150: Val loss 2.603, Val took 4.711s\n",
      "Iter 11150: Train loss 0.450, Learning Rate 1.000e-04, It/sec 11.741, Tokens/sec 1441.816, Trained Tokens 1845697, Peak mem 19.940 GB\n",
      "Iter 11160: Train loss 0.424, Learning Rate 1.000e-04, It/sec 1.458, Tokens/sec 238.740, Trained Tokens 1847335, Peak mem 19.940 GB\n",
      "Iter 11170: Train loss 0.467, Learning Rate 1.000e-04, It/sec 1.832, Tokens/sec 242.710, Trained Tokens 1848660, Peak mem 19.940 GB\n",
      "Iter 11180: Train loss 0.366, Learning Rate 1.000e-04, It/sec 1.493, Tokens/sec 267.924, Trained Tokens 1850454, Peak mem 19.940 GB\n",
      "Iter 11190: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.662, Tokens/sec 262.956, Trained Tokens 1852036, Peak mem 19.940 GB\n",
      "Iter 11200: Val loss 2.554, Val took 3.246s\n",
      "Iter 11200: Train loss 0.458, Learning Rate 1.000e-04, It/sec 18.028, Tokens/sec 2601.482, Trained Tokens 1853479, Peak mem 19.940 GB\n",
      "Iter 11200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011200_adapters.safetensors.\n",
      "Iter 11210: Train loss 0.459, Learning Rate 1.000e-04, It/sec 1.352, Tokens/sec 232.558, Trained Tokens 1855199, Peak mem 19.940 GB\n",
      "Iter 11220: Train loss 0.506, Learning Rate 1.000e-04, It/sec 1.782, Tokens/sec 234.461, Trained Tokens 1856515, Peak mem 19.940 GB\n",
      "Iter 11230: Train loss 0.414, Learning Rate 1.000e-04, It/sec 1.448, Tokens/sec 257.669, Trained Tokens 1858294, Peak mem 19.940 GB\n",
      "Iter 11240: Train loss 0.477, Learning Rate 1.000e-04, It/sec 1.564, Tokens/sec 257.622, Trained Tokens 1859941, Peak mem 19.940 GB\n",
      "Iter 11250: Val loss 2.704, Val took 3.754s\n",
      "Iter 11250: Train loss 0.467, Learning Rate 1.000e-04, It/sec 13.952, Tokens/sec 2840.556, Trained Tokens 1861977, Peak mem 19.940 GB\n",
      "Iter 11260: Train loss 0.446, Learning Rate 1.000e-04, It/sec 1.743, Tokens/sec 250.695, Trained Tokens 1863415, Peak mem 19.940 GB\n",
      "Iter 11270: Train loss 0.423, Learning Rate 1.000e-04, It/sec 1.611, Tokens/sec 259.961, Trained Tokens 1865029, Peak mem 19.940 GB\n",
      "Iter 11280: Train loss 0.412, Learning Rate 1.000e-04, It/sec 1.660, Tokens/sec 246.116, Trained Tokens 1866512, Peak mem 19.940 GB\n",
      "Iter 11290: Train loss 0.618, Learning Rate 1.000e-04, It/sec 1.150, Tokens/sec 262.844, Trained Tokens 1868797, Peak mem 19.940 GB\n",
      "Iter 11300: Val loss 2.767, Val took 4.496s\n",
      "Iter 11300: Train loss 0.543, Learning Rate 1.000e-04, It/sec 11.419, Tokens/sec 3351.572, Trained Tokens 1871732, Peak mem 19.940 GB\n",
      "Iter 11300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011300_adapters.safetensors.\n",
      "Iter 11310: Train loss 0.562, Learning Rate 1.000e-04, It/sec 1.542, Tokens/sec 237.109, Trained Tokens 1873270, Peak mem 19.940 GB\n",
      "Iter 11320: Train loss 0.422, Learning Rate 1.000e-04, It/sec 1.538, Tokens/sec 251.787, Trained Tokens 1874907, Peak mem 19.940 GB\n",
      "Iter 11330: Train loss 0.444, Learning Rate 1.000e-04, It/sec 1.478, Tokens/sec 243.048, Trained Tokens 1876551, Peak mem 19.940 GB\n",
      "Iter 11340: Train loss 0.475, Learning Rate 1.000e-04, It/sec 1.205, Tokens/sec 269.206, Trained Tokens 1878785, Peak mem 19.940 GB\n",
      "Iter 11350: Val loss 2.962, Val took 4.705s\n",
      "Iter 11350: Train loss 0.460, Learning Rate 1.000e-04, It/sec 16.939, Tokens/sec 2073.327, Trained Tokens 1880009, Peak mem 19.940 GB\n",
      "Iter 11360: Train loss 0.433, Learning Rate 1.000e-04, It/sec 1.472, Tokens/sec 242.350, Trained Tokens 1881655, Peak mem 19.940 GB\n",
      "Iter 11370: Train loss 0.512, Learning Rate 1.000e-04, It/sec 1.462, Tokens/sec 250.266, Trained Tokens 1883367, Peak mem 19.940 GB\n",
      "Iter 11380: Train loss 0.462, Learning Rate 1.000e-04, It/sec 1.883, Tokens/sec 240.434, Trained Tokens 1884644, Peak mem 19.940 GB\n",
      "Iter 11390: Train loss 0.465, Learning Rate 1.000e-04, It/sec 1.635, Tokens/sec 246.192, Trained Tokens 1886150, Peak mem 19.940 GB\n",
      "Iter 11400: Val loss 2.684, Val took 4.361s\n",
      "Iter 11400: Train loss 0.470, Learning Rate 1.000e-04, It/sec 22.013, Tokens/sec 3687.222, Trained Tokens 1887825, Peak mem 19.940 GB\n",
      "Iter 11400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011400_adapters.safetensors.\n",
      "Iter 11410: Train loss 0.444, Learning Rate 1.000e-04, It/sec 1.429, Tokens/sec 241.651, Trained Tokens 1889516, Peak mem 19.940 GB\n",
      "Iter 11420: Train loss 0.457, Learning Rate 1.000e-04, It/sec 1.729, Tokens/sec 252.221, Trained Tokens 1890975, Peak mem 19.940 GB\n",
      "Iter 11430: Train loss 0.443, Learning Rate 1.000e-04, It/sec 1.662, Tokens/sec 246.047, Trained Tokens 1892455, Peak mem 19.940 GB\n",
      "Iter 11440: Train loss 0.491, Learning Rate 1.000e-04, It/sec 1.234, Tokens/sec 264.539, Trained Tokens 1894599, Peak mem 19.940 GB\n",
      "Iter 11450: Val loss 2.629, Val took 3.749s\n",
      "Iter 11450: Train loss 0.531, Learning Rate 1.000e-04, It/sec 29.456, Tokens/sec 4371.337, Trained Tokens 1896083, Peak mem 19.940 GB\n",
      "Iter 11460: Train loss 0.523, Learning Rate 1.000e-04, It/sec 1.728, Tokens/sec 245.899, Trained Tokens 1897506, Peak mem 19.940 GB\n",
      "Iter 11470: Train loss 0.592, Learning Rate 1.000e-04, It/sec 1.461, Tokens/sec 260.159, Trained Tokens 1899287, Peak mem 19.940 GB\n",
      "Iter 11480: Train loss 0.636, Learning Rate 1.000e-04, It/sec 0.978, Tokens/sec 228.208, Trained Tokens 1901620, Peak mem 19.940 GB\n",
      "Iter 11490: Train loss 0.453, Learning Rate 1.000e-04, It/sec 2.044, Tokens/sec 238.705, Trained Tokens 1902788, Peak mem 19.940 GB\n",
      "Iter 11500: Val loss 2.709, Val took 4.108s\n",
      "Iter 11500: Train loss 0.399, Learning Rate 1.000e-04, It/sec 9.143, Tokens/sec 1727.148, Trained Tokens 1904677, Peak mem 19.940 GB\n",
      "Iter 11500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011500_adapters.safetensors.\n",
      "Iter 11510: Train loss 0.454, Learning Rate 1.000e-04, It/sec 0.930, Tokens/sec 271.734, Trained Tokens 1907599, Peak mem 19.940 GB\n",
      "Iter 11520: Train loss 0.374, Learning Rate 1.000e-04, It/sec 1.130, Tokens/sec 260.636, Trained Tokens 1909906, Peak mem 19.940 GB\n",
      "Iter 11530: Train loss 0.412, Learning Rate 1.000e-04, It/sec 1.870, Tokens/sec 248.864, Trained Tokens 1911237, Peak mem 19.940 GB\n",
      "Iter 11540: Train loss 0.745, Learning Rate 1.000e-04, It/sec 0.909, Tokens/sec 254.797, Trained Tokens 1914039, Peak mem 19.940 GB\n",
      "Iter 11550: Val loss 2.589, Val took 3.390s\n",
      "Iter 11550: Train loss 0.419, Learning Rate 1.000e-04, It/sec 22.774, Tokens/sec 2946.958, Trained Tokens 1915333, Peak mem 19.940 GB\n",
      "Iter 11560: Train loss 0.524, Learning Rate 1.000e-04, It/sec 0.918, Tokens/sec 263.177, Trained Tokens 1918200, Peak mem 19.940 GB\n",
      "Iter 11570: Train loss 0.436, Learning Rate 1.000e-04, It/sec 1.625, Tokens/sec 267.302, Trained Tokens 1919845, Peak mem 19.940 GB\n",
      "Iter 11580: Train loss 0.428, Learning Rate 1.000e-04, It/sec 1.646, Tokens/sec 239.502, Trained Tokens 1921300, Peak mem 19.940 GB\n",
      "Iter 11590: Train loss 0.439, Learning Rate 1.000e-04, It/sec 1.359, Tokens/sec 265.138, Trained Tokens 1923251, Peak mem 19.940 GB\n",
      "Iter 11600: Val loss 2.553, Val took 3.341s\n",
      "Iter 11600: Train loss 0.419, Learning Rate 1.000e-04, It/sec 29.289, Tokens/sec 4296.668, Trained Tokens 1924718, Peak mem 19.940 GB\n",
      "Iter 11600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011600_adapters.safetensors.\n",
      "Iter 11610: Train loss 0.451, Learning Rate 1.000e-04, It/sec 1.496, Tokens/sec 220.383, Trained Tokens 1926191, Peak mem 19.940 GB\n",
      "Iter 11620: Train loss 0.407, Learning Rate 1.000e-04, It/sec 1.554, Tokens/sec 237.247, Trained Tokens 1927718, Peak mem 19.940 GB\n",
      "Iter 11630: Train loss 0.447, Learning Rate 1.000e-04, It/sec 1.599, Tokens/sec 237.338, Trained Tokens 1929202, Peak mem 19.940 GB\n",
      "Iter 11640: Train loss 0.419, Learning Rate 1.000e-04, It/sec 1.828, Tokens/sec 254.474, Trained Tokens 1930594, Peak mem 19.940 GB\n",
      "Iter 11650: Val loss 2.476, Val took 3.111s\n",
      "Iter 11650: Train loss 0.468, Learning Rate 1.000e-04, It/sec 29.176, Tokens/sec 3066.401, Trained Tokens 1931645, Peak mem 19.940 GB\n",
      "Iter 11660: Train loss 0.424, Learning Rate 1.000e-04, It/sec 1.629, Tokens/sec 231.044, Trained Tokens 1933063, Peak mem 19.940 GB\n",
      "Iter 11670: Train loss 0.391, Learning Rate 1.000e-04, It/sec 1.402, Tokens/sec 255.667, Trained Tokens 1934887, Peak mem 19.940 GB\n",
      "Iter 11680: Train loss 0.437, Learning Rate 1.000e-04, It/sec 1.678, Tokens/sec 253.379, Trained Tokens 1936397, Peak mem 19.940 GB\n",
      "Iter 11690: Train loss 0.445, Learning Rate 1.000e-04, It/sec 2.018, Tokens/sec 247.149, Trained Tokens 1937622, Peak mem 19.940 GB\n",
      "Iter 11700: Val loss 2.478, Val took 3.593s\n",
      "Iter 11700: Train loss 0.489, Learning Rate 1.000e-04, It/sec 7.833, Tokens/sec 1305.769, Trained Tokens 1939289, Peak mem 19.940 GB\n",
      "Iter 11700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011700_adapters.safetensors.\n",
      "Iter 11710: Train loss 0.400, Learning Rate 1.000e-04, It/sec 1.572, Tokens/sec 248.186, Trained Tokens 1940868, Peak mem 19.940 GB\n",
      "Iter 11720: Train loss 0.449, Learning Rate 1.000e-04, It/sec 1.608, Tokens/sec 265.071, Trained Tokens 1942516, Peak mem 19.940 GB\n",
      "Iter 11730: Train loss 0.432, Learning Rate 1.000e-04, It/sec 1.666, Tokens/sec 243.521, Trained Tokens 1943978, Peak mem 19.940 GB\n",
      "Iter 11740: Train loss 0.495, Learning Rate 1.000e-04, It/sec 2.091, Tokens/sec 249.074, Trained Tokens 1945169, Peak mem 19.940 GB\n",
      "Iter 11750: Val loss 2.814, Val took 3.739s\n",
      "Iter 11750: Train loss 0.451, Learning Rate 1.000e-04, It/sec 17.604, Tokens/sec 2323.770, Trained Tokens 1946489, Peak mem 19.940 GB\n",
      "Iter 11760: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.393, Tokens/sec 224.492, Trained Tokens 1948100, Peak mem 19.940 GB\n",
      "Iter 11770: Train loss 0.468, Learning Rate 1.000e-04, It/sec 1.400, Tokens/sec 252.483, Trained Tokens 1949904, Peak mem 19.940 GB\n",
      "Iter 11780: Train loss 0.512, Learning Rate 1.000e-04, It/sec 1.666, Tokens/sec 248.974, Trained Tokens 1951398, Peak mem 19.940 GB\n",
      "Iter 11790: Train loss 0.506, Learning Rate 1.000e-04, It/sec 2.229, Tokens/sec 236.962, Trained Tokens 1952461, Peak mem 19.940 GB\n",
      "Iter 11800: Val loss 3.085, Val took 3.697s\n",
      "Iter 11800: Train loss 0.424, Learning Rate 1.000e-04, It/sec 22.827, Tokens/sec 5330.166, Trained Tokens 1954796, Peak mem 19.940 GB\n",
      "Iter 11800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011800_adapters.safetensors.\n",
      "Iter 11810: Train loss 0.484, Learning Rate 1.000e-04, It/sec 1.490, Tokens/sec 236.763, Trained Tokens 1956385, Peak mem 19.940 GB\n",
      "Iter 11820: Train loss 0.499, Learning Rate 1.000e-04, It/sec 0.950, Tokens/sec 262.766, Trained Tokens 1959151, Peak mem 19.940 GB\n",
      "Iter 11830: Train loss 0.416, Learning Rate 1.000e-04, It/sec 1.708, Tokens/sec 251.043, Trained Tokens 1960621, Peak mem 19.940 GB\n",
      "Iter 11840: Train loss 0.485, Learning Rate 1.000e-04, It/sec 1.472, Tokens/sec 258.560, Trained Tokens 1962378, Peak mem 19.940 GB\n",
      "Iter 11850: Val loss 2.402, Val took 3.500s\n",
      "Iter 11850: Train loss 0.482, Learning Rate 1.000e-04, It/sec 11.859, Tokens/sec 1623.486, Trained Tokens 1963747, Peak mem 19.940 GB\n",
      "Iter 11860: Train loss 0.532, Learning Rate 1.000e-04, It/sec 2.084, Tokens/sec 230.332, Trained Tokens 1964852, Peak mem 19.940 GB\n",
      "Iter 11870: Train loss 0.557, Learning Rate 1.000e-04, It/sec 1.424, Tokens/sec 266.492, Trained Tokens 1966724, Peak mem 19.940 GB\n",
      "Iter 11880: Train loss 0.510, Learning Rate 1.000e-04, It/sec 1.906, Tokens/sec 253.849, Trained Tokens 1968056, Peak mem 19.940 GB\n",
      "Iter 11890: Train loss 0.483, Learning Rate 1.000e-04, It/sec 1.635, Tokens/sec 241.837, Trained Tokens 1969535, Peak mem 19.940 GB\n",
      "Iter 11900: Val loss 2.727, Val took 4.859s\n",
      "Iter 11900: Train loss 0.476, Learning Rate 1.000e-04, It/sec 27.006, Tokens/sec 3405.420, Trained Tokens 1970796, Peak mem 19.940 GB\n",
      "Iter 11900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0011900_adapters.safetensors.\n",
      "Iter 11910: Train loss 0.422, Learning Rate 1.000e-04, It/sec 1.356, Tokens/sec 265.359, Trained Tokens 1972753, Peak mem 19.940 GB\n",
      "Iter 11920: Train loss 0.397, Learning Rate 1.000e-04, It/sec 1.354, Tokens/sec 183.028, Trained Tokens 1974105, Peak mem 19.940 GB\n",
      "Iter 11930: Train loss 0.438, Learning Rate 1.000e-04, It/sec 1.660, Tokens/sec 234.440, Trained Tokens 1975517, Peak mem 19.940 GB\n",
      "Iter 11940: Train loss 0.472, Learning Rate 1.000e-04, It/sec 1.895, Tokens/sec 227.373, Trained Tokens 1976717, Peak mem 19.940 GB\n",
      "Iter 11950: Val loss 2.516, Val took 3.680s\n",
      "Iter 11950: Train loss 0.569, Learning Rate 1.000e-04, It/sec 8.417, Tokens/sec 2254.983, Trained Tokens 1979396, Peak mem 19.940 GB\n",
      "Iter 11960: Train loss 0.471, Learning Rate 1.000e-04, It/sec 1.929, Tokens/sec 207.558, Trained Tokens 1980472, Peak mem 19.940 GB\n",
      "Iter 11970: Train loss 0.437, Learning Rate 1.000e-04, It/sec 1.789, Tokens/sec 239.240, Trained Tokens 1981809, Peak mem 19.940 GB\n",
      "Iter 11980: Train loss 0.452, Learning Rate 1.000e-04, It/sec 1.279, Tokens/sec 247.865, Trained Tokens 1983747, Peak mem 19.940 GB\n",
      "Iter 11990: Train loss 0.502, Learning Rate 1.000e-04, It/sec 1.129, Tokens/sec 265.227, Trained Tokens 1986097, Peak mem 19.940 GB\n",
      "Iter 12000: Val loss 2.209, Val took 3.958s\n",
      "Iter 12000: Train loss 0.504, Learning Rate 1.000e-04, It/sec 19.338, Tokens/sec 2989.649, Trained Tokens 1987643, Peak mem 19.940 GB\n",
      "Iter 12000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012000_adapters.safetensors.\n",
      "Iter 12010: Train loss 0.376, Learning Rate 1.000e-04, It/sec 1.644, Tokens/sec 245.585, Trained Tokens 1989137, Peak mem 19.940 GB\n",
      "Iter 12020: Train loss 0.553, Learning Rate 1.000e-04, It/sec 1.157, Tokens/sec 258.857, Trained Tokens 1991374, Peak mem 19.940 GB\n",
      "Iter 12030: Train loss 0.481, Learning Rate 1.000e-04, It/sec 1.870, Tokens/sec 253.591, Trained Tokens 1992730, Peak mem 19.940 GB\n",
      "Iter 12040: Train loss 0.454, Learning Rate 1.000e-04, It/sec 1.905, Tokens/sec 251.312, Trained Tokens 1994049, Peak mem 19.940 GB\n",
      "Iter 12050: Val loss 2.296, Val took 3.549s\n",
      "Iter 12050: Train loss 0.447, Learning Rate 1.000e-04, It/sec 16.479, Tokens/sec 2465.268, Trained Tokens 1995545, Peak mem 19.940 GB\n",
      "Iter 12060: Train loss 0.463, Learning Rate 1.000e-04, It/sec 1.401, Tokens/sec 261.181, Trained Tokens 1997409, Peak mem 19.940 GB\n",
      "Iter 12070: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.516, Tokens/sec 260.419, Trained Tokens 1999127, Peak mem 19.940 GB\n",
      "Iter 12080: Train loss 0.422, Learning Rate 1.000e-04, It/sec 1.364, Tokens/sec 248.979, Trained Tokens 2000953, Peak mem 19.940 GB\n",
      "Iter 12090: Train loss 0.405, Learning Rate 1.000e-04, It/sec 1.482, Tokens/sec 246.941, Trained Tokens 2002619, Peak mem 19.940 GB\n",
      "Iter 12100: Val loss 2.482, Val took 3.287s\n",
      "Iter 12100: Train loss 0.484, Learning Rate 1.000e-04, It/sec 12.004, Tokens/sec 1505.243, Trained Tokens 2003873, Peak mem 19.940 GB\n",
      "Iter 12100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012100_adapters.safetensors.\n",
      "Iter 12110: Train loss 0.528, Learning Rate 1.000e-04, It/sec 1.408, Tokens/sec 253.616, Trained Tokens 2005674, Peak mem 19.940 GB\n",
      "Iter 12120: Train loss 0.430, Learning Rate 1.000e-04, It/sec 1.695, Tokens/sec 245.837, Trained Tokens 2007124, Peak mem 19.940 GB\n",
      "Iter 12130: Train loss 0.422, Learning Rate 1.000e-04, It/sec 1.492, Tokens/sec 256.986, Trained Tokens 2008847, Peak mem 19.940 GB\n",
      "Iter 12140: Train loss 0.445, Learning Rate 1.000e-04, It/sec 1.210, Tokens/sec 253.385, Trained Tokens 2010941, Peak mem 19.940 GB\n",
      "Iter 12150: Val loss 3.000, Val took 3.894s\n",
      "Iter 12150: Train loss 0.421, Learning Rate 1.000e-04, It/sec 12.265, Tokens/sec 2658.995, Trained Tokens 2013109, Peak mem 19.940 GB\n",
      "Iter 12160: Train loss 0.508, Learning Rate 1.000e-04, It/sec 1.754, Tokens/sec 251.669, Trained Tokens 2014544, Peak mem 19.940 GB\n",
      "Iter 12170: Train loss 0.456, Learning Rate 1.000e-04, It/sec 1.278, Tokens/sec 260.808, Trained Tokens 2016585, Peak mem 19.940 GB\n",
      "Iter 12180: Train loss 0.418, Learning Rate 1.000e-04, It/sec 1.549, Tokens/sec 252.383, Trained Tokens 2018214, Peak mem 19.940 GB\n",
      "Iter 12190: Train loss 0.411, Learning Rate 1.000e-04, It/sec 1.345, Tokens/sec 248.159, Trained Tokens 2020059, Peak mem 19.940 GB\n",
      "Iter 12200: Val loss 2.482, Val took 3.553s\n",
      "Iter 12200: Train loss 0.470, Learning Rate 1.000e-04, It/sec 10.504, Tokens/sec 2416.926, Trained Tokens 2022360, Peak mem 19.940 GB\n",
      "Iter 12200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012200_adapters.safetensors.\n",
      "Iter 12210: Train loss 0.501, Learning Rate 1.000e-04, It/sec 1.997, Tokens/sec 254.236, Trained Tokens 2023633, Peak mem 19.940 GB\n",
      "Iter 12220: Train loss 0.423, Learning Rate 1.000e-04, It/sec 1.617, Tokens/sec 261.275, Trained Tokens 2025249, Peak mem 19.940 GB\n",
      "Iter 12230: Train loss 0.446, Learning Rate 1.000e-04, It/sec 1.637, Tokens/sec 244.176, Trained Tokens 2026741, Peak mem 19.940 GB\n",
      "Iter 12240: Train loss 0.475, Learning Rate 1.000e-04, It/sec 2.056, Tokens/sec 254.133, Trained Tokens 2027977, Peak mem 19.940 GB\n",
      "Iter 12250: Val loss 2.596, Val took 2.715s\n",
      "Iter 12250: Train loss 0.459, Learning Rate 1.000e-04, It/sec 24.943, Tokens/sec 3970.937, Trained Tokens 2029569, Peak mem 19.940 GB\n",
      "Iter 12260: Train loss 0.625, Learning Rate 1.000e-04, It/sec 1.125, Tokens/sec 264.603, Trained Tokens 2031922, Peak mem 19.940 GB\n",
      "Iter 12270: Train loss 0.523, Learning Rate 1.000e-04, It/sec 1.597, Tokens/sec 253.846, Trained Tokens 2033512, Peak mem 19.940 GB\n",
      "Iter 12280: Train loss 0.506, Learning Rate 1.000e-04, It/sec 1.907, Tokens/sec 243.915, Trained Tokens 2034791, Peak mem 19.940 GB\n",
      "Iter 12290: Train loss 0.462, Learning Rate 1.000e-04, It/sec 1.572, Tokens/sec 243.736, Trained Tokens 2036341, Peak mem 19.940 GB\n",
      "Iter 12300: Val loss 2.759, Val took 2.760s\n",
      "Iter 12300: Train loss 0.471, Learning Rate 1.000e-04, It/sec 39.969, Tokens/sec 4432.542, Trained Tokens 2037450, Peak mem 19.940 GB\n",
      "Iter 12300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012300_adapters.safetensors.\n",
      "Iter 12310: Train loss 0.454, Learning Rate 1.000e-04, It/sec 1.367, Tokens/sec 248.080, Trained Tokens 2039265, Peak mem 19.940 GB\n",
      "Iter 12320: Train loss 0.457, Learning Rate 1.000e-04, It/sec 1.921, Tokens/sec 241.696, Trained Tokens 2040523, Peak mem 19.940 GB\n",
      "Iter 12330: Train loss 0.401, Learning Rate 1.000e-04, It/sec 1.601, Tokens/sec 251.179, Trained Tokens 2042092, Peak mem 19.940 GB\n",
      "Iter 12340: Train loss 0.441, Learning Rate 1.000e-04, It/sec 1.611, Tokens/sec 246.758, Trained Tokens 2043624, Peak mem 19.940 GB\n",
      "Iter 12350: Val loss 2.627, Val took 3.458s\n",
      "Iter 12350: Train loss 0.416, Learning Rate 1.000e-04, It/sec 9.960, Tokens/sec 1922.335, Trained Tokens 2045554, Peak mem 19.940 GB\n",
      "Iter 12360: Train loss 0.431, Learning Rate 1.000e-04, It/sec 1.588, Tokens/sec 248.631, Trained Tokens 2047120, Peak mem 19.940 GB\n",
      "Iter 12370: Train loss 0.504, Learning Rate 1.000e-04, It/sec 1.455, Tokens/sec 251.612, Trained Tokens 2048849, Peak mem 19.940 GB\n",
      "Iter 12380: Train loss 0.447, Learning Rate 1.000e-04, It/sec 2.142, Tokens/sec 247.438, Trained Tokens 2050004, Peak mem 19.940 GB\n",
      "Iter 12390: Train loss 0.469, Learning Rate 1.000e-04, It/sec 1.444, Tokens/sec 242.139, Trained Tokens 2051681, Peak mem 19.940 GB\n",
      "Iter 12400: Val loss 2.821, Val took 3.506s\n",
      "Iter 12400: Train loss 0.408, Learning Rate 1.000e-04, It/sec 8.319, Tokens/sec 1231.194, Trained Tokens 2053161, Peak mem 19.940 GB\n",
      "Iter 12400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012400_adapters.safetensors.\n",
      "Iter 12410: Train loss 0.399, Learning Rate 1.000e-04, It/sec 1.537, Tokens/sec 241.482, Trained Tokens 2054732, Peak mem 19.940 GB\n",
      "Iter 12420: Train loss 0.416, Learning Rate 1.000e-04, It/sec 1.211, Tokens/sec 264.399, Trained Tokens 2056915, Peak mem 19.940 GB\n",
      "Iter 12430: Train loss 0.429, Learning Rate 1.000e-04, It/sec 1.818, Tokens/sec 269.025, Trained Tokens 2058395, Peak mem 19.940 GB\n",
      "Iter 12440: Train loss 0.408, Learning Rate 1.000e-04, It/sec 1.371, Tokens/sec 258.360, Trained Tokens 2060279, Peak mem 19.940 GB\n",
      "Iter 12450: Val loss 2.653, Val took 3.269s\n",
      "Iter 12450: Train loss 0.458, Learning Rate 1.000e-04, It/sec 16.663, Tokens/sec 2071.215, Trained Tokens 2061522, Peak mem 19.940 GB\n",
      "Iter 12460: Train loss 0.431, Learning Rate 1.000e-04, It/sec 1.244, Tokens/sec 259.229, Trained Tokens 2063606, Peak mem 19.940 GB\n",
      "Iter 12470: Train loss 0.493, Learning Rate 1.000e-04, It/sec 1.270, Tokens/sec 267.277, Trained Tokens 2065710, Peak mem 19.940 GB\n",
      "Iter 12480: Train loss 0.482, Learning Rate 1.000e-04, It/sec 1.837, Tokens/sec 250.780, Trained Tokens 2067075, Peak mem 19.940 GB\n",
      "Iter 12490: Train loss 0.390, Learning Rate 1.000e-04, It/sec 1.677, Tokens/sec 240.854, Trained Tokens 2068511, Peak mem 19.940 GB\n",
      "Iter 12500: Val loss 2.599, Val took 3.966s\n",
      "Iter 12500: Train loss 0.448, Learning Rate 1.000e-04, It/sec 16.155, Tokens/sec 2507.324, Trained Tokens 2070063, Peak mem 19.940 GB\n",
      "Iter 12500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012500_adapters.safetensors.\n",
      "Iter 12510: Train loss 0.569, Learning Rate 1.000e-04, It/sec 1.210, Tokens/sec 259.064, Trained Tokens 2072204, Peak mem 19.940 GB\n",
      "Iter 12520: Train loss 0.372, Learning Rate 1.000e-04, It/sec 1.566, Tokens/sec 269.599, Trained Tokens 2073926, Peak mem 19.940 GB\n",
      "Iter 12530: Train loss 0.420, Learning Rate 1.000e-04, It/sec 1.858, Tokens/sec 262.341, Trained Tokens 2075338, Peak mem 19.940 GB\n",
      "Iter 12540: Train loss 0.415, Learning Rate 1.000e-04, It/sec 1.576, Tokens/sec 261.406, Trained Tokens 2076997, Peak mem 19.940 GB\n",
      "Iter 12550: Val loss 2.333, Val took 3.367s\n",
      "Iter 12550: Train loss 0.416, Learning Rate 1.000e-04, It/sec 16.537, Tokens/sec 2922.083, Trained Tokens 2078764, Peak mem 19.940 GB\n",
      "Iter 12560: Train loss 0.453, Learning Rate 1.000e-04, It/sec 1.690, Tokens/sec 243.765, Trained Tokens 2080206, Peak mem 19.940 GB\n",
      "Iter 12570: Train loss 0.452, Learning Rate 1.000e-04, It/sec 1.801, Tokens/sec 251.036, Trained Tokens 2081600, Peak mem 19.940 GB\n",
      "Iter 12580: Train loss 0.530, Learning Rate 1.000e-04, It/sec 1.238, Tokens/sec 271.298, Trained Tokens 2083791, Peak mem 19.940 GB\n",
      "Iter 12590: Train loss 0.426, Learning Rate 1.000e-04, It/sec 1.512, Tokens/sec 248.967, Trained Tokens 2085438, Peak mem 19.940 GB\n",
      "Iter 12600: Val loss 2.488, Val took 4.060s\n",
      "Iter 12600: Train loss 0.496, Learning Rate 1.000e-04, It/sec 12.209, Tokens/sec 1620.200, Trained Tokens 2086765, Peak mem 19.940 GB\n",
      "Iter 12600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012600_adapters.safetensors.\n",
      "Iter 12610: Train loss 0.417, Learning Rate 1.000e-04, It/sec 1.713, Tokens/sec 248.105, Trained Tokens 2088213, Peak mem 19.940 GB\n",
      "Iter 12620: Train loss 0.441, Learning Rate 1.000e-04, It/sec 1.405, Tokens/sec 253.377, Trained Tokens 2090017, Peak mem 19.940 GB\n",
      "Iter 12630: Train loss 0.483, Learning Rate 1.000e-04, It/sec 1.650, Tokens/sec 249.408, Trained Tokens 2091529, Peak mem 19.940 GB\n",
      "Iter 12640: Train loss 0.438, Learning Rate 1.000e-04, It/sec 1.971, Tokens/sec 258.357, Trained Tokens 2092840, Peak mem 19.940 GB\n",
      "Iter 12650: Val loss 2.706, Val took 3.629s\n",
      "Iter 12650: Train loss 0.504, Learning Rate 1.000e-04, It/sec 8.196, Tokens/sec 1031.847, Trained Tokens 2094099, Peak mem 19.940 GB\n",
      "Iter 12660: Train loss 0.595, Learning Rate 1.000e-04, It/sec 0.771, Tokens/sec 251.218, Trained Tokens 2097356, Peak mem 19.940 GB\n",
      "Iter 12670: Train loss 0.463, Learning Rate 1.000e-04, It/sec 2.113, Tokens/sec 249.750, Trained Tokens 2098538, Peak mem 19.940 GB\n",
      "Iter 12680: Train loss 0.551, Learning Rate 1.000e-04, It/sec 1.098, Tokens/sec 260.991, Trained Tokens 2100916, Peak mem 19.940 GB\n",
      "Iter 12690: Train loss 0.516, Learning Rate 1.000e-04, It/sec 2.039, Tokens/sec 244.896, Trained Tokens 2102117, Peak mem 19.940 GB\n",
      "Iter 12700: Val loss 3.021, Val took 3.513s\n",
      "Iter 12700: Train loss 0.479, Learning Rate 1.000e-04, It/sec 28.544, Tokens/sec 5097.923, Trained Tokens 2103903, Peak mem 19.940 GB\n",
      "Iter 12700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012700_adapters.safetensors.\n",
      "Iter 12710: Train loss 0.476, Learning Rate 1.000e-04, It/sec 1.436, Tokens/sec 209.875, Trained Tokens 2105365, Peak mem 19.940 GB\n",
      "Iter 12720: Train loss 0.352, Learning Rate 1.000e-04, It/sec 1.353, Tokens/sec 259.367, Trained Tokens 2107282, Peak mem 19.940 GB\n",
      "Iter 12730: Train loss 0.353, Learning Rate 1.000e-04, It/sec 1.512, Tokens/sec 260.654, Trained Tokens 2109006, Peak mem 19.940 GB\n",
      "Iter 12740: Train loss 0.383, Learning Rate 1.000e-04, It/sec 1.833, Tokens/sec 246.410, Trained Tokens 2110350, Peak mem 19.940 GB\n",
      "Iter 12750: Val loss 2.507, Val took 3.265s\n",
      "Iter 12750: Train loss 0.410, Learning Rate 1.000e-04, It/sec 28.200, Tokens/sec 5067.600, Trained Tokens 2112147, Peak mem 19.940 GB\n",
      "Iter 12760: Train loss 0.446, Learning Rate 1.000e-04, It/sec 1.455, Tokens/sec 222.516, Trained Tokens 2113676, Peak mem 19.940 GB\n",
      "Iter 12770: Train loss 0.464, Learning Rate 1.000e-04, It/sec 1.466, Tokens/sec 252.152, Trained Tokens 2115396, Peak mem 19.940 GB\n",
      "Iter 12780: Train loss 0.429, Learning Rate 1.000e-04, It/sec 1.400, Tokens/sec 242.142, Trained Tokens 2117126, Peak mem 19.940 GB\n",
      "Iter 12790: Train loss 0.389, Learning Rate 1.000e-04, It/sec 1.434, Tokens/sec 241.521, Trained Tokens 2118810, Peak mem 19.940 GB\n",
      "Iter 12800: Val loss 3.433, Val took 6.127s\n",
      "Iter 12800: Train loss 0.453, Learning Rate 1.000e-04, It/sec 16.636, Tokens/sec 2633.480, Trained Tokens 2120393, Peak mem 19.940 GB\n",
      "Iter 12800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012800_adapters.safetensors.\n",
      "Iter 12810: Train loss 0.487, Learning Rate 1.000e-04, It/sec 1.083, Tokens/sec 248.537, Trained Tokens 2122687, Peak mem 19.940 GB\n",
      "Iter 12820: Train loss 0.396, Learning Rate 1.000e-04, It/sec 1.500, Tokens/sec 267.941, Trained Tokens 2124473, Peak mem 19.940 GB\n",
      "Iter 12830: Train loss 0.381, Learning Rate 1.000e-04, It/sec 1.466, Tokens/sec 243.615, Trained Tokens 2126135, Peak mem 19.940 GB\n",
      "Iter 12840: Train loss 0.533, Learning Rate 1.000e-04, It/sec 2.194, Tokens/sec 239.333, Trained Tokens 2127226, Peak mem 19.940 GB\n",
      "Iter 12850: Val loss 2.373, Val took 3.788s\n",
      "Iter 12850: Train loss 0.527, Learning Rate 1.000e-04, It/sec 15.724, Tokens/sec 4001.632, Trained Tokens 2129771, Peak mem 19.940 GB\n",
      "Iter 12860: Train loss 0.430, Learning Rate 1.000e-04, It/sec 1.080, Tokens/sec 214.934, Trained Tokens 2131761, Peak mem 19.940 GB\n",
      "Iter 12870: Train loss 0.427, Learning Rate 1.000e-04, It/sec 1.835, Tokens/sec 256.921, Trained Tokens 2133161, Peak mem 19.940 GB\n",
      "Iter 12880: Train loss 0.409, Learning Rate 1.000e-04, It/sec 1.442, Tokens/sec 252.299, Trained Tokens 2134911, Peak mem 19.940 GB\n",
      "Iter 12890: Train loss 0.431, Learning Rate 1.000e-04, It/sec 1.766, Tokens/sec 238.021, Trained Tokens 2136259, Peak mem 19.940 GB\n",
      "Iter 12900: Val loss 2.412, Val took 2.917s\n",
      "Iter 12900: Train loss 0.437, Learning Rate 1.000e-04, It/sec 14.329, Tokens/sec 1917.215, Trained Tokens 2137597, Peak mem 19.940 GB\n",
      "Iter 12900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0012900_adapters.safetensors.\n",
      "Iter 12910: Train loss 0.572, Learning Rate 1.000e-04, It/sec 1.021, Tokens/sec 252.525, Trained Tokens 2140071, Peak mem 19.940 GB\n",
      "Iter 12920: Train loss 0.491, Learning Rate 1.000e-04, It/sec 1.877, Tokens/sec 249.891, Trained Tokens 2141402, Peak mem 19.940 GB\n",
      "Iter 12930: Train loss 0.383, Learning Rate 1.000e-04, It/sec 1.571, Tokens/sec 255.721, Trained Tokens 2143030, Peak mem 19.940 GB\n",
      "Iter 12940: Train loss 0.397, Learning Rate 1.000e-04, It/sec 1.442, Tokens/sec 251.695, Trained Tokens 2144775, Peak mem 19.940 GB\n",
      "Iter 12950: Val loss 2.749, Val took 3.921s\n",
      "Iter 12950: Train loss 0.505, Learning Rate 1.000e-04, It/sec 3.502, Tokens/sec 447.501, Trained Tokens 2146053, Peak mem 19.940 GB\n",
      "Iter 12960: Train loss 0.426, Learning Rate 1.000e-04, It/sec 1.510, Tokens/sec 243.130, Trained Tokens 2147663, Peak mem 19.940 GB\n",
      "Iter 12970: Train loss 0.415, Learning Rate 1.000e-04, It/sec 1.199, Tokens/sec 261.063, Trained Tokens 2149840, Peak mem 19.940 GB\n",
      "Iter 12980: Train loss 0.439, Learning Rate 1.000e-04, It/sec 1.517, Tokens/sec 266.072, Trained Tokens 2151594, Peak mem 19.940 GB\n",
      "Iter 12990: Train loss 0.471, Learning Rate 1.000e-04, It/sec 2.004, Tokens/sec 253.967, Trained Tokens 2152861, Peak mem 19.940 GB\n",
      "Iter 13000: Val loss 2.523, Val took 3.462s\n",
      "Iter 13000: Train loss 0.466, Learning Rate 1.000e-04, It/sec 21.802, Tokens/sec 3023.926, Trained Tokens 2154248, Peak mem 19.940 GB\n",
      "Iter 13000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013000_adapters.safetensors.\n",
      "Iter 13010: Train loss 0.558, Learning Rate 1.000e-04, It/sec 1.110, Tokens/sec 249.156, Trained Tokens 2156492, Peak mem 19.940 GB\n",
      "Iter 13020: Train loss 0.546, Learning Rate 1.000e-04, It/sec 1.314, Tokens/sec 272.895, Trained Tokens 2158569, Peak mem 19.940 GB\n",
      "Iter 13030: Train loss 0.491, Learning Rate 1.000e-04, It/sec 1.896, Tokens/sec 215.056, Trained Tokens 2159703, Peak mem 19.940 GB\n",
      "Iter 13040: Train loss 0.460, Learning Rate 1.000e-04, It/sec 1.507, Tokens/sec 262.516, Trained Tokens 2161445, Peak mem 19.940 GB\n",
      "Iter 13050: Val loss 2.476, Val took 3.434s\n",
      "Iter 13050: Train loss 0.426, Learning Rate 1.000e-04, It/sec 11.213, Tokens/sec 1951.036, Trained Tokens 2163185, Peak mem 19.940 GB\n",
      "Iter 13060: Train loss 0.514, Learning Rate 1.000e-04, It/sec 1.747, Tokens/sec 248.563, Trained Tokens 2164608, Peak mem 19.940 GB\n",
      "Iter 13070: Train loss 0.442, Learning Rate 1.000e-04, It/sec 2.005, Tokens/sec 245.204, Trained Tokens 2165831, Peak mem 19.940 GB\n",
      "Iter 13080: Train loss 0.475, Learning Rate 1.000e-04, It/sec 1.674, Tokens/sec 240.286, Trained Tokens 2167266, Peak mem 19.940 GB\n",
      "Iter 13090: Train loss 0.458, Learning Rate 1.000e-04, It/sec 2.156, Tokens/sec 240.847, Trained Tokens 2168383, Peak mem 19.940 GB\n",
      "Iter 13100: Val loss 2.681, Val took 3.058s\n",
      "Iter 13100: Train loss 0.457, Learning Rate 1.000e-04, It/sec 15.973, Tokens/sec 2213.869, Trained Tokens 2169769, Peak mem 19.940 GB\n",
      "Iter 13100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013100_adapters.safetensors.\n",
      "Iter 13110: Train loss 0.433, Learning Rate 1.000e-04, It/sec 1.525, Tokens/sec 244.990, Trained Tokens 2171376, Peak mem 19.940 GB\n",
      "Iter 13120: Train loss 0.523, Learning Rate 1.000e-04, It/sec 1.325, Tokens/sec 252.319, Trained Tokens 2173280, Peak mem 19.940 GB\n",
      "Iter 13130: Train loss 0.404, Learning Rate 1.000e-04, It/sec 1.620, Tokens/sec 227.122, Trained Tokens 2174682, Peak mem 19.940 GB\n",
      "Iter 13140: Train loss 0.383, Learning Rate 1.000e-04, It/sec 1.224, Tokens/sec 247.773, Trained Tokens 2176706, Peak mem 19.940 GB\n",
      "Iter 13150: Val loss 2.702, Val took 3.257s\n",
      "Iter 13150: Train loss 0.463, Learning Rate 1.000e-04, It/sec 27.360, Tokens/sec 3031.450, Trained Tokens 2177814, Peak mem 19.940 GB\n",
      "Iter 13160: Train loss 0.400, Learning Rate 1.000e-04, It/sec 1.391, Tokens/sec 227.176, Trained Tokens 2179447, Peak mem 19.940 GB\n",
      "Iter 13170: Train loss 0.441, Learning Rate 1.000e-04, It/sec 2.028, Tokens/sec 239.526, Trained Tokens 2180628, Peak mem 19.940 GB\n",
      "Iter 13180: Train loss 0.450, Learning Rate 1.000e-04, It/sec 2.116, Tokens/sec 226.872, Trained Tokens 2181700, Peak mem 19.940 GB\n",
      "Iter 13190: Train loss 0.423, Learning Rate 1.000e-04, It/sec 1.336, Tokens/sec 264.206, Trained Tokens 2183677, Peak mem 19.940 GB\n",
      "Iter 13200: Val loss 2.600, Val took 3.955s\n",
      "Iter 13200: Train loss 0.397, Learning Rate 1.000e-04, It/sec 12.549, Tokens/sec 1756.905, Trained Tokens 2185077, Peak mem 19.940 GB\n",
      "Iter 13200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013200_adapters.safetensors.\n",
      "Iter 13210: Train loss 0.452, Learning Rate 1.000e-04, It/sec 1.522, Tokens/sec 216.863, Trained Tokens 2186502, Peak mem 19.940 GB\n",
      "Iter 13220: Train loss 0.561, Learning Rate 1.000e-04, It/sec 1.165, Tokens/sec 259.403, Trained Tokens 2188728, Peak mem 19.940 GB\n",
      "Iter 13230: Train loss 0.467, Learning Rate 1.000e-04, It/sec 1.445, Tokens/sec 244.512, Trained Tokens 2190420, Peak mem 19.940 GB\n",
      "Iter 13240: Train loss 0.494, Learning Rate 1.000e-04, It/sec 1.023, Tokens/sec 257.195, Trained Tokens 2192933, Peak mem 19.940 GB\n",
      "Iter 13250: Val loss 2.466, Val took 3.873s\n",
      "Iter 13250: Train loss 0.401, Learning Rate 1.000e-04, It/sec 13.144, Tokens/sec 1980.811, Trained Tokens 2194440, Peak mem 19.940 GB\n",
      "Iter 13260: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.632, Tokens/sec 257.797, Trained Tokens 2196020, Peak mem 19.940 GB\n",
      "Iter 13270: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.513, Tokens/sec 261.195, Trained Tokens 2197746, Peak mem 19.940 GB\n",
      "Iter 13280: Train loss 0.548, Learning Rate 1.000e-04, It/sec 0.742, Tokens/sec 262.781, Trained Tokens 2201287, Peak mem 19.940 GB\n",
      "Iter 13290: Train loss 0.447, Learning Rate 1.000e-04, It/sec 1.738, Tokens/sec 253.172, Trained Tokens 2202744, Peak mem 19.940 GB\n",
      "Iter 13300: Val loss 2.450, Val took 3.516s\n",
      "Iter 13300: Train loss 0.400, Learning Rate 1.000e-04, It/sec 12.342, Tokens/sec 1835.278, Trained Tokens 2204231, Peak mem 19.940 GB\n",
      "Iter 13300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013300_adapters.safetensors.\n",
      "Iter 13310: Train loss 0.490, Learning Rate 1.000e-04, It/sec 1.942, Tokens/sec 245.252, Trained Tokens 2205494, Peak mem 19.940 GB\n",
      "Iter 13320: Train loss 0.403, Learning Rate 1.000e-04, It/sec 1.635, Tokens/sec 256.847, Trained Tokens 2207065, Peak mem 19.940 GB\n",
      "Iter 13330: Train loss 0.405, Learning Rate 1.000e-04, It/sec 1.847, Tokens/sec 255.242, Trained Tokens 2208447, Peak mem 19.940 GB\n",
      "Iter 13340: Train loss 0.417, Learning Rate 1.000e-04, It/sec 1.267, Tokens/sec 255.996, Trained Tokens 2210467, Peak mem 19.940 GB\n",
      "Iter 13350: Val loss 2.506, Val took 3.774s\n",
      "Iter 13350: Train loss 0.450, Learning Rate 1.000e-04, It/sec 16.905, Tokens/sec 2084.413, Trained Tokens 2211700, Peak mem 19.940 GB\n",
      "Iter 13360: Train loss 0.351, Learning Rate 1.000e-04, It/sec 1.062, Tokens/sec 262.838, Trained Tokens 2214174, Peak mem 19.940 GB\n",
      "Iter 13370: Train loss 0.435, Learning Rate 1.000e-04, It/sec 1.470, Tokens/sec 227.809, Trained Tokens 2215724, Peak mem 19.940 GB\n",
      "Iter 13380: Train loss 0.573, Learning Rate 1.000e-04, It/sec 1.130, Tokens/sec 251.280, Trained Tokens 2217948, Peak mem 19.940 GB\n",
      "Iter 13390: Train loss 0.441, Learning Rate 1.000e-04, It/sec 1.661, Tokens/sec 233.474, Trained Tokens 2219354, Peak mem 19.940 GB\n",
      "Iter 13400: Val loss 2.549, Val took 3.783s\n",
      "Iter 13400: Train loss 0.448, Learning Rate 1.000e-04, It/sec 33.594, Tokens/sec 5344.740, Trained Tokens 2220945, Peak mem 19.940 GB\n",
      "Iter 13400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013400_adapters.safetensors.\n",
      "Iter 13410: Train loss 0.430, Learning Rate 1.000e-04, It/sec 1.814, Tokens/sec 252.996, Trained Tokens 2222340, Peak mem 19.940 GB\n",
      "Iter 13420: Train loss 0.461, Learning Rate 1.000e-04, It/sec 1.620, Tokens/sec 244.851, Trained Tokens 2223851, Peak mem 19.940 GB\n",
      "Iter 13430: Train loss 0.478, Learning Rate 1.000e-04, It/sec 1.809, Tokens/sec 238.726, Trained Tokens 2225171, Peak mem 19.940 GB\n",
      "Iter 13440: Train loss 0.420, Learning Rate 1.000e-04, It/sec 1.487, Tokens/sec 254.709, Trained Tokens 2226884, Peak mem 19.940 GB\n",
      "Iter 13450: Val loss 2.792, Val took 5.357s\n",
      "Iter 13450: Train loss 0.413, Learning Rate 1.000e-04, It/sec 12.208, Tokens/sec 2033.910, Trained Tokens 2228550, Peak mem 19.940 GB\n",
      "Iter 13460: Train loss 0.469, Learning Rate 1.000e-04, It/sec 1.502, Tokens/sec 266.552, Trained Tokens 2230325, Peak mem 19.940 GB\n",
      "Iter 13470: Train loss 0.465, Learning Rate 1.000e-04, It/sec 1.887, Tokens/sec 253.240, Trained Tokens 2231667, Peak mem 19.940 GB\n",
      "Iter 13480: Train loss 0.442, Learning Rate 1.000e-04, It/sec 1.902, Tokens/sec 252.390, Trained Tokens 2232994, Peak mem 19.940 GB\n",
      "Iter 13490: Train loss 0.431, Learning Rate 1.000e-04, It/sec 1.557, Tokens/sec 256.513, Trained Tokens 2234642, Peak mem 19.940 GB\n",
      "Iter 13500: Val loss 2.637, Val took 3.826s\n",
      "Iter 13500: Train loss 0.491, Learning Rate 1.000e-04, It/sec 27.551, Tokens/sec 4835.162, Trained Tokens 2236397, Peak mem 19.940 GB\n",
      "Iter 13500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013500_adapters.safetensors.\n",
      "Iter 13510: Train loss 0.486, Learning Rate 1.000e-04, It/sec 1.933, Tokens/sec 242.801, Trained Tokens 2237653, Peak mem 19.940 GB\n",
      "Iter 13520: Train loss 0.537, Learning Rate 1.000e-04, It/sec 1.978, Tokens/sec 250.471, Trained Tokens 2238919, Peak mem 19.940 GB\n",
      "Iter 13530: Train loss 0.502, Learning Rate 1.000e-04, It/sec 1.203, Tokens/sec 273.738, Trained Tokens 2241195, Peak mem 19.940 GB\n",
      "Iter 13540: Train loss 0.540, Learning Rate 1.000e-04, It/sec 1.063, Tokens/sec 272.843, Trained Tokens 2243761, Peak mem 19.940 GB\n",
      "Iter 13550: Val loss 2.442, Val took 3.187s\n",
      "Iter 13550: Train loss 0.443, Learning Rate 1.000e-04, It/sec 10.451, Tokens/sec 1599.054, Trained Tokens 2245291, Peak mem 19.940 GB\n",
      "Iter 13560: Train loss 0.461, Learning Rate 1.000e-04, It/sec 1.444, Tokens/sec 230.246, Trained Tokens 2246885, Peak mem 19.940 GB\n",
      "Iter 13570: Train loss 0.428, Learning Rate 1.000e-04, It/sec 1.614, Tokens/sec 242.651, Trained Tokens 2248388, Peak mem 19.940 GB\n",
      "Iter 13580: Train loss 0.421, Learning Rate 1.000e-04, It/sec 1.546, Tokens/sec 251.178, Trained Tokens 2250013, Peak mem 19.940 GB\n",
      "Iter 13590: Train loss 0.383, Learning Rate 1.000e-04, It/sec 1.480, Tokens/sec 237.934, Trained Tokens 2251621, Peak mem 19.940 GB\n",
      "Iter 13600: Val loss 2.813, Val took 4.145s\n",
      "Iter 13600: Train loss 0.361, Learning Rate 1.000e-04, It/sec 11.148, Tokens/sec 1795.959, Trained Tokens 2253232, Peak mem 19.940 GB\n",
      "Iter 13600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013600_adapters.safetensors.\n",
      "Iter 13610: Train loss 0.437, Learning Rate 1.000e-04, It/sec 1.592, Tokens/sec 238.145, Trained Tokens 2254728, Peak mem 19.940 GB\n",
      "Iter 13620: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.510, Tokens/sec 260.923, Trained Tokens 2256456, Peak mem 19.940 GB\n",
      "Iter 13630: Train loss 0.415, Learning Rate 1.000e-04, It/sec 1.605, Tokens/sec 240.877, Trained Tokens 2257957, Peak mem 19.940 GB\n",
      "Iter 13640: Train loss 0.414, Learning Rate 1.000e-04, It/sec 1.540, Tokens/sec 256.521, Trained Tokens 2259623, Peak mem 19.940 GB\n",
      "Iter 13650: Val loss 2.423, Val took 2.671s\n",
      "Iter 13650: Train loss 0.455, Learning Rate 1.000e-04, It/sec 18.280, Tokens/sec 2248.464, Trained Tokens 2260853, Peak mem 19.940 GB\n",
      "Iter 13660: Train loss 0.476, Learning Rate 1.000e-04, It/sec 1.476, Tokens/sec 263.071, Trained Tokens 2262635, Peak mem 19.940 GB\n",
      "Iter 13670: Train loss 0.388, Learning Rate 1.000e-04, It/sec 1.430, Tokens/sec 250.231, Trained Tokens 2264385, Peak mem 19.940 GB\n",
      "Iter 13680: Train loss 0.477, Learning Rate 1.000e-04, It/sec 1.941, Tokens/sec 246.945, Trained Tokens 2265657, Peak mem 19.940 GB\n",
      "Iter 13690: Train loss 0.469, Learning Rate 1.000e-04, It/sec 1.327, Tokens/sec 263.215, Trained Tokens 2267640, Peak mem 19.940 GB\n",
      "Iter 13700: Val loss 2.770, Val took 3.613s\n",
      "Iter 13700: Train loss 0.466, Learning Rate 1.000e-04, It/sec 3.488, Tokens/sec 492.793, Trained Tokens 2269053, Peak mem 19.940 GB\n",
      "Iter 13700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013700_adapters.safetensors.\n",
      "Iter 13710: Train loss 0.479, Learning Rate 1.000e-04, It/sec 1.989, Tokens/sec 226.963, Trained Tokens 2270194, Peak mem 19.940 GB\n",
      "Iter 13720: Train loss 0.399, Learning Rate 1.000e-04, It/sec 1.460, Tokens/sec 244.645, Trained Tokens 2271870, Peak mem 19.940 GB\n",
      "Iter 13730: Train loss 0.438, Learning Rate 1.000e-04, It/sec 1.562, Tokens/sec 250.391, Trained Tokens 2273473, Peak mem 19.940 GB\n",
      "Iter 13740: Train loss 0.450, Learning Rate 1.000e-04, It/sec 1.825, Tokens/sec 245.335, Trained Tokens 2274817, Peak mem 19.940 GB\n",
      "Iter 13750: Val loss 2.676, Val took 4.726s\n",
      "Iter 13750: Train loss 0.507, Learning Rate 1.000e-04, It/sec 5.391, Tokens/sec 874.406, Trained Tokens 2276439, Peak mem 19.940 GB\n",
      "Iter 13760: Train loss 0.402, Learning Rate 1.000e-04, It/sec 1.229, Tokens/sec 258.968, Trained Tokens 2278547, Peak mem 19.940 GB\n",
      "Iter 13770: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.476, Tokens/sec 261.951, Trained Tokens 2280322, Peak mem 19.940 GB\n",
      "Iter 13780: Train loss 0.425, Learning Rate 1.000e-04, It/sec 1.703, Tokens/sec 251.869, Trained Tokens 2281801, Peak mem 19.940 GB\n",
      "Iter 13790: Train loss 0.408, Learning Rate 1.000e-04, It/sec 1.698, Tokens/sec 243.013, Trained Tokens 2283232, Peak mem 19.940 GB\n",
      "Iter 13800: Val loss 2.949, Val took 3.831s\n",
      "Iter 13800: Train loss 0.508, Learning Rate 1.000e-04, It/sec 29.571, Tokens/sec 4196.182, Trained Tokens 2284651, Peak mem 19.940 GB\n",
      "Iter 13800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013800_adapters.safetensors.\n",
      "Iter 13810: Train loss 0.530, Learning Rate 1.000e-04, It/sec 1.074, Tokens/sec 251.990, Trained Tokens 2286998, Peak mem 19.940 GB\n",
      "Iter 13820: Train loss 0.473, Learning Rate 1.000e-04, It/sec 0.935, Tokens/sec 270.832, Trained Tokens 2289896, Peak mem 19.940 GB\n",
      "Iter 13830: Train loss 0.431, Learning Rate 1.000e-04, It/sec 1.775, Tokens/sec 253.510, Trained Tokens 2291324, Peak mem 19.940 GB\n",
      "Iter 13840: Train loss 0.415, Learning Rate 1.000e-04, It/sec 1.314, Tokens/sec 266.667, Trained Tokens 2293353, Peak mem 19.940 GB\n",
      "Iter 13850: Val loss 3.212, Val took 3.402s\n",
      "Iter 13850: Train loss 0.366, Learning Rate 1.000e-04, It/sec 10.260, Tokens/sec 1977.076, Trained Tokens 2295280, Peak mem 19.940 GB\n",
      "Iter 13860: Train loss 0.463, Learning Rate 1.000e-04, It/sec 1.414, Tokens/sec 255.359, Trained Tokens 2297086, Peak mem 19.940 GB\n",
      "Iter 13870: Train loss 0.415, Learning Rate 1.000e-04, It/sec 1.484, Tokens/sec 253.106, Trained Tokens 2298791, Peak mem 19.940 GB\n",
      "Iter 13880: Train loss 0.400, Learning Rate 1.000e-04, It/sec 1.735, Tokens/sec 259.267, Trained Tokens 2300285, Peak mem 19.940 GB\n",
      "Iter 13890: Train loss 0.473, Learning Rate 1.000e-04, It/sec 2.146, Tokens/sec 248.350, Trained Tokens 2301442, Peak mem 19.940 GB\n",
      "Iter 13900: Val loss 2.996, Val took 3.522s\n",
      "Iter 13900: Train loss 0.502, Learning Rate 1.000e-04, It/sec 23.211, Tokens/sec 3015.051, Trained Tokens 2302741, Peak mem 19.940 GB\n",
      "Iter 13900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0013900_adapters.safetensors.\n",
      "Iter 13910: Train loss 0.483, Learning Rate 1.000e-04, It/sec 1.582, Tokens/sec 199.029, Trained Tokens 2303999, Peak mem 19.940 GB\n",
      "Iter 13920: Train loss 0.456, Learning Rate 1.000e-04, It/sec 1.706, Tokens/sec 242.306, Trained Tokens 2305419, Peak mem 19.940 GB\n",
      "Iter 13930: Train loss 0.594, Learning Rate 1.000e-04, It/sec 1.105, Tokens/sec 266.656, Trained Tokens 2307833, Peak mem 19.940 GB\n",
      "Iter 13940: Train loss 0.450, Learning Rate 1.000e-04, It/sec 1.834, Tokens/sec 234.187, Trained Tokens 2309110, Peak mem 19.940 GB\n",
      "Iter 13950: Val loss 2.715, Val took 3.218s\n",
      "Iter 13950: Train loss 0.570, Learning Rate 1.000e-04, It/sec 16.063, Tokens/sec 3782.766, Trained Tokens 2311465, Peak mem 19.940 GB\n",
      "Iter 13960: Train loss 0.381, Learning Rate 1.000e-04, It/sec 1.443, Tokens/sec 244.964, Trained Tokens 2313163, Peak mem 19.940 GB\n",
      "Iter 13970: Train loss 0.471, Learning Rate 1.000e-04, It/sec 1.613, Tokens/sec 234.945, Trained Tokens 2314620, Peak mem 19.940 GB\n",
      "Iter 13980: Train loss 0.413, Learning Rate 1.000e-04, It/sec 1.810, Tokens/sec 251.195, Trained Tokens 2316008, Peak mem 19.940 GB\n",
      "Iter 13990: Train loss 0.372, Learning Rate 1.000e-04, It/sec 1.492, Tokens/sec 264.160, Trained Tokens 2317779, Peak mem 19.940 GB\n",
      "Iter 14000: Val loss 2.675, Val took 4.140s\n",
      "Iter 14000: Train loss 0.387, Learning Rate 1.000e-04, It/sec 28.906, Tokens/sec 4775.223, Trained Tokens 2319431, Peak mem 19.940 GB\n",
      "Iter 14000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014000_adapters.safetensors.\n",
      "Iter 14010: Train loss 0.435, Learning Rate 1.000e-04, It/sec 1.578, Tokens/sec 218.711, Trained Tokens 2320817, Peak mem 19.940 GB\n",
      "Iter 14020: Train loss 0.414, Learning Rate 1.000e-04, It/sec 1.162, Tokens/sec 259.197, Trained Tokens 2323047, Peak mem 19.940 GB\n",
      "Iter 14030: Train loss 0.472, Learning Rate 1.000e-04, It/sec 2.191, Tokens/sec 246.932, Trained Tokens 2324174, Peak mem 19.940 GB\n",
      "Iter 14040: Train loss 0.351, Learning Rate 1.000e-04, It/sec 1.488, Tokens/sec 256.894, Trained Tokens 2325901, Peak mem 19.940 GB\n",
      "Iter 14050: Val loss 2.430, Val took 4.098s\n",
      "Iter 14050: Train loss 0.495, Learning Rate 1.000e-04, It/sec 13.066, Tokens/sec 3557.840, Trained Tokens 2328624, Peak mem 19.940 GB\n",
      "Iter 14060: Train loss 0.398, Learning Rate 1.000e-04, It/sec 1.549, Tokens/sec 232.333, Trained Tokens 2330124, Peak mem 19.940 GB\n",
      "Iter 14070: Train loss 0.405, Learning Rate 1.000e-04, It/sec 1.797, Tokens/sec 247.974, Trained Tokens 2331504, Peak mem 19.940 GB\n",
      "Iter 14080: Train loss 0.428, Learning Rate 1.000e-04, It/sec 1.772, Tokens/sec 250.533, Trained Tokens 2332918, Peak mem 19.940 GB\n",
      "Iter 14090: Train loss 0.416, Learning Rate 1.000e-04, It/sec 2.006, Tokens/sec 242.679, Trained Tokens 2334128, Peak mem 19.940 GB\n",
      "Iter 14100: Val loss 2.623, Val took 3.806s\n",
      "Iter 14100: Train loss 0.400, Learning Rate 1.000e-04, It/sec 22.573, Tokens/sec 4343.020, Trained Tokens 2336052, Peak mem 19.940 GB\n",
      "Iter 14100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014100_adapters.safetensors.\n",
      "Iter 14110: Train loss 0.433, Learning Rate 1.000e-04, It/sec 1.540, Tokens/sec 191.692, Trained Tokens 2337297, Peak mem 19.940 GB\n",
      "Iter 14120: Train loss 0.407, Learning Rate 1.000e-04, It/sec 1.131, Tokens/sec 232.862, Trained Tokens 2339356, Peak mem 19.940 GB\n",
      "Iter 14130: Train loss 0.402, Learning Rate 1.000e-04, It/sec 1.267, Tokens/sec 249.365, Trained Tokens 2341324, Peak mem 19.940 GB\n",
      "Iter 14140: Train loss 0.378, Learning Rate 1.000e-04, It/sec 1.697, Tokens/sec 264.378, Trained Tokens 2342882, Peak mem 19.940 GB\n",
      "Iter 14150: Val loss 2.757, Val took 3.689s\n",
      "Iter 14150: Train loss 0.448, Learning Rate 1.000e-04, It/sec 9.961, Tokens/sec 1260.115, Trained Tokens 2344147, Peak mem 19.940 GB\n",
      "Iter 14160: Train loss 0.466, Learning Rate 1.000e-04, It/sec 1.417, Tokens/sec 208.964, Trained Tokens 2345622, Peak mem 19.940 GB\n",
      "Iter 14170: Train loss 0.467, Learning Rate 1.000e-04, It/sec 1.293, Tokens/sec 259.831, Trained Tokens 2347632, Peak mem 19.940 GB\n",
      "Iter 14180: Train loss 0.436, Learning Rate 1.000e-04, It/sec 1.497, Tokens/sec 252.475, Trained Tokens 2349319, Peak mem 19.940 GB\n",
      "Iter 14190: Train loss 0.365, Learning Rate 1.000e-04, It/sec 1.497, Tokens/sec 254.930, Trained Tokens 2351022, Peak mem 19.940 GB\n",
      "Iter 14200: Val loss 2.469, Val took 3.623s\n",
      "Iter 14200: Train loss 0.395, Learning Rate 1.000e-04, It/sec 12.330, Tokens/sec 1875.458, Trained Tokens 2352543, Peak mem 19.940 GB\n",
      "Iter 14200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014200_adapters.safetensors.\n",
      "Iter 14210: Train loss 0.441, Learning Rate 1.000e-04, It/sec 1.227, Tokens/sec 204.799, Trained Tokens 2354212, Peak mem 19.940 GB\n",
      "Iter 14220: Train loss 0.402, Learning Rate 1.000e-04, It/sec 1.477, Tokens/sec 251.825, Trained Tokens 2355917, Peak mem 19.940 GB\n",
      "Iter 14230: Train loss 0.462, Learning Rate 1.000e-04, It/sec 1.842, Tokens/sec 249.020, Trained Tokens 2357269, Peak mem 19.940 GB\n",
      "Iter 14240: Train loss 0.416, Learning Rate 1.000e-04, It/sec 1.951, Tokens/sec 255.811, Trained Tokens 2358580, Peak mem 19.940 GB\n",
      "Iter 14250: Val loss 2.458, Val took 3.679s\n",
      "Iter 14250: Train loss 0.481, Learning Rate 1.000e-04, It/sec 25.428, Tokens/sec 3753.228, Trained Tokens 2360056, Peak mem 19.940 GB\n",
      "Iter 14260: Train loss 0.454, Learning Rate 1.000e-04, It/sec 1.698, Tokens/sec 250.253, Trained Tokens 2361530, Peak mem 19.940 GB\n",
      "Iter 14270: Train loss 0.507, Learning Rate 1.000e-04, It/sec 1.068, Tokens/sec 254.664, Trained Tokens 2363914, Peak mem 19.940 GB\n",
      "Iter 14280: Train loss 0.467, Learning Rate 1.000e-04, It/sec 2.197, Tokens/sec 239.010, Trained Tokens 2365002, Peak mem 19.940 GB\n",
      "Iter 14290: Train loss 0.426, Learning Rate 1.000e-04, It/sec 1.516, Tokens/sec 243.576, Trained Tokens 2366609, Peak mem 19.940 GB\n",
      "Iter 14300: Val loss 2.759, Val took 3.184s\n",
      "Iter 14300: Train loss 0.594, Learning Rate 1.000e-04, It/sec 28.047, Tokens/sec 6217.999, Trained Tokens 2368826, Peak mem 19.940 GB\n",
      "Iter 14300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014300_adapters.safetensors.\n",
      "Iter 14310: Train loss 0.487, Learning Rate 1.000e-04, It/sec 1.581, Tokens/sec 247.734, Trained Tokens 2370393, Peak mem 19.940 GB\n",
      "Iter 14320: Train loss 0.394, Learning Rate 1.000e-04, It/sec 1.689, Tokens/sec 255.236, Trained Tokens 2371904, Peak mem 19.940 GB\n",
      "Iter 14330: Train loss 0.550, Learning Rate 1.000e-04, It/sec 1.374, Tokens/sec 253.867, Trained Tokens 2373751, Peak mem 19.940 GB\n",
      "Iter 14340: Train loss 0.427, Learning Rate 1.000e-04, It/sec 1.650, Tokens/sec 245.909, Trained Tokens 2375241, Peak mem 19.940 GB\n",
      "Iter 14350: Val loss 2.418, Val took 4.332s\n",
      "Iter 14350: Train loss 0.409, Learning Rate 1.000e-04, It/sec 27.287, Tokens/sec 4868.056, Trained Tokens 2377025, Peak mem 19.940 GB\n",
      "Iter 14360: Train loss 0.401, Learning Rate 1.000e-04, It/sec 1.569, Tokens/sec 261.617, Trained Tokens 2378692, Peak mem 19.940 GB\n",
      "Iter 14370: Train loss 0.389, Learning Rate 1.000e-04, It/sec 1.829, Tokens/sec 239.570, Trained Tokens 2380002, Peak mem 19.940 GB\n",
      "Iter 14380: Train loss 0.441, Learning Rate 1.000e-04, It/sec 1.265, Tokens/sec 263.205, Trained Tokens 2382083, Peak mem 19.940 GB\n",
      "Iter 14390: Train loss 0.391, Learning Rate 1.000e-04, It/sec 1.689, Tokens/sec 228.529, Trained Tokens 2383436, Peak mem 19.940 GB\n",
      "Iter 14400: Val loss 2.474, Val took 4.074s\n",
      "Iter 14400: Train loss 0.409, Learning Rate 1.000e-04, It/sec 12.111, Tokens/sec 1849.303, Trained Tokens 2384963, Peak mem 19.940 GB\n",
      "Iter 14400: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014400_adapters.safetensors.\n",
      "Iter 14410: Train loss 0.377, Learning Rate 1.000e-04, It/sec 1.353, Tokens/sec 241.736, Trained Tokens 2386750, Peak mem 19.940 GB\n",
      "Iter 14420: Train loss 0.456, Learning Rate 1.000e-04, It/sec 2.297, Tokens/sec 233.368, Trained Tokens 2387766, Peak mem 19.940 GB\n",
      "Iter 14430: Train loss 0.420, Learning Rate 1.000e-04, It/sec 1.538, Tokens/sec 256.482, Trained Tokens 2389434, Peak mem 19.940 GB\n",
      "Iter 14440: Train loss 0.426, Learning Rate 1.000e-04, It/sec 1.107, Tokens/sec 262.319, Trained Tokens 2391804, Peak mem 19.940 GB\n",
      "Iter 14450: Val loss 2.151, Val took 3.843s\n",
      "Iter 14450: Train loss 0.435, Learning Rate 1.000e-04, It/sec 34.871, Tokens/sec 5938.568, Trained Tokens 2393507, Peak mem 19.940 GB\n",
      "Iter 14460: Train loss 0.478, Learning Rate 1.000e-04, It/sec 1.716, Tokens/sec 220.903, Trained Tokens 2394794, Peak mem 19.940 GB\n",
      "Iter 14470: Train loss 0.454, Learning Rate 1.000e-04, It/sec 2.147, Tokens/sec 236.009, Trained Tokens 2395893, Peak mem 19.940 GB\n",
      "Iter 14480: Train loss 0.545, Learning Rate 1.000e-04, It/sec 1.057, Tokens/sec 271.215, Trained Tokens 2398460, Peak mem 19.940 GB\n",
      "Iter 14490: Train loss 0.393, Learning Rate 1.000e-04, It/sec 1.302, Tokens/sec 252.629, Trained Tokens 2400401, Peak mem 19.940 GB\n",
      "Iter 14500: Val loss 2.512, Val took 2.535s\n",
      "Iter 14500: Train loss 0.363, Learning Rate 1.000e-04, It/sec 22.580, Tokens/sec 4100.555, Trained Tokens 2402217, Peak mem 19.940 GB\n",
      "Iter 14500: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014500_adapters.safetensors.\n",
      "Iter 14510: Train loss 0.413, Learning Rate 1.000e-04, It/sec 1.607, Tokens/sec 229.094, Trained Tokens 2403643, Peak mem 19.940 GB\n",
      "Iter 14520: Train loss 0.384, Learning Rate 1.000e-04, It/sec 1.672, Tokens/sec 251.966, Trained Tokens 2405150, Peak mem 19.940 GB\n",
      "Iter 14530: Train loss 0.402, Learning Rate 1.000e-04, It/sec 1.284, Tokens/sec 268.057, Trained Tokens 2407238, Peak mem 19.940 GB\n",
      "Iter 14540: Train loss 0.462, Learning Rate 1.000e-04, It/sec 2.065, Tokens/sec 248.828, Trained Tokens 2408443, Peak mem 19.940 GB\n",
      "Iter 14550: Val loss 2.706, Val took 3.768s\n",
      "Iter 14550: Train loss 0.409, Learning Rate 1.000e-04, It/sec 9.728, Tokens/sec 1796.844, Trained Tokens 2410290, Peak mem 19.940 GB\n",
      "Iter 14560: Train loss 0.413, Learning Rate 1.000e-04, It/sec 1.827, Tokens/sec 245.721, Trained Tokens 2411635, Peak mem 19.940 GB\n",
      "Iter 14570: Train loss 0.421, Learning Rate 1.000e-04, It/sec 1.942, Tokens/sec 248.222, Trained Tokens 2412913, Peak mem 19.940 GB\n",
      "Iter 14580: Train loss 0.354, Learning Rate 1.000e-04, It/sec 1.282, Tokens/sec 254.349, Trained Tokens 2414897, Peak mem 19.940 GB\n",
      "Iter 14590: Train loss 0.356, Learning Rate 1.000e-04, It/sec 1.531, Tokens/sec 260.649, Trained Tokens 2416600, Peak mem 19.940 GB\n",
      "Iter 14600: Val loss 2.474, Val took 4.563s\n",
      "Iter 14600: Train loss 0.484, Learning Rate 1.000e-04, It/sec 22.231, Tokens/sec 5428.916, Trained Tokens 2419042, Peak mem 19.940 GB\n",
      "Iter 14600: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014600_adapters.safetensors.\n",
      "Iter 14610: Train loss 0.406, Learning Rate 1.000e-04, It/sec 1.582, Tokens/sec 235.383, Trained Tokens 2420530, Peak mem 19.940 GB\n",
      "Iter 14620: Train loss 0.479, Learning Rate 1.000e-04, It/sec 1.023, Tokens/sec 261.995, Trained Tokens 2423091, Peak mem 19.940 GB\n",
      "Iter 14630: Train loss 0.389, Learning Rate 1.000e-04, It/sec 1.269, Tokens/sec 260.915, Trained Tokens 2425147, Peak mem 19.940 GB\n",
      "Iter 14640: Train loss 0.356, Learning Rate 1.000e-04, It/sec 1.272, Tokens/sec 244.637, Trained Tokens 2427071, Peak mem 19.940 GB\n",
      "Iter 14650: Val loss 2.408, Val took 3.702s\n",
      "Iter 14650: Train loss 0.416, Learning Rate 1.000e-04, It/sec 11.720, Tokens/sec 1579.813, Trained Tokens 2428419, Peak mem 19.940 GB\n",
      "Iter 14660: Train loss 0.364, Learning Rate 1.000e-04, It/sec 1.209, Tokens/sec 257.961, Trained Tokens 2430553, Peak mem 19.940 GB\n",
      "Iter 14670: Train loss 0.449, Learning Rate 1.000e-04, It/sec 1.409, Tokens/sec 258.127, Trained Tokens 2432385, Peak mem 19.940 GB\n",
      "Iter 14680: Train loss 0.459, Learning Rate 1.000e-04, It/sec 2.108, Tokens/sec 238.404, Trained Tokens 2433516, Peak mem 19.940 GB\n",
      "Iter 14690: Train loss 0.504, Learning Rate 1.000e-04, It/sec 2.173, Tokens/sec 233.126, Trained Tokens 2434589, Peak mem 19.940 GB\n",
      "Iter 14700: Val loss 2.524, Val took 3.590s\n",
      "Iter 14700: Train loss 0.431, Learning Rate 1.000e-04, It/sec 8.206, Tokens/sec 1514.011, Trained Tokens 2436434, Peak mem 19.940 GB\n",
      "Iter 14700: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014700_adapters.safetensors.\n",
      "Iter 14710: Train loss 0.452, Learning Rate 1.000e-04, It/sec 1.773, Tokens/sec 245.756, Trained Tokens 2437820, Peak mem 19.940 GB\n",
      "Iter 14720: Train loss 0.515, Learning Rate 1.000e-04, It/sec 2.118, Tokens/sec 241.052, Trained Tokens 2438958, Peak mem 19.940 GB\n",
      "Iter 14730: Train loss 0.423, Learning Rate 1.000e-04, It/sec 1.745, Tokens/sec 239.293, Trained Tokens 2440329, Peak mem 19.940 GB\n",
      "Iter 14740: Train loss 0.366, Learning Rate 1.000e-04, It/sec 1.387, Tokens/sec 247.099, Trained Tokens 2442111, Peak mem 19.940 GB\n",
      "Iter 14750: Val loss 2.771, Val took 4.442s\n",
      "Iter 14750: Train loss 0.444, Learning Rate 1.000e-04, It/sec 12.120, Tokens/sec 1620.439, Trained Tokens 2443448, Peak mem 19.940 GB\n",
      "Iter 14760: Train loss 0.399, Learning Rate 1.000e-04, It/sec 1.587, Tokens/sec 236.772, Trained Tokens 2444940, Peak mem 19.940 GB\n",
      "Iter 14770: Train loss 0.413, Learning Rate 1.000e-04, It/sec 2.100, Tokens/sec 250.931, Trained Tokens 2446135, Peak mem 19.940 GB\n",
      "Iter 14780: Train loss 0.396, Learning Rate 1.000e-04, It/sec 1.888, Tokens/sec 242.935, Trained Tokens 2447422, Peak mem 19.940 GB\n",
      "Iter 14790: Train loss 0.325, Learning Rate 1.000e-04, It/sec 1.564, Tokens/sec 272.116, Trained Tokens 2449162, Peak mem 19.940 GB\n",
      "Iter 14800: Val loss 2.744, Val took 2.833s\n",
      "Iter 14800: Train loss 0.426, Learning Rate 1.000e-04, It/sec 16.278, Tokens/sec 3219.757, Trained Tokens 2451140, Peak mem 19.940 GB\n",
      "Iter 14800: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014800_adapters.safetensors.\n",
      "Iter 14810: Train loss 0.422, Learning Rate 1.000e-04, It/sec 1.786, Tokens/sec 219.812, Trained Tokens 2452371, Peak mem 19.940 GB\n",
      "Iter 14820: Train loss 0.379, Learning Rate 1.000e-04, It/sec 1.742, Tokens/sec 246.125, Trained Tokens 2453784, Peak mem 19.940 GB\n",
      "Iter 14830: Train loss 0.361, Learning Rate 1.000e-04, It/sec 1.440, Tokens/sec 253.396, Trained Tokens 2455544, Peak mem 19.940 GB\n",
      "Iter 14840: Train loss 0.406, Learning Rate 1.000e-04, It/sec 1.688, Tokens/sec 233.994, Trained Tokens 2456930, Peak mem 19.940 GB\n",
      "Iter 14850: Val loss 2.859, Val took 4.070s\n",
      "Iter 14850: Train loss 0.398, Learning Rate 1.000e-04, It/sec 17.366, Tokens/sec 2375.734, Trained Tokens 2458298, Peak mem 19.940 GB\n",
      "Iter 14860: Train loss 0.378, Learning Rate 1.000e-04, It/sec 1.415, Tokens/sec 236.563, Trained Tokens 2459970, Peak mem 19.940 GB\n",
      "Iter 14870: Train loss 0.396, Learning Rate 1.000e-04, It/sec 1.691, Tokens/sec 249.786, Trained Tokens 2461447, Peak mem 19.940 GB\n",
      "Iter 14880: Train loss 0.356, Learning Rate 1.000e-04, It/sec 1.543, Tokens/sec 240.209, Trained Tokens 2463004, Peak mem 19.940 GB\n",
      "Iter 14890: Train loss 0.452, Learning Rate 1.000e-04, It/sec 1.082, Tokens/sec 272.653, Trained Tokens 2465523, Peak mem 19.940 GB\n",
      "Iter 14900: Val loss 2.644, Val took 4.187s\n",
      "Iter 14900: Train loss 0.417, Learning Rate 1.000e-04, It/sec 18.639, Tokens/sec 2201.212, Trained Tokens 2466704, Peak mem 19.940 GB\n",
      "Iter 14900: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0014900_adapters.safetensors.\n",
      "Iter 14910: Train loss 0.379, Learning Rate 1.000e-04, It/sec 1.731, Tokens/sec 254.158, Trained Tokens 2468172, Peak mem 19.940 GB\n",
      "Iter 14920: Train loss 0.388, Learning Rate 1.000e-04, It/sec 1.376, Tokens/sec 260.461, Trained Tokens 2470065, Peak mem 19.940 GB\n",
      "Iter 14930: Train loss 0.440, Learning Rate 1.000e-04, It/sec 1.826, Tokens/sec 239.957, Trained Tokens 2471379, Peak mem 19.940 GB\n",
      "Iter 14940: Train loss 0.388, Learning Rate 1.000e-04, It/sec 1.695, Tokens/sec 257.325, Trained Tokens 2472897, Peak mem 19.940 GB\n",
      "Iter 14950: Val loss 2.346, Val took 3.778s\n",
      "Iter 14950: Train loss 0.358, Learning Rate 1.000e-04, It/sec 10.841, Tokens/sec 2151.987, Trained Tokens 2474882, Peak mem 19.940 GB\n",
      "Iter 14960: Train loss 0.430, Learning Rate 1.000e-04, It/sec 1.431, Tokens/sec 248.041, Trained Tokens 2476615, Peak mem 19.940 GB\n",
      "Iter 14970: Train loss 0.382, Learning Rate 1.000e-04, It/sec 1.542, Tokens/sec 247.824, Trained Tokens 2478222, Peak mem 19.940 GB\n",
      "Iter 14980: Train loss 0.418, Learning Rate 1.000e-04, It/sec 1.201, Tokens/sec 249.297, Trained Tokens 2480297, Peak mem 19.940 GB\n",
      "Iter 14990: Train loss 0.379, Learning Rate 1.000e-04, It/sec 1.607, Tokens/sec 245.301, Trained Tokens 2481823, Peak mem 19.940 GB\n",
      "Iter 15000: Val loss 2.581, Val took 3.342s\n",
      "Iter 15000: Train loss 0.382, Learning Rate 1.000e-04, It/sec 12.238, Tokens/sec 1904.294, Trained Tokens 2483379, Peak mem 19.940 GB\n",
      "Iter 15000: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0015000_adapters.safetensors.\n",
      "Iter 15010: Train loss 0.363, Learning Rate 1.000e-04, It/sec 1.307, Tokens/sec 243.730, Trained Tokens 2485244, Peak mem 19.940 GB\n",
      "Iter 15020: Train loss 0.518, Learning Rate 1.000e-04, It/sec 1.349, Tokens/sec 259.610, Trained Tokens 2487169, Peak mem 19.940 GB\n",
      "Iter 15030: Train loss 0.376, Learning Rate 1.000e-04, It/sec 1.651, Tokens/sec 249.815, Trained Tokens 2488682, Peak mem 19.940 GB\n",
      "Iter 15040: Train loss 0.461, Learning Rate 1.000e-04, It/sec 0.918, Tokens/sec 272.405, Trained Tokens 2491648, Peak mem 19.940 GB\n",
      "Iter 15050: Val loss 2.194, Val took 3.443s\n",
      "Iter 15050: Train loss 0.380, Learning Rate 1.000e-04, It/sec 9.451, Tokens/sec 1710.595, Trained Tokens 2493458, Peak mem 19.940 GB\n",
      "Iter 15060: Train loss 0.378, Learning Rate 1.000e-04, It/sec 1.881, Tokens/sec 255.465, Trained Tokens 2494816, Peak mem 19.940 GB\n",
      "Iter 15070: Train loss 0.399, Learning Rate 1.000e-04, It/sec 1.447, Tokens/sec 261.247, Trained Tokens 2496622, Peak mem 19.940 GB\n",
      "Iter 15080: Train loss 0.547, Learning Rate 1.000e-04, It/sec 1.128, Tokens/sec 262.335, Trained Tokens 2498948, Peak mem 19.940 GB\n",
      "Iter 15090: Train loss 0.413, Learning Rate 1.000e-04, It/sec 1.791, Tokens/sec 246.827, Trained Tokens 2500326, Peak mem 19.940 GB\n",
      "Iter 15100: Val loss 2.590, Val took 3.289s\n",
      "Iter 15100: Train loss 0.448, Learning Rate 1.000e-04, It/sec 5.636, Tokens/sec 1005.516, Trained Tokens 2502110, Peak mem 19.940 GB\n",
      "Iter 15100: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0015100_adapters.safetensors.\n",
      "Iter 15110: Train loss 0.551, Learning Rate 1.000e-04, It/sec 1.906, Tokens/sec 245.277, Trained Tokens 2503397, Peak mem 19.940 GB\n",
      "Iter 15120: Train loss 0.470, Learning Rate 1.000e-04, It/sec 1.778, Tokens/sec 237.135, Trained Tokens 2504731, Peak mem 19.940 GB\n",
      "Iter 15130: Train loss 0.417, Learning Rate 1.000e-04, It/sec 1.449, Tokens/sec 263.963, Trained Tokens 2506553, Peak mem 19.940 GB\n",
      "Iter 15140: Train loss 0.474, Learning Rate 1.000e-04, It/sec 1.766, Tokens/sec 249.900, Trained Tokens 2507968, Peak mem 19.940 GB\n",
      "Iter 15150: Val loss 2.476, Val took 3.318s\n",
      "Iter 15150: Train loss 0.434, Learning Rate 1.000e-04, It/sec 16.316, Tokens/sec 2617.147, Trained Tokens 2509572, Peak mem 19.940 GB\n",
      "Iter 15160: Train loss 0.457, Learning Rate 1.000e-04, It/sec 1.110, Tokens/sec 257.579, Trained Tokens 2511893, Peak mem 19.940 GB\n",
      "Iter 15170: Train loss 0.542, Learning Rate 1.000e-04, It/sec 2.453, Tokens/sec 235.976, Trained Tokens 2512855, Peak mem 19.940 GB\n",
      "Iter 15180: Train loss 0.421, Learning Rate 1.000e-04, It/sec 2.126, Tokens/sec 244.048, Trained Tokens 2514003, Peak mem 19.940 GB\n",
      "Iter 15190: Train loss 0.421, Learning Rate 1.000e-04, It/sec 2.043, Tokens/sec 265.036, Trained Tokens 2515300, Peak mem 19.940 GB\n",
      "Iter 15200: Val loss 3.053, Val took 4.220s\n",
      "Iter 15200: Train loss 0.340, Learning Rate 1.000e-04, It/sec 21.566, Tokens/sec 3756.837, Trained Tokens 2517042, Peak mem 19.940 GB\n",
      "Iter 15200: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0015200_adapters.safetensors.\n",
      "Iter 15210: Train loss 0.410, Learning Rate 1.000e-04, It/sec 1.737, Tokens/sec 246.528, Trained Tokens 2518461, Peak mem 19.940 GB\n",
      "Iter 15220: Train loss 0.409, Learning Rate 1.000e-04, It/sec 1.899, Tokens/sec 246.113, Trained Tokens 2519757, Peak mem 19.940 GB\n",
      "Iter 15230: Train loss 0.401, Learning Rate 1.000e-04, It/sec 1.193, Tokens/sec 268.505, Trained Tokens 2522008, Peak mem 19.940 GB\n",
      "Iter 15240: Train loss 0.394, Learning Rate 1.000e-04, It/sec 1.629, Tokens/sec 258.253, Trained Tokens 2523593, Peak mem 19.940 GB\n",
      "Iter 15250: Val loss 2.756, Val took 4.009s\n",
      "Iter 15250: Train loss 0.340, Learning Rate 1.000e-04, It/sec 9.918, Tokens/sec 2048.166, Trained Tokens 2525658, Peak mem 19.940 GB\n",
      "Iter 15260: Train loss 0.402, Learning Rate 1.000e-04, It/sec 1.984, Tokens/sec 247.805, Trained Tokens 2526907, Peak mem 19.940 GB\n",
      "Iter 15270: Train loss 0.323, Learning Rate 1.000e-04, It/sec 1.501, Tokens/sec 262.347, Trained Tokens 2528655, Peak mem 19.940 GB\n",
      "Iter 15280: Train loss 0.368, Learning Rate 1.000e-04, It/sec 1.428, Tokens/sec 252.206, Trained Tokens 2530421, Peak mem 19.940 GB\n",
      "Iter 15290: Train loss 0.426, Learning Rate 1.000e-04, It/sec 1.683, Tokens/sec 249.064, Trained Tokens 2531901, Peak mem 19.940 GB\n",
      "Iter 15300: Val loss 2.744, Val took 3.824s\n",
      "Iter 15300: Train loss 0.477, Learning Rate 1.000e-04, It/sec 11.654, Tokens/sec 2827.182, Trained Tokens 2534327, Peak mem 19.940 GB\n",
      "Iter 15300: Saved adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors and ../trained_models/adapters_30_2_1e4/0015300_adapters.safetensors.\n",
      "Iter 15310: Train loss 0.360, Learning Rate 1.000e-04, It/sec 1.682, Tokens/sec 249.048, Trained Tokens 2535808, Peak mem 19.940 GB\n",
      "Iter 15320: Train loss 0.430, Learning Rate 1.000e-04, It/sec 1.833, Tokens/sec 260.635, Trained Tokens 2537230, Peak mem 19.940 GB\n",
      "Iter 15330: Train loss 0.382, Learning Rate 1.000e-04, It/sec 1.636, Tokens/sec 266.771, Trained Tokens 2538861, Peak mem 19.940 GB\n",
      "Iter 15340: Train loss 0.498, Learning Rate 1.000e-04, It/sec 2.247, Tokens/sec 244.425, Trained Tokens 2539949, Peak mem 19.940 GB\n",
      "Iter 15350: Val loss 2.917, Val took 3.412s\n",
      "Iter 15350: Train loss 0.418, Learning Rate 1.000e-04, It/sec 16.359, Tokens/sec 2358.981, Trained Tokens 2541391, Peak mem 19.940 GB\n",
      "Iter 15360: Val loss 2.481, Val took 3.351s\n",
      "Iter 15360: Train loss 0.373, Learning Rate 1.000e-04, It/sec 29.837, Tokens/sec 4845.463, Trained Tokens 2543015, Peak mem 19.940 GB\n",
      "Saved final adapter weights to ../trained_models/adapters_30_2_1e4/adapters.safetensors.\n",
      "\n",
      " Starting Evaluation\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 205/205 [45:00<00:00, 13.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on 205 test samples:\n",
      "Accuracy: 0.000\n",
      "F1 Score: 0.000\n",
      "Perplexity: 8.845\n",
      "\n",
      "ROUGE Scores:\n",
      "rouge1: 0.094\n",
      "rouge2: 0.027\n",
      "rougeL: 0.073\n",
      " View run MLX-30_2_1e4 at: http://127.0.0.1:5000/#/experiments/880645134898555871/runs/9fc877b8c05c4e8daec990013378a71b\n",
      " View experiment at: http://127.0.0.1:5000/#/experiments/880645134898555871\n"
     ]
    }
   ],
   "source": [
    "# Checl if the output folder exists, if not, create it:\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Output folder {output_path} created\")\n",
    "else:\n",
    "    print(f\"Output folder {output_path} already exists\")\n",
    "\n",
    "\n",
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "if optimizer == \"adam\":\n",
    "    opt = optim.Adam(learning_rate=learning_rate_value)\n",
    "else:\n",
    "    opt = optim.AdamW(learning_rate=learning_rate_value, weight_decay=weight_decay_value)\n",
    "\n",
    "# Make a class to record the training stats:\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "        try:\n",
    "            mlflow.log_metric(\"train_loss\", info[\"train_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log train metric to MLflow: {e}\")\n",
    "            \n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        self.val_accuracies.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        # log validation loss\n",
    "        try:\n",
    "            mlflow.log_metric(\"val_loss\", info[\"val_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log validation metric to MLflow: {e}\")\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "with mlflow.start_run(run_name=corrida_name):\n",
    "    mlflow.log_params({\n",
    "        \"num_train_epoch\": lora_config[\"lora_parameters\"][\"epochs\"],\n",
    "        \"max_steps\": training_args.iters,\n",
    "        \"lora_r\": lora_config[\"lora_parameters\"][\"rank\"],\n",
    "        \"lora_dropout\":lora_config[\"lora_parameters\"][\"dropout\"],\n",
    "        \"lora_layers\":lora_config[\"lora_layers\"],\n",
    "        \"lora_layeres_scale\":lora_config[\"lora_parameters\"][\"scale\"],\n",
    "        \"batch_size\":training_args.batch_size,\n",
    "        \"optimizer\":optimizer,\n",
    "        \"learning_rate\":learning_rate_value,\n",
    "        # \"weight_decay\": weight_decay_value,\n",
    "        \"scheduler\":lr_scheduler\n",
    "    })\n",
    "\n",
    "    # Train model:\n",
    "    train(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        optimizer=opt,\n",
    "        train_dataset=train_set,\n",
    "        val_dataset=valid_set,\n",
    "        training_callback=metrics,\n",
    "    )\n",
    "\n",
    "    print(\"\\n Starting Evaluation\")\n",
    "    # Evaluate model and log metrics in the same run\n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, f1, perplexity, rouge_scores = evaluate_model(model, tokenizer, test_set[:num_test])\n",
    "\n",
    "    # Log final metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": metrics.train_losses[-1][1],\n",
    "        \"final_val_loss\": metrics.val_losses[-1][1],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"rouge1\": rouge_scores['rouge1'],\n",
    "        \"rouge2\": rouge_scores['rouge2'],\n",
    "        \"rougeL\": rouge_scores['rougeL']\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nResults on {num_test} test samples:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\") \n",
    "    print(f\"Perplexity: {perplexity:.3f}\")\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for key, score in rouge_scores.items():\n",
    "        print(f\"{key}: {score:.3f}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved every 100 iterations along with the final adapters in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ac329358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000100_adapters.safetensors 0001200_adapters.safetensors\n",
      "0000200_adapters.safetensors 0001300_adapters.safetensors\n",
      "0000300_adapters.safetensors 0001400_adapters.safetensors\n",
      "0000400_adapters.safetensors 0001500_adapters.safetensors\n",
      "0000500_adapters.safetensors 0001600_adapters.safetensors\n",
      "0000600_adapters.safetensors 0001700_adapters.safetensors\n",
      "0000700_adapters.safetensors 0001800_adapters.safetensors\n",
      "0000800_adapters.safetensors 0001900_adapters.safetensors\n",
      "0000900_adapters.safetensors 0002000_adapters.safetensors\n",
      "0001000_adapters.safetensors adapters.safetensors\n",
      "0001100_adapters.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls ../trained_models/adapters2k/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training and validation losses to see how well the adapters fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x30efe5af0>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_its, train_losses = zip(*metrics.train_losses)\n",
    "val_its, val_losses = zip(*metrics.val_losses)\n",
    "plt.plot(train_its, train_losses, '-o')\n",
    "plt.plot(val_its, val_losses, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', \"Valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "The adapted weights are: $\\tilde{W} = W + c \\cdot \\mathbf{b}^\\top \\mathbf{a}$. Note, this process can be destructive if the inputs are in low precision and they have very different magnitudes. Tuning the `scale` parameter, $c$, prior to fine-tuning can improve the model performance after fusion.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlx_lm.fuse --model {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `lora_fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = load(\"lora_fused_model\")\n",
    "# num_correct = 0\n",
    "# for prompt, answer in tqdm.tqdm(test_set[:num_test]):\n",
    "#     response = generate(model, tokenizer, prompt, max_tokens=2)\n",
    "#     num_correct += (response==answer)\n",
    "# test_acc = num_correct / num_test\n",
    "# print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your LoRA adapters are not working well it's critical to plot both the trianing loss and validation loss over the duration of fine-tuning. There are really only two cases to consider: underfitting or overfitting. And you can figure out which regime you are in based on the above plot.\n",
    "\n",
    "**Underfitting**: The trianing loss is not low enough and the validation loss closely matches the training loss. You could also measure the accuracy on the training set itself for question-answering style tasks like HellaSwag. If you are in this regime you have a few options to improve the results:\n",
    "\n",
    "- Use more adapters. Increase `lora_layers` or adapt more of the linear layers within a given block by setting `lora_parameters[\"keys\"]`.\n",
    "- Use a higher rank. A higher rank means more parameters per adapter.\n",
    "- If you are using dropout, decrease the droupout rate or turn it off entirely.\n",
    "- Sometimes, underfitting issues are really optimization issues. In these cases it can be helpful to tune the learning rate or learning rate schedule.\n",
    "- If none of the above works, try a bigger model. For example, try Phi-3 medium instead of Phi-3 tiny.\n",
    "\n",
    "**Overfitting**: The trianing loss keeps going down but the validation loss stops going down and even starts to go up. If you are in this regime you also have a few options:\n",
    "\n",
    "- The best thing to do is to use more trianing data if you have it.\n",
    "- Contrary to the underfitting regime decreasing the capacity of the model can help. For example, use fewer adapters, a lower LoRA rank, or a smaller model size.\n",
    "- If you are not using dropout, use it.\n",
    "\n",
    "If you find your adapters work well pre-fusion but stop working post-fusion, try tuning the `scale` parameter, $c$, prior to fine-tuning. Typically the adapters have a smaller magnitude than the weights, so using a larger scale helps.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "Fine-tuning a large LM with LoRA requires a machine with a decent amount of memory. Here are some tips to reduce memory use should you need to do so. \n",
    "\n",
    "- Try quantization (QLoRA). You can use QLoRA by generating a quantized model with `mlx_lm.convert` and the `-q` flag or by using an already quantized model from HuggingFace.\n",
    "\n",
    "- Try using a smaller batch size. You can set the `batch_size` parameter in the `TrainingArgs` or pass `--batch-size` if you are using the CLI. The default is 4 so setting this to 2 or 1 will reduce memory consumption. Note, this may slow things down a little..\n",
    "\n",
    "- Reduce the number of layers to fine-tune with by setting `lora_layers` to a smaller value or passing `--lora-layers` if you are using the CLI. The default is `16`, so you can try `8` or `4`. This reduces the amount of memory needed for back propagation. It may also reduce the quality of the fine-tuned model and you may need to compensate with a larger `rank`.\n",
    "\n",
    "- Longer examples require more memory. If it makes sense for your data, one thing you can do is break your examples into smaller sequences when making the `train`, `valid`, and `test` data sets.\n",
    "\n",
    "- Gradient checkpointing lets you trade-off memory use (less) for computation (more) by recomputing instead of storing intermediate values needed by the backward pass. You can use gradient checkpointing by passing `grad_checkpoint=True` to the `TrainingArgs` or the `--grad-checkpoint` flag if using the CLI. Gradient checkpointing will be more helpful for larger batch sizes or sequence lengths with smaller or quantized models.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project.\n",
    "- We also have an example of [LoRA fine-tuning in MLX Swift](https://github.com/ml-explore/mlx-swift-examples/tree/main/Applications/LoRATrainingExample)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
