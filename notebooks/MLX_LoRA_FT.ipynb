{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to [LoRA fine-tune](https://arxiv.org/abs/2106.09685) an LLM with MLX LM. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it in the right format for MLX LM.\n",
    "2. Setup and run LoRA training. We'll show how to capture the training logs and plot some statistics to visualize the performance.\n",
    "3. Evaluate on the test set. We'll compute the final question-answer accuracy of the fine-tuned model.\n",
    "4. Fuse the resulting adapters into the base model and upload to Hugging Face.\n",
    "5. Discuss tips for debugging accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install mlx-lm\n",
    "# pip install matplotlib\n",
    "# pip install rouge-score\n",
    "# pip install scikit-learn\n",
    "# pip install tqdm\n",
    "# pip install numpy\n",
    "# pip install json\n",
    "# pip install pathlib\n",
    "# pip install transformers\n",
    "# pip install sentencepiece\n",
    "# pip install datasets\n",
    "# pip install torch\n",
    "# pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1131315",
   "metadata": {},
   "source": [
    "### MLFOW CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1cf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d438bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name=\"MLX1.0\"\n",
    "\n",
    "# base_model = \"Llama-3.2-1B-Instruct\"\n",
    "# four_bits = True \n",
    "# dataset_path = \"FAQ_All.jsonl\"\n",
    "# dataset_type = \"alpaca_chat.load_qa\"\n",
    "output_dir = \"../trained_models/adapters/adapters.safetensors\"\n",
    "sequence_len = 2048\n",
    "lora_layers = 16\n",
    "lora_layers_scale = 16.0\n",
    "grad_checkpoint_value = True\n",
    "\n",
    "epochs = 13\n",
    "steps = 600\n",
    "lora_r = 64\n",
    "# lora_alpha = 16\n",
    "lora_dropout = 0.01\n",
    "# gradient_accumulation_steps = 4\n",
    "batch_size = 8\n",
    "optimizer = \"adamw\"\n",
    "learning_rate_value = 2e-6\n",
    "weight_decay_value = 0.02\n",
    "lr_scheduler = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0069b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://35vh82lm-5000.usw3.devtunnels.ms/\")\n",
    "mlflow.set_experiment(experiment_name=exp_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Hiperparametros FT MLX\"):\n",
    "    mlflow.log_param(\"num_train_epoch\", epochs)\n",
    "    mlflow.log_param(\"max_steps\", steps)\n",
    "    # mlflow.log_param(\"lora_alpha\", lora_alpha)\n",
    "    mlflow.log_param(\"lora_r\", lora_r)\n",
    "    mlflow.log_param(\"lora_dropout\", lora_dropout)\n",
    "    mlflow.log_param(\"lora_layers\", lora_layers)\n",
    "    mlflow.log_param(\"lora_layers_scale\",lora_layers_scale)\n",
    "    #mlflow.log_param(\"gradient_accumulation_steps\", gradient_accumulation_steps)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"optimizer\", optimizer)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate_value)\n",
    "    mlflow.log_param(\"weight_decay\", weight_decay_value)\n",
    "    mlflow.log_param(\"scheduler\", lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61698208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello stats: 512 lenght trainging dataset\n",
      "An example:\n",
      "\n",
      "{\n",
      "  \"question\": \"\\u00bfPuedo presentar p\\u00f3liza de fianza en formato electr\\u00f3nico?\",\n",
      "  \"answer\": \"S\\u00ed, es v\\u00e1lido este tipo de garant\\u00eda.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "save_path = \"./dataset/FAQ_All.jsonl\"\n",
    "\n",
    "with open(save_path, 'r') as file:\n",
    "\tdataset = [json.loads(line) for line in file]\n",
    "\n",
    "print(f\"Hello stats: {len(dataset)} lenght trainging dataset\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b607237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "valid_set = [dataset[i] for i in perm[:valid_size]]\n",
    "train_set = [dataset[i] for i in perm[valid_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c38c4e",
   "metadata": {},
   "source": [
    "Finally, put the data splits in the MLX LM training format. The format simply expects the data to be in a container which supports random access to the individual examples (e.g. a Python `list`):\n",
    "```\n",
    "[\"An example for the model.\", \"Another example for the model.\", ...]\n",
    "```\n",
    "For more details, see the [documentation on supported formats](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea738f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return [\n",
    "        f\"Question: {clean_text(t['question'])}\\n\"\n",
    "        f\"Answer: {clean_text(t['answer'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\n",
    "\n",
    "train_set, valid_set = map(preprocess, (train_set, valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [Phi-3 mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). At 3.8 billion parameters, Phi-3 mini is a high-quality model that is also fast to fine-tune on most Apple silicon machines. Also, it has a [permissive MIT License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE).\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ff309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danny/Docs/GitHub/Cuentame-Mas_API/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm.utils import load, generate\n",
    "from mlx_lm.tuner.trainer import train, evaluate, TrainingArgs\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters and make the training arguments. See the [training argument class](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/trainer.py#L31-L63) for a more detailed list of training parameters. \n",
    "\n",
    "Recall the LoRA update is $W^\\top \\mathbf{x} + c \\cdot \\mathbf{a} \\mathbf{b}^\\top \\mathbf{x}$ where $\\mathbf{a}$ has shape `(D, rank)`.\n",
    "\n",
    "With that in mind, the LoRA parameters to attend to are:\n",
    "- `lora_layers`: The number of Transformer blocks from the top of the model to adapt.\n",
    "- `rank`: The rank of the low-rank adapters. A larger rank implies more adapter parameters per linear layer.\n",
    "- `scale`: This is the constant $c$ that scales the low-rank update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0851dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save the adapter config and weights\n",
    "adapter_path = Path(\"../trained_models/adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"lora_layers\": lora_layers,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": lora_r,\n",
    "    \"scale\": lora_layers_scale,\n",
    "    \"dropout\": lora_dropout,\n",
    "    \"epochs\": epochs\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)    \n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=output_dir,\n",
    "    iters=steps,\n",
    "    steps_per_eval=50,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_length=sequence_len,\n",
    "    grad_checkpoint=grad_checkpoint_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the Phi-3 mini model. Note this may take a few minutes to download from HuggingFace if you haven't downloaded it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0b16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../original_models/Llama-3.2-1B-Instruct-bf16\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the model, freeze it's parameters so we don't train them. Then convert linear layers to LoRA layers using the MLX LM utility `linear_to_lora_layers`. The adapters in the `LoRA` layers are not frozen, so they will be included in the model's `trainable_parameters`. Check-out the [LoRA layer implementation](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/lora.py#L72-L104) to see how it all works.\n",
    "\n",
    "By default, MLX LM only adapts the query, key, and value projection matrices for Phi-3. You can specify the layers to adapt by setting `lora_parameters[\"keys\"]` to a list of layer names. In this case it defaults to `[\"attn.qkv_proj\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 6815744\n"
     ]
    }
   ],
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"lora_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer, but you can specify any [optimizer](https://ml-explore.github.io/mlx/build/html/python/optimizers/common_optimizers.html) with any [scheduler](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html). We also added a custom class to capture the training and validation loss to plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "984516d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 600\n",
      "Iter 1: Val loss 2.076, Val took 13.780s\n",
      "Iter 10: Train loss 1.335, Learning Rate 2.000e-05, It/sec 0.631, Tokens/sec 414.487, Trained Tokens 6569, Peak mem 22.604 GB\n",
      "Iter 20: Train loss 1.404, Learning Rate 2.000e-05, It/sec 0.229, Tokens/sec 153.356, Trained Tokens 13272, Peak mem 22.604 GB\n",
      "Iter 30: Train loss 1.460, Learning Rate 2.000e-05, It/sec 0.468, Tokens/sec 321.140, Trained Tokens 20133, Peak mem 22.604 GB\n",
      "Iter 40: Train loss 1.537, Learning Rate 2.000e-05, It/sec 0.117, Tokens/sec 114.821, Trained Tokens 29930, Peak mem 22.655 GB\n",
      "Iter 50: Val loss 2.003, Val took 9.554s\n",
      "Iter 50: Train loss 1.369, Learning Rate 2.000e-05, It/sec 4.564, Tokens/sec 2790.953, Trained Tokens 36045, Peak mem 22.655 GB\n",
      "Iter 60: Train loss 1.071, Learning Rate 2.000e-05, It/sec 0.531, Tokens/sec 336.486, Trained Tokens 42377, Peak mem 22.655 GB\n",
      "Iter 70: Train loss 0.898, Learning Rate 2.000e-05, It/sec 0.716, Tokens/sec 353.973, Trained Tokens 47321, Peak mem 22.655 GB\n",
      "Iter 80: Train loss 0.982, Learning Rate 2.000e-05, It/sec 0.388, Tokens/sec 296.959, Trained Tokens 54970, Peak mem 22.655 GB\n",
      "Iter 90: Train loss 1.072, Learning Rate 2.000e-05, It/sec 0.322, Tokens/sec 208.917, Trained Tokens 61453, Peak mem 22.655 GB\n",
      "Iter 100: Val loss 2.020, Val took 6.551s\n",
      "Iter 100: Train loss 1.086, Learning Rate 2.000e-05, It/sec 0.534, Tokens/sec 492.374, Trained Tokens 70671, Peak mem 22.655 GB\n",
      "Iter 100: Saved adapter weights to ../trained_models/adapters/adapters.safetensors and ../trained_models/adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.865, Learning Rate 2.000e-05, It/sec 0.375, Tokens/sec 307.194, Trained Tokens 78872, Peak mem 22.655 GB\n",
      "Iter 120: Train loss 0.738, Learning Rate 2.000e-05, It/sec 0.540, Tokens/sec 374.311, Trained Tokens 85806, Peak mem 22.655 GB\n",
      "Iter 130: Train loss 0.649, Learning Rate 2.000e-05, It/sec 0.657, Tokens/sec 361.871, Trained Tokens 91317, Peak mem 22.655 GB\n",
      "Iter 140: Train loss 0.847, Learning Rate 2.000e-05, It/sec 0.355, Tokens/sec 275.173, Trained Tokens 99063, Peak mem 22.655 GB\n",
      "Iter 150: Val loss 2.084, Val took 6.536s\n",
      "Iter 150: Train loss 0.910, Learning Rate 2.000e-05, It/sec 3.765, Tokens/sec 3594.117, Trained Tokens 108609, Peak mem 22.655 GB\n",
      "Iter 160: Train loss 0.430, Learning Rate 2.000e-05, It/sec 0.970, Tokens/sec 421.696, Trained Tokens 112956, Peak mem 22.655 GB\n",
      "Iter 170: Train loss 0.587, Learning Rate 2.000e-05, It/sec 0.313, Tokens/sec 241.202, Trained Tokens 120650, Peak mem 22.655 GB\n",
      "Iter 180: Train loss 0.633, Learning Rate 2.000e-05, It/sec 0.522, Tokens/sec 338.789, Trained Tokens 127137, Peak mem 22.655 GB\n",
      "Iter 190: Train loss 0.627, Learning Rate 2.000e-05, It/sec 0.529, Tokens/sec 415.058, Trained Tokens 134980, Peak mem 22.655 GB\n",
      "Iter 200: Val loss 2.225, Val took 9.502s\n",
      "Iter 200: Train loss 0.498, Learning Rate 2.000e-05, It/sec 7.955, Tokens/sec 4492.403, Trained Tokens 140627, Peak mem 22.655 GB\n",
      "Iter 200: Saved adapter weights to ../trained_models/adapters/adapters.safetensors and ../trained_models/adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.508, Learning Rate 2.000e-05, It/sec 0.135, Tokens/sec 102.474, Trained Tokens 148211, Peak mem 22.655 GB\n",
      "Iter 220: Train loss 0.521, Learning Rate 2.000e-05, It/sec 0.406, Tokens/sec 343.129, Trained Tokens 156662, Peak mem 22.655 GB\n",
      "Iter 230: Train loss 0.480, Learning Rate 2.000e-05, It/sec 0.471, Tokens/sec 384.355, Trained Tokens 164814, Peak mem 22.655 GB\n",
      "Iter 240: Train loss 0.361, Learning Rate 2.000e-05, It/sec 0.674, Tokens/sec 372.123, Trained Tokens 170338, Peak mem 22.655 GB\n",
      "Iter 250: Val loss 2.263, Val took 9.058s\n",
      "Iter 250: Train loss 0.517, Learning Rate 2.000e-05, It/sec 14.682, Tokens/sec 11910.076, Trained Tokens 178450, Peak mem 22.655 GB\n",
      "Iter 260: Train loss 0.406, Learning Rate 2.000e-05, It/sec 0.505, Tokens/sec 394.303, Trained Tokens 186260, Peak mem 22.655 GB\n",
      "Iter 270: Train loss 0.339, Learning Rate 2.000e-05, It/sec 0.351, Tokens/sec 248.844, Trained Tokens 193350, Peak mem 22.655 GB\n",
      "Iter 280: Train loss 0.269, Learning Rate 2.000e-05, It/sec 0.641, Tokens/sec 363.237, Trained Tokens 199016, Peak mem 22.655 GB\n",
      "Iter 290: Train loss 0.369, Learning Rate 2.000e-05, It/sec 0.348, Tokens/sec 220.391, Trained Tokens 205357, Peak mem 22.655 GB\n",
      "Iter 300: Val loss 2.406, Val took 9.212s\n",
      "Iter 300: Train loss 0.459, Learning Rate 2.000e-05, It/sec 2.348, Tokens/sec 2033.152, Trained Tokens 214015, Peak mem 22.655 GB\n",
      "Iter 300: Saved adapter weights to ../trained_models/adapters/adapters.safetensors and ../trained_models/adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.390, Learning Rate 2.000e-05, It/sec 0.514, Tokens/sec 368.734, Trained Tokens 221184, Peak mem 22.655 GB\n",
      "Iter 320: Train loss 0.419, Learning Rate 2.000e-05, It/sec 0.159, Tokens/sec 163.648, Trained Tokens 231464, Peak mem 22.655 GB\n",
      "Iter 330: Train loss 0.297, Learning Rate 2.000e-05, It/sec 0.363, Tokens/sec 245.081, Trained Tokens 238215, Peak mem 22.655 GB\n",
      "Iter 340: Train loss 0.246, Learning Rate 2.000e-05, It/sec 0.634, Tokens/sec 413.748, Trained Tokens 244740, Peak mem 22.655 GB\n",
      "Iter 350: Val loss 2.450, Val took 8.382s\n",
      "Iter 350: Train loss 0.311, Learning Rate 2.000e-05, It/sec 2.694, Tokens/sec 1624.691, Trained Tokens 250770, Peak mem 22.655 GB\n",
      "Iter 360: Train loss 0.292, Learning Rate 2.000e-05, It/sec 0.444, Tokens/sec 363.330, Trained Tokens 258960, Peak mem 22.655 GB\n",
      "Iter 370: Train loss 0.211, Learning Rate 2.000e-05, It/sec 0.838, Tokens/sec 401.870, Trained Tokens 263757, Peak mem 22.655 GB\n",
      "Iter 380: Train loss 0.215, Learning Rate 2.000e-05, It/sec 0.595, Tokens/sec 378.464, Trained Tokens 270123, Peak mem 22.655 GB\n",
      "Iter 390: Train loss 0.266, Learning Rate 2.000e-05, It/sec 0.249, Tokens/sec 192.290, Trained Tokens 277858, Peak mem 22.655 GB\n",
      "Iter 400: Val loss 2.495, Val took 7.599s\n",
      "Iter 400: Train loss 0.374, Learning Rate 2.000e-05, It/sec 18.253, Tokens/sec 17428.340, Trained Tokens 287406, Peak mem 22.655 GB\n",
      "Iter 400: Saved adapter weights to ../trained_models/adapters/adapters.safetensors and ../trained_models/adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.217, Learning Rate 2.000e-05, It/sec 0.824, Tokens/sec 452.143, Trained Tokens 292891, Peak mem 22.655 GB\n",
      "Iter 420: Train loss 0.258, Learning Rate 2.000e-05, It/sec 0.424, Tokens/sec 371.263, Trained Tokens 301646, Peak mem 22.655 GB\n",
      "Iter 430: Train loss 0.194, Learning Rate 2.000e-05, It/sec 0.343, Tokens/sec 210.426, Trained Tokens 307782, Peak mem 22.655 GB\n",
      "Iter 440: Train loss 0.226, Learning Rate 2.000e-05, It/sec 0.570, Tokens/sec 371.786, Trained Tokens 314307, Peak mem 22.655 GB\n",
      "Iter 450: Val loss 2.597, Val took 7.351s\n",
      "Iter 450: Train loss 0.299, Learning Rate 2.000e-05, It/sec 0.213, Tokens/sec 189.936, Trained Tokens 323209, Peak mem 22.655 GB\n",
      "Iter 460: Train loss 0.222, Learning Rate 2.000e-05, It/sec 0.676, Tokens/sec 353.417, Trained Tokens 328437, Peak mem 22.655 GB\n",
      "Iter 470: Train loss 0.178, Learning Rate 2.000e-05, It/sec 0.381, Tokens/sec 228.201, Trained Tokens 334430, Peak mem 22.655 GB\n",
      "Iter 480: Train loss 0.196, Learning Rate 2.000e-05, It/sec 0.574, Tokens/sec 366.819, Trained Tokens 340824, Peak mem 22.655 GB\n",
      "Iter 490: Train loss 0.239, Learning Rate 2.000e-05, It/sec 0.445, Tokens/sec 386.082, Trained Tokens 349495, Peak mem 22.655 GB\n",
      "Iter 500: Val loss 2.638, Val took 6.721s\n",
      "Iter 500: Train loss 0.286, Learning Rate 2.000e-05, It/sec 2.855, Tokens/sec 2739.502, Trained Tokens 359092, Peak mem 22.655 GB\n",
      "Iter 500: Saved adapter weights to ../trained_models/adapters/adapters.safetensors and ../trained_models/adapters/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.204, Learning Rate 2.000e-05, It/sec 0.829, Tokens/sec 444.982, Trained Tokens 364460, Peak mem 22.655 GB\n",
      "Iter 520: Train loss 0.174, Learning Rate 2.000e-05, It/sec 0.556, Tokens/sec 363.564, Trained Tokens 371000, Peak mem 22.655 GB\n",
      "Iter 530: Train loss 0.207, Learning Rate 2.000e-05, It/sec 0.569, Tokens/sec 391.417, Trained Tokens 377875, Peak mem 22.655 GB\n",
      "Iter 540: Train loss 0.280, Learning Rate 2.000e-05, It/sec 0.107, Tokens/sec 96.505, Trained Tokens 386871, Peak mem 22.655 GB\n",
      "Iter 550: Val loss 2.715, Val took 11.883s\n",
      "Iter 550: Train loss 0.194, Learning Rate 2.000e-05, It/sec 2.572, Tokens/sec 1576.720, Trained Tokens 393002, Peak mem 22.655 GB\n",
      "Iter 560: Train loss 0.170, Learning Rate 2.000e-05, It/sec 0.550, Tokens/sec 383.938, Trained Tokens 399982, Peak mem 22.655 GB\n",
      "Iter 570: Train loss 0.230, Learning Rate 2.000e-05, It/sec 0.183, Tokens/sec 194.558, Trained Tokens 410615, Peak mem 22.655 GB\n",
      "Iter 580: Train loss 0.166, Learning Rate 2.000e-05, It/sec 0.644, Tokens/sec 437.216, Trained Tokens 417399, Peak mem 22.655 GB\n",
      "Iter 590: Train loss 0.153, Learning Rate 2.000e-05, It/sec 0.694, Tokens/sec 489.421, Trained Tokens 424448, Peak mem 22.655 GB\n",
      "Iter 600: Val loss 2.707, Val took 8.166s\n",
      "Iter 600: Train loss 0.197, Learning Rate 2.000e-05, It/sec 9.321, Tokens/sec 5112.719, Trained Tokens 429933, Peak mem 22.655 GB\n",
      "Iter 600: Saved adapter weights to ../trained_models/adapters/adapters.safetensors and ../trained_models/adapters/0000600_adapters.safetensors.\n",
      "Saved final adapter weights to ../trained_models/adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "if optimizer == \"adam\":\n",
    "    opt = optim.Adam(learning_rate=learning_rate_value)\n",
    "else:\n",
    "    opt = optim.AdamW(learning_rate=learning_rate_value, weight_decay=weight_decay_value)\n",
    "\n",
    "# Make a class to record the training stats:\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "\n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        self.val_accuracies.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "# Train model:\n",
    "train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=train_set,\n",
    "    val_dataset=valid_set,\n",
    "    training_callback=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved every 100 iterations along with the final adapters in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac329358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(86353) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000100_adapters.safetensors 0000500_adapters.safetensors\n",
      "0000200_adapters.safetensors 0000600_adapters.safetensors\n",
      "0000300_adapters.safetensors adapter_config.json\n",
      "0000400_adapters.safetensors adapters.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls ../trained_models/adapters/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training and validation losses to see how well the adapters fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x164f04710>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdFElEQVR4nO3deXhU1f0G8PdO9pDMZCE7IQk7MSxhDypEAQERQW2lVMTdnwitlFoVbQW1iki1aLXgCu4IVRZBQRYBgbCGACHshBAgCyEkk5B95v7+uJlJJpl9z+T9PE8ekjt3Zs5ceLhvzvmecwRRFEUQEREReQiZqxtAREREZE8MN0RERORRGG6IiIjIozDcEBERkUdhuCEiIiKPwnBDREREHoXhhoiIiDyKt6sb4GxqtRpXrlxBcHAwBEFwdXOIiIjIDKIooqKiArGxsZDJjPfNtLtwc+XKFcTHx7u6GURERGSF/Px8dOrUyeg57S7cBAcHA5Aujlwud3FriIiIyBxKpRLx8fHa+7gx7S7caIai5HI5ww0REVEbY05JCQuKiYiIyKMw3BAREZFHYbghIiIij9Luam7MpVKpUF9f7+pmtFk+Pj7w8vJydTOIiKgdYrhpQRRFFBYWoqyszNVNafNCQkIQHR3N9YSIiMipGG5a0ASbyMhIBAYG8sZsBVEUUVVVheLiYgBATEyMi1tERETtCcNNMyqVShtswsPDXd2cNi0gIAAAUFxcjMjISA5RERGR07CguBlNjU1gYKCLW+IZNNeRtUtERORMDDd6cCjKPngdiYjIFTgsRURE1NaoVUDeHqCyCAiKAhKGAzIO/2sw3BAREbUlOeuAjc8DyitNx+SxwLiFQPLdrmsX4Dahi8NSDqJSi8g4dw1rsy4j49w1qNSiq5tkscTERCxevNjVzSAiIo2cdcDK6brBBgCUBdLxnHWuaRcgvffiFODzu4DvH5P+XJzikjax58YBNmYX4JUfc1BQXqM9FqPwx7yJyRiXYv9p0aZqW+bNm4f58+db/LoHDhxAhw4drGwVERHZlVol9dhA3y/LIgAB2PgC0GuC83tLNKGrZds0oev+L5zaq8RwY2cbswsw46vMVv/0CstrMOOrTCyZNsDuAaegoED7/XfffYeXX34Zp06d0h4LCgrSfi+KIlQqFby9Tf/VR0RE2LWdRERkgwu7WvfY6BAB5WXgn1GATwDg5Qt4+zX+6Q94+wJefgaONXvM26/FscZzDR2TeQEb/gp3Cl0MNyaIoojqepVZ56rUIuatO27srxfz1+Xg5m4d4SUzPZMowMfLrBlH0dHR2u8VCgUEQdAe2759O2677Tb89NNP+Pvf/45jx47hl19+QXx8PObMmYO9e/fixo0b6N27NxYsWIDRo0drXysxMRGzZ8/G7NmzAUg9RB9//DE2bNiATZs2IS4uDm+//TbuvtvFY7xERJ6qNBc4vx04/ytwZot5z1HXA7XutARHY+jK2wMk3eqUd2S4MaG6XoXklzfZ5bVEAIXKGvSZ/4tZ5+e8OhaBvvb5K3rhhRfwr3/9C126dEFoaCjy8/Nx55134vXXX4efnx+++OILTJw4EadOnULnzp0Nvs4rr7yCt956C4sWLcJ//vMfPPDAA8jLy0NYWJhd2klE1K5VlQK5OxoDzXbg+gXLX+O+z4DY/kBDLaCqlf5sqAVUdRYeqwMaavQcq232nDqgpgyoVZpuV2WR5Z/FSgw37cSrr76KMWPGaH8OCwtDv379tD+/9tprWL16NdatW4dZs2YZfJ2HH34YU6dOBQC88cYbeO+997B//36MGzfOcY0nIvJU9TVA/l7g3K9SmCk4Ap3hHZk30Gkw0OU2IPFWqVC3ogD6h4AEadbUTZOdW3OT+5tUPGxKUJTj29KI4caEAB8v5Lw61qxz9+eW4uFlB0yet/yRwRiSZLqnI8DHfv84Bw0apPNzZWUl5s+fjw0bNqCgoAANDQ2orq7GxYsXjb5O3759td936NABcrlcu4cUERGZoFYDhUebemYuZki9I81F9Aa6pEtfiTcDfsFNj41f2Fi4K0A34DSWMIx70/nFxAnDpVClNBG6EoY7rUkMNyYIgmD20NCt3SMQo/BHYXmNob9eRCv8cWv3CLNqbuyp5aynZ599Fps3b8a//vUvdOvWDQEBAfjd736Huro6o6/j4+Oj87MgCFCr1XZvLxGRx7ie11Q3c34HUF2q+3hQNND1NinMJI0E5EYmnSTfLc080rvOzZuuWedG5iWtseNGoYvhxo68ZALmTUzGjK8yDf31Yt7EZKcHG312796Nhx9+GPfccw8AqSfnwoULrm0UEZG7sWZRuqpS4MJvUqA59ytwPVf3cd8gIPGWxt6Z24CInoAl29Uk3y3NPHKDxfJ02uRGoYvhxs7GpcRgybQBrda5iXbgOjfW6N69O3744QdMnDgRgiDgH//4B3tgiIiaM3cl4IZaIH9fU93MlcPQ+fVW8AI6DZKCTJd06Xsv3V5wi8m8nDbzyGxuFLoYbhxgXEoMxiRHY39uKYorahAZ7I8hSWFu0WOj8c477+DRRx/F8OHD0bFjRzz//PNQKs2odiciag9MLUo35jXpsfO/AnkZQEO17nkde0pBputtQMLNgL/cSQ13MTcJXYIoim1vXwAbKJVKKBQKlJeXQy7X/cdWU1OD3NxcJCUlwd/f30Ut9By8nkTUJqlV0rYBRhfMayEoqqkIuEu61MNDdmXs/t0Se26IiIhEEagsBkrPA6c3mhds4gYBKfdJYSayt2V1M+RQDDdEROQeHL2jtCgCFYVSgCk9D5Sea/Z9LlBXadnrDZsB9Pmd/dpHdsNwQ0RErmdu8a4parW0yF2rAJMr/VlfZeTJAhASDwSENi6mZ4ITF6UjyzDcEBGRa1m6o7RaDVRcAa4173lp1gPTsri3OUEGhHQGwroAYV0b/2z8Ck2QNo3U1ty4z6J0ZBmGGyIich21SuqxMbjlMIB1f5JW8r1+oSnAqGoNv6bgJQWVlgEmvCugiJd2tDbGDRelI8sw3BARkevk7TFdvFtTBuz9r+4xmTcQmqinByZJ6pmxdR0ZN1uUjizDcENERK6hVknrxJij2xigx9imEKOIB7wcfAtzo0XpyDIMN0RE5DyiKG0ceXQlkP194w7XZrj5GdcsDucmi9KRZWSubgC5h/T0dMyePVv7c2JiIhYvXmz0OYIgYM2aNQ5tFxF5iOsXgJ2LgA+GAh+OADLel4KNnwLwCTTyRAGQx7F4lyzCnhtHcfR6Dc1MnDgR9fX12LhxY6vHfvvtN4wYMQJHjhxB3759zX7NAwcOtNpJnIjIIjeuAcd/AI6tkvZe0vD2B3qMA/reD3QbDZze1Fi8C7B4l+yB4cYR7LVeg5kee+wx3Hfffbh06RI6deqk89iyZcswaNAgi4INAERERNiziUTUXtRVAad+koadzm0F1A3ScUEGJI0A+twP9J6ou9cSi3fJzjgsZW+a9RpaVv9r1mvIWWf3t7zrrrsQERGB5cuX6xyvrKzEqlWrMHnyZEydOhVxcXEIDAxEnz598O233xp9zZbDUmfOnMGIESPg7++P5ORkbN682e6fg4jaKFUDcHYL8MOTwKJuwPePAWc2ScEmph9wx+vAX3KA6WuB1Af0byKZfDcwOxt4aD1w36fSn7OPMdiQVdhzY4oomljRshm1Cvj5ORher0GQfjPpkm5eF6tPoFl7lXh7e2P69OlYvnw5XnrpJQiNz1m1ahVUKhWmTZuGVatW4fnnn4dcLseGDRvw4IMPomvXrhgyZIjpj6VW495770VUVBT27duH8vJynfocImqHRBG4nAkcaywMvnG16bHQRKDP76Vemoge5r8mi3fJThhuTKmvAt6w1+6uotSj82a8eae/eAXwNa/u5dFHH8WiRYuwY8cOpKenA5CGpO677z4kJCTg2Wef1Z77pz/9CZs2bcLKlSvNCjdbtmzByZMnsWnTJsTGStfijTfewPjx4837HETkOa6dk4acjq2UFtTTCAwHbrpXqqPpNJibSJJLMdx4iF69emH48OH47LPPkJ6ejrNnz+K3337Dq6++CpVKhTfeeAMrV67E5cuXUVdXh9raWgQGGpuh0OTEiROIj4/XBhsASEtLc9RHISJ3U1ks9c4cXQlcyWw67hMorQPT536g6222L5xHZCcuDTcLFizADz/8gJMnTyIgIADDhw/HwoUL0bNnT4PPWb58OR555BGdY35+fqipqXFMI30CpR4Uc+TtAb42Y4fYB/5n3rRGo9MjW3vsscfwpz/9CR988AGWLVuGrl27YuTIkVi4cCHeffddLF68GH369EGHDh0we/Zs1NXVWfT6ROQhzJnNWVsBnNwAHP0OOL8dENXSccFLCjJ97peCjV+Q05tPZIpLw82OHTswc+ZMDB48GA0NDXjxxRdxxx13ICcnx+g0ZLlcjlOnTml/FhzZ/SkIZg8NoevtUnW/qc3Wut7ukGmN999/P5555hl88803+OKLLzBjxgwIgoDdu3dj0qRJmDZtGgCphub06dNITk4263V79+6N/Px8FBQUICYmBgCwd+9eu7efiJzA2GzOnuOBs1ulIaeTP+luQBk3SBpyuuleIIizKcm9uTTctFyXZfny5YiMjMShQ4cwYsQIg88TBAHR0dGObp7lXLzZWlBQEKZMmYK5c+dCqVTi4YcfBgB0794d//vf/7Bnzx6EhobinXfeQVFRkdnhZvTo0ejRowceeughLFq0CEqlEi+99JJDPgMROZDB3bevACsfBHyDgLrKpuPh3aQemj6/kzadJGoj3GoqeHl5OQAgLCzM6HmVlZVISEhAfHw8Jk2ahOPHjxs8t7a2FkqlUufLoTTrNchjdI/LY6XjDp7W+Nhjj+H69esYO3astkbm73//OwYMGICxY8ciPT0d0dHRmDx5stmvKZPJsHr1alRXV2PIkCF4/PHH8frrrzvoExCRQxjdfbtRXSUQGAEMnQE8sQ2YdRBIf57BhtocQRRFI//SnUetVuPuu+9GWVkZdu3aZfC8jIwMnDlzBn379kV5eTn+9a9/YefOnTh+/HirBewAYP78+XjllVdaHS8vL4dcrrvWQk1NDXJzc5GUlAR/f38bP5DzVih2V3a9nkRkGVUDUJYHlJyWvs7vBM5tMf28B9cCXdMd3jwiSymVSigUCr3375bcJtzMmDEDP//8M3bt2qU3pBhSX1+P3r17Y+rUqXjttddaPV5bW4va2lrtz0qlEvHx8Y4PN8TrSeQMtZXAtTNAyRkpxFw9JX1feg5QWTFp4L5PpWEoIjdjSbhxi6ngs2bNwvr167Fz506Lgg0A+Pj4IDU1FWfPntX7uJ+fH/z8/OzRTCIiy9mjF1cUpedremGuNv5ZcgZQXjL8PO8AoGM3oGMPwMsPOPKN6fcKirKsbURuyKXhRhRF/OlPf8Lq1auxfft2JCUlWfwaKpUKx44dw5133umAFhIR2cDSfeZU9dLu2c17YDQhprbc8Pt0iJACTMfuQMeeTd8r4gFZY2mlWgXkbjc9m5O7b5MHcGm4mTlzJr755husXbsWwcHBKCwsBAAoFAoEBAQAAKZPn464uDgsWLAAAPDqq69i2LBh6NatG8rKyrBo0SLk5eXh8ccfd9nnICJqxeDMpMZ95kbPA4JjGkNMY4ApPQ+o6/W/niCTtjXQF2ICjU/CAODy2ZxEzuTScLNkyRIA0G4XoLFs2TLtNOaLFy9CJmua1HX9+nU88cQTKCwsRGhoKAYOHIg9e/aYPa3ZHG5ShtTm8TpSu2V0ZlLjsS3z9T/XJ7AxvPRoDDCN34d3BbxtHGLn7tvUTrhNQbGzGCtIUqlUOH36NCIjIxEeHu6iFnqOa9euobi4GD169ICXF38bJA9VdwO4drZxCOmMVNx7+TBw/bzp50b1AeKHNOuN6QHI45qGkhyFszmpDWpzBcXuwsvLCyEhISguLgYABAYGOnb1Yw8liiKqqqpQXFyMkJAQBhtyDkfesNVqoOJK4/DRWenPa2ek740V9Jpyy2zXzEzi7tvk4RhuWtCsfKwJOGS9kJAQ91xJmjyPpYW7hujrhSk5Le2EXV9l+HmB4UB496aZSQ31wK+tl6ZohTOTiByC4aYFQRAQExODyMhI1NcbKOwjk3x8fNhjQ85hqnC35crgajWgvNxsbZgzTd8rLxt+H5k3EJrUOITUGGLCu+sv6FWrgEOfcmYSkYsw3Bjg5eXFmzORuzOncPfHZ4DCY1KPzLUzFvTCNH6FN9bChCYAXj7mtYszk4hciuGGiNquvD26Q1H6VJcCO9/SPSbzBsK66AkxZk6rNgdnJhG5DMMNEbVdlUXmnZdwC9DjjqahJEt6YWyRfDfQawJnJhE5GcMNEbVNFUVAzo/mnZv+gutmB3FmEpHTMdwQUdtSUQjsfhc4+BnQUGPiZBbuErVHDDdE1DboCzWdhgBd0oGdixpPYuEuETHcEJG7qygEdi0GDi3TDTW3zQW63AYIAhDdh4W7RKTFcENE7klfqIkfKtXPaEKNBgt3iagZhhsici/KAmD3YuDgMkBVKx2LHwqkz5WGoAxticLCXSJqxHBDRO5BeaWxp2Z5s1AzrLGnJt1wqCEiaoHhhohcy1CouW0ukDSSoYaILMZwQ0SuobwC7Po3cOjzplDTOa1xTRqGGiKyHsMNETmXNtQsB1R10rHOwxtDzQiGGiKyGcMNETlH+WUp1GR+rhtqbpsLJN7KUENEdsNwQ0SOpS/UJNws9dQw1BCRAzDcEJFjlF9qDDVftAg1czllm4gciuGGiCyjVhlfLK/8EvDbO8DhL5uFmltcu3klEbUrDDdEZL6cdQa2OVgIxA2QQk3mF4C6Xnos8VZg5PMMNUTkVAw3RGSenHXAyunQ3ZwS0orCKx8EBC9AVEnHEm9trKm5xenNJCJiuCEi09QqqcemZbABmo6JKmn46ba5DDVE5FIMN0RkmFot1dbkrNUdijKEvTVE5AYYbojcmaniXXuoUQJlecD1C8D1xj+b/6xZPdgclUX2bRsRkRUYbojclbHi3eS7zX8dVb00g6llaLl+QfqqLjX+fMELCOwI3DAjuARFmd8uIiIHYbghckdGi3enA/d/0RRwRBGoutYYWHJbhJgL0iJ6mkJfQwLDgZAEIDSx8avx+5AEQNEJEGTA4hTp/fXW3QhS8EoYbtPHJiKyB4YbIndjTvHumhlA1rdSiCnLA+oqjb+ml19TYNGEFk2ICUkA/OWm2zVuYWPgElq0rXGF4XFv2n/IjIjICgw3RO4mb4/p4t26SuD0T7rHgmN1e12ah5igKEAms61dyXdLPUZ6h8retGyojIjIgRhuiNxN6Xnzzuv/AHDTPVJ4UcQDPv4ObRYAKcD0muD4ImciIhsw3BC5i6pSYN9SYM/75p3fb6prVv6VeXHFYSJyaww3RK524xqw9wNg30dAXYV0TOYNqBsMPIHFu0RExjDcELlK5VUg4z/A/k+A+hvSsagUYMTfAAjAqocaT2TxLhGRJRhuiJytogjY8x5w4FOgoVo6Ft1X2mCy551Nhb8Ci3eJiKzBcEPkLMorwO53gUPLgYYa6VjsACnU9BgLCILu+SzeJSKyCsMNkaOV5QO7FwOZXwCqOulYp8HAyBeAbqNah5rmWLxLRGQxhhsiR7meB+x6Bzj8NaCul451TpN6arqkGw81RERkNYYbInsrPQ/89jZwZEXTjKfEW6VQk3gLQw0RkYMx3BDZS8lZ4Ld/AUdXNu3l1OU2YORznLZNROREDDdEtio+KYWa7O8BUS0d6zZGCjXxQ1zbNiKidojhhshaRceBnYuA42ugXYumx3hg5N+AuIGubBkRUbvGcENkqYKjwM63gBM/Nh3rdZfUUxPTz3XtIiIiAAw3ROa7nCn11JzS7MYtAMmTpBWFo1Nc2jQiImrCcEMEAGqV4cXyLh0EdiwEzvzSeLIApNwHjHgWiOztsiYTEZF+DDf2YuzmSO4tZ53+bQ4GPQbk7QbObZOOCTKgz/3ArX8FInq4pq1ERGQSw409GLo5jlvIPYDcXc46YOV06G5OCenvcttr0veCF9BvKnDrHCC8q9ObSERElmG4sZXBm2OBdPz+Lxhw3JVaJYXSln93zfkEAv/3G9Cxm9OaRUREtpG5ugFtmtGbY+OxjS9I55H7UKuAwmPAL3/X7W3Tp74KqChwTruIiMgu2HNji7w9Jm6OIqC8LNVtJI1wWrOohapS4NIBIH8/cGm/NOuprtL851cWOa5tRERkdww3tjD3pvfdg0CPsVLASRoBhHR2bLvaM7UKKD4hhZj8A9Kf1862Ps83GAjvAhQcMf2aQVH2bycRETkMw40tzL3p1ZQBR7+TvgAgNKkp6CSNBIIiHNZEj1dVKk3VvrRf6pm5nAnUVbQ+L7wb0GmItB1C/BAgopd0fHGKVB+ld2hRkArDuS8UEVGbwnBji4Th0s3P6M0xBpj0X2lo6vwO4PIh4Hqu9JX5uXRaZHJT2Em4GQgIceKHcDJbpsyr1cDVk01BJn8/cO1M6/N8g4C4AU1hptNgIDBM/2uOW9hYEC5A9++wcefucW9ySj8RURsjiKJoZKqI51EqlVAoFCgvL4dcLrf9BbWzpQC9N8eWs6VqK4C8DCB3h/RVeEz39QQZENNfCjpdRgLxwwDfQNvb6Q4snTJffR24dKhZr8whoFbZ+rywrk0hJn6IFBYtCSR62xUnBRvOdCMicguW3L8ZbuzBlpvjjWtA3i6pVyd3Z+ueCJmPdMNOGikFnriBgLevee1yp4UFDU2Z14TA338uLYynKfrNPwCUnGr9Oj4dpF6Z+CFSz0ynwUCHcNvb507XioiIWmG4McIh4Qaw381ReUUKObk7pcCjvKT7uE8g0DlN6tVJGgFE99X/Pu60sKBa1VjbYmxmWcthoUZhXRqHlwZLf0YmA14cTSUiam/aTLhZsGABfvjhB5w8eRIBAQEYPnw4Fi5ciJ49exp93qpVq/CPf/wDFy5cQPfu3bFw4ULceeedZr2nw8KNI4giUHq+Kezk7gSqSnTP8Q8BEm9p6tmJ6CntVm2sl8TWhQXVKml4qEZp4M9y3Z9LLwBXDpl+XS8/3eGlToOBDh2tbycREXmMNhNuxo0bhz/84Q8YPHgwGhoa8OKLLyI7Oxs5OTno0KGD3ufs2bMHI0aMwIIFC3DXXXfhm2++wcKFC5GZmYmUFNM7M7epcNOSKALFOU1B58Ku1jUoHSKl2UL11QZeRACCo4GHfwLqb5gOJvr+1DcbyR7u+QjoN8Uxr01ERG1amwk3LV29ehWRkZHYsWMHRozQv+jdlClTcOPGDaxfv157bNiwYejfvz+WLl1q8j3adLhpSdUgrdOS21ivc3Ev0GAo1DiAtz/gJwf85Xr+VDT9XFkI7H7X9Os9tB5IutXx7SYiojbHkvu3WxUvlJeXAwDCwgxM2wWQkZGBOXPm6BwbO3Ys1qxZo/f82tpa1NbWan9WKvXMtmmrvLyBTgOlr1vnAA21wM5F0pcpMh8gINRIMFEYeEwuPeYnt6yw+dgqridDRERO4TbhRq1WY/bs2bj55puNDi8VFhYiKkp38byoqCgUFhbqPX/BggV45ZVX7NpWt+XtJ9XemBNuHlztvF4SmRfXkyEiIqdxm40zZ86ciezsbKxYscKurzt37lyUl5drv/Lz8+36+m5Hs7CgJjS0IkjT1J3dS5J8t1TILI/RPS6P5c7pRERkV27RczNr1iysX78eO3fuRKdOnYyeGx0djaIi3T2dioqKEB0drfd8Pz8/+Pn52a2tbs+de0mS7wZ6TeB6MkRE5FAu7bkRRRGzZs3C6tWrsW3bNiQlJZl8TlpaGrZu3apzbPPmzUhLS3NUM9sed+4lkXlJw2F9fif9yWBDRER25tKem5kzZ+Kbb77B2rVrERwcrK2bUSgUCAgIAABMnz4dcXFxWLBgAQDgmWeewciRI/H2229jwoQJWLFiBQ4ePIiPPvrIZZ/DLbGXhIiI2imXTgUXBP11IcuWLcPDDz8MAEhPT0diYiKWL1+ufXzVqlX4+9//rl3E76233vLMRfyIiIgIQBte58YZGG6IiIjaHkvu324zW4qIiIjIHhhuiIiIyKMw3BAREZFHYbghIiIij8JwQ0RERB6F4YaIiIg8CsMNEREReRSGGyIiIvIoDDdERETkURhuiIiIyKMw3BAREZFHYbghIiIij8JwQ0RERB6F4YaIiIg8CsMNEREReRSGGyIiIvIoDDdERETkURhuiIiIyKMw3BAREZFHYbghIiIij8JwQ0RERB6F4YaIiIg8CsMNEREReRSGGyIiIvIoDDdERETkURhuiIiIyKMw3BAREZFHYbghIiIij8JwQ0RERB6F4YaIiIg8CsMNEREReRSGGyIiIvIoDDdERETkURhuiIiIyKMw3BAREZFHYbghIiIij8JwQ0RERB6F4YaIiIg8CsMNEREReRSGGyIiIvIoDDdERETkURhuiIiIyKMw3BAREZFHYbghIiIij8JwQ0RERB6F4YaIiIg8CsMNEREReRSGGyIiIvIo3q5uAFlPpRaxP7cUxRU1iAz2x5CkMHjJBFc3i4iIyKUYbtqojdkFeOXHHBSU12iPxSj8MW9iMsalxLiwZURERK7FYak2aGN2AWZ8lakTbACgsLwGM77KxMbsAhe1jIiIyPUYbtoYlVrEKz/mQNTzmObYKz/mQKXWdwYREZHnY7hpY/bnlrbqsWlOBFBQXoP9uaXOaxQREZEbYbhpY4orDAcba84jIiLyNAw3bUxksL9dzyMiIvI0Lg03O3fuxMSJExEbGwtBELBmzRqj52/fvh2CILT6KiwsdE6D3cCQpDBEKwwHFwHSrKkhSWHOaxQREZEbsSrc5Ofn49KlS9qf9+/fj9mzZ+Ojjz6y6HVu3LiBfv364YMPPrDoeadOnUJBQYH2KzIy0qLnt2VeMgGjexn/vPMmJnO9GyIiaresWufmj3/8I5588kk8+OCDKCwsxJgxY3DTTTfh66+/RmFhIV5++WWzXmf8+PEYP368xe8fGRmJkJAQs86tra1FbW2t9melUmnx+7mTEwVKrDokBctgP29U1DZoH/P3kWHxlP5c54aIiNo1q3pusrOzMWTIEADAypUrkZKSgj179uDrr7/G8uXL7dk+vfr374+YmBiMGTMGu3fvNnruggULoFAotF/x8fEOb5+jVNY2YObXmahtUCO9ZwQy/zEG3z4xDH8b2xMAIIoibuke4eJWEhERuZZV4aa+vh5+fn4AgC1btuDuu+8GAPTq1QsFBY5bQC4mJgZLly7F999/j++//x7x8fFIT09HZmamwefMnTsX5eXl2q/8/HyHtc8RVGoRGeeuYe3hy/i/Lw7ifMkNRMv98c79/eHjLUNa13A8nd4VSR07oLZBxNYTRa5uMhERkUtZNSx10003YenSpZgwYQI2b96M1157DQBw5coVhIeH27WBzfXs2RM9e/bU/jx8+HCcO3cO//73v/Hll1/qfY6fn582iLU1+rZYAIBpwzojrIOv9mdBEDCxbwze23YWPx65gkn945zdVCIiIrdhVc/NwoUL8eGHHyI9PR1Tp05Fv379AADr1q3TDlc5y5AhQ3D27FmnvqczGNpiAQDe/uV0qy0WJvaLBQDsOH0V5VX1TmkjERGRO7Kq5yY9PR0lJSVQKpUIDQ3VHn/yyScRGBhot8aZIysrCzExnlVAa2yLBY1XfszBmORo7ayo7lHB6BUdjJOFFdh0vBD3D267tUVERES2sKrnprq6GrW1tdpgk5eXh8WLF+PUqVMWTcuurKxEVlYWsrKyAAC5ubnIysrCxYsXAUj1MtOnT9eev3jxYqxduxZnz55FdnY2Zs+ejW3btmHmzJnWfAy3Ze0WC5remx+PXnFk84iIiNyaVeFm0qRJ+OKLLwAAZWVlGDp0KN5++21MnjwZS5YsMft1Dh48iNTUVKSmpgIA5syZg9TUVO1U8oKCAm3QAYC6ujr89a9/RZ8+fTBy5EgcOXIEW7ZswahRo6z5GG7L2i0W7uor9WDtPluCkspafU8hIiLyeFaFm8zMTNx6660AgP/973+IiopCXl4evvjiC7z33ntmv056ejpEUWz1pZlOvnz5cmzfvl17/nPPPYezZ8+iuroa165dw6+//orbbrvNmo/g1jr4mjda2HKLhYTwDujXSQG1CPx8zHGz1oiIiNyZVeGmqqoKwcHBAIBffvkF9957L2QyGYYNG4a8vDy7NtDTaad6Z11GxrlrKKmoxTubTxl9jrEtFrRDU0cYboiIqH2yqqC4W7duWLNmDe655x5s2rQJf/nLXwAAxcXFkMvldm2gJ9M31dtbJqBBLSLIzxuVtQ0QAJ3CYs2mCoa2WJjQNwb/3HAC+y+UoqC8GjGKAId+BiIiIndjVc/Nyy+/jGeffRaJiYkYMmQI0tLSAEi9OJr6GTLO0FTvBrUUZeaM6YGl0wa02iQzWuGPJdMGGNxiIUYRgCGJUo/OhqPsvSEiovZHEEXR2IxjgwoLC1FQUIB+/fpBJpMy0v79+yGXy9GrVy+7NtKelEolFAoFysvLXdbLpFKLuGXhNqMzomIU/tj1/O0ApNlTxRU1iAyWhqJMbYr5RcYFvLz2OPp1UmDtrFvs2nYiIiJXsOT+bdWwFABER0cjOjpauzt4p06dnL6AX1tlaqo30DTVO61rONK6Wrbq8/iUGMxfdxxHLpVjzeFLEATB7GBERETU1lk1LKVWq/Hqq69CoVAgISEBCQkJCAkJwWuvvQa1Wm3vNnoca6d6mysi2A89oqSC79nfHcEzK7Iw9eO9uGXhtlYrGxMREXkaq3puXnrpJXz66ad48803cfPNNwMAdu3ahfnz56Ompgavv/66XRvpaVpO4bb1vJY2ZhfgZGFFq+OF5TWY8VWm0ZodIiKits6qcPP555/jk08+0e4GDgB9+/ZFXFwcnn76aYYbE4YkhUHu7w1lTYPexwVIhcP6pnqbotm6QR+x8bVbbt1ARETkSawaliotLdVbNNyrVy+UlpbqeQY1t/f8NVTWGg42gOGp3qZYu3UDERGRp7Aq3PTr1w/vv/9+q+Pvv/8++vbta3OjPE3zhfp+yLyEp748CLUIDEkMtXiqtymOruchIiJyd1YNS7311luYMGECtmzZol3jJiMjA/n5+fjpp5/s2sC2Tt9CfQCQ1DEQXzw2FD5eMounehvj6HoeIiIid2dVz83IkSNx+vRp3HPPPSgrK0NZWRnuvfdeHD9+HF9++aW929hmGVqoDwByS6qw/VQxvGQC0rqGY1L/OKR1Dbe5DmZIUhhiFP4w9CrGtm4gIiLyBFYv4qfPkSNHMGDAAKhUKnu9pN05axE/Uwv1aYqGdz1/u90LezWhCtC/dQNnSxERUVtjyf3bqp4bMs2Vhb3jUmKwxIqtG4iIiDyB1SsUk3GuLuwdlxKDMcnRWLTxJJbuPI/e0cFY/+dbOf2biIg8HntuHMQdCnu9ZAImpcYBAC6VVYO5hoiI2gOLem7uvfdeo4+XlZXZ0haPoinsLSyvgb6iJlsW6rNEt8gg+HrLUFHTgPzSanQOD3To+xEREbmaRT03CoXC6FdCQgKmT5/uqLa2KV4yAfMmJgNAq5lLti7UZwkfLxl6Nu4zlX2l3KHvRURE5A4s6rlZtmyZo9rhkTSFvS3XuYlW+GPexGSnFfbeFCvHscvlOH6lHHf2YTExERF5NhYUO5imsNeeC/VZ6qZYacrc8StKp70nERGRqzDcOIFmoT5XuSlOAYDhhoiI2gfOlmoHekfLIROAqxW1KFZyTykiIvJsDDftQICvF7pEBAFg7w0REXk+hpt2oqnuhjOmiIjIszHctBMsKiYiovaC4aaduCmWRcVERNQ+MNy0E5qem4ulVVDW1Lu4NURERI7DcNNOhAT6Ii4kAACQw94bIiLyYAw37Yim9yb7MouKiYjIczHctCOauhv23BARkSdjuGlHOGOKiIjaA4abduSmOCncnL1aiZp6lYtbQ0RE5BgMN+1ItNwf4R18oVKLOFVY4ermEBEROQTDTTsiCAKSOTRFREQejuGmndEUFWdzGwYiIvJQDDftDIuKiYjI0zHctDOacHOyQIkGldrFrSEiIrI/hpt2JjG8Azr4eqG2QY3zJTdc3RwiIiK7Y7hpZ2Sy5kXFrLshIiLPw3DTDml3CL/MuhsiIvI8DDftkKbnhjOmiIjIEzHctEOaouKcK0qIouji1hAREdmXt6sbQM7XPTIYPl4ClDUNuHS9GvFhgU57b5VaxP7cUhRX1CAy2B9DksLgJROc9v5EROT5GG7aIV9vGXpGByP7shLHr5TbPdwYCjAbswvwyo85KCiv0Z4bo/DHvInJGJcSY9c2EBFR+8Vw0071jpYj+7ISqzMvQxHga7ceFEMB5u5+MfhoZy5aDoIVltdgxleZWDJtAAMOERHZhSC2s6ILpVIJhUKB8vJyyOVyVzfHJTZmF+C5/x2FsqZBe8wePSgbswsw46vMVgHGFAFAtMIfu56/nUNURESklyX3bxYUtzOaANI82ABNPSgbswusel2VWsQrP+ZYHGwAQARQUF6D/bmlVr03ERFRcww37YixAKI59sqPOVCpLY8o+3NLdYairFFcYdvziYiIAIabdsVUALGlB8UewSQy2N/m1yAiImJBcTtibgCxJqjYEkw0NTdDksKsfg0iIiIN9ty0I+YGEGuCypCkMEQrTD/PULnwvInJLCYmIiK7YLhpR4YkhSFG4W8wYAiQZk1Z04PiJRNwR3KUwdcVAPzfiKRWAcjPW8Zp4EREZFcMN+2Il0zAvInJAOzfg3K1ohZrs64AAIL9dUc7oxX+WDJtAObemYxdz9+Ob58Yhr+N7Sm1SQBG9dYfioiIiKzBmpt2ZlxKDJZMG9BqoT0BwHtT+1vdgzJ/3XGUV9cjJU6O758ajsyLZXq3WPCSCUjrGo6hSWH4+LfzKKuqx9FL5RiYEGqPj0dEROTanpudO3di4sSJiI2NhSAIWLNmjcnnbN++HQMGDICfnx+6deuG5cuXO7ydnmZcSoy2B2XxlP4IDfSBCCDAx7qs+8vxQmw4VgAvmYCF9/WFn48X0rqGY1L/OKR1DdfbEySTCUjrEg4A2HO2xJaPQ0REpMOl4ebGjRvo168fPvjgA7POz83NxYQJE3DbbbchKysLs2fPxuOPP45NmzY5uKWeR9ODMjk1DpNT4wAAPx0zbwE/lVpExrlrWJt1GVtPFOHva44BAJ4c0QU3xSrMbsPwbh0BAHvOXbOw9URERIa5dFhq/PjxGD9+vNnnL126FElJSXj77bcBAL1798auXbvw73//G2PHjnVUMz3ehD4xWLb7AjbnFKG2QQU/by+D5+rbOwoAIoP98Myo7ha97/CuUs/NoYvXUVOvgr+P4fclIiIyV5sqKM7IyMDo0aN1jo0dOxYZGRkGn1NbWwulUqnzRboGdA5FtNwfFbUN2HXG8BCRZusGfQsBFlfUYvupYovet0vHDoiS+6GuQY1DedctbjcREZE+bSrcFBYWIipKd2ZNVFQUlEolqqur9T5nwYIFUCgU2q/4+HhnNLVNkckEjEuJBgBsMDA0ZWrvKAGWb90gCAJu7qoZmmLdDRER2UebCjfWmDt3LsrLy7Vf+fn5rm6SW5rQV5olpRmaaslRWzekNQ5Nse6GiIjspU2Fm+joaBQVFekcKyoqglwuR0BAgN7n+Pn5QS6X63xRawM7hyJK7oeKmgbs1jN7yVFbN2iKio9eKkdFTb1FzyUiItKnTYWbtLQ0bN26VefY5s2bkZaW5qIWeQ6ZTMD4xjVu1h9tPTTlqK0b4kICkBgeCJVatGrDTiIiopZcGm4qKyuRlZWFrKwsANJU76ysLFy8eBGANKQ0ffp07flPPfUUzp8/j+eeew4nT57Ef//7X6xcuRJ/+ctfXNF8j3NnH8NDU5qtGwyxZeuGtK6cEk5ERPbj0nBz8OBBpKamIjU1FQAwZ84cpKam4uWXXwYAFBQUaIMOACQlJWHDhg3YvHkz+vXrh7fffhuffPIJp4HbyaCEUEQG6x+a8pIJmDJYfzG2Zok+a7duuLmbVHejbziMiIjIUi5d5yY9PR2iaHh2jb7Vh9PT03H48GEHtqr9kskE3NknBsv3XMCGo4W4vVfTzLSKmnqsOngJABDo64WquqaenWiFP+ZNTLZ664ZhjSsVnyyswLXKWoQH+dnwKYiIqL3j3lKkQxNuNucUoq6hD3y9pc691zecwOWyasSHBWD9n25FzhWl3r2jrNExyA+9ooNxsrACe8+XamduOZOm5sden4mIiFyH4YZ0aIamiitq8elv5xEbGoAr16ux4kA+BAFY9Lt+UAT4aKdw28vwrh1xsrACu8+VOD3c6Ft1OcbG3igiInIdhhvSIZMJSI6Ro7jiKhZuOqXz2O09I7VDSPY2vGs4PtudiwwnFxVrVl1uOThaWF6DGV9lYsm0AQw4RERtTJuaCk6OtzG7ANtPX9X72LaTxdiYbd7mmpYa0iUMMgHILbmBK2X6V5u2N2OrLmuOWbrqMhERuR7DDWlpbvbGOOpmL/f3Qd9OIQCcNyXcUasuExGRazHckJarb/bDtVsxOGdKuKNWXSYiItdiuCEtV9/shzcu5rf9VDHWHr6MjHPXHDok5KhVl4mIyLVYUExarr7Zl96obfyzHs98lwXAsbOWYhT+kAmAofwkQFrDx5pVl4mIyHXYc0Nami0WDK3uYssWC6ZszC7AMyuyWh3XzFqytZBZpRaRce4a1mZJPUJFyho8svyANti0/My2rrpMRESuw54b0vKSCZg3MRkzvsqEAOjMInLkzd7UrCUBUiHzmORoq95b3zo2PjIB9WoRcSEBmHV7N7y39YzO47auukxERK7DnhvSMS4lBkumDUB0i00yoxX+DlvzxZGFzJp1bFq+fn1jl80TI5IwdUhn7Hr+dswY2RUAkBIrx67nb2ewISJqo9hzQ62MS4nBmORop21H4KhCZmM9Qhof7jiPB4clwksmYESPCCzZcQ5VdSoORRERtWEMN6SXl0yw+xYLhjiqkNlUjxDQ1COU1jUcnUIDAACXy6ohiiIEgQGHiKgt4rAUuZyjCpkt7RGKkvtDEIDaBjVKKussei8iInIfDDfkcppCZqD1rCUNawqZLe0R8vWWIarxe2dtAUFERPbHcENuwVAhMwDMGdPDquJea3qE4poNTRERUdvEcENuY1xKDHY9fzu+fWIY3v1Df4zqHQkAOHKp3KrXa94j1JKhqe2xIVK4Yc8NEVHbxXBDbkVTyDypfxxevLM3AGDLiSKcu1pp1euNS4nBf/6Y2qr3xtDU9rjGcHPpOsMNEVFbxdlS5La6RgRhdO8obDlRhE9+y8WCe/tY9TrhHfwgApD7e+PVSSmIkhue2h4XIg2LcViKiKjtYs8NubUnR3QBAHyfeQkllbVWvcb2U8UAgNHJUZicGoe0ruEGi5M1NTccliIiarsYbsitDU4MRb/4ENQ1qPFFRp5Vr/FrY7i5rWekyXPjQgIBsOeGiKgtY7ghtyYIAp68Veq9+TLjAqrrVBY9/9L1KpwuqoRMAEZ0jzB5fmzjsFRZVT1u1DZY3mAiInI5hhtye2NvikJ8WACuV9Vj0aaT2p29VWpjGytItp+6CgAYmBAKRaCPyfOD/X0g95dK0Tg0RUTUNrGgmNyet5cMaV3CkV96CZ/tvqA9HmPGzt2aept0M4akNGJDAqAsrMClsmp0jwq2ut1EROQa7Lkht7cxuwCrDl5qdbywvAYzvsrExuwCvc+rqVdh99lrAMyrt9HoxKJiIqI2jeGG3Jqxnb01x175MUfvENX+3FJU16sQJfdD7xjze2A0a91c5lo3RERtEsMNuTVTO3uLaNrZu6Xms6Qs2eGbqxQTEbVtDDfk1izd2bs5TTGxJfU2APeXIiJq6xhuyK1ZurO3Rm7JDeSW3ICPl4Cbu4Vb9J6xHJYiImrTGG7IrZna2RtovbM30DRLanBiGIL9TU8Bb65TY7gpVNagQaW26LlEROR6DDfk1prv7G0o4Lx8V3Kr7RR+bRySsmSWlEbHID/4esmgFqWAQ0REbQvDDbm9cSkxWDJtAKIV+oeoyqvrdX6uqmvA3vONU8B7mV6VuCWZTEBM40rFV8oYboiI2hou4kdtwriUGIxJjsb+3FIUV9QgMtgfxy6X4Y2fTuL1n07gtl6RiJJLgSTj3DXUNajRKTQAXSOCrHq/uJAA5F2rwuWyKgBhJs8nIiL3wZ4bajO8ZALSuoZjUn9pZ+/HbumCfp0UqKhpwLy1x7XnWTsFvDkWFRMRtV0MN9RmeckEvHlfX3jLBGw8Xoifjl5BxrkSbDgqrVg8skdHq19bu5Afh6WIiNocDktRm9Y7Ro6nRnbF+7+exaxvD6P5QsV/X5ONBrVodO8pQ7jWDRFR28WeG2rzekRJdTUtd2AoUtYa3XvKmKYtGKpsbp9KLSLj3DWLdjMnIiLrseeG2jSVWsSCn0/qfUyENH38lR9zMCY5utV0cWPitFsw1EAURatrdzZmF+CVH3N0tpBovpu5Si3qFEkPSQqzqJ1ERNQaww21aZbsPZXW1fyVijXTzqvrVbheVY+wDr4Wt21jdgFmfJXZatNPzW7mT45IwrojBQaDDxERWYfDUtSm2bL3lDH+Pl6ICPYDYN0GmqZ2MxcBfLgzt1Uw0wQfa4bSiIhIwnBDbZq1e0+ZQzM0dcmK6eCmepQM0YShV37MYW0OEZGVGG6oTTO195QA/XtPmaNpOrjl4cbSnqLmmg+lERGR5RhuqE0ztveU5ud5E1vvPWUOzXRwU8NS+mZD2aMk2JaARETUnrGgmNo8zd5TLWclRdtYnBvbWFRsbJVifbOhwjr4oqauwar3bM6aoTQiImK4IQ+hb+8pW6dVx4UGAgCulOsPN4ZmQ5XeqAMAdAzyRUllHQRAb2GxIQKkYGbNUBoREXFYijxIy72nbF0vJs7I/lLGZkNpeHvJ8N8/tt7NPEbhj/8bkQQBrYfSNKwdSiMiIvbcEBmkCTfXbtShuk6FAF8v7WPmzIYqLK9BaAdf7Hr+dr09SqmdQ1sNaQHAo7ckcZ0bIiIbMNwQGSAP8EaQnzcqaxtwpbwaXSOCtI9Zsr6OpkeppZZDaTtPX8X3mZdxMO+6TasiExG1dxyWIjJAEATEhugvKrbX+jrNh9JeGN8bvl4yHMkvQ+bF69Y1moiIGG6IjGnaY0o33DhifZ2IYD9MTo0FAHy6K9ea5hIRERhuiIzSrHXTciE/zfo6+gqKbVlf59FbkgAAG7MLkV9q+47kRETtEcMNkRGxRmZMjUuJQd84eavj0Qp/LJk2wKqi4F7RctzavSPUIrB8zwWLn9+cvsUF7cFRr0tEZC8sKCYywtgWDNcqa5FTUAEA+Nfv+8LHS2aX9XUeuyUJv50pwXcH8vHM6O6Q+/voPU+lFg2u66NvcUF77DjuqNclIrInhhsiI4yFmx+PXEGDWkTfTgr8bmC83d5zZI8IdIsMwtniSqw8kI/Hb+3S6hxjIQOA3sUFNTuOW9urZGjRQltfl4jI3jgsRWSEpuamsLym1fDL6sOXAQD3pMbZ9T0FQcBjjbU3n+3Kxa4zV3WGgDQho+X6OIXlNXjqq0y88MMxvbVAtuw4bmzRQu5kTkTuxi3CzQcffIDExET4+/tj6NCh2L9/v8Fzly9fDkEQdL78/bkHDzlGZLA/vGUCGtSizto2Z4srceRSObxlAib2i7X7+96TGocgP29cKa/BtE/345kVWZj68V7c/OZWk+GlrKre4Otau+O4qUULuZM5EbkTl4eb7777DnPmzMG8efOQmZmJfv36YezYsSguLjb4HLlcjoKCAu1XXl6eE1tM7YmXTNBun9C8qHj14UsAgPSeEegY5Gf3991+qhiVta033yxU1hoNL+YqLK+2qCjYkkULiYhczeU1N++88w6eeOIJPPLIIwCApUuXYsOGDfjss8/wwgsv6H2OIAiIjo426/Vra2tRW1ur/VmpVNreaGpX4kICcOl6NS6XVWMQALVaxOpMzZBUJ7u/n2YIyJFe23BCu8EnYLoo2F6LFhIROYNLe27q6upw6NAhjB49WntMJpNh9OjRyMjIMPi8yspKJCQkID4+HpMmTcLx48cNnrtgwQIoFArtV3y8/Qo/qX1oWVS8N/carpTXINjfG6N6R9r9/czZt8pWzYMN0FQUvDG7QO/5jli0kIjIUVwabkpKSqBSqRAVFaVzPCoqCoWFhXqf07NnT3z22WdYu3YtvvrqK6jVagwfPhyXLl3Se/7cuXNRXl6u/crPz7f75yDPpikq1qxSrOm1uatvDPx9vAw+z1q2DO0IAEICfYzuOK6PqaJgzaKFxp7v6p3Muf4OEWm4fFjKUmlpaUhLS9P+PHz4cPTu3RsffvghXnvttVbn+/n5wc/P/jUR1H7ENVvIr7pOhZ+OSb0b9w6w/5AUYP3QjiZWvHlvHwBoNVU8rIMPSm+YV2xsaKPPKUPisWJ/618QhiSFuXQaONffIaLmXBpuOnbsCC8vLxQVFekcLyoqMrumxsfHB6mpqTh79qwjmkjUtEpxWTV+ySnEjToV4sMCMCgh1CHvpxkCKiyvMbi9gyLQB/7eXihUNt3Mo1vczJvvOB4Z7I9CZQ3+8l2Wyfc31nN0qlBatHDasM4YnBiGGzUNeHFNNg5cKMXpogr0iAq25KPaBdffIaKWXBpufH19MXDgQGzduhWTJ08GAKjVamzduhWzZs0y6zVUKhWOHTuGO++804EtpfZMu7/U9Wr80KyQWBAcMwSjGQKa8VUmBEDnpt28d6ZleGm5MrJmx3GNjHPXzHp/Qz1H565W4vDFMnjJBPx5VHfteTvPlGDj8UK888tpLH1woCUf1Wam1t8RIPVgjUmOdumQGRE5l8ungs+ZMwcff/wxPv/8c5w4cQIzZszAjRs3tLOnpk+fjrlz52rPf/XVV/HLL7/g/PnzyMzMxLRp05CXl4fHH3/cVR+BPFysQgo3N+pU2HH6KgDgbgesbdPcuJQYLJk2QDsNXaP5vlWa8DKpfxzSuoabvHnbWhSsqTUa0b2jTgCac0cPCAKw8Xghjl0qN/sz2gPX3yEifVxeczNlyhRcvXoVL7/8MgoLC9G/f39s3LhRW2R88eJFyGRNGez69et44oknUFhYiNDQUAwcOBB79uxBcrLhYkciW+w4XQyZADSvT33w030Or+cYlxJjsnfGEsZ6hDQMFQWr1aJ2ReaWtUY9ooIxuX8cVh++jH/9cgqfPzrEqvZZw17r7xjbp4uI2h5BFMV2NaVAqVRCoVCgvLwccnnrHZ2JmjNUz6G57bXFeg59xbeANPvr/T8O0PucPedK8MeP9yHY3xsHXhrdapZY3rUbGPX2DjSoRax6Kg2DE50zJTzj3DVM/XivyfO+fWKY3iJpgMXIRG2FJfdvlw9LEbkrT91PaVxKDHY9fzu+fWIY3v1Df/x5VDcAwG9nSvSuigxAW2tkaPp7QngH/H6QtIbUWxtPIuNciVOmZA9JCkNksPHZkMaG2ozt02Vs3R8icm8uH5YicleW1HMY6hVwV82LjVVqEeuPFuD81Rv4em8e/m9kV51zq+oa8LMZ09//PKobVh3Mx4EL1zH1433a447sBZEJQJTcD8UVtQbPmdA3Ru8QE4uRiTwXe26IDGgv+yl5yQTMaAw0H/+Wi5p6lc7jm45L0987hwUanf5+JL8MDXp6aRzZC7LhWAGOXVbCSwZ0DPLVeayDr9TD9NXePGRdLGu1wB+LkYk8F3tuiAxoT/spTU6Nw+ItZ3C5rBrfHcjHQ8MTtY9phqTuHRBncPq7sf2wHNULUl5Vj/nrpPecdVt3/HlUd52i4IEJoXjyy4PYfuoq7l2yW6cgPEbhj3E3mbeWVlsPr0TtEXtuiAxoT/sp+XjJ8FS61Hvz4Y5zqGtQA5B6XXadLQEA3Gtkk1BX9IK88dMJlFTWoltkEJ6+rWurqfG+3jLtlP2WHUoF5TVYtueCWe/jCeG1JW5VQZ6OPTdEBpizmJ6r91Oyp98P7IT/bD2DK+U1WHP4Mu4fHI/Vhy9DFIEhiWHoHB5o8LnOGMJrPl27pKIW3x2UtoFYcG8f+Hm3LnJWqUUs2nTK6vcTIK0r5AnhtTnODqP2gOGGyAjNYnotbwYttzrwBP4+XnhyRBf8c8MJfPDrGcSG+OPzPbkAgMmpxhctdPQQnqHp67d272hw2rklu6tbuu5PW8WtKqi9YLghMsHei+m5sz8O7Yx/bz6NvNJqTPt0v/b4u1vPIKyDr8Ebn6n9sADrh/AM3ZABYNeZEmzMLtDbLnN7iR69ORE/Zxe2CkIv3tnbo270nB1G7QlrbojMYOlWB23VztNXcaNO1ep4sbLW6IwnzRAeAIM1SuNSLL9pGrshaxhaa8jcXqIxydE66/5opsgfu+zcrSQcjbPDqD1huCEiAKZnPAHGFy00tB9WBz+pHubrvRex7/w1iwpZbbkhW1IQ3jy8/n1CbwDA+qNXcPFaldH2tSXtZWkDIoDDUkTUyB6LFuobwhuUEIo/fXsYG48X4g8f74XYYkq2sdolW27I1haE3xSrwMgeEdhx+io+3HkOr9/Tx6w2uLv2tLQBEXtuiAiA/X6zbzmE5+Mtw/g+0poyLXeyM7XAn603ZHN2V9fn6cZp8asOXfKYngxNT5YhnrS0ARF7bogIgON+s1epRbz580m9j5kqZNXckA31KJkzXduagvAhSWEY0DkEmRfLsGz3BTw/rpepj2mQu+w47iUT8OKdvfGnbw8bPMfTZodR+8VwQ0QATM94snbdF1uGu7xkAmaP6o7nfzimtz2AeTfk5ntpmUMQBMxI74YnvjiIrzLyMCO9K+T+PmY/X8Pd1pS5XlUHQNqTq3m5kwDgvan9PWp2GLVvHJYiIgDGZzzZsmihrcNdB/KuAwB8vHTf19TQkq1G9YpEj6ggVNQ24I0NJyxezdfddhyvqmvAe1vPApD+Hr99Yhj+PaU/QgN9IALw8Wq9EKKzceVkshf23BCRliMWLbRluOvYpXL879AlAMA3TwxDg0p02vCOTCZgeNdwnC6qxIoD+VhxQFoR2ZyeF3PXlLm9VxQO5V13ymdatvsCSiprER8WgKlDEuDrLf1ue7JAiQ93nsfqw5cwLsW8/bYcwd16uahtY7ghIh32XrTQnAX+IoP9Wg13iaKIV9cfBwBM7h9rcCViR9mYXYDP9+S1Om7Oar7mDsUNW7AVpTfqtMcddTMvr6rHhzvOAQD+OqanNtgAwD0D4vDhzvPYdrIYZVV1CAn0NfQyDuPOKye7smbKXeq12iKGGyJqxdIaFVOvZWhKtoaPl4CaehU6+DX9l/TTsUIcuHAd/j4yPGdDQa81bF3N19yhuObBBnDczXzpznNQ1jSgV3SwdjNRjV7RciTHyJFToMSPRwvw4LAEu72vOdx55WRX9iaxJ8s2rLkhIoczNCU7MtgPwf7euFxWg2dXHUF9gxoZ567hf4fyMW9tNgDg/0Z0RWxIgFPba+tqvtauFWPOYonm0tSvfJlxAZ/8dh4A8OwdPSHTExDuHRAHAFidecmm97SGu66c7MqaKXer12qL2HNDRE5haLgrK/86pn60Dz9nFyL1tc2orG3QPkcmAF0iOji9rbYWQfeMCoa3TECDFQHFnMUSTdH3W7+Pl4B6lVrv+Xf3i8UbP51A5sUyXCi5gcSOhq+5vYdK3HHlZFf2JrlzT1Zbwp4bInIafXt0DUwIw5TB8QCgE2wAabry7BVZTv9N1ZYi6NoGFZ7+5pBVwaY5a2/mhn7rr1eJePpr/b/1R8r9cUv3CADA6sOXjb72LQu3YerHe/HMiixM/Xgvblm4zaa/H3dcOdmVvUnu2pPV1jDcEJFLqdQitpwoMnqOPYZpLGFqXyoA8POWITlW3mL6cgn+tuoI9p4vRZCfN166s3erVYHDOpi3Xo41N3NbNhq9N7VxaOrwZYgtl5KG44ZKBieGwt/H+K3I2Ssnu7I3yR17stoiDksRkUvZY08rezOnCLq2QY273vsNtQ1qFFfU6jwmE4D/PjAAI3pE4NFbknSGcQYmhGLkol+Nzh6z9mZuy7W846YodPD1wsXSKhzKu45BzWanOXKo5Jv9F1FTr3+4TOO5cb0cOgTTcqgtJMBxAdRer8k9wIxjuCEil3LX31QNrfkTo/DHQ8MT8cGvZ5F/vVrvc9WitGgeoH/mmang9OdR3ay6mdtyLQN9vTEuJQbfZ17CD4cv64QbewXQliHC20vAa+ulneh/NyAOu89d03kfzUrKO04VY3L/WAiC/QOOvvokbzPGNBzVmzQkKQxBfl6orFXpfdzalcKbaw9TzBluiMil3Pk3VUNF0ADw6W+5qECD3ueZ6skwFJykol8RPx4pwJRBnfXObDLG1mt574A4fJ95CeuPXMG8icnw85ZWLbZHANUXIjTh5c4+0Vj0+35Qi9C51oIAPPDJPqzJuoJhXcORENbBrjdkQ+vrNDTrSDIUQFM7hzokEGw7WWww2GjYsgdYe5liznBDRC7lqD2t7EVfz0vGuWu4Wllr4Bnm9WToC04dg3xx9/u7sefcNXyy6zweu6WLwd+wW/72PTAhFD+bqHsxdS2HdQlHtNwfhcoaLN1+DokdOyAy2B/hZi7sZyg0GQoRmtKfMcnREAQBXgJaXa85Y3pg0aZTeOF73f3FbL0hm1OfFBLoA39vLxQqa3SOlVXVY9PxQmRevI4BnUOten99zl+txJzvsgAAt/WKwMmCilY9ZkOSwqz+zO68WKK9MdwQkUsZq2+xZU8rR7LXUJq+4PTyxGTM/eEYFm48iY92nkdJZesVjAG0+u3bz1uG2mZdDtZcSy+ZgL6dFCjMqcG/t5zRHm+5r5c+hoZpzAkRb208ibv7xeptV1K4/mnptt6QTQ21AUBZVT2+fmwAZDJBJ2DOWZmFtVlX8JfvsvDjrFtw/IrSqh6l5gE12N8Hb2zIQUVtA4YkhuGjBwdBJgjax8ur6zFv7XHsyy3FlpwijE6Osujztrcp5gw3RORyjtjTypEcOZT2h8Hx+Hb/RRy9VK4TbADphv7UV5l6n6cJNo/fmoRBCaFWXcuN2QX4Jaf1zLV6VdMt0dAwTWJ4B+i7J5oTIgz1cqnUIl7bkKP3OZbckPXVmJgbUEtu1GJS/zidY69OSsGB3FLkXavC0AVbUV3XNIxkbo+SvuEhAFAEeOP9B1Lh4yUV/jS/JpeuV+Ojnecxd/UxDEoMtWirDGcV7rtLPQ/DDRG5BXvvaeVIjhxKU4tSiNHHnMnwG44WYO743hZfS81v9sboG6YJC/TB9ap6ZJy/hn9vOYNnRnXXed/m5xqjL2zY44ZsqMbE3L3K9AVURYAP7h8cj8VbzugEG8C8HiVDw0MAUF7dgMy863qfO2dMD2w5UYTzV29g/rrjmDK4s9l/v84o3Heneh6GGyJyG/bc08qRHDmUJgUDw/U8pjS/2VtyLW0ZpvnuQD5eXH0M7209g8/3XEB5db32OQobplXbekM2FCIKymuw7sgVo69pLKCq1CK+a9wlviVTPUqmhumMPdffxwv/+n0/3PffPViTdQVrspo+g6kQ4ejCfXer5+EifkREVjC0X1a0wt+m/8jtMeXdmtewZJim5SrTfxzaGeNTogFAJ9jo+7klAYbrdSy5IesupngNdQ1qk7U+vt4yCECrxRpNBVRLepRatmvv+WtmP1efYqX+3kJTiyneFCs3WTsVJfezqrfRVD0P4PyFONlzQ0RkJUcMpdljyrs1r2HLb/YqtYjD+WUmn2tpL5ep4T+Nb/bl4S/fZekOl3XwQekN48GqrkGNv4zugRUHLlpUn2RuENycU4g5K7N0Xtvcnix972Fs6LBljxHQNK0+JNAHH+44p1M7pU+grxfqGtTw9ZZZ9G/aHRfiZLghIrKBvYfSzL2h62NLrY8tdUT7c0sN1gk1F9rBF6U3moqkTYUIU8N/mp9/PNq6t8JUsNFI7BiIXc/fbtHN3Nwg+NnuC62OmerJMvYe5oaI97edbRXYAGlG3TOjuuPLvXk6j0UE++FGbQNyS6ow5cMMFFfU6gRFU0Ne7rgQJ8MNEZEbMfeGbu9aH1vqiMy9af1jQm9EKwIs6uUyNpPuHxN6Y+7qbLMDgz6Rwf4WB1RbAqgpxkKkudf531tO6z1e26BGl4gOesNc5sXrmPrRXhy9XN7qec3rZvT1VHbwMy9KOHMhToYbIiI3Y2pqPNB6nRt7TJu3dkq+uTetaEWAVb1chob/9ueWWh1sbOnlMjeAWtMmwHCItDUcNB+2avn3MKBzKIL8vVFW1fp6aoa8XvjhGOavy9Hp1ekY5Avo2Wi15fs6eyFOhhsiIjdkqp7HUdPmrakjcsYq0/p6V6wd5rDH4pDGguCdKdH4VM+QVEshAT4oaxbOTIVIW3uMjNW+7M8t1Rtsmj9Xelz3HM1aTEG+3qisa3CbhTgZboiI3JSx4RJHTpu39LVdtcq0uT0ZYRbW+pjLWI+SOeHmgz+2nlZv7BrZq8dIXyi0tR6mg78X3vpdX7y2wT0W4mS4ISIim7lilWlze4x2/O02HMq77pDFIfUFQXPbNaxxKr0ljF3nPwyO19k2wxB9odDWIa8iZS1CO/haXJztKAw3RERkF85eZdrcHiNfb5lTF4d0dE+Wsd3qVxzIt2p40B5F0sUVNW6zECcX8SMiIrvR3NyaL/LnSI5aTNHd26XvOmtCFWD5woTGnmsuZ86GMkUQRRNlzh5GqVRCoVCgvLwccrnc1c0hIiI7cJcNG92hXbbs8aTvudFyP9Q0qFFeVW+0R2jX87c79LNZcv9muCEiIvIwtoQqfc/dnFOIGY070usbZnNGLxnDjREMN0RERJZz9a7flty/WVBMREREJjm7YNwWDDdERERkFneZDWUKZ0sRERGRR2G4ISIiIo/CcENEREQeheGGiIiIPArDDREREXkUhhsiIiLyKAw3RERE5FEYboiIiMijMNwQERGRR2l3KxRrttJSKpUubgkRERGZS3PfNmdLzHYXbioqKgAA8fHxLm4JERERWaqiogIKhcLoOe1uV3C1Wo0rV64gODgYgmC/zb6USiXi4+ORn5/P3cZN4LWyDK+X+XitzMdrZT5eK8s46nqJooiKigrExsZCJjNeVdPuem5kMhk6derksNeXy+X8x28mXivL8HqZj9fKfLxW5uO1sowjrpepHhsNFhQTERGRR2G4ISIiIo/CcGMnfn5+mDdvHvz8/FzdFLfHa2UZXi/z8VqZj9fKfLxWlnGH69XuCoqJiIjIs7HnhoiIiDwKww0RERF5FIYbIiIi8igMN0RERORRGG7s5IMPPkBiYiL8/f0xdOhQ7N+/39VNcrqdO3di4sSJiI2NhSAIWLNmjc7joiji5ZdfRkxMDAICAjB69GicOXNG55zS0lI88MADkMvlCAkJwWOPPYbKykonfgrHW7BgAQYPHozg4GBERkZi8uTJOHXqlM45NTU1mDlzJsLDwxEUFIT77rsPRUVFOudcvHgREyZMQGBgICIjI/G3v/0NDQ0NzvwoTrFkyRL07dtXuyBYWloafv75Z+3jvFaGvfnmmxAEAbNnz9Ye4/WSzJ8/H4Ig6Hz16tVL+zivU2uXL1/GtGnTEB4ejoCAAPTp0wcHDx7UPu5W/8eLZLMVK1aIvr6+4meffSYeP35cfOKJJ8SQkBCxqKjI1U1zqp9++kl86aWXxB9++EEEIK5evVrn8TfffFNUKBTimjVrxCNHjoh33323mJSUJFZXV2vPGTdunNivXz9x79694m+//SZ269ZNnDp1qpM/iWONHTtWXLZsmZidnS1mZWWJd955p9i5c2exsrJSe85TTz0lxsfHi1u3bhUPHjwoDhs2TBw+fLj28YaGBjElJUUcPXq0ePjwYfGnn34SO3bsKM6dO9cVH8mh1q1bJ27YsEE8ffq0eOrUKfHFF18UfXx8xOzsbFEUea0M2b9/v5iYmCj27dtXfOaZZ7THeb0k8+bNE2+66SaxoKBA+3X16lXt47xOukpLS8WEhATx4YcfFvft2yeeP39e3LRpk3j27FntOe70fzzDjR0MGTJEnDlzpvZnlUolxsbGigsWLHBhq1yrZbhRq9VidHS0uGjRIu2xsrIy0c/PT/z2229FURTFnJwcEYB44MAB7Tk///yzKAiCePnyZae13dmKi4tFAOKOHTtEUZSui4+Pj7hq1SrtOSdOnBABiBkZGaIoSkFSJpOJhYWF2nOWLFkiyuVysba21rkfwAVCQ0PFTz75hNfKgIqKCrF79+7i5s2bxZEjR2rDDa9Xk3nz5on9+vXT+xivU2vPP/+8eMsttxh83N3+j+ewlI3q6upw6NAhjB49WntMJpNh9OjRyMjIcGHL3Etubi4KCwt1rpNCocDQoUO11ykjIwMhISEYNGiQ9pzRo0dDJpNh3759Tm+zs5SXlwMAwsLCAACHDh1CfX29zrXq1asXOnfurHOt+vTpg6ioKO05Y8eOhVKpxPHjx53YeudSqVRYsWIFbty4gbS0NF4rA2bOnIkJEyboXBeA/7ZaOnPmDGJjY9GlSxc88MADuHjxIgBeJ33WrVuHQYMG4fe//z0iIyORmpqKjz/+WPu4u/0fz3Bjo5KSEqhUKp1/4AAQFRWFwsJCF7XK/WiuhbHrVFhYiMjISJ3Hvb29ERYW5rHXUq1WY/bs2bj55puRkpICQLoOvr6+CAkJ0Tm35bXSdy01j3maY8eOISgoCH5+fnjqqaewevVqJCcn81rpsWLFCmRmZmLBggWtHuP1ajJ06FAsX74cGzduxJIlS5Cbm4tbb70VFRUVvE56nD9/HkuWLEH37t2xadMmzJgxA3/+85/x+eefA3C//+Pb3a7gRO5k5syZyM7Oxq5du1zdFLfWs2dPZGVloby8HP/73//w0EMPYceOHa5ultvJz8/HM888g82bN8Pf39/VzXFr48eP137ft29fDB06FAkJCVi5ciUCAgJc2DL3pFarMWjQILzxxhsAgNTUVGRnZ2Pp0qV46KGHXNy61thzY6OOHTvCy8urVRV9UVERoqOjXdQq96O5FsauU3R0NIqLi3Ueb2hoQGlpqUdey1mzZmH9+vX49ddf0alTJ+3x6Oho1NXVoaysTOf8ltdK37XUPOZpfH190a1bNwwcOBALFixAv3798O677/JatXDo0CEUFxdjwIAB8Pb2hre3N3bs2IH33nsP3t7eiIqK4vUyICQkBD169MDZs2f570qPmJgYJCcn6xzr3bu3dijP3f6PZ7ixka+vLwYOHIitW7dqj6nVamzduhVpaWkubJl7SUpKQnR0tM51UiqV2Ldvn/Y6paWloaysDIcOHdKes23bNqjVagwdOtTpbXYUURQxa9YsrF69Gtu2bUNSUpLO4wMHDoSPj4/OtTp16hQuXryoc62OHTum8x/F5s2bIZfLW/0H5InUajVqa2t5rVoYNWoUjh07hqysLO3XoEGD8MADD2i/5/XSr7KyEufOnUNMTAz/Xelx8803t1qy4vTp00hISADghv/H27U8uZ1asWKF6OfnJy5fvlzMyckRn3zySTEkJESnir49qKioEA8fPiwePnxYBCC+88474uHDh8W8vDxRFKVpgiEhIeLatWvFo0ePipMmTdI7TTA1NVXct2+fuGvXLrF79+4eNxV8xowZokKhELdv364zDbWqqkp7zlNPPSV27txZ3LZtm3jw4EExLS1NTEtL0z6umYZ6xx13iFlZWeLGjRvFiIgIj5yG+sILL4g7duwQc3NzxaNHj4ovvPCCKAiC+Msvv4iiyGtlSvPZUqLI66Xx17/+Vdy+fbuYm5sr7t69Wxw9erTYsWNHsbi4WBRFXqeW9u/fL3p7e4uvv/66eObMGfHrr78WAwMDxa+++kp7jjv9H89wYyf/+c9/xM6dO4u+vr7ikCFDxL1797q6SU7366+/igBafT300EOiKEpTBf/xj3+IUVFRop+fnzhq1Cjx1KlTOq9x7do1cerUqWJQUJAol8vFRx55RKyoqHDBp3EcfdcIgLhs2TLtOdXV1eLTTz8thoaGioGBgeI999wjFhQU6LzOhQsXxPHjx4sBAQFix44dxb/+9a9ifX29kz+N4z366KNiQkKC6OvrK0ZERIijRo3SBhtR5LUypWW44fWSTJkyRYyJiRF9fX3FuLg4ccqUKTprtvA6tfbjjz+KKSkpop+fn9irVy/xo48+0nncnf6PF0RRFO3bF0RERETkOqy5ISIiIo/CcENEREQeheGGiIiIPArDDREREXkUhhsiIiLyKAw3RERE5FEYboiIiMijMNwQERGRR2G4IaJ2JzExEYsXL3Z1M4jIQRhuiMihHn74YUyePBkAkJ6ejtmzZzvtvZcvX46QkJBWxw8cOIAnn3zSae0gIufydnUDiIgsVVdXB19fX6ufHxERYcfWEJG7Yc8NETnFww8/jB07duDdd9+FIAgQBAEXLlwAAGRnZ2P8+PEICgpCVFQUHnzwQZSUlGifm56ejlmzZmH27Nno2LEjxo4dCwB455130KdPH3To0AHx8fF4+umnUVlZCQDYvn07HnnkEZSXl2vfb/78+QBaD0tdvHgRkyZNQlBQEORyOe6//34UFRVpH58/fz769++PL7/8EomJiVAoFPjDH/6AiooKx140IrIKww0ROcW7776LtLQ0PPHEEygoKEBBQQHi4+NRVlaG22+/HampqTh48CA2btyIoqIi3H///TrP//zzz+Hr64vdu3dj6dKlAACZTIb33nsPx48fx+eff45t27bhueeeAwAMHz4cixcvhlwu177fs88+26pdarUakyZNQmlpKXbs2IHNmzfj/PnzmDJlis55586dw5o1a7B+/XqsX78eO3bswJtvvumgq0VEtuCwFBE5hUKhgK+vLwIDAxEdHa09/v777yM1NRVvvPGG9thnn32G+Ph4nD59Gj169AAAdO/eHW+99ZbOazav30lMTMQ///lPPPXUU/jvf/8LX19fKBQKCIKg834tbd26FceOHUNubi7i4+MBAF988QVuuukmHDhwAIMHDwYghaDly5cjODgYAPDggw9i69ateP311227MERkd+y5ISKXOnLkCH799VcEBQVpv3r16gVA6i3RGDhwYKvnbtmyBaNGjUJcXByCg4Px4IMP4tq1a6iqqjL7/U+cOIH4+HhtsAGA5ORkhISE4MSJE9pjiYmJ2mADADExMSguLrbosxKRc7DnhohcqrKyEhMnTsTChQtbPRYTE6P9vkOHDjqPXbhwAXfddRdmzJiB119/HWFhYdi1axcee+wx1NXVITAw0K7t9PHx0flZEASo1Wq7vgcR2QfDDRE5ja+vL1Qqlc6xAQMG4Pvvv0diYiK8vc3/L+nQoUNQq9V4++23IZNJndArV640+X4t9e7dG/n5+cjPz9f23uTk5KCsrAzJyclmt4eI3AeHpYjIaRITE7Fv3z5cuHABJSUlUKvVmDlzJkpLSzF16lQcOHAA586dw6ZNm/DII48YDSbdunVDfX09/vOf/+D8+fP48ssvtYXGzd+vsrISW7duRUlJid7hqtGjR6NPnz544IEHkJmZif3792P69OkYOXIkBg0aZPdrQESOx3BDRE7z7LPPwsvLC8nJyYiIiMDFixcRGxuL3bt3Q6VS4Y477kCfPn0we/ZshISEaHtk9OnXrx/eeecdLFy4ECkpKfj666+xYMECnXOGDx+Op556ClOmTEFERESrgmRAGl5au3YtQkNDMWLECIwePRpdunTBd999Z/fPT0TOIYiiKLq6EURERET2wp4bIiIi8igMN0RERORRGG6IiIjIozDcEBERkUdhuCEiIiKPwnBDREREHoXhhoiIiDwKww0RERF5FIYbIiIi8igMN0RERORRGG6IiIjIo/w/5kXiVN3V07oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_its, train_losses = zip(*metrics.train_losses)\n",
    "val_its, val_losses = zip(*metrics.val_losses)\n",
    "plt.plot(train_its, train_losses, '-o')\n",
    "plt.plot(val_its, val_losses, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', \"Valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f216c",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each example in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b03c1040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 103/103 [03:12<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on 103 test samples:\n",
      "Accuracy: 0.019\n",
      "F1 Score: 0.038\n",
      "Perplexity: 4.603\n",
      "\n",
      "ROUGE Scores:\n",
      "rouge1: 0.303\n",
      "rouge2: 0.161\n",
      "rougeL: 0.247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    # Adjust generation parameters\n",
    "    generation_params = {\n",
    "        \"max_tokens\": 100,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    for example in tqdm(dataset):\n",
    "        # Get prompt and true answer\n",
    "        prompt = example[\"question\"]\n",
    "        true_answer = example[\"answer\"]\n",
    "        \n",
    "        # Generate prediction\n",
    "        response = generate(model, tokenizer, prompt, **generation_params)\n",
    "        \n",
    "        # Store prediction and true label\n",
    "        all_preds.append(response)\n",
    "        all_labels.append(true_answer)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(true_answer, response)\n",
    "        for key, score in scores.items():\n",
    "            rouge_scores[key].append(score.fmeasure)\n",
    "\n",
    "        # Calculate loss/perplexity\n",
    "        tokens = tokenizer.encode(prompt + true_answer)\n",
    "        tokens = mx.array(tokens)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(tokens[None])[0]\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        targets = tokens[1:]\n",
    "        logits = logits[:-1]\n",
    "        loss = mx.mean(nn.losses.cross_entropy(logits, targets))\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = sum(1 for x,y in zip(all_preds, all_labels) if x.strip() == y.strip()) / len(all_preds)\n",
    "    \n",
    "    # Convert predictions and labels to match format for F1\n",
    "    pred_labels = [1 if p.strip() == l.strip() else 0 for p,l in zip(all_preds, all_labels)]\n",
    "    true_labels = [1] * len(all_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='binary')\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge_scores = {key: np.mean(scores) for key, scores in rouge_scores.items()}\n",
    "\n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    perplexity = float(mx.exp(mx.array(avg_loss)))\n",
    "\n",
    "    return accuracy, f1, perplexity, avg_rouge_scores\n",
    "\n",
    "# Load test dataset\n",
    "test_set_path = \"./dataset/All_validation.jsonl\" \n",
    "with open(test_set_path, 'r') as file:\n",
    "    test_set = [json.loads(line) for line in file]\n",
    "\n",
    "# Define number of test samples\n",
    "num_test = len(test_set)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Evaluating model...\")\n",
    "accuracy, f1, perplexity, rouge_scrs = evaluate_model(model, tokenizer, test_set[:num_test])\n",
    "\n",
    "print(f\"Results on {num_test} test samples:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\") \n",
    "print(f\"Perplexity: {perplexity:.3f}\")\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for key, score in rouge_scrs.items():\n",
    "    print(f\"{key}: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "The adapted weights are: $\\tilde{W} = W + c \\cdot \\mathbf{b}^\\top \\mathbf{a}$. Note, this process can be destructive if the inputs are in low precision and they have very different magnitudes. Tuning the `scale` parameter, $c$, prior to fine-tuning can improve the model performance after fusion.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlx_lm.fuse --model {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `lora_fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate test accuracy 0.750\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"lora_fused_model\")\n",
    "num_correct = 0\n",
    "for prompt, answer in tqdm.tqdm(test_set[:num_test]):\n",
    "    response = generate(model, tokenizer, prompt, max_tokens=2)\n",
    "    num_correct += (response==answer)\n",
    "test_acc = num_correct / num_test\n",
    "print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your LoRA adapters are not working well it's critical to plot both the trianing loss and validation loss over the duration of fine-tuning. There are really only two cases to consider: underfitting or overfitting. And you can figure out which regime you are in based on the above plot.\n",
    "\n",
    "**Underfitting**: The trianing loss is not low enough and the validation loss closely matches the training loss. You could also measure the accuracy on the training set itself for question-answering style tasks like HellaSwag. If you are in this regime you have a few options to improve the results:\n",
    "\n",
    "- Use more adapters. Increase `lora_layers` or adapt more of the linear layers within a given block by setting `lora_parameters[\"keys\"]`.\n",
    "- Use a higher rank. A higher rank means more parameters per adapter.\n",
    "- If you are using dropout, decrease the droupout rate or turn it off entirely.\n",
    "- Sometimes, underfitting issues are really optimization issues. In these cases it can be helpful to tune the learning rate or learning rate schedule.\n",
    "- If none of the above works, try a bigger model. For example, try Phi-3 medium instead of Phi-3 tiny.\n",
    "\n",
    "**Overfitting**: The trianing loss keeps going down but the validation loss stops going down and even starts to go up. If you are in this regime you also have a few options:\n",
    "\n",
    "- The best thing to do is to use more trianing data if you have it.\n",
    "- Contrary to the underfitting regime decreasing the capacity of the model can help. For example, use fewer adapters, a lower LoRA rank, or a smaller model size.\n",
    "- If you are not using dropout, use it.\n",
    "\n",
    "If you find your adapters work well pre-fusion but stop working post-fusion, try tuning the `scale` parameter, $c$, prior to fine-tuning. Typically the adapters have a smaller magnitude than the weights, so using a larger scale helps.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "Fine-tuning a large LM with LoRA requires a machine with a decent amount of memory. Here are some tips to reduce memory use should you need to do so. \n",
    "\n",
    "- Try quantization (QLoRA). You can use QLoRA by generating a quantized model with `mlx_lm.convert` and the `-q` flag or by using an already quantized model from HuggingFace.\n",
    "\n",
    "- Try using a smaller batch size. You can set the `batch_size` parameter in the `TrainingArgs` or pass `--batch-size` if you are using the CLI. The default is 4 so setting this to 2 or 1 will reduce memory consumption. Note, this may slow things down a little..\n",
    "\n",
    "- Reduce the number of layers to fine-tune with by setting `lora_layers` to a smaller value or passing `--lora-layers` if you are using the CLI. The default is `16`, so you can try `8` or `4`. This reduces the amount of memory needed for back propagation. It may also reduce the quality of the fine-tuned model and you may need to compensate with a larger `rank`.\n",
    "\n",
    "- Longer examples require more memory. If it makes sense for your data, one thing you can do is break your examples into smaller sequences when making the `train`, `valid`, and `test` data sets.\n",
    "\n",
    "- Gradient checkpointing lets you trade-off memory use (less) for computation (more) by recomputing instead of storing intermediate values needed by the backward pass. You can use gradient checkpointing by passing `grad_checkpoint=True` to the `TrainingArgs` or the `--grad-checkpoint` flag if using the CLI. Gradient checkpointing will be more helpful for larger batch sizes or sequence lengths with smaller or quantized models.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project.\n",
    "- We also have an example of [LoRA fine-tuning in MLX Swift](https://github.com/ml-explore/mlx-swift-examples/tree/main/Applications/LoRATrainingExample)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
