{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to [LoRA fine-tune](https://arxiv.org/abs/2106.09685) an LLM with MLX LM. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it in the right format for MLX LM.\n",
    "2. Setup and run LoRA training. We'll show how to capture the training logs and plot some statistics to visualize the performance.\n",
    "3. Evaluate on the test set. We'll compute the final question-answer accuracy of the fine-tuned model.\n",
    "4. Fuse the resulting adapters into the base model and upload to Hugging Face.\n",
    "5. Discuss tips for debugging accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install mlx-lm\n",
    "# pip install matplotlib\n",
    "# pip install rouge-score\n",
    "# pip install scikit-learn\n",
    "# pip install tqdm\n",
    "# pip install numpy\n",
    "# pip install json\n",
    "# pip install pathlib\n",
    "# pip install transformers\n",
    "# pip install sentencepiece\n",
    "# pip install datasets\n",
    "# pip install torch\n",
    "# pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1131315",
   "metadata": {},
   "source": [
    "### MLFOW CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d1cf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae62a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d438bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_name=\"finetuning_comparativa\"\n",
    "exp_name=\"MLX1.0\"\n",
    "corrida_name=\"MLX-40_16_4e4\"\n",
    "# base_model = \"Llama-3.2-1B-Instruct\"\n",
    "# four_bits = True \n",
    "# dataset_path = \"FAQ_All.jsonl\"\n",
    "# dataset_type = \"alpaca_chat.load_qa\"\n",
    "# output_dir = \"../trained_models/adapters/adapters.safetensors\"\n",
    "output_path = \"../trained_models/adapters_40_16_4e4/\"\n",
    "output_dir = output_path + 'adapters.safetensors'\n",
    "sequence_len = 2048\n",
    "lora_layers = 8\n",
    "lora_layers_scale = 20.0\n",
    "grad_checkpoint_value = True\n",
    "\n",
    "\n",
    "lora_r = 8\n",
    "# lora_alpha = 16\n",
    "lora_dropout = 0.0\n",
    "# gradient_accumulation_steps = 4\n",
    "optimizer = \"adam\"\n",
    "# weight_decay_value = 0.02\n",
    "lr_scheduler = \"linear\"\n",
    "ds_len = 1024\n",
    "learning_rate_value = 4e-4\n",
    "batch_size = 16\n",
    "\n",
    "epochs = 40\n",
    "steps = ((ds_len // batch_size) * epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c0069b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/880645134898555871', creation_time=1733877767863, experiment_id='880645134898555871', last_update_time=1733877767863, lifecycle_stage='active', name='MLX1.0', tags={'mlflow.sharedViewState.b8046cb6ae65fcd7b63c4c15f84c0e6dddbdafa9dffbad22fd28a284a2c0bde2': 'deflate;eJzlV91v2zgM/1cGPxeDJMuWlLeu6HbFpbtD2xsGDEOmDyrx6o/AkrO6Q//3o+s4Sdfl7uWADriHPJj8ifyRFBnqexJAt3b1tigjtMksSU6SpnXQvul/hx6/dYxtYboI4XWIuo2LWFSwB50Gm8y8LgOcJI/6m0E9S07ncwSVhQfb2xJ25k9tLDbDeaejDhDDpPn0+SSpGgflB2hD0dT7E2X56qqrA54JUIKN4M6asqtQMvuUrHWrq/D6i9HRrhahuIcvCJykJcZWF/Vy0er4RFF31SK2uqgXsG7s6lBV6btFiLAOg7ACjN6iVFvbtdr2T4SeLoJtWngibJtuCfS5iD0XzZ+I1tCuS7grIjrBZLQY8/ndWtcOXDL7/jBK/izqevj+tEX8VjgH9f77QxEKU5Ro5FKv8VjC0kwRR63huedcGWkyJnlKc+pSJwVNZrHtsHhAhFMgdJ4qwlPwiotcgwfQxPs8FxMuzVOaplxmSjKufG5SlTpKUm6419yaCWcUFZRLIFamnFprrLBWGmMyQgwjbsIxz6Q3QkkClBNPFAG0TpXNhHJasAlnNVPGZwjgOQefyowJZ2hKMpVqne38ogswTJMcKIacEamApY4RklJwmqd7nOIMhNWZcJwZKjEECjQzufSWq1283GZgnRWUesYzlivwyD4zEgUeUzjhcuEkyxlTqTecO2eAEu0URd6WYConnHK5d9xRyoTmQLhmQhCvMqTsMcQdP+WtFMJISzLLQToNVilCMPdCakF38VJFMSqCNrEqDssi8kx4ZrG21kIqJ5xOrSaMypxLyS0wSZnKBXPGSQ3C+BH3cHirLrEdsQHfXlxd3ywoWVz99f4aL6xtKuwVwJ48W2HDD234Pem6AlORUJHifVAsF5liKjOKNMsWz8R+PZh6c3qVjB7Omq6ON83ZaCuZUTJ1wjW2OPb/xTNz9/lXU/JNPQwPHARxaIPt5CnCO6hhaPJdPUZj2xm2bd7k4eTnTO+rJfMbdv/iTKeJcpRpHzMuqjJ/eaZFrcvtDC2bEI4yXpEmZ2Zz+4sw3uDvH/neBr/pVAkvznf/f3CUq82/qj7S5sW5jn96R3k2qt/c37b81+DJjncXzWhVVZtfg+f8KM8O5K2LLu55zi/en78E0YMBMKx7NVxXTRNXNaBgho7vTu+KcG11CTcj0QGjh7+E8DNhq+sljNvO48nRybCOHax/HwfN5QEJ1PV79EhvkuEK1SIZDDuMW1KxrHHA/tHFssBNcxdnF+Bd2RhdftSTpR818114uFJu1ccqRGknelbRl6/QbuD9P+vz+XBh2ab3+MpykOZaPz5khiWofLVd0ofsF2Fd6n7KdBGu4PEpdFCQfy3Q0TXkm4j9V6333q97zG31X7vHnGwK+HaJmazwxbSH4uWcFyFOj4qdgVOLS4nDzB14GwkMycdhtQb3QZcdhIvdUjjqV4WD82od+6fi5XDmDYZRd2W5/fzhtaO72FyBx8uxOq+1KQ9oLnflfrR61tS+WA6Pned38idX/vg1fXj4G5Bw6oU=',\n",
       " 'mlflow.sharedViewState.bb15b83a1c8297e8bcab8dc38315c49bda8d8fec1b37088f0195711a12855512': 'deflate;eJzlVktv2kAQ/iuRz6jCJEDNjVCSokJaAY0iRRFZ1oPZsN619uFgEP+9s5iHm+D2UolIPfiw8/zmm/XOrD0NRNH5DeMGlNfyvIonVQjqOvsGGZ6JMYpNrQH9SRuizMSwGI5GbU291oxwDRVvqx87dctr9/toxNkMaEY5HMK3qWGp8w+JIRqM3msenypeLEPg96A0k+LowfnF0AqNPho4UANhR3Ibo6T1WIT3PJJWUXhGw6J04ILqt9IvefpnD9MqjN5dJkSEEHqt9SaX/GBCuPPjzuIrC0MQx/M902zKODPZgCRHt9zMJUXsN73haDzxq5Phz7sRIqAyTogCLKczR65cBWvPWoZpPL95eRlcBrVGsx7Ugvo0qMpIoY/JEhfquj308gwdaYUZy04ey2v5VWQOsDQ6QnaQut67cKvGy5RfpcLxjhwaV9euaUzfggBFtjKjLOyD7dpPqVWEZt6mchrpKo5qs7S2OjvSmT/RVGKaMqSZqV81Y944P1ImCJ8YRZiYcKl1KeJ5VTZq03TxQRCn+P0R70LPUhtwODveBFTCYYn/ZilW2ngJMuPLs2NV0kbgl+KUQZauFurqY+Cslf9dft2P4zj9GDj7pTgtfF6EJjRHnP3eXfccQAsPgJuUAkaxlGYuAAUtTLxsL5keUcJhnAN1NsSNBH1KqIiIIJ9DW888iTaQFCbng9MMCiBQlx2tc3h7Gc5EhWCwbJ2PPRYJfGC/W8MZDulDnVbDLZdTwh/IPtJbTf9QHhPRTl3WId+3zawW++fv0OHB+z/781RcWHb0lq8sBZoF2e6A283rIsfsOAyZTjjJ9kwzPYTtFlloyF8bVLqGvDZN9kLIMfsoQ27jf50eOUkZvA6QyZitCqZ4OftMm/2WeAjQpriUhMjc+2yOfXytEgjvCbege4etMEc4ZyF048Rkv4sj53ONdQjL+e74Zn8l1sghzPB2zLuCTHkhaXTo9zZqR4oZw46vT1zKE3e+/J5uNr8AWwkZ8g=='}>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"https://4z0r6nts-5000.usw3.devtunnels.ms/\") \n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(experiment_name=exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61698208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello stats: 1024 lenght trainging dataset\n",
      "An example:\n",
      "\n",
      "{\n",
      "  \"pregunta\": \"\\u00bfPuedo presentar una p\\u00f3liza de bonos en formato electr\\u00f3nico?\",\n",
      "  \"respuesta\": \"Este tipo de garant\\u00eda es v\\u00e1lida, \\u00bfno?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# jsonl file path\n",
    "\"\"\" save_path = \"./dataset/FAQ_All.jsonl\"\n",
    "with open(save_path, 'r') as file:\n",
    "\tdataset = [json.loads(line) for line in file]\n",
    "\"\"\"\n",
    "\n",
    "# csv file path\n",
    "save_path = \"./datasets/Parph_Data/FAQs_1000.csv\"\n",
    "dataset = []\n",
    "with open(save_path, 'r', encoding='utf-8') as file:\n",
    "\treader = csv.DictReader(file)\n",
    "\tfor row in reader:\n",
    "\t\tdataset.append(row)\n",
    "\n",
    "print(f\"Hello stats: {len(dataset)} lenght trainging dataset\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b607237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "valid_set = [dataset[i] for i in perm[:valid_size]]\n",
    "train_set = [dataset[i] for i in perm[valid_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c38c4e",
   "metadata": {},
   "source": [
    "Finally, put the data splits in the MLX LM training format. The format simply expects the data to be in a container which supports random access to the individual examples (e.g. a Python `list`):\n",
    "```\n",
    "[\"An example for the model.\", \"Another example for the model.\", ...]\n",
    "```\n",
    "For more details, see the [documentation on supported formats](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea738f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\"\"\"def preprocess(dataset):\n",
    "    return [\n",
    "        f\"Question: {clean_text(t['question'])}\\n\"\n",
    "        f\"Answer: {clean_text(t['answer'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\"\"\"\n",
    "def preprocess(dataset):\n",
    "    return [\n",
    "        f\"pregunta: {clean_text(t['pregunta'])}\\n\"\n",
    "        f\"respuesta: {clean_text(t['respuesta'])}\\n\"\n",
    "        for t in dataset\n",
    "    ]\n",
    "\n",
    "train_set, valid_set = map(preprocess, (train_set, valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [Phi-3 mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). At 3.8 billion parameters, Phi-3 mini is a high-quality model that is also fast to fine-tune on most Apple silicon machines. Also, it has a [permissive MIT License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE).\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3ff309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm.utils import load, generate\n",
    "from mlx_lm.tuner.trainer import train, evaluate, TrainingArgs\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters and make the training arguments. See the [training argument class](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/trainer.py#L31-L63) for a more detailed list of training parameters. \n",
    "\n",
    "Recall the LoRA update is $W^\\top \\mathbf{x} + c \\cdot \\mathbf{a} \\mathbf{b}^\\top \\mathbf{x}$ where $\\mathbf{a}$ has shape `(D, rank)`.\n",
    "\n",
    "With that in mind, the LoRA parameters to attend to are:\n",
    "- `lora_layers`: The number of Transformer blocks from the top of the model to adapt.\n",
    "- `rank`: The rank of the low-rank adapters. A larger rank implies more adapter parameters per linear layer.\n",
    "- `scale`: This is the constant $c$ that scales the low-rank update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0851dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save the adapter config and weights\n",
    "adapter_path = Path(\"../trained_models/adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lora_config = {\n",
    " \"lora_layers\": lora_layers,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": lora_r,\n",
    "    \"scale\": lora_layers_scale,\n",
    "    \"dropout\": lora_dropout,\n",
    "    \"epochs\": epochs\n",
    "}}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)    \n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=output_dir,\n",
    "    iters=steps,\n",
    "    steps_per_eval=50,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_length=sequence_len,\n",
    "    grad_checkpoint=grad_checkpoint_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the Phi-3 mini model. Note this may take a few minutes to download from HuggingFace if you haven't downloaded it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb0b16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../original_models/Llama-3.2-1B-Instruct-bf16\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the model, freeze it's parameters so we don't train them. Then convert linear layers to LoRA layers using the MLX LM utility `linear_to_lora_layers`. The adapters in the `LoRA` layers are not frozen, so they will be included in the model's `trainable_parameters`. Check-out the [LoRA layer implementation](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/lora.py#L72-L104) to see how it all works.\n",
    "\n",
    "By default, MLX LM only adapts the query, key, and value projection matrices for Phi-3. You can specify the layers to adapt by setting `lora_parameters[\"keys\"]` to a list of layer names. In this case it defaults to `[\"attn.qkv_proj\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50e1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 425984\n"
     ]
    }
   ],
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"lora_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34ee27",
   "metadata": {},
   "source": [
    "### Evaluate Functions\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each example in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37e55f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > 0.8\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    # Adjust generation parameters\n",
    "    generation_params = {\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    for example in tqdm(dataset):\n",
    "        # Get prompt and true answer\n",
    "        prompt = example[\"prompt\"]\n",
    "        true_answer = example[\"response\"]\n",
    "        \n",
    "        # Generate prediction\n",
    "        response = generate(model, tokenizer, prompt, **generation_params)\n",
    "        \n",
    "        # Store prediction and true label\n",
    "        all_preds.append(response)\n",
    "        all_labels.append(true_answer)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(true_answer, response)\n",
    "        for key, score in scores.items():\n",
    "            rouge_scores[key].append(score.fmeasure)\n",
    "\n",
    "        # Calculate loss/perplexity\n",
    "        tokens = tokenizer.encode(prompt + true_answer)\n",
    "        tokens = mx.array(tokens)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(tokens[None])[0]\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        targets = tokens[1:]\n",
    "        logits = logits[:-1]\n",
    "        loss = mx.mean(nn.losses.cross_entropy(logits, targets))\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = sum(1 for x,y in zip(all_preds, all_labels) if similar(x.strip(), y.strip())) / len(all_preds)\n",
    "    \n",
    "    # Convert predictions and labels to match format for F1\n",
    "    pred_labels = [1 if similar(p.strip(), l.strip()) else 0 for p,l in zip(all_preds, all_labels)]\n",
    "    true_labels = [1] * len(all_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='binary')\n",
    "    \n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge_scores = {key: np.mean(scores) for key, scores in rouge_scores.items()}\n",
    "\n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    perplexity = float(mx.exp(mx.array(avg_loss)))\n",
    "\n",
    "    return accuracy, f1, perplexity, avg_rouge_scores\n",
    "\n",
    "# Load test dataset\n",
    "test_set_path = \"./datasets/Parph_Data/FAQs_200_testing.jsonl\" \n",
    "with open(test_set_path, 'r') as file:\n",
    "    test_set = [json.loads(line) for line in file]\n",
    "\n",
    "# Define number of test samples\n",
    "num_test = len(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer, but you can specify any [optimizer](https://ml-explore.github.io/mlx/build/html/python/optimizers/common_optimizers.html) with any [scheduler](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html). We also added a custom class to capture the training and validation loss to plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "984516d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder ../trained_models/adapters_40_16_4e4/ created\n",
      "Starting training..., iters: 2560\n",
      "Iter 1: Val loss 3.565, Val took 11.775s\n",
      "Iter 10: Train loss 3.093, Learning Rate 4.000e-04, It/sec 0.335, Tokens/sec 454.427, Trained Tokens 13551, Peak mem 19.940 GB\n",
      "Iter 20: Train loss 2.999, Learning Rate 4.000e-04, It/sec 0.379, Tokens/sec 543.151, Trained Tokens 27884, Peak mem 19.940 GB\n",
      "Iter 30: Train loss 2.780, Learning Rate 4.000e-04, It/sec 0.232, Tokens/sec 404.634, Trained Tokens 45354, Peak mem 19.940 GB\n",
      "Iter 40: Train loss 2.593, Learning Rate 4.000e-04, It/sec 0.571, Tokens/sec 535.608, Trained Tokens 54737, Peak mem 19.940 GB\n",
      "Iter 50: Val loss 2.645, Val took 11.048s\n",
      "Iter 50: Train loss 2.485, Learning Rate 4.000e-04, It/sec 4.396, Tokens/sec 3800.134, Trained Tokens 63381, Peak mem 19.940 GB\n",
      "Iter 60: Train loss 2.329, Learning Rate 4.000e-04, It/sec 0.560, Tokens/sec 525.115, Trained Tokens 72758, Peak mem 19.940 GB\n",
      "Iter 70: Train loss 2.392, Learning Rate 4.000e-04, It/sec 0.417, Tokens/sec 522.060, Trained Tokens 85264, Peak mem 19.940 GB\n",
      "Iter 80: Train loss 2.502, Learning Rate 4.000e-04, It/sec 0.230, Tokens/sec 392.565, Trained Tokens 102354, Peak mem 19.940 GB\n",
      "Iter 90: Train loss 2.278, Learning Rate 4.000e-04, It/sec 0.401, Tokens/sec 528.087, Trained Tokens 115515, Peak mem 19.940 GB\n",
      "Iter 100: Val loss 2.382, Val took 10.743s\n",
      "Iter 100: Train loss 2.215, Learning Rate 4.000e-04, It/sec 2.626, Tokens/sec 3491.146, Trained Tokens 128810, Peak mem 19.940 GB\n",
      "Iter 100: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 2.047, Learning Rate 4.000e-04, It/sec 0.393, Tokens/sec 413.115, Trained Tokens 139323, Peak mem 19.940 GB\n",
      "Iter 120: Train loss 1.962, Learning Rate 4.000e-04, It/sec 0.419, Tokens/sec 426.243, Trained Tokens 149484, Peak mem 19.940 GB\n",
      "Iter 130: Train loss 2.028, Learning Rate 4.000e-04, It/sec 0.361, Tokens/sec 412.347, Trained Tokens 160908, Peak mem 19.940 GB\n",
      "Iter 140: Train loss 2.199, Learning Rate 4.000e-04, It/sec 0.305, Tokens/sec 513.543, Trained Tokens 177720, Peak mem 19.940 GB\n",
      "Iter 150: Val loss 2.245, Val took 10.345s\n",
      "Iter 150: Train loss 1.945, Learning Rate 4.000e-04, It/sec 3.349, Tokens/sec 4031.354, Trained Tokens 189756, Peak mem 19.940 GB\n",
      "Iter 160: Train loss 1.886, Learning Rate 4.000e-04, It/sec 0.165, Tokens/sec 295.629, Trained Tokens 207710, Peak mem 19.940 GB\n",
      "Iter 170: Train loss 1.807, Learning Rate 4.000e-04, It/sec 0.358, Tokens/sec 485.278, Trained Tokens 221277, Peak mem 19.940 GB\n",
      "Iter 180: Train loss 1.640, Learning Rate 4.000e-04, It/sec 0.446, Tokens/sec 475.688, Trained Tokens 231944, Peak mem 19.940 GB\n",
      "Iter 190: Train loss 1.811, Learning Rate 4.000e-04, It/sec 0.398, Tokens/sec 520.430, Trained Tokens 245014, Peak mem 19.940 GB\n",
      "Iter 200: Val loss 2.189, Val took 12.023s\n",
      "Iter 200: Train loss 1.821, Learning Rate 4.000e-04, It/sec 2.507, Tokens/sec 3254.212, Trained Tokens 257994, Peak mem 19.940 GB\n",
      "Iter 200: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 1.517, Learning Rate 4.000e-04, It/sec 0.493, Tokens/sec 464.192, Trained Tokens 267407, Peak mem 19.940 GB\n",
      "Iter 220: Train loss 1.602, Learning Rate 4.000e-04, It/sec 0.383, Tokens/sec 467.792, Trained Tokens 279616, Peak mem 19.940 GB\n",
      "Iter 230: Train loss 1.552, Learning Rate 4.000e-04, It/sec 0.434, Tokens/sec 494.152, Trained Tokens 291001, Peak mem 19.940 GB\n",
      "Iter 240: Train loss 1.583, Learning Rate 4.000e-04, It/sec 0.265, Tokens/sec 372.538, Trained Tokens 305044, Peak mem 19.940 GB\n",
      "Iter 250: Val loss 2.151, Val took 10.054s\n",
      "Iter 250: Train loss 1.759, Learning Rate 4.000e-04, It/sec 1.465, Tokens/sec 2225.101, Trained Tokens 320232, Peak mem 19.940 GB\n",
      "Iter 260: Train loss 1.420, Learning Rate 4.000e-04, It/sec 0.476, Tokens/sec 528.466, Trained Tokens 331335, Peak mem 19.940 GB\n",
      "Iter 270: Train loss 1.426, Learning Rate 4.000e-04, It/sec 0.418, Tokens/sec 518.228, Trained Tokens 343729, Peak mem 19.940 GB\n",
      "Iter 280: Train loss 1.315, Learning Rate 4.000e-04, It/sec 0.441, Tokens/sec 475.399, Trained Tokens 354509, Peak mem 19.940 GB\n",
      "Iter 290: Train loss 1.380, Learning Rate 4.000e-04, It/sec 0.260, Tokens/sec 362.015, Trained Tokens 368455, Peak mem 19.940 GB\n",
      "Iter 300: Val loss 2.110, Val took 11.878s\n",
      "Iter 300: Train loss 1.635, Learning Rate 4.000e-04, It/sec 2.660, Tokens/sec 4386.717, Trained Tokens 384949, Peak mem 19.940 GB\n",
      "Iter 300: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 1.336, Learning Rate 4.000e-04, It/sec 0.408, Tokens/sec 497.623, Trained Tokens 397160, Peak mem 19.940 GB\n",
      "Iter 320: Train loss 1.194, Learning Rate 4.000e-04, It/sec 0.400, Tokens/sec 480.101, Trained Tokens 409159, Peak mem 19.940 GB\n",
      "Iter 330: Train loss 1.293, Learning Rate 4.000e-04, It/sec 0.378, Tokens/sec 511.335, Trained Tokens 422674, Peak mem 19.940 GB\n",
      "Iter 340: Train loss 1.194, Learning Rate 4.000e-04, It/sec 0.410, Tokens/sec 431.489, Trained Tokens 433191, Peak mem 19.940 GB\n",
      "Iter 350: Val loss 2.122, Val took 12.555s\n",
      "Iter 350: Train loss 1.322, Learning Rate 4.000e-04, It/sec 1.447, Tokens/sec 1909.963, Trained Tokens 446389, Peak mem 19.940 GB\n",
      "Iter 360: Train loss 1.410, Learning Rate 4.000e-04, It/sec 0.205, Tokens/sec 315.846, Trained Tokens 461790, Peak mem 19.940 GB\n",
      "Iter 370: Train loss 1.101, Learning Rate 4.000e-04, It/sec 0.222, Tokens/sec 353.049, Trained Tokens 477661, Peak mem 19.940 GB\n",
      "Iter 380: Train loss 1.105, Learning Rate 4.000e-04, It/sec 0.354, Tokens/sec 476.060, Trained Tokens 491098, Peak mem 19.940 GB\n",
      "Iter 390: Train loss 1.224, Learning Rate 4.000e-04, It/sec 0.314, Tokens/sec 426.447, Trained Tokens 504659, Peak mem 19.940 GB\n",
      "Iter 400: Val loss 2.132, Val took 12.291s\n",
      "Iter 400: Train loss 1.092, Learning Rate 4.000e-04, It/sec 4.863, Tokens/sec 4910.328, Trained Tokens 514757, Peak mem 19.940 GB\n",
      "Iter 400: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 1.148, Learning Rate 4.000e-04, It/sec 0.404, Tokens/sec 416.768, Trained Tokens 525064, Peak mem 19.940 GB\n",
      "Iter 420: Train loss 1.059, Learning Rate 4.000e-04, It/sec 0.232, Tokens/sec 337.916, Trained Tokens 539629, Peak mem 19.940 GB\n",
      "Iter 430: Train loss 1.036, Learning Rate 4.000e-04, It/sec 0.357, Tokens/sec 427.283, Trained Tokens 551588, Peak mem 19.940 GB\n",
      "Iter 440: Train loss 1.100, Learning Rate 4.000e-04, It/sec 0.313, Tokens/sec 382.384, Trained Tokens 563786, Peak mem 19.940 GB\n",
      "Iter 450: Val loss 2.149, Val took 14.317s\n",
      "Iter 450: Train loss 1.038, Learning Rate 4.000e-04, It/sec 2.991, Tokens/sec 3464.946, Trained Tokens 575370, Peak mem 19.940 GB\n",
      "Iter 460: Train loss 1.157, Learning Rate 4.000e-04, It/sec 0.308, Tokens/sec 439.198, Trained Tokens 589609, Peak mem 19.940 GB\n",
      "Iter 470: Train loss 1.176, Learning Rate 4.000e-04, It/sec 0.182, Tokens/sec 332.606, Trained Tokens 607917, Peak mem 19.940 GB\n",
      "Iter 480: Train loss 0.941, Learning Rate 4.000e-04, It/sec 0.362, Tokens/sec 368.415, Trained Tokens 618087, Peak mem 19.940 GB\n",
      "Iter 490: Train loss 0.927, Learning Rate 4.000e-04, It/sec 0.347, Tokens/sec 406.313, Trained Tokens 629802, Peak mem 19.940 GB\n",
      "Iter 500: Val loss 2.099, Val took 12.054s\n",
      "Iter 500: Train loss 0.998, Learning Rate 4.000e-04, It/sec 2.046, Tokens/sec 2839.907, Trained Tokens 643680, Peak mem 19.940 GB\n",
      "Iter 500: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.935, Learning Rate 4.000e-04, It/sec 0.390, Tokens/sec 416.392, Trained Tokens 654360, Peak mem 19.940 GB\n",
      "Iter 520: Train loss 0.826, Learning Rate 4.000e-04, It/sec 0.310, Tokens/sec 389.299, Trained Tokens 666909, Peak mem 19.940 GB\n",
      "Iter 530: Train loss 0.838, Learning Rate 4.000e-04, It/sec 0.299, Tokens/sec 378.156, Trained Tokens 679557, Peak mem 19.940 GB\n",
      "Iter 540: Train loss 0.813, Learning Rate 4.000e-04, It/sec 0.393, Tokens/sec 390.071, Trained Tokens 689485, Peak mem 19.940 GB\n",
      "Iter 550: Val loss 2.119, Val took 12.459s\n",
      "Iter 550: Train loss 0.987, Learning Rate 4.000e-04, It/sec 1.797, Tokens/sec 2361.619, Trained Tokens 702626, Peak mem 19.940 GB\n",
      "Iter 560: Train loss 1.044, Learning Rate 4.000e-04, It/sec 0.189, Tokens/sec 308.043, Trained Tokens 718935, Peak mem 19.940 GB\n",
      "Iter 570: Train loss 0.677, Learning Rate 4.000e-04, It/sec 0.380, Tokens/sec 390.376, Trained Tokens 729220, Peak mem 19.940 GB\n",
      "Iter 580: Train loss 0.875, Learning Rate 4.000e-04, It/sec 0.227, Tokens/sec 332.345, Trained Tokens 743858, Peak mem 19.940 GB\n",
      "Iter 590: Train loss 0.776, Learning Rate 4.000e-04, It/sec 0.365, Tokens/sec 447.769, Trained Tokens 756140, Peak mem 19.940 GB\n",
      "Iter 600: Val loss 2.184, Val took 12.042s\n",
      "Iter 600: Train loss 0.757, Learning Rate 4.000e-04, It/sec 3.796, Tokens/sec 3599.993, Trained Tokens 765624, Peak mem 19.940 GB\n",
      "Iter 600: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 0.997, Learning Rate 4.000e-04, It/sec 0.276, Tokens/sec 496.651, Trained Tokens 783600, Peak mem 19.940 GB\n",
      "Iter 620: Train loss 0.806, Learning Rate 4.000e-04, It/sec 0.230, Tokens/sec 332.154, Trained Tokens 798023, Peak mem 19.940 GB\n",
      "Iter 630: Train loss 0.708, Learning Rate 4.000e-04, It/sec 0.399, Tokens/sec 466.015, Trained Tokens 809692, Peak mem 19.940 GB\n",
      "Iter 640: Train loss 0.785, Learning Rate 4.000e-04, It/sec 0.369, Tokens/sec 455.994, Trained Tokens 822057, Peak mem 19.940 GB\n",
      "Iter 650: Val loss 2.194, Val took 12.409s\n",
      "Iter 650: Train loss 0.737, Learning Rate 4.000e-04, It/sec 3.776, Tokens/sec 4131.967, Trained Tokens 833001, Peak mem 19.940 GB\n",
      "Iter 660: Train loss 0.787, Learning Rate 4.000e-04, It/sec 0.313, Tokens/sec 432.802, Trained Tokens 846827, Peak mem 19.940 GB\n",
      "Iter 670: Train loss 0.655, Learning Rate 4.000e-04, It/sec 0.395, Tokens/sec 434.309, Trained Tokens 857822, Peak mem 19.940 GB\n",
      "Iter 680: Train loss 0.619, Learning Rate 4.000e-04, It/sec 0.412, Tokens/sec 436.312, Trained Tokens 868417, Peak mem 19.940 GB\n",
      "Iter 690: Train loss 0.832, Learning Rate 4.000e-04, It/sec 0.299, Tokens/sec 437.899, Trained Tokens 883048, Peak mem 19.940 GB\n",
      "Iter 700: Val loss 2.204, Val took 14.676s\n",
      "Iter 700: Train loss 0.686, Learning Rate 4.000e-04, It/sec 3.010, Tokens/sec 3869.420, Trained Tokens 895902, Peak mem 19.940 GB\n",
      "Iter 700: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 0.816, Learning Rate 4.000e-04, It/sec 0.237, Tokens/sec 352.284, Trained Tokens 910752, Peak mem 19.940 GB\n",
      "Iter 720: Train loss 0.667, Learning Rate 4.000e-04, It/sec 0.365, Tokens/sec 476.965, Trained Tokens 923820, Peak mem 19.940 GB\n",
      "Iter 730: Train loss 0.595, Learning Rate 4.000e-04, It/sec 0.450, Tokens/sec 504.461, Trained Tokens 935021, Peak mem 19.940 GB\n",
      "Iter 740: Train loss 0.632, Learning Rate 4.000e-04, It/sec 0.447, Tokens/sec 440.858, Trained Tokens 944889, Peak mem 19.940 GB\n",
      "Iter 750: Val loss 2.188, Val took 13.750s\n",
      "Iter 750: Train loss 0.652, Learning Rate 4.000e-04, It/sec 3.944, Tokens/sec 4721.386, Trained Tokens 956860, Peak mem 19.940 GB\n",
      "Iter 760: Train loss 0.887, Learning Rate 4.000e-04, It/sec 0.219, Tokens/sec 358.035, Trained Tokens 973178, Peak mem 19.940 GB\n",
      "Iter 770: Train loss 0.763, Learning Rate 4.000e-04, It/sec 0.217, Tokens/sec 359.250, Trained Tokens 989735, Peak mem 19.940 GB\n",
      "Iter 780: Train loss 0.583, Learning Rate 4.000e-04, It/sec 0.422, Tokens/sec 490.937, Trained Tokens 1001359, Peak mem 19.940 GB\n",
      "Iter 790: Train loss 0.623, Learning Rate 4.000e-04, It/sec 0.324, Tokens/sec 436.886, Trained Tokens 1014860, Peak mem 19.940 GB\n",
      "Iter 800: Val loss 2.247, Val took 11.449s\n",
      "Iter 800: Train loss 0.638, Learning Rate 4.000e-04, It/sec 5.147, Tokens/sec 7076.887, Trained Tokens 1028610, Peak mem 19.940 GB\n",
      "Iter 800: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 0.645, Learning Rate 4.000e-04, It/sec 0.385, Tokens/sec 510.415, Trained Tokens 1041884, Peak mem 19.940 GB\n",
      "Iter 820: Train loss 0.618, Learning Rate 4.000e-04, It/sec 0.475, Tokens/sec 444.698, Trained Tokens 1051246, Peak mem 19.940 GB\n",
      "Iter 830: Train loss 0.678, Learning Rate 4.000e-04, It/sec 0.229, Tokens/sec 363.241, Trained Tokens 1067077, Peak mem 19.940 GB\n",
      "Iter 840: Train loss 0.535, Learning Rate 4.000e-04, It/sec 0.347, Tokens/sec 453.328, Trained Tokens 1080126, Peak mem 19.940 GB\n",
      "Iter 850: Val loss 2.220, Val took 11.617s\n",
      "Iter 850: Train loss 0.641, Learning Rate 4.000e-04, It/sec 1.916, Tokens/sec 2959.600, Trained Tokens 1095573, Peak mem 19.940 GB\n",
      "Iter 860: Train loss 0.585, Learning Rate 4.000e-04, It/sec 0.422, Tokens/sec 455.980, Trained Tokens 1106386, Peak mem 19.940 GB\n",
      "Iter 870: Train loss 0.630, Learning Rate 4.000e-04, It/sec 0.501, Tokens/sec 433.030, Trained Tokens 1115035, Peak mem 19.940 GB\n",
      "Iter 880: Train loss 0.538, Learning Rate 4.000e-04, It/sec 0.364, Tokens/sec 416.847, Trained Tokens 1126490, Peak mem 19.940 GB\n",
      "Iter 890: Train loss 0.546, Learning Rate 4.000e-04, It/sec 0.332, Tokens/sec 415.059, Trained Tokens 1139005, Peak mem 19.940 GB\n",
      "Iter 900: Val loss 2.265, Val took 13.675s\n",
      "Iter 900: Train loss 0.592, Learning Rate 4.000e-04, It/sec 6.725, Tokens/sec 7292.177, Trained Tokens 1149849, Peak mem 19.940 GB\n",
      "Iter 900: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 0.688, Learning Rate 4.000e-04, It/sec 0.188, Tokens/sec 330.732, Trained Tokens 1167421, Peak mem 19.940 GB\n",
      "Iter 920: Train loss 0.611, Learning Rate 4.000e-04, It/sec 0.388, Tokens/sec 453.362, Trained Tokens 1179104, Peak mem 19.940 GB\n",
      "Iter 930: Train loss 0.641, Learning Rate 4.000e-04, It/sec 0.204, Tokens/sec 338.841, Trained Tokens 1195696, Peak mem 19.940 GB\n",
      "Iter 940: Train loss 0.547, Learning Rate 4.000e-04, It/sec 0.285, Tokens/sec 398.441, Trained Tokens 1209691, Peak mem 19.940 GB\n",
      "Iter 950: Val loss 2.285, Val took 12.822s\n",
      "Iter 950: Train loss 0.570, Learning Rate 4.000e-04, It/sec 7.892, Tokens/sec 6622.181, Trained Tokens 1218082, Peak mem 19.940 GB\n",
      "Iter 960: Train loss 0.580, Learning Rate 4.000e-04, It/sec 0.290, Tokens/sec 455.919, Trained Tokens 1233821, Peak mem 19.940 GB\n",
      "Iter 970: Train loss 0.525, Learning Rate 4.000e-04, It/sec 0.399, Tokens/sec 434.841, Trained Tokens 1244723, Peak mem 19.940 GB\n",
      "Iter 980: Train loss 0.524, Learning Rate 4.000e-04, It/sec 0.322, Tokens/sec 420.068, Trained Tokens 1257777, Peak mem 19.940 GB\n",
      "Iter 990: Train loss 0.586, Learning Rate 4.000e-04, It/sec 0.198, Tokens/sec 282.691, Trained Tokens 1272047, Peak mem 19.940 GB\n",
      "Iter 1000: Val loss 2.311, Val took 11.344s\n",
      "Iter 1000: Train loss 0.529, Learning Rate 4.000e-04, It/sec 11.103, Tokens/sec 14144.446, Trained Tokens 1284786, Peak mem 19.940 GB\n",
      "Iter 1000: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001000_adapters.safetensors.\n",
      "Iter 1010: Train loss 0.552, Learning Rate 4.000e-04, It/sec 0.336, Tokens/sec 427.251, Trained Tokens 1297507, Peak mem 19.940 GB\n",
      "Iter 1020: Train loss 0.533, Learning Rate 4.000e-04, It/sec 0.336, Tokens/sec 377.105, Trained Tokens 1308720, Peak mem 19.940 GB\n",
      "Iter 1030: Train loss 0.504, Learning Rate 4.000e-04, It/sec 0.334, Tokens/sec 418.807, Trained Tokens 1321266, Peak mem 19.940 GB\n",
      "Iter 1040: Train loss 0.500, Learning Rate 4.000e-04, It/sec 0.418, Tokens/sec 424.305, Trained Tokens 1331415, Peak mem 19.940 GB\n",
      "Iter 1050: Val loss 2.307, Val took 10.900s\n",
      "Iter 1050: Train loss 0.506, Learning Rate 4.000e-04, It/sec 2.547, Tokens/sec 3368.374, Trained Tokens 1344642, Peak mem 19.940 GB\n",
      "Iter 1060: Train loss 0.631, Learning Rate 4.000e-04, It/sec 0.242, Tokens/sec 339.238, Trained Tokens 1358683, Peak mem 19.940 GB\n",
      "Iter 1070: Train loss 0.492, Learning Rate 4.000e-04, It/sec 0.323, Tokens/sec 437.111, Trained Tokens 1372215, Peak mem 19.940 GB\n",
      "Iter 1080: Train loss 0.475, Learning Rate 4.000e-04, It/sec 0.322, Tokens/sec 426.476, Trained Tokens 1385444, Peak mem 19.940 GB\n",
      "Iter 1090: Train loss 0.455, Learning Rate 4.000e-04, It/sec 0.311, Tokens/sec 405.407, Trained Tokens 1398470, Peak mem 19.940 GB\n",
      "Iter 1100: Val loss 2.303, Val took 11.083s\n",
      "Iter 1100: Train loss 0.567, Learning Rate 4.000e-04, It/sec 3.790, Tokens/sec 5424.747, Trained Tokens 1412782, Peak mem 19.940 GB\n",
      "Iter 1100: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001100_adapters.safetensors.\n",
      "Iter 1110: Train loss 0.518, Learning Rate 4.000e-04, It/sec 0.395, Tokens/sec 400.491, Trained Tokens 1422912, Peak mem 19.940 GB\n",
      "Iter 1120: Train loss 0.515, Learning Rate 4.000e-04, It/sec 0.323, Tokens/sec 373.854, Trained Tokens 1434501, Peak mem 19.940 GB\n",
      "Iter 1130: Train loss 0.509, Learning Rate 4.000e-04, It/sec 0.295, Tokens/sec 415.435, Trained Tokens 1448595, Peak mem 19.940 GB\n",
      "Iter 1140: Train loss 0.534, Learning Rate 4.000e-04, It/sec 0.214, Tokens/sec 346.233, Trained Tokens 1464794, Peak mem 19.940 GB\n",
      "Iter 1150: Val loss 2.335, Val took 12.970s\n",
      "Iter 1150: Train loss 0.500, Learning Rate 4.000e-04, It/sec 7.369, Tokens/sec 7436.428, Trained Tokens 1474885, Peak mem 19.940 GB\n",
      "Iter 1160: Train loss 0.537, Learning Rate 4.000e-04, It/sec 0.340, Tokens/sec 408.106, Trained Tokens 1486883, Peak mem 19.940 GB\n",
      "Iter 1170: Train loss 0.488, Learning Rate 4.000e-04, It/sec 0.273, Tokens/sec 348.594, Trained Tokens 1499655, Peak mem 19.940 GB\n",
      "Iter 1180: Train loss 0.570, Learning Rate 4.000e-04, It/sec 0.303, Tokens/sec 435.447, Trained Tokens 1514025, Peak mem 19.940 GB\n",
      "Iter 1190: Train loss 0.538, Learning Rate 4.000e-04, It/sec 0.201, Tokens/sec 320.355, Trained Tokens 1529949, Peak mem 19.940 GB\n",
      "Iter 1200: Val loss 2.349, Val took 13.582s\n",
      "Iter 1200: Train loss 0.524, Learning Rate 4.000e-04, It/sec 2.173, Tokens/sec 2191.919, Trained Tokens 1540034, Peak mem 19.940 GB\n",
      "Iter 1200: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001200_adapters.safetensors.\n",
      "Iter 1210: Train loss 0.455, Learning Rate 4.000e-04, It/sec 0.306, Tokens/sec 390.341, Trained Tokens 1552789, Peak mem 19.940 GB\n",
      "Iter 1220: Train loss 0.491, Learning Rate 4.000e-04, It/sec 0.327, Tokens/sec 416.623, Trained Tokens 1565531, Peak mem 19.940 GB\n",
      "Iter 1230: Train loss 0.483, Learning Rate 4.000e-04, It/sec 0.294, Tokens/sec 389.592, Trained Tokens 1578788, Peak mem 19.940 GB\n",
      "Iter 1240: Train loss 0.429, Learning Rate 4.000e-04, It/sec 0.326, Tokens/sec 393.316, Trained Tokens 1590856, Peak mem 19.940 GB\n",
      "Iter 1250: Val loss 2.367, Val took 13.423s\n",
      "Iter 1250: Train loss 0.463, Learning Rate 4.000e-04, It/sec 3.721, Tokens/sec 3381.443, Trained Tokens 1599944, Peak mem 19.940 GB\n",
      "Iter 1260: Train loss 0.494, Learning Rate 4.000e-04, It/sec 0.320, Tokens/sec 393.589, Trained Tokens 1612232, Peak mem 19.940 GB\n",
      "Iter 1270: Train loss 0.548, Learning Rate 4.000e-04, It/sec 0.175, Tokens/sec 330.296, Trained Tokens 1631080, Peak mem 19.940 GB\n",
      "Iter 1280: Train loss 0.545, Learning Rate 4.000e-04, It/sec 0.225, Tokens/sec 303.639, Trained Tokens 1644593, Peak mem 19.940 GB\n",
      "Iter 1290: Train loss 0.458, Learning Rate 4.000e-04, It/sec 0.321, Tokens/sec 365.917, Trained Tokens 1656004, Peak mem 19.940 GB\n",
      "Iter 1300: Val loss 2.357, Val took 14.541s\n",
      "Iter 1300: Train loss 0.426, Learning Rate 4.000e-04, It/sec 1.487, Tokens/sec 2007.900, Trained Tokens 1669506, Peak mem 19.940 GB\n",
      "Iter 1300: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001300_adapters.safetensors.\n",
      "Iter 1310: Train loss 0.530, Learning Rate 4.000e-04, It/sec 0.480, Tokens/sec 402.266, Trained Tokens 1677886, Peak mem 19.940 GB\n",
      "Iter 1320: Train loss 0.493, Learning Rate 4.000e-04, It/sec 0.294, Tokens/sec 409.847, Trained Tokens 1691808, Peak mem 19.940 GB\n",
      "Iter 1330: Train loss 0.426, Learning Rate 4.000e-04, It/sec 0.273, Tokens/sec 395.935, Trained Tokens 1706336, Peak mem 19.940 GB\n",
      "Iter 1340: Train loss 0.538, Learning Rate 4.000e-04, It/sec 0.220, Tokens/sec 300.790, Trained Tokens 1719995, Peak mem 19.940 GB\n",
      "Iter 1350: Val loss 2.408, Val took 14.002s\n",
      "Iter 1350: Train loss 0.473, Learning Rate 4.000e-04, It/sec 2.027, Tokens/sec 2714.372, Trained Tokens 1733383, Peak mem 19.940 GB\n",
      "Iter 1360: Train loss 0.453, Learning Rate 4.000e-04, It/sec 0.347, Tokens/sec 458.305, Trained Tokens 1746592, Peak mem 19.940 GB\n",
      "Iter 1370: Train loss 0.435, Learning Rate 4.000e-04, It/sec 0.330, Tokens/sec 389.793, Trained Tokens 1758418, Peak mem 19.940 GB\n",
      "Iter 1380: Train loss 0.433, Learning Rate 4.000e-04, It/sec 0.339, Tokens/sec 424.125, Trained Tokens 1770939, Peak mem 19.940 GB\n",
      "Iter 1390: Train loss 0.438, Learning Rate 4.000e-04, It/sec 0.431, Tokens/sec 484.584, Trained Tokens 1782170, Peak mem 19.940 GB\n",
      "Iter 1400: Val loss 2.421, Val took 10.367s\n",
      "Iter 1400: Train loss 0.524, Learning Rate 4.000e-04, It/sec 7.267, Tokens/sec 9506.622, Trained Tokens 1795252, Peak mem 19.940 GB\n",
      "Iter 1400: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001400_adapters.safetensors.\n",
      "Iter 1410: Train loss 0.393, Learning Rate 4.000e-04, It/sec 0.441, Tokens/sec 492.440, Trained Tokens 1806427, Peak mem 19.940 GB\n",
      "Iter 1420: Train loss 0.457, Learning Rate 4.000e-04, It/sec 0.310, Tokens/sec 492.773, Trained Tokens 1822344, Peak mem 19.940 GB\n",
      "Iter 1430: Train loss 0.395, Learning Rate 4.000e-04, It/sec 0.397, Tokens/sec 503.683, Trained Tokens 1835017, Peak mem 19.940 GB\n",
      "Iter 1440: Train loss 0.443, Learning Rate 4.000e-04, It/sec 0.353, Tokens/sec 486.068, Trained Tokens 1848770, Peak mem 19.940 GB\n",
      "Iter 1450: Val loss 2.433, Val took 12.823s\n",
      "Iter 1450: Train loss 0.401, Learning Rate 4.000e-04, It/sec 5.859, Tokens/sec 7115.160, Trained Tokens 1860914, Peak mem 19.940 GB\n",
      "Iter 1460: Train loss 0.471, Learning Rate 4.000e-04, It/sec 0.454, Tokens/sec 484.266, Trained Tokens 1871582, Peak mem 19.940 GB\n",
      "Iter 1470: Train loss 0.481, Learning Rate 4.000e-04, It/sec 0.262, Tokens/sec 368.660, Trained Tokens 1885674, Peak mem 19.940 GB\n",
      "Iter 1480: Train loss 0.414, Learning Rate 4.000e-04, It/sec 0.392, Tokens/sec 538.542, Trained Tokens 1899404, Peak mem 19.940 GB\n",
      "Iter 1490: Train loss 0.374, Learning Rate 4.000e-04, It/sec 0.378, Tokens/sec 471.090, Trained Tokens 1911872, Peak mem 19.940 GB\n",
      "Iter 1500: Val loss 2.409, Val took 10.249s\n",
      "Iter 1500: Train loss 0.428, Learning Rate 4.000e-04, It/sec 8.573, Tokens/sec 13114.435, Trained Tokens 1927170, Peak mem 19.940 GB\n",
      "Iter 1500: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001500_adapters.safetensors.\n",
      "Iter 1510: Train loss 0.476, Learning Rate 4.000e-04, It/sec 0.502, Tokens/sec 494.972, Trained Tokens 1937021, Peak mem 19.940 GB\n",
      "Iter 1520: Train loss 0.485, Learning Rate 4.000e-04, It/sec 0.259, Tokens/sec 359.042, Trained Tokens 1950896, Peak mem 19.940 GB\n",
      "Iter 1530: Train loss 0.449, Learning Rate 4.000e-04, It/sec 0.417, Tokens/sec 508.475, Trained Tokens 1963080, Peak mem 19.940 GB\n",
      "Iter 1540: Train loss 0.414, Learning Rate 4.000e-04, It/sec 0.233, Tokens/sec 391.360, Trained Tokens 1979855, Peak mem 19.940 GB\n",
      "Iter 1550: Val loss 2.387, Val took 12.029s\n",
      "Iter 1550: Train loss 0.414, Learning Rate 4.000e-04, It/sec 3.076, Tokens/sec 3717.545, Trained Tokens 1991942, Peak mem 19.940 GB\n",
      "Iter 1560: Train loss 0.459, Learning Rate 4.000e-04, It/sec 0.484, Tokens/sec 480.271, Trained Tokens 2001873, Peak mem 19.940 GB\n",
      "Iter 1570: Train loss 0.462, Learning Rate 4.000e-04, It/sec 0.449, Tokens/sec 500.307, Trained Tokens 2013025, Peak mem 19.940 GB\n",
      "Iter 1580: Train loss 0.421, Learning Rate 4.000e-04, It/sec 0.373, Tokens/sec 501.262, Trained Tokens 2026461, Peak mem 19.940 GB\n",
      "Iter 1590: Train loss 0.437, Learning Rate 4.000e-04, It/sec 0.481, Tokens/sec 507.643, Trained Tokens 2037020, Peak mem 19.940 GB\n",
      "Iter 1600: Val loss 2.402, Val took 10.352s\n",
      "Iter 1600: Train loss 0.442, Learning Rate 4.000e-04, It/sec 5.067, Tokens/sec 7510.816, Trained Tokens 2051844, Peak mem 19.940 GB\n",
      "Iter 1600: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001600_adapters.safetensors.\n",
      "Iter 1610: Train loss 0.423, Learning Rate 4.000e-04, It/sec 0.486, Tokens/sec 525.266, Trained Tokens 2062656, Peak mem 19.940 GB\n",
      "Iter 1620: Train loss 0.456, Learning Rate 4.000e-04, It/sec 0.477, Tokens/sec 489.110, Trained Tokens 2072911, Peak mem 19.940 GB\n",
      "Iter 1630: Train loss 0.397, Learning Rate 4.000e-04, It/sec 0.299, Tokens/sec 512.306, Trained Tokens 2090064, Peak mem 19.940 GB\n",
      "Iter 1640: Train loss 0.344, Learning Rate 4.000e-04, It/sec 0.369, Tokens/sec 501.944, Trained Tokens 2103684, Peak mem 19.940 GB\n",
      "Iter 1650: Val loss 2.443, Val took 11.929s\n",
      "Iter 1650: Train loss 0.425, Learning Rate 4.000e-04, It/sec 0.517, Tokens/sec 885.161, Trained Tokens 2120806, Peak mem 19.940 GB\n",
      "Iter 1660: Train loss 0.410, Learning Rate 4.000e-04, It/sec 0.434, Tokens/sec 448.907, Trained Tokens 2131149, Peak mem 19.940 GB\n",
      "Iter 1670: Train loss 0.377, Learning Rate 4.000e-04, It/sec 0.364, Tokens/sec 490.951, Trained Tokens 2144648, Peak mem 19.940 GB\n",
      "Iter 1680: Train loss 0.470, Learning Rate 4.000e-04, It/sec 0.491, Tokens/sec 499.285, Trained Tokens 2154827, Peak mem 19.940 GB\n",
      "Iter 1690: Train loss 0.451, Learning Rate 4.000e-04, It/sec 0.431, Tokens/sec 501.155, Trained Tokens 2166449, Peak mem 19.940 GB\n",
      "Iter 1700: Val loss 2.403, Val took 10.352s\n",
      "Iter 1700: Train loss 0.455, Learning Rate 4.000e-04, It/sec 8.392, Tokens/sec 11543.980, Trained Tokens 2180205, Peak mem 19.940 GB\n",
      "Iter 1700: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001700_adapters.safetensors.\n",
      "Iter 1710: Train loss 0.450, Learning Rate 4.000e-04, It/sec 0.445, Tokens/sec 489.755, Trained Tokens 2191217, Peak mem 19.940 GB\n",
      "Iter 1720: Train loss 0.360, Learning Rate 4.000e-04, It/sec 0.311, Tokens/sec 504.678, Trained Tokens 2207421, Peak mem 19.940 GB\n",
      "Iter 1730: Train loss 0.397, Learning Rate 4.000e-04, It/sec 0.351, Tokens/sec 476.094, Trained Tokens 2220980, Peak mem 19.940 GB\n",
      "Iter 1740: Train loss 0.398, Learning Rate 4.000e-04, It/sec 0.388, Tokens/sec 486.596, Trained Tokens 2233536, Peak mem 19.940 GB\n",
      "Iter 1750: Val loss 2.456, Val took 12.163s\n",
      "Iter 1750: Train loss 0.433, Learning Rate 4.000e-04, It/sec 2.778, Tokens/sec 3196.117, Trained Tokens 2245041, Peak mem 19.940 GB\n",
      "Iter 1760: Train loss 0.450, Learning Rate 4.000e-04, It/sec 0.252, Tokens/sec 372.794, Trained Tokens 2259814, Peak mem 19.940 GB\n",
      "Iter 1770: Train loss 0.391, Learning Rate 4.000e-04, It/sec 0.381, Tokens/sec 457.575, Trained Tokens 2271811, Peak mem 19.940 GB\n",
      "Iter 1780: Train loss 0.382, Learning Rate 4.000e-04, It/sec 0.352, Tokens/sec 461.268, Trained Tokens 2284923, Peak mem 19.940 GB\n",
      "Iter 1790: Train loss 0.420, Learning Rate 4.000e-04, It/sec 0.402, Tokens/sec 436.676, Trained Tokens 2295782, Peak mem 19.940 GB\n",
      "Iter 1800: Val loss 2.424, Val took 10.103s\n",
      "Iter 1800: Train loss 0.369, Learning Rate 4.000e-04, It/sec 3.197, Tokens/sec 3873.013, Trained Tokens 2307897, Peak mem 19.940 GB\n",
      "Iter 1800: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001800_adapters.safetensors.\n",
      "Iter 1810: Train loss 0.367, Learning Rate 4.000e-04, It/sec 0.335, Tokens/sec 477.244, Trained Tokens 2322152, Peak mem 19.940 GB\n",
      "Iter 1820: Train loss 0.454, Learning Rate 4.000e-04, It/sec 0.220, Tokens/sec 316.707, Trained Tokens 2336526, Peak mem 19.940 GB\n",
      "Iter 1830: Train loss 0.433, Learning Rate 4.000e-04, It/sec 0.556, Tokens/sec 521.807, Trained Tokens 2345908, Peak mem 19.940 GB\n",
      "Iter 1840: Train loss 0.355, Learning Rate 4.000e-04, It/sec 0.342, Tokens/sec 519.465, Trained Tokens 2361090, Peak mem 19.940 GB\n",
      "Iter 1850: Val loss 2.416, Val took 9.936s\n",
      "Iter 1850: Train loss 0.421, Learning Rate 4.000e-04, It/sec 5.309, Tokens/sec 7520.181, Trained Tokens 2375254, Peak mem 19.940 GB\n",
      "Iter 1860: Train loss 0.425, Learning Rate 4.000e-04, It/sec 0.401, Tokens/sec 495.522, Trained Tokens 2387607, Peak mem 19.940 GB\n",
      "Iter 1870: Train loss 0.381, Learning Rate 4.000e-04, It/sec 0.366, Tokens/sec 457.827, Trained Tokens 2400112, Peak mem 19.940 GB\n",
      "Iter 1880: Train loss 0.428, Learning Rate 4.000e-04, It/sec 0.373, Tokens/sec 486.366, Trained Tokens 2413139, Peak mem 19.940 GB\n",
      "Iter 1890: Train loss 0.414, Learning Rate 4.000e-04, It/sec 0.475, Tokens/sec 493.522, Trained Tokens 2423530, Peak mem 19.940 GB\n",
      "Iter 1900: Val loss 2.467, Val took 10.844s\n",
      "Iter 1900: Train loss 0.396, Learning Rate 4.000e-04, It/sec 5.963, Tokens/sec 6125.694, Trained Tokens 2433802, Peak mem 19.940 GB\n",
      "Iter 1900: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0001900_adapters.safetensors.\n",
      "Iter 1910: Train loss 0.395, Learning Rate 4.000e-04, It/sec 0.412, Tokens/sec 499.611, Trained Tokens 2445924, Peak mem 19.940 GB\n",
      "Iter 1920: Train loss 0.405, Learning Rate 4.000e-04, It/sec 0.325, Tokens/sec 525.611, Trained Tokens 2462107, Peak mem 19.940 GB\n",
      "Iter 1930: Train loss 0.364, Learning Rate 4.000e-04, It/sec 0.401, Tokens/sec 475.125, Trained Tokens 2473948, Peak mem 19.940 GB\n",
      "Iter 1940: Train loss 0.410, Learning Rate 4.000e-04, It/sec 0.224, Tokens/sec 359.571, Trained Tokens 2490026, Peak mem 19.940 GB\n",
      "Iter 1950: Val loss 2.490, Val took 11.375s\n",
      "Iter 1950: Train loss 0.336, Learning Rate 4.000e-04, It/sec 2.001, Tokens/sec 2587.595, Trained Tokens 2502960, Peak mem 19.940 GB\n",
      "Iter 1960: Train loss 0.476, Learning Rate 4.000e-04, It/sec 0.528, Tokens/sec 513.605, Trained Tokens 2512679, Peak mem 19.940 GB\n",
      "Iter 1970: Train loss 0.399, Learning Rate 4.000e-04, It/sec 0.393, Tokens/sec 455.570, Trained Tokens 2524267, Peak mem 19.940 GB\n",
      "Iter 1980: Train loss 0.396, Learning Rate 4.000e-04, It/sec 0.375, Tokens/sec 490.701, Trained Tokens 2537354, Peak mem 19.940 GB\n",
      "Iter 1990: Train loss 0.394, Learning Rate 4.000e-04, It/sec 0.250, Tokens/sec 389.088, Trained Tokens 2552891, Peak mem 19.940 GB\n",
      "Iter 2000: Val loss 2.534, Val took 9.860s\n",
      "Iter 2000: Train loss 0.343, Learning Rate 4.000e-04, It/sec 2.926, Tokens/sec 3986.704, Trained Tokens 2566515, Peak mem 19.940 GB\n",
      "Iter 2000: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0002000_adapters.safetensors.\n",
      "Iter 2010: Train loss 0.338, Learning Rate 4.000e-04, It/sec 0.195, Tokens/sec 357.170, Trained Tokens 2584863, Peak mem 19.940 GB\n",
      "Iter 2020: Train loss 0.416, Learning Rate 4.000e-04, It/sec 0.443, Tokens/sec 498.367, Trained Tokens 2596119, Peak mem 19.940 GB\n",
      "Iter 2030: Train loss 0.435, Learning Rate 4.000e-04, It/sec 0.484, Tokens/sec 505.327, Trained Tokens 2606549, Peak mem 19.940 GB\n",
      "Iter 2040: Train loss 0.449, Learning Rate 4.000e-04, It/sec 0.465, Tokens/sec 506.029, Trained Tokens 2617440, Peak mem 19.940 GB\n",
      "Iter 2050: Val loss 2.481, Val took 12.729s\n",
      "Iter 2050: Train loss 0.352, Learning Rate 4.000e-04, It/sec 1.412, Tokens/sec 2032.682, Trained Tokens 2631837, Peak mem 19.940 GB\n",
      "Iter 2060: Train loss 0.398, Learning Rate 4.000e-04, It/sec 0.223, Tokens/sec 361.074, Trained Tokens 2648046, Peak mem 19.940 GB\n",
      "Iter 2070: Train loss 0.402, Learning Rate 4.000e-04, It/sec 0.340, Tokens/sec 405.153, Trained Tokens 2659969, Peak mem 19.940 GB\n",
      "Iter 2080: Train loss 0.420, Learning Rate 4.000e-04, It/sec 0.396, Tokens/sec 465.781, Trained Tokens 2671724, Peak mem 19.940 GB\n",
      "Iter 2090: Train loss 0.452, Learning Rate 4.000e-04, It/sec 0.462, Tokens/sec 486.816, Trained Tokens 2682270, Peak mem 19.940 GB\n",
      "Iter 2100: Val loss 2.531, Val took 11.617s\n",
      "Iter 2100: Train loss 0.421, Learning Rate 4.000e-04, It/sec 5.434, Tokens/sec 5698.754, Trained Tokens 2692758, Peak mem 19.940 GB\n",
      "Iter 2100: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0002100_adapters.safetensors.\n",
      "Iter 2110: Train loss 0.398, Learning Rate 4.000e-04, It/sec 0.372, Tokens/sec 500.927, Trained Tokens 2706228, Peak mem 19.940 GB\n",
      "Iter 2120: Train loss 0.382, Learning Rate 4.000e-04, It/sec 0.384, Tokens/sec 480.947, Trained Tokens 2718738, Peak mem 19.940 GB\n",
      "Iter 2130: Train loss 0.422, Learning Rate 4.000e-04, It/sec 0.442, Tokens/sec 494.981, Trained Tokens 2729947, Peak mem 19.940 GB\n",
      "Iter 2140: Train loss 0.501, Learning Rate 4.000e-04, It/sec 0.229, Tokens/sec 387.731, Trained Tokens 2746854, Peak mem 19.940 GB\n",
      "Iter 2150: Val loss 2.473, Val took 9.893s\n",
      "Iter 2150: Train loss 0.445, Learning Rate 4.000e-04, It/sec 10.792, Tokens/sec 14875.456, Trained Tokens 2760638, Peak mem 19.940 GB\n",
      "Iter 2160: Train loss 0.417, Learning Rate 4.000e-04, It/sec 0.463, Tokens/sec 474.551, Trained Tokens 2770888, Peak mem 19.940 GB\n",
      "Iter 2170: Train loss 0.479, Learning Rate 4.000e-04, It/sec 0.402, Tokens/sec 517.358, Trained Tokens 2783743, Peak mem 19.940 GB\n",
      "Iter 2180: Train loss 0.434, Learning Rate 4.000e-04, It/sec 0.428, Tokens/sec 542.047, Trained Tokens 2796421, Peak mem 19.940 GB\n",
      "Iter 2190: Train loss 0.387, Learning Rate 4.000e-04, It/sec 0.399, Tokens/sec 549.517, Trained Tokens 2810198, Peak mem 19.940 GB\n",
      "Iter 2200: Val loss 2.548, Val took 11.841s\n",
      "Iter 2200: Train loss 0.474, Learning Rate 4.000e-04, It/sec 5.927, Tokens/sec 5908.840, Trained Tokens 2820168, Peak mem 19.940 GB\n",
      "Iter 2200: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0002200_adapters.safetensors.\n",
      "Iter 2210: Train loss 0.458, Learning Rate 4.000e-04, It/sec 0.478, Tokens/sec 497.685, Trained Tokens 2830573, Peak mem 19.940 GB\n",
      "Iter 2220: Train loss 0.405, Learning Rate 4.000e-04, It/sec 0.201, Tokens/sec 361.673, Trained Tokens 2848525, Peak mem 19.940 GB\n",
      "Iter 2230: Train loss 0.375, Learning Rate 4.000e-04, It/sec 0.388, Tokens/sec 489.979, Trained Tokens 2861161, Peak mem 19.940 GB\n",
      "Iter 2240: Train loss 0.368, Learning Rate 4.000e-04, It/sec 0.349, Tokens/sec 496.409, Trained Tokens 2875393, Peak mem 19.940 GB\n",
      "Iter 2250: Val loss 2.463, Val took 12.778s\n",
      "Iter 2250: Train loss 0.393, Learning Rate 4.000e-04, It/sec 5.073, Tokens/sec 5251.992, Trained Tokens 2885746, Peak mem 19.940 GB\n",
      "Iter 2260: Train loss 0.379, Learning Rate 4.000e-04, It/sec 0.443, Tokens/sec 518.267, Trained Tokens 2897437, Peak mem 19.940 GB\n",
      "Iter 2270: Train loss 0.412, Learning Rate 4.000e-04, It/sec 0.337, Tokens/sec 470.073, Trained Tokens 2911405, Peak mem 19.940 GB\n",
      "Iter 2280: Train loss 0.411, Learning Rate 4.000e-04, It/sec 0.242, Tokens/sec 396.021, Trained Tokens 2927796, Peak mem 19.940 GB\n",
      "Iter 2290: Train loss 0.459, Learning Rate 4.000e-04, It/sec 0.449, Tokens/sec 495.460, Trained Tokens 2938828, Peak mem 19.940 GB\n",
      "Iter 2300: Val loss 2.501, Val took 11.665s\n",
      "Iter 2300: Train loss 0.460, Learning Rate 4.000e-04, It/sec 5.214, Tokens/sec 5415.486, Trained Tokens 2949214, Peak mem 19.940 GB\n",
      "Iter 2300: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0002300_adapters.safetensors.\n",
      "Iter 2310: Train loss 0.383, Learning Rate 4.000e-04, It/sec 0.317, Tokens/sec 458.378, Trained Tokens 2963686, Peak mem 19.940 GB\n",
      "Iter 2320: Train loss 0.449, Learning Rate 4.000e-04, It/sec 0.210, Tokens/sec 355.422, Trained Tokens 2980575, Peak mem 19.940 GB\n",
      "Iter 2330: Train loss 0.432, Learning Rate 4.000e-04, It/sec 0.470, Tokens/sec 506.757, Trained Tokens 2991355, Peak mem 19.940 GB\n",
      "Iter 2340: Train loss 0.465, Learning Rate 4.000e-04, It/sec 0.367, Tokens/sec 454.617, Trained Tokens 3003757, Peak mem 19.940 GB\n",
      "Iter 2350: Val loss 2.510, Val took 11.315s\n",
      "Iter 2350: Train loss 0.395, Learning Rate 4.000e-04, It/sec 2.678, Tokens/sec 3846.165, Trained Tokens 3018117, Peak mem 19.940 GB\n",
      "Iter 2360: Train loss 0.477, Learning Rate 4.000e-04, It/sec 0.244, Tokens/sec 361.699, Trained Tokens 3032953, Peak mem 19.940 GB\n",
      "Iter 2370: Train loss 0.438, Learning Rate 4.000e-04, It/sec 0.481, Tokens/sec 507.753, Trained Tokens 3043518, Peak mem 19.940 GB\n",
      "Iter 2380: Train loss 0.358, Learning Rate 4.000e-04, It/sec 0.351, Tokens/sec 447.982, Trained Tokens 3056294, Peak mem 19.940 GB\n",
      "Iter 2390: Train loss 0.430, Learning Rate 4.000e-04, It/sec 0.455, Tokens/sec 499.083, Trained Tokens 3067269, Peak mem 19.940 GB\n",
      "Iter 2400: Val loss 2.461, Val took 9.911s\n",
      "Iter 2400: Train loss 0.439, Learning Rate 4.000e-04, It/sec 5.330, Tokens/sec 7321.211, Trained Tokens 3081006, Peak mem 19.940 GB\n",
      "Iter 2400: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0002400_adapters.safetensors.\n",
      "Iter 2410: Train loss 0.391, Learning Rate 4.000e-04, It/sec 0.390, Tokens/sec 491.149, Trained Tokens 3093608, Peak mem 19.940 GB\n",
      "Iter 2420: Train loss 0.377, Learning Rate 4.000e-04, It/sec 0.433, Tokens/sec 527.617, Trained Tokens 3105791, Peak mem 19.940 GB\n",
      "Iter 2430: Train loss 0.412, Learning Rate 4.000e-04, It/sec 0.402, Tokens/sec 523.124, Trained Tokens 3118807, Peak mem 19.940 GB\n",
      "Iter 2440: Train loss 0.490, Learning Rate 4.000e-04, It/sec 0.286, Tokens/sec 360.893, Trained Tokens 3131438, Peak mem 19.940 GB\n",
      "Iter 2450: Val loss 2.475, Val took 13.531s\n",
      "Iter 2450: Train loss 0.433, Learning Rate 4.000e-04, It/sec 2.651, Tokens/sec 2939.375, Trained Tokens 3142526, Peak mem 19.940 GB\n",
      "Iter 2460: Train loss 0.414, Learning Rate 4.000e-04, It/sec 0.394, Tokens/sec 447.428, Trained Tokens 3153888, Peak mem 19.940 GB\n",
      "Iter 2470: Train loss 0.330, Learning Rate 4.000e-04, It/sec 0.333, Tokens/sec 502.781, Trained Tokens 3169000, Peak mem 19.940 GB\n",
      "Iter 2480: Train loss 0.373, Learning Rate 4.000e-04, It/sec 0.368, Tokens/sec 518.173, Trained Tokens 3183098, Peak mem 19.940 GB\n",
      "Iter 2490: Train loss 0.475, Learning Rate 4.000e-04, It/sec 0.259, Tokens/sec 324.448, Trained Tokens 3195644, Peak mem 19.940 GB\n",
      "Iter 2500: Val loss 2.475, Val took 11.422s\n",
      "Iter 2500: Train loss 0.384, Learning Rate 4.000e-04, It/sec 1.918, Tokens/sec 2478.277, Trained Tokens 3208567, Peak mem 19.940 GB\n",
      "Iter 2500: Saved adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors and ../trained_models/adapters_40_16_4e4/0002500_adapters.safetensors.\n",
      "Iter 2510: Train loss 0.433, Learning Rate 4.000e-04, It/sec 0.253, Tokens/sec 337.451, Trained Tokens 3221886, Peak mem 19.940 GB\n",
      "Iter 2520: Train loss 0.397, Learning Rate 4.000e-04, It/sec 0.482, Tokens/sec 511.830, Trained Tokens 3232497, Peak mem 19.940 GB\n",
      "Iter 2530: Train loss 0.361, Learning Rate 4.000e-04, It/sec 0.424, Tokens/sec 516.797, Trained Tokens 3244696, Peak mem 19.940 GB\n",
      "Iter 2540: Train loss 0.346, Learning Rate 4.000e-04, It/sec 0.327, Tokens/sec 480.530, Trained Tokens 3259391, Peak mem 19.940 GB\n",
      "Iter 2550: Val loss 2.497, Val took 12.366s\n",
      "Iter 2550: Train loss 0.435, Learning Rate 4.000e-04, It/sec 1.942, Tokens/sec 2409.891, Trained Tokens 3271800, Peak mem 19.940 GB\n",
      "Iter 2560: Val loss 2.532, Val took 11.082s\n",
      "Iter 2560: Train loss 0.378, Learning Rate 4.000e-04, It/sec 4.389, Tokens/sec 6946.806, Trained Tokens 3287627, Peak mem 19.940 GB\n",
      "Saved final adapter weights to ../trained_models/adapters_40_16_4e4/adapters.safetensors.\n",
      "\n",
      " Starting Evaluation\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 205/205 [12:47<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on 205 test samples:\n",
      "Accuracy: 0.000\n",
      "F1 Score: 0.000\n",
      "Perplexity: 10.079\n",
      "\n",
      "ROUGE Scores:\n",
      "rouge1: 0.187\n",
      "rouge2: 0.060\n",
      "rougeL: 0.144\n",
      " View run MLX-40_16_4e4 at: http://127.0.0.1:5000/#/experiments/880645134898555871/runs/7f15b0a5e6dd4352a816ed1ec0b674b1\n",
      " View experiment at: http://127.0.0.1:5000/#/experiments/880645134898555871\n"
     ]
    }
   ],
   "source": [
    "# Checl if the output folder exists, if not, create it:\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Output folder {output_path} created\")\n",
    "else:\n",
    "    print(f\"Output folder {output_path} already exists\")\n",
    "\n",
    "\n",
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "if optimizer == \"adam\":\n",
    "    opt = optim.Adam(learning_rate=learning_rate_value)\n",
    "else:\n",
    "    opt = optim.AdamW(learning_rate=learning_rate_value, weight_decay=weight_decay_value)\n",
    "\n",
    "# Make a class to record the training stats:\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "        try:\n",
    "            mlflow.log_metric(\"train_loss\", info[\"train_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log train metric to MLflow: {e}\")\n",
    "            \n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        self.val_accuracies.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        # log validation loss\n",
    "        try:\n",
    "            mlflow.log_metric(\"val_loss\", info[\"val_loss\"], step=info[\"iteration\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not log validation metric to MLflow: {e}\")\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "with mlflow.start_run(run_name=corrida_name):\n",
    "    mlflow.log_params({\n",
    "        \"num_train_epoch\": lora_config[\"lora_parameters\"][\"epochs\"],\n",
    "        \"max_steps\": training_args.iters,\n",
    "        \"lora_r\": lora_config[\"lora_parameters\"][\"rank\"],\n",
    "        \"lora_dropout\":lora_config[\"lora_parameters\"][\"dropout\"],\n",
    "        \"lora_layers\":lora_config[\"lora_layers\"],\n",
    "        \"lora_layeres_scale\":lora_config[\"lora_parameters\"][\"scale\"],\n",
    "        \"batch_size\":training_args.batch_size,\n",
    "        \"optimizer\":optimizer,\n",
    "        \"learning_rate\":learning_rate_value,\n",
    "        # \"weight_decay\": weight_decay_value,\n",
    "        \"scheduler\":lr_scheduler\n",
    "    })\n",
    "\n",
    "    # Train model:\n",
    "    train(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        optimizer=opt,\n",
    "        train_dataset=train_set,\n",
    "        val_dataset=valid_set,\n",
    "        training_callback=metrics,\n",
    "    )\n",
    "\n",
    "    print(\"\\n Starting Evaluation\")\n",
    "    # Evaluate model and log metrics in the same run\n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, f1, perplexity, rouge_scores = evaluate_model(model, tokenizer, test_set[:num_test])\n",
    "\n",
    "    # Log final metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": metrics.train_losses[-1][1],\n",
    "        \"final_val_loss\": metrics.val_losses[-1][1],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"rouge1\": rouge_scores['rouge1'],\n",
    "        \"rouge2\": rouge_scores['rouge2'],\n",
    "        \"rougeL\": rouge_scores['rougeL']\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nResults on {num_test} test samples:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\") \n",
    "    print(f\"Perplexity: {perplexity:.3f}\")\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for key, score in rouge_scores.items():\n",
    "        print(f\"{key}: {score:.3f}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved every 100 iterations along with the final adapters in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac329358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000100_adapters.safetensors 0001200_adapters.safetensors\n",
      "0000200_adapters.safetensors 0001300_adapters.safetensors\n",
      "0000300_adapters.safetensors 0001400_adapters.safetensors\n",
      "0000400_adapters.safetensors 0001500_adapters.safetensors\n",
      "0000500_adapters.safetensors 0001600_adapters.safetensors\n",
      "0000600_adapters.safetensors 0001700_adapters.safetensors\n",
      "0000700_adapters.safetensors 0001800_adapters.safetensors\n",
      "0000800_adapters.safetensors 0001900_adapters.safetensors\n",
      "0000900_adapters.safetensors 0002000_adapters.safetensors\n",
      "0001000_adapters.safetensors adapters.safetensors\n",
      "0001100_adapters.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls ../trained_models/adapters2k/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training and validation losses to see how well the adapters fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3b62043e0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_its, train_losses = zip(*metrics.train_losses)\n",
    "val_its, val_losses = zip(*metrics.val_losses)\n",
    "plt.plot(train_its, train_losses, '-o')\n",
    "plt.plot(val_its, val_losses, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', \"Valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "The adapted weights are: $\\tilde{W} = W + c \\cdot \\mathbf{b}^\\top \\mathbf{a}$. Note, this process can be destructive if the inputs are in low precision and they have very different magnitudes. Tuning the `scale` parameter, $c$, prior to fine-tuning can improve the model performance after fusion.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlx_lm.fuse --model {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `lora_fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = load(\"lora_fused_model\")\n",
    "# num_correct = 0\n",
    "# for prompt, answer in tqdm.tqdm(test_set[:num_test]):\n",
    "#     response = generate(model, tokenizer, prompt, max_tokens=2)\n",
    "#     num_correct += (response==answer)\n",
    "# test_acc = num_correct / num_test\n",
    "# print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your LoRA adapters are not working well it's critical to plot both the trianing loss and validation loss over the duration of fine-tuning. There are really only two cases to consider: underfitting or overfitting. And you can figure out which regime you are in based on the above plot.\n",
    "\n",
    "**Underfitting**: The trianing loss is not low enough and the validation loss closely matches the training loss. You could also measure the accuracy on the training set itself for question-answering style tasks like HellaSwag. If you are in this regime you have a few options to improve the results:\n",
    "\n",
    "- Use more adapters. Increase `lora_layers` or adapt more of the linear layers within a given block by setting `lora_parameters[\"keys\"]`.\n",
    "- Use a higher rank. A higher rank means more parameters per adapter.\n",
    "- If you are using dropout, decrease the droupout rate or turn it off entirely.\n",
    "- Sometimes, underfitting issues are really optimization issues. In these cases it can be helpful to tune the learning rate or learning rate schedule.\n",
    "- If none of the above works, try a bigger model. For example, try Phi-3 medium instead of Phi-3 tiny.\n",
    "\n",
    "**Overfitting**: The trianing loss keeps going down but the validation loss stops going down and even starts to go up. If you are in this regime you also have a few options:\n",
    "\n",
    "- The best thing to do is to use more trianing data if you have it.\n",
    "- Contrary to the underfitting regime decreasing the capacity of the model can help. For example, use fewer adapters, a lower LoRA rank, or a smaller model size.\n",
    "- If you are not using dropout, use it.\n",
    "\n",
    "If you find your adapters work well pre-fusion but stop working post-fusion, try tuning the `scale` parameter, $c$, prior to fine-tuning. Typically the adapters have a smaller magnitude than the weights, so using a larger scale helps.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "Fine-tuning a large LM with LoRA requires a machine with a decent amount of memory. Here are some tips to reduce memory use should you need to do so. \n",
    "\n",
    "- Try quantization (QLoRA). You can use QLoRA by generating a quantized model with `mlx_lm.convert` and the `-q` flag or by using an already quantized model from HuggingFace.\n",
    "\n",
    "- Try using a smaller batch size. You can set the `batch_size` parameter in the `TrainingArgs` or pass `--batch-size` if you are using the CLI. The default is 4 so setting this to 2 or 1 will reduce memory consumption. Note, this may slow things down a little..\n",
    "\n",
    "- Reduce the number of layers to fine-tune with by setting `lora_layers` to a smaller value or passing `--lora-layers` if you are using the CLI. The default is `16`, so you can try `8` or `4`. This reduces the amount of memory needed for back propagation. It may also reduce the quality of the fine-tuned model and you may need to compensate with a larger `rank`.\n",
    "\n",
    "- Longer examples require more memory. If it makes sense for your data, one thing you can do is break your examples into smaller sequences when making the `train`, `valid`, and `test` data sets.\n",
    "\n",
    "- Gradient checkpointing lets you trade-off memory use (less) for computation (more) by recomputing instead of storing intermediate values needed by the backward pass. You can use gradient checkpointing by passing `grad_checkpoint=True` to the `TrainingArgs` or the `--grad-checkpoint` flag if using the CLI. Gradient checkpointing will be more helpful for larger batch sizes or sequence lengths with smaller or quantized models.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project.\n",
    "- We also have an example of [LoRA fine-tuning in MLX Swift](https://github.com/ml-explore/mlx-swift-examples/tree/main/Applications/LoRATrainingExample)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
